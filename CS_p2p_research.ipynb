{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "CS_p2p_research.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXfCC9mymKYo"
      },
      "source": [
        "# Reseach "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LVpkjvgzqu-"
      },
      "source": [
        "The dataset is kaggle from 2007 to 2018q4 datasets.\n",
        "For analysis, follow Measuring Financial Risks: The Application of\n",
        "Network Theory in Fintech Risk Management\n",
        "\n",
        "Grade Assigned: loan grade\n",
        "\n",
        "Annual income: The self-reported annual income provided by the borrower\n",
        "Loan amount: The listed amount of the loan applied for by the borrower\n",
        "\n",
        "Loan over income ratio: Loan amount divided by annual income\n",
        "\n",
        "Ownership: Binary variable taking value [1] if the borrower owns/morgages his home\n",
        "Total number of accounts: The total number of credit lines currently in the borrower’s credit file\n",
        "\n",
        "Fico score: The upper boundary range the borrower’s FICO belongs to.\n",
        "\n",
        "Inquiries The number of inquiries in past 6 months\n",
        "\n",
        "Address_group1-3 Locations were classified into four groups based on default odds ratios\n",
        "\n",
        "Months since last delinquency The number of months since the borrower’s last delinquency.\n",
        "Revolving balance Total credit revolving balance\n",
        "Revolving line utilization rate Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n",
        "Verification status Indicates if income was verified by Lending club\n",
        "Debt ratio A ratio calculated using the borrower’s total monthly debt payments\n",
        "on the total debt obligations\n",
        "Purpose1-3 Debt purposes were classified into four groups based on default odds ratios\n",
        "income_group1-3 Interaction between annual income and purpose group\n",
        "loan_group1-3 Interaction between loan over income ratio and purpose group\n",
        "balance_group1-3 Interaction between revolving balance and purpose group\n",
        "util_group1-3 Interaction between revolving line utilization rate and purpose group\n",
        "total_group1-3 Interaction between total number of accounts and purpose group\n",
        "issue_year07-16 Dummies referring to the year\n",
        "fico_group1-3 Interaction between fico score and purpose group\n",
        "\n",
        "Grade Assigned: \n",
        "Annual income: \n",
        "Loan amount: \n",
        "Loan over income ratio: \n",
        "Ownership: \n",
        "Total number of accounts: \n",
        "Fico score: \n",
        "Inquiries \n",
        "Address_group1-3 \n",
        "Months since last delinquency \n",
        "Revolving balance Total credit \n",
        "Revolving line utilization rate \n",
        "Verification status \n",
        "Debt ratio \n",
        "Purpose1-3 Debt \n",
        "income_group1-3 \n",
        "loan_group1-3 \n",
        "balance_group1-3 \n",
        "util_group1-3 \n",
        "total_group1-3 \n",
        "issue_year07-16 \n",
        "fico_group1-3\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQdfihtjmS4P"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ0rrhFcA3No",
        "outputId": "fbd88ba0-7bf8-43bf-eead-b6716803f8b7"
      },
      "source": [
        "!pip install -q tensorflow==2.4.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 394.3 MB 10 kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 58.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 76.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 91.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0uNwhwnUvp1",
        "outputId": "7699e97e-086a-43a1-d357-666ce28e43b8"
      },
      "source": [
        "!pip install -q autokeras\n",
        "!pip install -q git+https://github.com/keras-team/keras-tuner/tree/1.0.2\n",
        "!pip install -q keras-rectified-adam"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Discarding git+https://github.com/keras-team/keras-tuner/tree/1.0.2. Command errored out with exit status 128: git clone -q https://github.com/keras-team/keras-tuner/tree/1.0.2 /tmp/pip-req-build-_yk9736o Check the logs for full command output.\u001b[0m\n",
            "\u001b[31mERROR: Command errored out with exit status 128: git clone -q https://github.com/keras-team/keras-tuner/tree/1.0.2 /tmp/pip-req-build-_yk9736o Check the logs for full command output.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxIEXiOOPnIf",
        "outputId": "60092ff0-c48b-430e-e298-4bffcb55a667"
      },
      "source": [
        "!pip install -q -U keras-tuner\n",
        "!pip install -q -U autokeras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 97 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 454.3 MB 15 kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 64.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 69.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 83.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb5pNjy1xOzC",
        "outputId": "eb864913-d250-42b4-90c5-a25c330e3a36"
      },
      "source": [
        "!pip install -q -U tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 491 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 583 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 614 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 645 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 675 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 706 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 737 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 768 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 860 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 870 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 880 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 890 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 911 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 921 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 942 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 952 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 962 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 983 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 993 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZY-cOqL9GyB"
      },
      "source": [
        "!pip install -q catboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aoM7Yre8v5e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier, Pool, metrics, cv\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19H4MawCxDxD"
      },
      "source": [
        "from tensorflow_addons.metrics import MultiLabelConfusionMatrix as MCM\n",
        "from tensorflow_addons.metrics import F1Score as F1\n",
        "from tensorflow_addons.optimizers import AdamW\n",
        "from tensorflow_addons.optimizers import RectifiedAdam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-UTm8TFgx1q"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yl0isc5KSlG"
      },
      "source": [
        "import pkg_resources\n",
        "pkg_resources.require(\"tensorflow==2.4.1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB8QL1oTwEiU"
      },
      "source": [
        "# import autokeras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QATQx0Fow6S2"
      },
      "source": [
        "import keras_tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMJtGEXtk8WI"
      },
      "source": [
        "keras_tuner.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XgilmOfzWBL"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
        "from tensorflow.keras.layers import Dropout, Activation, Lambda\n",
        "from tensorflow.keras.layers import LSTM, Conv1D, Conv2D, Flatten\n",
        "from tensorflow.keras.layers import MaxPooling1D, MaxPooling2D, Reshape, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras import backend as K\n",
        "#from tensorflow.keras.engine import Layer\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "from tensorflow.keras.layers import Concatenate, Layer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#from keras_radam.training import RAdamOptimizer\n",
        "import keras_tuner as kt\n",
        "#import autokeras as ak\n",
        "\n",
        "from typing import Union, Callable\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RRACM8vWBRnL",
        "outputId": "8e1ccf87-0db2-4c8a-d11a-fced2e1a6579"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXHxGwDlXspW"
      },
      "source": [
        "import datetime\n",
        "import math\n",
        "from tensorboard.plugins.hparams import api as hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKC4EYcH0Oyz"
      },
      "source": [
        "import sys\n",
        "pd.set_option('display.max_rows', 100)\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B7KFgjcgAo0"
      },
      "source": [
        "!cp '/content/drive/MyDrive/論文/The credit scoring of P2P lending: a transformer and dynamic conv/LCdata_2.tar.gz' /content\n",
        "!tar -zxf /LCdata_2.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WzRO2-1gU9v",
        "outputId": "385a56f3-6c0e-4c81-a165-bb9027599c7e"
      },
      "source": [
        "!tar -C /content/LCdata_2 -zcvf LCdata_2.tar.gz /content/LCdata_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "/content/LCdata_2/\n",
            "/content/LCdata_2/train_v_2.pkl\n",
            "/content/LCdata_2/train_m_2.pkl\n",
            "/content/LCdata_2/train_y_2.pkl\n",
            "/content/LCdata_2/train_i_2.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RhG_jIEjahE"
      },
      "source": [
        "!tar -zxvf LCdata_2.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNv0bYCMja9T",
        "outputId": "a2dfed32-5bf6-42ec-a881-bba433d2f781"
      },
      "source": [
        "!cp '/content/drive/MyDrive/論文/The credit scoring of P2P lending: a transformer and dynamic conv/LCdata_2.tar.gz' /content\n",
        "!tar -zxf /content/LCdata_2.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYGdNN_Flv4a"
      },
      "source": [
        "df_v_2 = pd.read_pickle(\"/content/content/LCdata_2/train_v_2.pkl\")\n",
        "df_i_2 = pd.read_pickle(\"/content/content/LCdata_2/train_i_2.pkl\")\n",
        "df_y_2 = pd.read_pickle(\"/content/content/LCdata_2/train_y_2.pkl\")\n",
        "df_m_2 = pd.read_pickle(\"/content/content/LCdata_2/train_i_2.pkl\")\n",
        "df_m_r_2 = pd.read_pickle(\"/content/content/LCdata_2/train_i_2.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA3QxWAAjPP8"
      },
      "source": [
        "df_m_2 = pd.read_pickle(\"/content/content/LCdata_2/train_i_2.pkl\")\n",
        "df_m_r_2 = pd.read_pickle(\"/content/content/LCdata_2/train_i_2.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVzaxuS8Jbr7"
      },
      "source": [
        "!cp '/content/drive/MyDrive/論文/The credit scoring of P2P lending: a transformer and dynamic conv/LCdata_3.tar.gz' /content\n",
        "!tar -zxf /content/LCdata_3.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plxZZ0mNJEIR"
      },
      "source": [
        "df_v_2 = pd.read_pickle(\"/content/content/LCdata_3/train_v_3.pkl\")\n",
        "df_i_2 = pd.read_pickle(\"/content/content/LCdata_3/train_i_3.pkl\")\n",
        "df_y_2 = pd.read_pickle(\"/content/content/LCdata_3/train_y_3.pkl\")\n",
        "df_m_2 = pd.read_pickle(\"/content/content/LCdata_3/train_i_3.pkl\")\n",
        "df_m_r_2 = pd.read_pickle(\"/content/content/LCdata_3/train_i_3.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdES8rr4JL7R"
      },
      "source": [
        "rm -rf /content/content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5qzjPzj9SQn"
      },
      "source": [
        "df_y_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeXL9zDYY0ES",
        "outputId": "e252cd7c-9f04-4263-a3c3-3d2df3e57e2d"
      },
      "source": [
        "df_v_2.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
              "       'fico_range_high', 'inq_last_6mths', 'mths_since_last_delinq',\n",
              "       'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n",
              "       'revol_util', 'total_acc', 'annual_inc_joint', 'dti_joint',\n",
              "       'tot_coll_amt', 'open_il_12m', 'open_il_24m', 'open_rv_12m',\n",
              "       'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi',\n",
              "       'total_cu_tl', 'inq_last_12m', 'acc_open_past_24mths', 'bc_open_to_buy',\n",
              "       'bc_util', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mort_acc',\n",
              "       'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl',\n",
              "       'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl',\n",
              "       'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats',\n",
              "       'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75',\n",
              "       'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim',\n",
              "       'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit',\n",
              "       'credit_age', 'desc', 'emp_title', 'grade', 'sub_grade', 'emp_length',\n",
              "       'pymnt_plan', 'home_ownership', 'verification_status', 'purpose',\n",
              "       'term', 'zip_code', 'addr_state', 'issue_d', 'initial_list_status',\n",
              "       'application_type', 'verification_status_joint'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwBqWr_xOxFj",
        "outputId": "4a26041e-d453-4ca1-d607-42c155d3291e"
      },
      "source": [
        "df_v_2.columns[:26]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
              "       'fico_range_high', 'inq_last_6mths', 'mths_since_last_delinq',\n",
              "       'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n",
              "       'revol_util', 'total_acc', 'annual_inc_joint', 'dti_joint',\n",
              "       'tot_coll_amt', 'open_il_12m', 'open_il_24m', 'open_rv_12m',\n",
              "       'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi',\n",
              "       'total_cu_tl'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 158,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz3wfxdbN403",
        "outputId": "fd367ace-22b0-40c5-8c0f-96d21840c2a8"
      },
      "source": [
        "df_v_2.columns[54:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['sub_grade', 'emp_length', 'home_ownership', 'verification_status',\n",
              "       'purpose', 'term', 'zip_code', 'addr_state', 'application_type',\n",
              "       'verification_status_joint'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 305,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "-f4bn8o4-Yqm",
        "outputId": "d1791469-f9a2-4590-c6e1-3a36e04fe111"
      },
      "source": [
        "df_v_2.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annual_inc</th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>dti</th>\n",
              "      <th>delinq_2yrs</th>\n",
              "      <th>fico_range_low</th>\n",
              "      <th>fico_range_high</th>\n",
              "      <th>inq_last_6mths</th>\n",
              "      <th>mths_since_last_delinq</th>\n",
              "      <th>mths_since_last_record</th>\n",
              "      <th>open_acc</th>\n",
              "      <th>pub_rec</th>\n",
              "      <th>revol_bal</th>\n",
              "      <th>revol_util</th>\n",
              "      <th>total_acc</th>\n",
              "      <th>annual_inc_joint</th>\n",
              "      <th>dti_joint</th>\n",
              "      <th>tot_coll_amt</th>\n",
              "      <th>open_il_12m</th>\n",
              "      <th>open_il_24m</th>\n",
              "      <th>open_rv_12m</th>\n",
              "      <th>open_rv_24m</th>\n",
              "      <th>max_bal_bc</th>\n",
              "      <th>all_util</th>\n",
              "      <th>total_rev_hi_lim</th>\n",
              "      <th>inq_fi</th>\n",
              "      <th>total_cu_tl</th>\n",
              "      <th>inq_last_12m</th>\n",
              "      <th>acc_open_past_24mths</th>\n",
              "      <th>bc_open_to_buy</th>\n",
              "      <th>bc_util</th>\n",
              "      <th>mo_sin_old_il_acct</th>\n",
              "      <th>mo_sin_old_rev_tl_op</th>\n",
              "      <th>mort_acc</th>\n",
              "      <th>num_accts_ever_120_pd</th>\n",
              "      <th>num_actv_bc_tl</th>\n",
              "      <th>num_actv_rev_tl</th>\n",
              "      <th>num_bc_sats</th>\n",
              "      <th>num_bc_tl</th>\n",
              "      <th>num_il_tl</th>\n",
              "      <th>num_op_rev_tl</th>\n",
              "      <th>num_rev_accts</th>\n",
              "      <th>num_rev_tl_bal_gt_0</th>\n",
              "      <th>num_sats</th>\n",
              "      <th>num_tl_op_past_12m</th>\n",
              "      <th>pct_tl_nvr_dlq</th>\n",
              "      <th>percent_bc_gt_75</th>\n",
              "      <th>pub_rec_bankruptcies</th>\n",
              "      <th>tax_liens</th>\n",
              "      <th>tot_hi_cred_lim</th>\n",
              "      <th>total_bal_ex_mort</th>\n",
              "      <th>total_bc_limit</th>\n",
              "      <th>total_il_high_credit_limit</th>\n",
              "      <th>credit_age</th>\n",
              "      <th>desc</th>\n",
              "      <th>emp_title</th>\n",
              "      <th>grade</th>\n",
              "      <th>sub_grade</th>\n",
              "      <th>emp_length</th>\n",
              "      <th>pymnt_plan</th>\n",
              "      <th>home_ownership</th>\n",
              "      <th>verification_status</th>\n",
              "      <th>purpose</th>\n",
              "      <th>term</th>\n",
              "      <th>zip_code</th>\n",
              "      <th>addr_state</th>\n",
              "      <th>issue_d</th>\n",
              "      <th>initial_list_status</th>\n",
              "      <th>application_type</th>\n",
              "      <th>verification_status_joint</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>55000.0</td>\n",
              "      <td>3600.0</td>\n",
              "      <td>5.910000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>675.0</td>\n",
              "      <td>679.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2765.0</td>\n",
              "      <td>29.700001</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>722.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>722.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>9300.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1506.0</td>\n",
              "      <td>37.200001</td>\n",
              "      <td>148.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>76.900002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>178050.0</td>\n",
              "      <td>7746.0</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>13734.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[[2.630921, -1.3062463, 14.80966, 4.0948644, -...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65000.0</td>\n",
              "      <td>24700.0</td>\n",
              "      <td>16.059999</td>\n",
              "      <td>1.0</td>\n",
              "      <td>715.0</td>\n",
              "      <td>719.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21470.0</td>\n",
              "      <td>19.200001</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6472.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>111800.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>57830.0</td>\n",
              "      <td>27.100000</td>\n",
              "      <td>113.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>97.400002</td>\n",
              "      <td>7.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>314017.0</td>\n",
              "      <td>39475.0</td>\n",
              "      <td>79300.0</td>\n",
              "      <td>24667.0</td>\n",
              "      <td>194.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[[3.0159009, -2.9674704, 8.356276, 1.3137902, ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>63000.0</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>10.780000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>695.0</td>\n",
              "      <td>699.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7869.0</td>\n",
              "      <td>56.200001</td>\n",
              "      <td>18.0</td>\n",
              "      <td>71000.0</td>\n",
              "      <td>13.85</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2081.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>14000.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2737.0</td>\n",
              "      <td>55.900002</td>\n",
              "      <td>125.0</td>\n",
              "      <td>184.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>218418.0</td>\n",
              "      <td>18696.0</td>\n",
              "      <td>6200.0</td>\n",
              "      <td>14877.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[[3.4516678, 1.3470607, 1.1577941, -4.85592, -...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>104433.0</td>\n",
              "      <td>10400.0</td>\n",
              "      <td>25.370001</td>\n",
              "      <td>1.0</td>\n",
              "      <td>695.0</td>\n",
              "      <td>699.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21929.0</td>\n",
              "      <td>64.500000</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9702.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>34000.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4567.0</td>\n",
              "      <td>77.500000</td>\n",
              "      <td>128.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>96.599998</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>439570.0</td>\n",
              "      <td>95768.0</td>\n",
              "      <td>20300.0</td>\n",
              "      <td>88097.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[[-9.124034, -0.4458597, 7.7888994, 1.3426188,...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>34000.0</td>\n",
              "      <td>11950.0</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>690.0</td>\n",
              "      <td>694.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8822.0</td>\n",
              "      <td>68.400002</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4522.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>12900.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>844.0</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>338.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16900.0</td>\n",
              "      <td>12798.0</td>\n",
              "      <td>9400.0</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   annual_inc  loan_amnt  ...  application_type  verification_status_joint\n",
              "0     55000.0     3600.0  ...               1.0                        1.0\n",
              "1     65000.0    24700.0  ...               1.0                        1.0\n",
              "2     63000.0    20000.0  ...               1.0                        1.0\n",
              "3    104433.0    10400.0  ...               1.0                        1.0\n",
              "4     34000.0    11950.0  ...               1.0                        1.0\n",
              "\n",
              "[5 rows x 69 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3UPaCwP4bYn"
      },
      "source": [
        "less_col = ['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
        "       'fico_range_high','annual_inc_joint', 'dti_joint', 'sub_grade', 'emp_length', 'home_ownership',\n",
        "       'verification_status', 'purpose', 'credit_age', 'verification_status_joint',  'zip_code', 'addr_state']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY8OfB_YNxV4"
      },
      "source": [
        "less_col = ['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
        "       'fico_range_high', 'sub_grade', 'emp_length', 'home_ownership',\n",
        "       'verification_status', 'purpose', 'term']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8zbCxDA_uh4"
      },
      "source": [
        "less_col = ['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
        "       'fico_range_high', 'sub_grade', 'emp_length', 'home_ownership',\n",
        "       'verification_status', 'purpose', 'term', 'credit_age']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1QuYvOi9SzQ"
      },
      "source": [
        "less_col = ['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
        "       'fico_range_high','annual_inc_joint', 'dti_joint', 'sub_grade', 'emp_length', 'home_ownership',\n",
        "       'verification_status', 'purpose', 'term', 'verification_status_joint', ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QreAHEpg9paK"
      },
      "source": [
        "less_col = ['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
        "       'fico_range_high', 'sub_grade', 'emp_length', 'home_ownership',\n",
        "       'verification_status', 'purpose', 'term',  'zip_code', 'addr_state']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8na9n3lv4Rn"
      },
      "source": [
        "less_col = ['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
        "       'fico_range_high','annual_inc_joint', 'dti_joint', 'sub_grade', 'emp_length', 'home_ownership',\n",
        "       'verification_status', 'purpose', 'term', 'credit_age', 'verification_status_joint',  'zip_code', 'addr_state']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmVRjfthgGx5"
      },
      "source": [
        "less_col = ['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
        "       'fico_range_high', 'sub_grade', 'emp_length', 'home_ownership',\n",
        "       'verification_status', 'purpose', 'term', 'inq_last_6mths', 'mths_since_last_delinq',\n",
        "       'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n",
        "       'revol_util', 'total_acc', 'annual_inc_joint', 'dti_joint']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B48_kocAmV-r"
      },
      "source": [
        "less_col = ['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
        "       'fico_range_high', 'sub_grade', 'emp_length', 'home_ownership',\n",
        "       'verification_status', 'purpose', 'term', 'inq_last_6mths', 'mths_since_last_delinq',\n",
        "      'open_acc', 'pub_rec', 'mths_since_last_record', 'revol_bal',\n",
        "       'revol_util', 'total_acc', 'annual_inc_joint', 'dti_joint', \n",
        "       'pub_rec_bankruptcies','credit_age']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY0Wyxj3_cIV"
      },
      "source": [
        "less_col = ['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
        "       'fico_range_high', 'sub_grade', 'emp_length', 'home_ownership',\n",
        "       'verification_status', 'purpose', 'term', 'inq_last_6mths', 'mths_since_last_delinq',\n",
        "      'open_acc', 'pub_rec', 'mths_since_last_record', 'revol_bal',\n",
        "       'revol_util', 'total_acc', 'annual_inc_joint', 'dti_joint', \n",
        "       'pub_rec_bankruptcies','credit_age']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ3jFz0IYRmm"
      },
      "source": [
        "less_col = df_i_2.columns[54:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvhBQMv1pA_6",
        "outputId": "142ef258-1946-495f-ca95-3cfd874a5bdf"
      },
      "source": [
        "len(set(less_col))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "execution_count": 109,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V28wI6_hwS-"
      },
      "source": [
        "cate_col = df_m_2.columns[55:]\n",
        "nume_col = df_m_2.columns[:54]\n",
        "\n",
        "\n",
        "#meta_index\n",
        "for i in cate_col:\n",
        "  df_m_2[ i ] = np.float32(0.0)\n",
        "for i in nume_col:\n",
        "  df_m_2.loc[df_m_2[i] % 2 ==0, i ] = np.float32(4.0)\n",
        "  df_m_2.loc[df_m_2[i] % 2 ==1, i ] = np.float32(0.0)\n",
        "  df_m_2.loc[df_m_2[i] == np.float32(4.0), i ] = np.float32(1.0)\n",
        "df_m_2.loc[df_m_2['emp_title']  ==108, 'emp_title' ] = np.float32(1.0)\n",
        "df_m_2.loc[df_m_2['emp_title']  ==109, 'emp_title' ] = np.float32(0.0)\n",
        "df_m_2.loc[df_m_2['emp_title']  ==110, 'emp_title' ] = np.float32(0.0)\n",
        "\n",
        "#reverse meta index\n",
        "\n",
        "for i in cate_col:\n",
        "  df_m_r_2[ i ] = np.float32(1.0)\n",
        "for i in nume_col:\n",
        "  df_m_r_2.loc[df_m_2[i] % 2 ==0, i ] = np.float32(4.0)\n",
        "  df_m_r_2.loc[df_m_2[i] % 2 ==1, i ] = np.float32(1.0)\n",
        "  df_m_r_2.loc[df_m_2[i] == np.float32(4.0), i ] = np.float32(0.0)\n",
        "df_m_r_2.loc[df_m_2['emp_title']  ==108, 'emp_title' ] = np.float32(0.0)\n",
        "df_m_r_2.loc[df_m_2['emp_title']  ==109, 'emp_title' ] = np.float32(1.0)\n",
        "df_m_r_2.loc[df_m_2['emp_title']  ==110, 'emp_title' ] = np.float32(1.0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3WRm3nZ2ecp"
      },
      "source": [
        "df_emb_v_2 = pd.DataFrame([])\n",
        "df_emb_i_2 = pd.DataFrame([])\n",
        "df_emb_v_2['desc'] = df_v_2.pop('desc')\n",
        "df_emb_v_2['emp_title'] = df_v_2.pop('emp_title')\n",
        "df_emb_i_2['desc'] = df_i_2.pop('desc')\n",
        "df_emb_i_2['emp_title'] = df_i_2.pop('emp_title')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDFr_GANjD9f"
      },
      "source": [
        "df_m_2 = df_m_2.drop(columns=['desc', 'emp_title'])\n",
        "df_m_r_2 = df_m_r_2.drop(columns=['desc', 'emp_title'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i42pUgj52G7k"
      },
      "source": [
        "needs = []\n",
        "for i in df_v_2.columns:\n",
        "  if not df_v_2[i].max() == df_v_2[i].min():\n",
        "    needs.append(i)\n",
        "for i in needs:\n",
        "  df_v_2[i] = df_v_2[i].map(lambda x : int(math.log(float(x+3))**2), na_action='ignore')\n",
        "\n",
        "df_v_2[needs] = df_v_2[needs].apply(lambda x: (x - x.min())/(x.max() - x.min()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egEYu8Fs13N6"
      },
      "source": [
        "#df_emb_v_2.emp_title = df_emb_v_2.emp_title.map(lambda x: x.tolist())\n",
        "#df_emb_v_2.desc = df_emb_v_2.desc.map(lambda x: x.tolist())\n",
        "df_emb_v_2.desc[df_emb_i_2.desc == 106] = df_emb_v_2.desc[df_emb_i_2.desc == 106].map(lambda x:x[0])\n",
        "df_emb_v_2.emp_title[df_emb_i_2.emp_title == 108] = df_emb_v_2.emp_title[df_emb_i_2.emp_title == 108].map(lambda x:x[0])\n",
        "df_emb_v_2.desc[df_emb_i_2.desc == 107] = df_emb_v_2.desc[df_emb_i_2.desc == 107].map(lambda x: [1.0]*128)\n",
        "df_emb_v_2.emp_title[df_emb_i_2.emp_title == 109] = df_emb_v_2.emp_title[df_emb_i_2.emp_title == 109].map(lambda x:[1.0]*128)\n",
        "df_emb_v_2.emp_title[df_emb_i_2.emp_title == 110] = df_emb_v_2.emp_title[df_emb_i_2.emp_title == 110].map(lambda x:[1.0]*128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1-NebwZCYsP"
      },
      "source": [
        "df_emb_v_2.desc[df_emb_i_2.desc == 106] = df_emb_v_2.desc[df_emb_i_2.desc == 106].map(lambda x:x.tolist())\n",
        "df_emb_v_2.emp_title[df_emb_i_2.emp_title == 108] = df_emb_v_2.emp_title[df_emb_i_2.emp_title == 108].map(lambda x:x.tolist())\n",
        "\n",
        "\n",
        "#df_emb_v_2 = [[i[0], i[1]] for i in df_emb_v_2]\n",
        "#tt = np.array(df_emb_v_2).copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5E5EiUpozvc"
      },
      "source": [
        "df_i_2 = df_i_2[df_i_2.columns[:15]]\n",
        "df_v_2 = df_v_2[df_v_2.columns[:15]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3ayVpMcoTuM"
      },
      "source": [
        "df_y_2.loan_status = df_y_2.loan_status.astype(str)\n",
        "df_y_2 = pd.get_dummies(df_y_2, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep8r1fZ3uwn-"
      },
      "source": [
        "drop_c = df_v_2.columns[:53]\n",
        "#df_v_2_1 = df_v_2.copy()\n",
        "for i in drop_c:\n",
        "  df_v_2.loc[df_i_2[i] % 2 ==1, i ] = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5q944P96CUA"
      },
      "source": [
        "df_v_2_1.isna().sum(), "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtuL6nQc7xC_"
      },
      "source": [
        "#remove issued\n",
        "df_v_2 = df_v_2.drop(columns=['issue_d', 'initial_list_status'], axis=1)\n",
        "df_i_2 = df_i_2.drop(columns=['issue_d', 'initial_list_status'], axis=1)\n",
        "df_m_2 = df_m_2.drop(columns=['issue_d', 'initial_list_status'], axis=1)\n",
        "df_m_r_2 = df_m_r_2.drop(columns=['issue_d', 'initial_list_status'], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzVvr0SPc0MM"
      },
      "source": [
        "df_v_2 = df_v_2.drop(columns=['pymnt_plan'], axis=1)\n",
        "df_i_2 = df_i_2.drop(columns=['pymnt_plan'], axis=1)\n",
        "df_m_2 = df_m_2.drop(columns=['pymnt_plan'], axis=1)\n",
        "df_m_r_2 = df_m_r_2.drop(columns=['pymnt_plan'], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4VfgLoSQiUb"
      },
      "source": [
        "df_v = df_v_2.copy()\n",
        "df_i = df_i_2.copy()\n",
        "df_y = df_y_2.copy()\n",
        "df_m = df_m_2.copy()\n",
        "df_m_r = df_m_r_2.copy()\n",
        "df_emb_i = df_emb_i_2.copy()\n",
        "df_emb_v = df_emb_v_2.copy()\n",
        "#df_emb_v = [[i[0], i[1]] for i in df_emb_v_2]\n",
        "#df_emb_v = np.array(df_emb_v_2)\n",
        "#df_y = df_y_2.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRQnL5MMwIWm"
      },
      "source": [
        "less_col = df_v_2.columns[54:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zygskYDVDSPm"
      },
      "source": [
        "less_col = ['annual_inc', 'grade', 'emp_length', 'home_ownership', 'verification_status',\n",
        "       'purpose', 'term',  'initial_list_status']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta3UKBL0_XD6"
      },
      "source": [
        "less_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXwE--kBYhXo"
      },
      "source": [
        "df_v_2 = df_v_2[less_col]\n",
        "df_i_2 =df_i_2[less_col]\n",
        "df_m_2 =df_m_2[less_col]\n",
        "df_m_r_2=df_m_r_2[less_col]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ-GJHInZBi4",
        "outputId": "acdcbc87-ec2b-40dd-ae8a-154a6c03dd3c"
      },
      "source": [
        "df_i_2.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['annual_inc', 'loan_amnt', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
              "       'fico_range_high', 'annual_inc_joint', 'dti_joint', 'sub_grade',\n",
              "       'emp_length', 'home_ownership', 'verification_status', 'purpose',\n",
              "       'credit_age', 'verification_status_joint', 'zip_code', 'addr_state'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GfEXO3qBA0X"
      },
      "source": [
        "for i in df_i_2.columns:\n",
        "  print(df_i_2[i].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O0ExBdnRCbm"
      },
      "source": [
        "df_v_2 = df_v.copy()\n",
        "df_i_2 = df_i.copy()\n",
        "df_y_2 = df_y.copy()\n",
        "df_m_2 = df_m.copy()\n",
        "df_m_r_2 = df_m_r.copy()\n",
        "df_emb_i_2 = df_emb_i.copy()\n",
        "df_emb_v_2 = df_emb_v.copy()\n",
        "#df_emb_v = [[i[0], i[1]] for i in df_emb_v_2]\n",
        "#df_emb_v = np.array(df_emb_v_2)\n",
        "#df_y = df_y_2.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo6ZWCXFYmxR"
      },
      "source": [
        "df_v_2 = df_v_2.to_numpy()\n",
        "df_i_2 = df_i_2.to_numpy()\n",
        "df_y_2 = df_y_2.to_numpy()\n",
        "df_m_2 = df_m_2.to_numpy()\n",
        "df_m_r_2 = df_m_r_2.to_numpy()\n",
        "df_emb_i_2 = df_emb_i_2.to_numpy()\n",
        "df_emb_v_2 = df_emb_v_2.to_numpy()\n",
        "df_emb_v_2 = [[i[0], i[1]] for i in df_emb_v_2]\n",
        "df_emb_v_2 = np.array(df_emb_v_2)\n",
        "df_y_2 = df_y_2.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5j6NS1gih_-"
      },
      "source": [
        "df_emb_v_2 = df_emb_v_2.drop('desc', axis=1)\n",
        "df_emb_i_2 = df_emb_i_2.drop('desc', axis=1)\n",
        "df_v_2 = df_v_2.to_numpy()\n",
        "df_i_2 = df_i_2.to_numpy()\n",
        "df_y_2 = df_y_2.to_numpy()\n",
        "df_m_2 = df_m_2.to_numpy()\n",
        "df_emb_i_2 = df_emb_i_2.to_numpy()\n",
        "df_emb_v_2 = df_emb_v_2.to_numpy()\n",
        "df_emb_v_2 = [[i[0]] for i in df_emb_v_2]\n",
        "df_emb_v_2 = np.array(df_emb_v_2)\n",
        "df_y_2 = df_y_2.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBb8q46pZWgJ",
        "outputId": "fd68040a-f52c-428b-a48a-2a7fe2bc6f12"
      },
      "source": [
        "df_v_2.shape, df_emb_v_2.shape, df_emb_i_2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1380929, 64), (1380929, 2, 128), (1380929, 2))"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0zhQG9JmhWT"
      },
      "source": [
        "# num of datasets\n",
        "df_v_2.isna().sum(), df_v_2.max(), df_v_2.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5vdzCbZ_uUX",
        "outputId": "b3ed9f12-05ed-4ad4-b5a5-39410548afb4"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/accepted_2007_to_2018Q4.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVt6O8diYXXa"
      },
      "source": [
        "log_dir=\"/content/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=log_dir, histogram_freq=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjVNxXhIefJo",
        "outputId": "e1cf4b98-7033-49d3-fb94-cf457350212d"
      },
      "source": [
        "!rm -r logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'logs': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vR0cL806zWm"
      },
      "source": [
        "dictionary_df = pd.read_excel(\"https://resources.lendingclub.com/LCDataDictionary.xlsx\")\n",
        "\n",
        "# Drop blank rows, strip white space, convert to Python dictionary, fix one key name\n",
        "dictionary_df.dropna(axis=\"index\", inplace=True)\n",
        "dictionary_df = dictionary_df.applymap(lambda x: x.strip())\n",
        "dictionary_df.set_index(\"LoanStatNew\", inplace=True)\n",
        "dictionary = dictionary_df[\"Description\"].to_dict()\n",
        "dictionary[\"verification_status_joint\"] = dictionary.pop(\"verified_status_joint\")\n",
        "\n",
        "dictionary\n",
        "# Print in order of dataset columns (which makes more sense than dictionary's order)\n",
        "\n",
        "for col in acp_loans.columns:\n",
        "    print(f\"•{col}: {dictionary[col]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIwYyIqt4JQM"
      },
      "source": [
        "## startegy sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYVR9b_Y4mg0",
        "outputId": "264fe21b-8ab1-4841-dad6-ce3464857881"
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y2wmOid2iam"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkNioa4nZjnm",
        "outputId": "7a8801a6-73b9-4e83-dcf3-b7cf81ecc956"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.72.83.234:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.72.83.234:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU')]\n",
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdtFGvnHKIrm"
      },
      "source": [
        "needs to pass strategy to tuner distribution_strategy variables for keras tuner\n",
        "and remove strategy.scope part "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OIv6ihOZqb_"
      },
      "source": [
        "with strategy.scope():\n",
        "  training_model = lstm_model(seq_len=100, stateful=False)\n",
        "  training_model.compile(\n",
        "      optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "training_model.fit(\n",
        "    input_fn(),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=10\n",
        ")\n",
        "training_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EaUnecU4U-g"
      },
      "source": [
        "## Dataset loads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8buBo4JHtpR"
      },
      "source": [
        "!gzip -d accepted_2007_to_2018Q4.csv.gz\n",
        "#!gzip -d rejected_2007_to_2018Q4.csv.gz\n",
        "acp_loans = pd.read_csv('./accepted_2007_to_2018Q4.csv', low_memory=False,)\n",
        "#rej_loans = pd.read_csv('./accepted_2007_to_2018Q4.csv', low_memory=False,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABqTW2xUwHIl"
      },
      "source": [
        "! gzip -d '/content/archive (1).zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1G-F24PrAWi"
      },
      "source": [
        "! gzip -d '/content/accepted_2007_to_2018Q4.csv.gz'\n",
        "dataset = pd.read_csv('/content/accepted_2007_to_2018Q4.csv', index_col=['id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWS_OIm9FOsU"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def pre_dataset(input, cv_n:int, batch_size:int=0, CV:int=0):\n",
        "  #cv_n = cv_n\n",
        "  y = input[['loan_status']]\n",
        "  x = input.drop(labels=['loan_status'], axis=1)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=101)\n",
        "  X_train, X_vali, y_train, y_vali = train_test_split(X_train, y_train, test_size=0.25, random_state=101, shuffle=True)\n",
        "  scaler = MinMaxScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_vali = scaler.transform(X_vali)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "  x_train = K.variable(X_train)\n",
        "  y_train = K.variable(y_train)\n",
        "\n",
        "  x_vali = K.variable(X_vali)\n",
        "  y_vali = K.variable(y_vali)\n",
        "\n",
        "  x_test = K.variable(X_test)\n",
        "  y_test = K.variable(y_test)\n",
        "  #y_train = np.asarray(y)\n",
        "  #x_train = np.asarray(x)\n",
        "  train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "  vali = tf.data.Dataset.from_tensor_slices((x_vali, y_vali))\n",
        "  test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "  if batch_size > 0:\n",
        "    train_dataset = train.shuffle(len(train), reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)\n",
        "    vali_dataset = vali.shuffle(len(train), reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)\n",
        "    #vali_n = len(train_dataset) // cv_n\n",
        "    #vali_dataset = train_dataset.take(vali_n)\n",
        "    #train_dataset = train_dataset.skip(vali_n)\n",
        "    test_dataset = test.shuffle(len(test), reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)\n",
        "  else:\n",
        "    dataset = dict_slices\n",
        "  return train_dataset, vali_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7vqJOVsdL5V"
      },
      "source": [
        "! gzip -d 'LC_dataset09-18q4.csv.gz'\n",
        "dataset = pd.read_csv('/content/LC_dataset09-18q4.csv')\n",
        "dataset = dataset.drop(labels=['id'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "rhpJND-thhVb",
        "outputId": "8030a42b-b9e9-4338-99b1-36d06d7e5da9"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>('emp_length : 1 year',)</th>\n",
              "      <th>('emp_length : 10+ years',)</th>\n",
              "      <th>('emp_length : 2 years',)</th>\n",
              "      <th>('emp_length : 3 years',)</th>\n",
              "      <th>('emp_length : 4 years',)</th>\n",
              "      <th>('emp_length : 5 years',)</th>\n",
              "      <th>('emp_length : 6 years',)</th>\n",
              "      <th>('emp_length : 7 years',)</th>\n",
              "      <th>('emp_length : 8 years',)</th>\n",
              "      <th>('emp_length : 9 years',)</th>\n",
              "      <th>('emp_length : &lt; 1 year',)</th>\n",
              "      <th>('emp_length : None',)</th>\n",
              "      <th>('verification_status : Not Verified',)</th>\n",
              "      <th>('verification_status : Source Verified',)</th>\n",
              "      <th>('verification_status : Verified',)</th>\n",
              "      <th>('home_ownership : ANY',)</th>\n",
              "      <th>('home_ownership : MORTGAGE',)</th>\n",
              "      <th>('home_ownership : NONE',)</th>\n",
              "      <th>('home_ownership : OTHER',)</th>\n",
              "      <th>('home_ownership : OWN',)</th>\n",
              "      <th>('home_ownership : RENT',)</th>\n",
              "      <th>('sub_grade : A1',)</th>\n",
              "      <th>('sub_grade : A2',)</th>\n",
              "      <th>('sub_grade : A3',)</th>\n",
              "      <th>('sub_grade : A4',)</th>\n",
              "      <th>('sub_grade : A5',)</th>\n",
              "      <th>('sub_grade : B1',)</th>\n",
              "      <th>('sub_grade : B2',)</th>\n",
              "      <th>('sub_grade : B3',)</th>\n",
              "      <th>('sub_grade : B4',)</th>\n",
              "      <th>('sub_grade : B5',)</th>\n",
              "      <th>('sub_grade : C1',)</th>\n",
              "      <th>('sub_grade : C2',)</th>\n",
              "      <th>('sub_grade : C3',)</th>\n",
              "      <th>('sub_grade : C4',)</th>\n",
              "      <th>('sub_grade : C5',)</th>\n",
              "      <th>('sub_grade : D1',)</th>\n",
              "      <th>('sub_grade : D2',)</th>\n",
              "      <th>('sub_grade : D3',)</th>\n",
              "      <th>('sub_grade : D4',)</th>\n",
              "      <th>...</th>\n",
              "      <th>('sub_grade : E4',)</th>\n",
              "      <th>('sub_grade : E5',)</th>\n",
              "      <th>('sub_grade : F1',)</th>\n",
              "      <th>('sub_grade : F2',)</th>\n",
              "      <th>('sub_grade : F3',)</th>\n",
              "      <th>('sub_grade : F4',)</th>\n",
              "      <th>('sub_grade : F5',)</th>\n",
              "      <th>('sub_grade : G1',)</th>\n",
              "      <th>('sub_grade : G2',)</th>\n",
              "      <th>('sub_grade : G3',)</th>\n",
              "      <th>('sub_grade : G4',)</th>\n",
              "      <th>('sub_grade : G5',)</th>\n",
              "      <th>('purpose : car',)</th>\n",
              "      <th>('purpose : credit_card',)</th>\n",
              "      <th>('purpose : debt_consolidation',)</th>\n",
              "      <th>('purpose : educational',)</th>\n",
              "      <th>('purpose : home_improvement',)</th>\n",
              "      <th>('purpose : house',)</th>\n",
              "      <th>('purpose : major_purchase',)</th>\n",
              "      <th>('purpose : medical',)</th>\n",
              "      <th>('purpose : moving',)</th>\n",
              "      <th>('purpose : other',)</th>\n",
              "      <th>('purpose : renewable_energy',)</th>\n",
              "      <th>('purpose : small_business',)</th>\n",
              "      <th>('purpose : vacation',)</th>\n",
              "      <th>('purpose : wedding',)</th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>term</th>\n",
              "      <th>inq_last_6mths</th>\n",
              "      <th>revol_util</th>\n",
              "      <th>delinq_2yrs</th>\n",
              "      <th>pub_rec</th>\n",
              "      <th>open_acc</th>\n",
              "      <th>total_acc</th>\n",
              "      <th>annual_inc</th>\n",
              "      <th>dti</th>\n",
              "      <th>loan_status</th>\n",
              "      <th>fico</th>\n",
              "      <th>revol_inc</th>\n",
              "      <th>credit_age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.188967</td>\n",
              "      <td>36</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>10.915107</td>\n",
              "      <td>5.91</td>\n",
              "      <td>1</td>\n",
              "      <td>677.0</td>\n",
              "      <td>8.712869</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.114599</td>\n",
              "      <td>36</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>11.082158</td>\n",
              "      <td>16.06</td>\n",
              "      <td>1</td>\n",
              "      <td>717.0</td>\n",
              "      <td>10.800559</td>\n",
              "      <td>194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.903538</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>11.050906</td>\n",
              "      <td>10.78</td>\n",
              "      <td>1</td>\n",
              "      <td>697.0</td>\n",
              "      <td>9.741261</td>\n",
              "      <td>186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.249657</td>\n",
              "      <td>60</td>\n",
              "      <td>3.0</td>\n",
              "      <td>64.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>11.556311</td>\n",
              "      <td>25.37</td>\n",
              "      <td>1</td>\n",
              "      <td>697.0</td>\n",
              "      <td>10.379379</td>\n",
              "      <td>213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.388570</td>\n",
              "      <td>36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10.434145</td>\n",
              "      <td>10.20</td>\n",
              "      <td>1</td>\n",
              "      <td>692.0</td>\n",
              "      <td>10.448523</td>\n",
              "      <td>342</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 84 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   ('emp_length : 1 year',)  ('emp_length : 10+ years',)  ...  revol_inc  credit_age\n",
              "0                       0.0                          1.0  ...   8.712869         150\n",
              "1                       0.0                          1.0  ...  10.800559         194\n",
              "2                       0.0                          1.0  ...   9.741261         186\n",
              "3                       0.0                          0.0  ...  10.379379         213\n",
              "4                       0.0                          0.0  ...  10.448523         342\n",
              "\n",
              "[5 rows x 84 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdJFFdKGh1Ff",
        "outputId": "547658b2-95f4-4055-e7c2-ffc577634004"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RangeIndex(start=0, stop=1346788, step=1)"
            ]
          },
          "execution_count": 40,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VA6EHJ-dTRv"
      },
      "source": [
        "cols = dataset.columns.tolist()\n",
        "print(cols[70])\n",
        "loan = cols.index(\"loan_amnt\")\n",
        "cols = [cols[70]] +  cols[:loan] + cols[71:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4QXU_88tfWb"
      },
      "source": [
        "cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3PYbAIMp1m7",
        "outputId": "9275e950-567b-4379-e0ca-dfc9e2fbf810"
      },
      "source": [
        "c = 0\n",
        "print(sum([int(i.startswith(\"('emp_length\")) for i  in cols ]))\n",
        "print(sum([int(i.startswith(\"('verification_status\")) for i  in cols ]))\n",
        "print(sum([int(i.startswith(\"('home_ownership\")) for i  in cols ]))\n",
        "print(sum([int(i.startswith(\"('sub_grade\")) for i  in cols ]))\n",
        "print(sum([int(i.startswith(\"('purpose\")) for i  in cols ]))\n",
        "print(sum([int((not i.startswith(\"('\"))) for i  in cols ]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n",
            "3\n",
            "6\n",
            "35\n",
            "14\n",
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm3r5Z7Di_rO"
      },
      "source": [
        "dataset = dataset[ cols]\n",
        "dataset.to_csv('/content/LC_dataset09-18q4-1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4wF91Sb6gU9"
      },
      "source": [
        "! gzip -d 'LC_dataset09-18q4.csv.gz'\n",
        "dataset = pd.read_csv('/content/LC_dataset09-18q4.csv')\n",
        "dataset = dataset.drop(labels=['id'], axis=1)\n",
        "\n",
        "batch_size = 1024\n",
        "train_dataset, vali_dataset, test_dataset = pre_dataset(dataset, 5, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RaJxZO4lLs2",
        "outputId": "888c64d4-91f2-4d09-c4b8-ba3b1451a8ef"
      },
      "source": [
        "train_dataset, vali_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<BatchDataset shapes: ((1024, 83), (1024, 1)), types: (tf.float32, tf.float32)>,\n",
              " <BatchDataset shapes: ((1024, 83), (1024, 1)), types: (tf.float32, tf.float32)>,\n",
              " <BatchDataset shapes: ((1024, 83), (1024, 1)), types: (tf.float32, tf.float32)>)"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSoQdqcn0VkW"
      },
      "source": [
        "# brief test\n",
        "# 0-label, 6-18 numerical, 1-5 category \n",
        "\n",
        "trainD_name = \"LC_dataset09-18q4-1.csv\"\n",
        "nume_field = 13 # \n",
        "cateNum = 5\n",
        "cateS = 1\n",
        "feature_field = nume_field + cateNum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH9prtjT0Dkz"
      },
      "source": [
        "cwd = os.path.abspath(os.getcwd())\n",
        "train_path = cwd +\"/\"+ trainD_name\n",
        "f1 = open(train_path,'r')\n",
        "dic= {}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ5teeEVYpyJ"
      },
      "source": [
        "# LC grade,  Income to Payment Ratio, annual income, fico, dti, revol util, term, revol to income, total acc, home ownerdhip\n",
        "# loan amnt, purpose, inquiries, credit age\n",
        "colm15 = ['loan_amnt', 'term', 'purpose', 'fico_range_low', 'fico_range_high', #'grade', \n",
        "        'sub_grade', 'inq_last_6mths', 'revol_util', 'revol_bal', 'delinq_2yrs', 'pub_rec', 'open_acc',\n",
        "        'total_acc', 'earliest_cr_line', 'issue_d', 'annual_inc', 'emp_length', 'home_ownership', \n",
        "         'verification_status', 'dti', 'loan_status']\n",
        "# revolving income ration, desc : len(desc), fico\n",
        "# target loan status\n",
        "# categorical : purpose, credit lebel(grade, sub_grade), homeownership, income verification, employ_length\n",
        "# numerical : term, loan_amnt, int_rate, fico, inq_last_6mths, revol_util, delinq_2yrs, pub_rec, opne_acc\n",
        "#   revolv_inc, total_acc, credit_age, annual_inc, DTI, desc\n",
        "\"\"\"\n",
        "colm = ['loan_amnt', 'term', 'int_rate', 'purpose', 'fico_range_low', 'fico_range_high', #'grade', \n",
        "        'sub_grade', 'inq_last_6mths', 'revol_util', 'revol_bal', 'delinq_2yrs', 'pub_rec', 'open_acc',\n",
        "        'total_acc', 'earliest_cr_line', 'annual_inc', 'emp_length', 'home_ownership', \n",
        "         'verification_status', 'dti', 'desc', 'loan_status']\"\"\"\n",
        "dataset = dataset[colm15]\n",
        "dataset = dataset.dropna(how='all')\n",
        "# {'Does not meet the credit policy. Status:Fully Paid', 'In Grace Period', \n",
        "# 'Does not meet the credit policy. Status:Charged Off', 'Fully Paid', 'Default', 'Late (31-120 days)', 'Current', 'Charged Off', 'Late (16-30 days)' }\n",
        "credit_policy = \"Does not meet the credit policy. Status:\"\n",
        "len_credit_policy = len(credit_policy)\n",
        "remove_credit_policy = (\n",
        "    lambda status: status[len_credit_policy:]\n",
        "    if credit_policy in str(status)\n",
        "    else status\n",
        ")\n",
        "dataset.loan_status = dataset.loan_status.map(remove_credit_policy)\n",
        "rows_to_drop = dataset[\n",
        "    (dataset.loan_status != \"Charged Off\") & (dataset.loan_status != \"Fully Paid\")\n",
        "].index\n",
        "dataset.drop(index=rows_to_drop, inplace=True)\n",
        "dataset.loan_status = dataset.loan_status.map({'Fully Paid':1, 'Charged Off':0})\n",
        "#dataset.loan_status = dataset.loan_status.replace(to_replace='Fully Paid')\n",
        "# fico handling\n",
        "\"\"\"\n",
        "fico = dataset.fico_range_high + dataset.fico_range_low\n",
        "dataset[['fico']] = fico/2\n",
        "\"\"\"\n",
        "dataset[['fico']] = (dataset.fico_range_high + dataset.fico_range_low) /2\n",
        "dataset = dataset.drop(labels=['fico_range_high', 'fico_range_low'], axis=1)\n",
        "# log \n",
        "dataset[['loan_amnt']] = dataset.loan_amnt.replace(to_replace=0, value=1)\n",
        "dataset[['loan_amnt']] = np.log(dataset.loan_amnt+1)\n",
        "dataset[['annual_inc']] = dataset.annual_inc.replace(to_replace=0, value=1)\n",
        "dataset[['annual_inc']] = np.log(dataset.annual_inc+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Q83hb3E-c7"
      },
      "source": [
        "# revol_inc\n",
        "month_inc = dataset.annual_inc /12\n",
        "dataset.revol_bal = np.log(dataset.revol_bal+1)\n",
        "dataset[['revol_inc']] = dataset.revol_bal / month_inc\n",
        "dataset = dataset.drop(labels=['revol_bal'], axis=1)\n",
        "# convertion of None data in categorical, \n",
        "# desc, emp_length, purpose, term, credit grade, ownership, \n",
        "dataset[['emp_length']] = dataset.emp_length.replace(to_replace=np.float32(None), value='None')\n",
        "#datset['desc'] = data.desc.replace(to_replace=np.float32(None), value='').str.len()\n",
        "# dropna\n",
        "#dataset = dataset.dropna(subset=['annual_inc', 'inq_last_6mths', 'revol_util', 'delinq_2yrs', 'pub_rec', 'open_acc', 'total_acc', 'dti'])\n",
        "dataset = dataset.dropna(subset=['annual_inc', 'inq_last_6mths', 'revol_util', 'delinq_2yrs', 'pub_rec', 'open_acc', 'total_acc', 'dti'])\n",
        "#dataset[['loan_status']] = dataset[[dataset.loan_status != '']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "OxNslu7qJpIY",
        "outputId": "514b44f3-d9dd-434f-9425-2e9c21911976"
      },
      "source": [
        "onehot(dataset.emp_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>emp_length : 1 year</th>\n",
              "      <th>emp_length : 10+ years</th>\n",
              "      <th>emp_length : 2 years</th>\n",
              "      <th>emp_length : 3 years</th>\n",
              "      <th>emp_length : 4 years</th>\n",
              "      <th>emp_length : 5 years</th>\n",
              "      <th>emp_length : 6 years</th>\n",
              "      <th>emp_length : 7 years</th>\n",
              "      <th>emp_length : 8 years</th>\n",
              "      <th>emp_length : 9 years</th>\n",
              "      <th>emp_length : &lt; 1 year</th>\n",
              "      <th>emp_length : None</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>68407277</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68355089</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68341763</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68476807</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68426831</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89905081</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88948836</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89996426</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90006534</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88224441</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1346788 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         emp_length : 1 year  ... emp_length : None\n",
              "id                            ...                  \n",
              "68407277                 0.0  ...               0.0\n",
              "68355089                 0.0  ...               0.0\n",
              "68341763                 0.0  ...               0.0\n",
              "68476807                 0.0  ...               0.0\n",
              "68426831                 0.0  ...               0.0\n",
              "...                      ...  ...               ...\n",
              "89905081                 0.0  ...               0.0\n",
              "88948836                 0.0  ...               0.0\n",
              "89996426                 0.0  ...               0.0\n",
              "90006534                 0.0  ...               0.0\n",
              "88224441                 0.0  ...               0.0\n",
              "\n",
              "[1346788 rows x 12 columns]"
            ]
          },
          "execution_count": 97,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9pyTSi4IM-P"
      },
      "source": [
        "# credit ages\n",
        "dataset[['credit_age']] = creditage(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhVhx8zqK3mi"
      },
      "source": [
        "dataset = dataset.drop(labels=['earliest_cr_line', 'issue_d'], axis=1)\n",
        "# term\n",
        "dataset.term = dataset.term.apply(lambda term: int(term[:3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "4lr_l7_YMI5b",
        "outputId": "14217648-d9e3-42ce-f347-885f97458756"
      },
      "source": [
        "emp = dataset.emp_length\n",
        "emp.index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index([  68407277,   68355089,   68341763,   68476807,   68426831,   68476668,\n",
              "         67275481,   68466926,   68616873,   68338832,\n",
              "       ...\n",
              "       '89968033', '89024450', '89846605', '89007204', '89867167', '89905081',\n",
              "       '88948836', '89996426', '90006534', '88224441'],\n",
              "      dtype='object', name='id', length=1346788)"
            ]
          },
          "execution_count": 88,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "-ZaQo6HvGekq",
        "outputId": "b6f3ff8c-8e62-40c0-aabe-996e8f53b028"
      },
      "source": [
        "# categolize\n",
        "purpose= onehot(dataset.purpose)\n",
        "dataset = pd.concat([purpose, dataset], axis=1)\n",
        "dataset = dataset.drop(labels=['purpose'], axis=1)\n",
        "\n",
        "sub_grade = onehot(dataset.sub_grade)\n",
        "dataset = pd.concat([sub_grade, dataset], axis=1)\n",
        "dataset = dataset.drop(labels=['sub_grade'], axis=1)\n",
        "\n",
        "home = onehot(dataset.home_ownership)\n",
        "dataset = pd.concat([home, dataset], axis=1)\n",
        "dataset = dataset.drop(labels=['home_ownership'], axis=1)\n",
        "\n",
        "verification = onehot(dataset.verification_status)\n",
        "dataset = pd.concat([verification, dataset], axis=1)\n",
        "dataset = dataset.drop(labels=['verification_status'], axis=1)\n",
        "\n",
        "employ = onehot(dataset.emp_length)\n",
        "dataset = pd.concat([employ, dataset], axis=1)\n",
        "dataset = dataset.drop(labels=['emp_length'], axis=1)\n",
        "display(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(emp_length : 1 year,)</th>\n",
              "      <th>(emp_length : 10+ years,)</th>\n",
              "      <th>(emp_length : 2 years,)</th>\n",
              "      <th>(emp_length : 3 years,)</th>\n",
              "      <th>(emp_length : 4 years,)</th>\n",
              "      <th>(emp_length : 5 years,)</th>\n",
              "      <th>(emp_length : 6 years,)</th>\n",
              "      <th>(emp_length : 7 years,)</th>\n",
              "      <th>(emp_length : 8 years,)</th>\n",
              "      <th>(emp_length : 9 years,)</th>\n",
              "      <th>(emp_length : &lt; 1 year,)</th>\n",
              "      <th>(emp_length : None,)</th>\n",
              "      <th>(verification_status : Not Verified,)</th>\n",
              "      <th>(verification_status : Source Verified,)</th>\n",
              "      <th>(verification_status : Verified,)</th>\n",
              "      <th>(home_ownership : ANY,)</th>\n",
              "      <th>(home_ownership : MORTGAGE,)</th>\n",
              "      <th>(home_ownership : NONE,)</th>\n",
              "      <th>(home_ownership : OTHER,)</th>\n",
              "      <th>(home_ownership : OWN,)</th>\n",
              "      <th>(home_ownership : RENT,)</th>\n",
              "      <th>(sub_grade : A1,)</th>\n",
              "      <th>(sub_grade : A2,)</th>\n",
              "      <th>(sub_grade : A3,)</th>\n",
              "      <th>(sub_grade : A4,)</th>\n",
              "      <th>(sub_grade : A5,)</th>\n",
              "      <th>(sub_grade : B1,)</th>\n",
              "      <th>(sub_grade : B2,)</th>\n",
              "      <th>(sub_grade : B3,)</th>\n",
              "      <th>(sub_grade : B4,)</th>\n",
              "      <th>(sub_grade : B5,)</th>\n",
              "      <th>(sub_grade : C1,)</th>\n",
              "      <th>(sub_grade : C2,)</th>\n",
              "      <th>(sub_grade : C3,)</th>\n",
              "      <th>(sub_grade : C4,)</th>\n",
              "      <th>(sub_grade : C5,)</th>\n",
              "      <th>(sub_grade : D1,)</th>\n",
              "      <th>(sub_grade : D2,)</th>\n",
              "      <th>(sub_grade : D3,)</th>\n",
              "      <th>(sub_grade : D4,)</th>\n",
              "      <th>...</th>\n",
              "      <th>(sub_grade : E4,)</th>\n",
              "      <th>(sub_grade : E5,)</th>\n",
              "      <th>(sub_grade : F1,)</th>\n",
              "      <th>(sub_grade : F2,)</th>\n",
              "      <th>(sub_grade : F3,)</th>\n",
              "      <th>(sub_grade : F4,)</th>\n",
              "      <th>(sub_grade : F5,)</th>\n",
              "      <th>(sub_grade : G1,)</th>\n",
              "      <th>(sub_grade : G2,)</th>\n",
              "      <th>(sub_grade : G3,)</th>\n",
              "      <th>(sub_grade : G4,)</th>\n",
              "      <th>(sub_grade : G5,)</th>\n",
              "      <th>(purpose : car,)</th>\n",
              "      <th>(purpose : credit_card,)</th>\n",
              "      <th>(purpose : debt_consolidation,)</th>\n",
              "      <th>(purpose : educational,)</th>\n",
              "      <th>(purpose : home_improvement,)</th>\n",
              "      <th>(purpose : house,)</th>\n",
              "      <th>(purpose : major_purchase,)</th>\n",
              "      <th>(purpose : medical,)</th>\n",
              "      <th>(purpose : moving,)</th>\n",
              "      <th>(purpose : other,)</th>\n",
              "      <th>(purpose : renewable_energy,)</th>\n",
              "      <th>(purpose : small_business,)</th>\n",
              "      <th>(purpose : vacation,)</th>\n",
              "      <th>(purpose : wedding,)</th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>term</th>\n",
              "      <th>inq_last_6mths</th>\n",
              "      <th>revol_util</th>\n",
              "      <th>delinq_2yrs</th>\n",
              "      <th>pub_rec</th>\n",
              "      <th>open_acc</th>\n",
              "      <th>total_acc</th>\n",
              "      <th>annual_inc</th>\n",
              "      <th>dti</th>\n",
              "      <th>loan_status</th>\n",
              "      <th>fico</th>\n",
              "      <th>revol_inc</th>\n",
              "      <th>credit_age</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>68407277</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.188967</td>\n",
              "      <td>36</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>10.915107</td>\n",
              "      <td>5.91</td>\n",
              "      <td>1</td>\n",
              "      <td>677.0</td>\n",
              "      <td>8.712869</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68355089</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.114599</td>\n",
              "      <td>36</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>11.082158</td>\n",
              "      <td>16.06</td>\n",
              "      <td>1</td>\n",
              "      <td>717.0</td>\n",
              "      <td>10.800559</td>\n",
              "      <td>194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68341763</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.903538</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>11.050906</td>\n",
              "      <td>10.78</td>\n",
              "      <td>1</td>\n",
              "      <td>697.0</td>\n",
              "      <td>9.741261</td>\n",
              "      <td>186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68476807</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.249657</td>\n",
              "      <td>60</td>\n",
              "      <td>3.0</td>\n",
              "      <td>64.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>11.556311</td>\n",
              "      <td>25.37</td>\n",
              "      <td>1</td>\n",
              "      <td>697.0</td>\n",
              "      <td>10.379379</td>\n",
              "      <td>213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68426831</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.388570</td>\n",
              "      <td>36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10.434145</td>\n",
              "      <td>10.20</td>\n",
              "      <td>1</td>\n",
              "      <td>692.0</td>\n",
              "      <td>10.448523</td>\n",
              "      <td>342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89905081</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.798183</td>\n",
              "      <td>60</td>\n",
              "      <td>1.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>11.775297</td>\n",
              "      <td>20.59</td>\n",
              "      <td>1</td>\n",
              "      <td>737.0</td>\n",
              "      <td>10.271199</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88948836</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.288784</td>\n",
              "      <td>60</td>\n",
              "      <td>1.0</td>\n",
              "      <td>85.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>12.105108</td>\n",
              "      <td>22.03</td>\n",
              "      <td>1</td>\n",
              "      <td>707.0</td>\n",
              "      <td>11.160037</td>\n",
              "      <td>177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89996426</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.373522</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>11.964007</td>\n",
              "      <td>10.34</td>\n",
              "      <td>0</td>\n",
              "      <td>737.0</td>\n",
              "      <td>11.657633</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90006534</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.680406</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>11.918397</td>\n",
              "      <td>12.25</td>\n",
              "      <td>1</td>\n",
              "      <td>667.0</td>\n",
              "      <td>9.010378</td>\n",
              "      <td>233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88224441</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.085851</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>11.608245</td>\n",
              "      <td>18.30</td>\n",
              "      <td>0</td>\n",
              "      <td>662.0</td>\n",
              "      <td>10.108027</td>\n",
              "      <td>210</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1346788 rows × 84 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          (emp_length : 1 year,)  ...  credit_age\n",
              "id                                ...            \n",
              "68407277                     0.0  ...         150\n",
              "68355089                     0.0  ...         194\n",
              "68341763                     0.0  ...         186\n",
              "68476807                     0.0  ...         213\n",
              "68426831                     0.0  ...         342\n",
              "...                          ...  ...         ...\n",
              "89905081                     0.0  ...         149\n",
              "88948836                     0.0  ...         177\n",
              "89996426                     0.0  ...          64\n",
              "90006534                     0.0  ...         233\n",
              "88224441                     0.0  ...         210\n",
              "\n",
              "[1346788 rows x 84 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHN9K3niNtXc"
      },
      "source": [
        "dataset.to_csv('./LC_dataset09-18q4.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es-OuNDEw5C3"
      },
      "source": [
        "data = pd.read_csv('LC_p2p (1).csv')\n",
        "display(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "Ecx8mTM0T8RR",
        "outputId": "9a22e0bb-cdb6-4530-a92d-468bfd3ed976"
      },
      "source": [
        "data = pd.read_csv('LC_p2p (1).csv')\n",
        "#dataset = pd.read_csv('LC_p2p_raw (3).csv')\n",
        "#dataset.dropna(how='all')\n",
        "data = data[['loan_amnt', 'term', 'int_rate', 'purpose', 'fico_range_low', 'fico_range_high', 'grade', 'sub_grade', 'inq_last_6mths', \n",
        "           'revol_util', 'revol_bal', 'delinq_2yrs', 'pub_rec', 'open_acc','total_acc', 'earliest_cr_line', 'annual_inc', 'emp_length', 'home_ownership', \n",
        "           'verification_status', 'dti', 'desc', 'loan_status']]\n",
        "display(data)\n",
        "print(len(data.dropna(subset=['loan_amnt', 'annual_inc', 'verification_status', 'loan_status'])))\n",
        "print(len(data.dropna(subset=['loan_status'])))\n",
        "print(set(data.dropna(subset=['annual_inc']).annual_inc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>term</th>\n",
              "      <th>int_rate</th>\n",
              "      <th>purpose</th>\n",
              "      <th>fico_range_low</th>\n",
              "      <th>fico_range_high</th>\n",
              "      <th>grade</th>\n",
              "      <th>sub_grade</th>\n",
              "      <th>inq_last_6mths</th>\n",
              "      <th>revol_util</th>\n",
              "      <th>revol_bal</th>\n",
              "      <th>delinq_2yrs</th>\n",
              "      <th>pub_rec</th>\n",
              "      <th>open_acc</th>\n",
              "      <th>total_acc</th>\n",
              "      <th>earliest_cr_line</th>\n",
              "      <th>annual_inc</th>\n",
              "      <th>emp_length</th>\n",
              "      <th>home_ownership</th>\n",
              "      <th>verification_status</th>\n",
              "      <th>dti</th>\n",
              "      <th>desc</th>\n",
              "      <th>loan_status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3600.0</td>\n",
              "      <td>36 months</td>\n",
              "      <td>13.99</td>\n",
              "      <td>debt_consolidation</td>\n",
              "      <td>675.0</td>\n",
              "      <td>679.0</td>\n",
              "      <td>C</td>\n",
              "      <td>C4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29.7</td>\n",
              "      <td>2765.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Aug-2003</td>\n",
              "      <td>55000.0</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>Not Verified</td>\n",
              "      <td>5.91</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fully Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24700.0</td>\n",
              "      <td>36 months</td>\n",
              "      <td>11.99</td>\n",
              "      <td>small_business</td>\n",
              "      <td>715.0</td>\n",
              "      <td>719.0</td>\n",
              "      <td>C</td>\n",
              "      <td>C1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>21470.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>Dec-1999</td>\n",
              "      <td>65000.0</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>Not Verified</td>\n",
              "      <td>16.06</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fully Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20000.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>10.78</td>\n",
              "      <td>home_improvement</td>\n",
              "      <td>695.0</td>\n",
              "      <td>699.0</td>\n",
              "      <td>B</td>\n",
              "      <td>B4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.2</td>\n",
              "      <td>7869.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>Aug-2000</td>\n",
              "      <td>63000.0</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>Not Verified</td>\n",
              "      <td>10.78</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fully Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>35000.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>14.85</td>\n",
              "      <td>debt_consolidation</td>\n",
              "      <td>785.0</td>\n",
              "      <td>789.0</td>\n",
              "      <td>C</td>\n",
              "      <td>C5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.6</td>\n",
              "      <td>7802.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>Sep-2008</td>\n",
              "      <td>110000.0</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>17.06</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Current</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10400.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>22.45</td>\n",
              "      <td>major_purchase</td>\n",
              "      <td>695.0</td>\n",
              "      <td>699.0</td>\n",
              "      <td>F</td>\n",
              "      <td>F1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>64.5</td>\n",
              "      <td>21929.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>Jun-1998</td>\n",
              "      <td>104433.0</td>\n",
              "      <td>3 years</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>25.37</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fully Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260662</th>\n",
              "      <td>24000.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>10.49</td>\n",
              "      <td>credit_card</td>\n",
              "      <td>725.0</td>\n",
              "      <td>729.0</td>\n",
              "      <td>B</td>\n",
              "      <td>B3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.4</td>\n",
              "      <td>22448.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>Feb-2001</td>\n",
              "      <td>125000.0</td>\n",
              "      <td>4 years</td>\n",
              "      <td>OWN</td>\n",
              "      <td>Not Verified</td>\n",
              "      <td>10.98</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Current</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260663</th>\n",
              "      <td>24000.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>12.79</td>\n",
              "      <td>home_improvement</td>\n",
              "      <td>665.0</td>\n",
              "      <td>669.0</td>\n",
              "      <td>C</td>\n",
              "      <td>C1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>84.4</td>\n",
              "      <td>49431.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>Dec-1999</td>\n",
              "      <td>95000.0</td>\n",
              "      <td>7 years</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>19.61</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Current</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260664</th>\n",
              "      <td>24000.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>10.49</td>\n",
              "      <td>debt_consolidation</td>\n",
              "      <td>695.0</td>\n",
              "      <td>699.0</td>\n",
              "      <td>B</td>\n",
              "      <td>B3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>21665.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>Feb-1991</td>\n",
              "      <td>108000.0</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>Not Verified</td>\n",
              "      <td>34.94</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Current</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260665</th>\n",
              "      <td>40000.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>10.49</td>\n",
              "      <td>debt_consolidation</td>\n",
              "      <td>705.0</td>\n",
              "      <td>709.0</td>\n",
              "      <td>B</td>\n",
              "      <td>B3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>64.9</td>\n",
              "      <td>8633.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>Feb-1995</td>\n",
              "      <td>227000.0</td>\n",
              "      <td>9 years</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>Verified</td>\n",
              "      <td>12.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Current</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260666</th>\n",
              "      <td>24000.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>14.49</td>\n",
              "      <td>debt_consolidation</td>\n",
              "      <td>660.0</td>\n",
              "      <td>664.0</td>\n",
              "      <td>C</td>\n",
              "      <td>C4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.1</td>\n",
              "      <td>17641.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>Jul-1999</td>\n",
              "      <td>110000.0</td>\n",
              "      <td>6 years</td>\n",
              "      <td>RENT</td>\n",
              "      <td>Not Verified</td>\n",
              "      <td>18.30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Charged Off</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2260667 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         loan_amnt        term  int_rate  ...    dti  desc  loan_status\n",
              "0           3600.0   36 months     13.99  ...   5.91   NaN   Fully Paid\n",
              "1          24700.0   36 months     11.99  ...  16.06   NaN   Fully Paid\n",
              "2          20000.0   60 months     10.78  ...  10.78   NaN   Fully Paid\n",
              "3          35000.0   60 months     14.85  ...  17.06   NaN      Current\n",
              "4          10400.0   60 months     22.45  ...  25.37   NaN   Fully Paid\n",
              "...            ...         ...       ...  ...    ...   ...          ...\n",
              "2260662    24000.0   60 months     10.49  ...  10.98   NaN      Current\n",
              "2260663    24000.0   60 months     12.79  ...  19.61   NaN      Current\n",
              "2260664    24000.0   60 months     10.49  ...  34.94   NaN      Current\n",
              "2260665    40000.0   60 months     10.49  ...  12.75   NaN      Current\n",
              "2260666    24000.0   60 months     14.49  ...  18.30   NaN  Charged Off\n",
              "\n",
              "[2260667 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2260663\n",
            "2260667\n",
            "{0.0, 1.0, 2.0, 3.0, 6.0, 8.0, 10.0, 15.0, 16.0, 20.0, 23.0, 25.0, 32.0, 33.0, 36.0, 39.0, 40.0, 50.0, 60.0, 66.0, 70.0, 100.0, 52125.68, 115.0, 118.0, 138.0, 150.0, 262300.0, 262325.0, 191.0, 200.0, 524500.0, 250.0, 262400.0, 260.0, 300.0, 52164.84, 262500.0, 52174.0, 52180.62, 262540.0, 52182.55, 400.0, 430.0, 52194.0, 487.0, 500.0, 262644.0, 52212.0, 262713.0, 262725.0, 587.0, 599.0, 600.0, 640.0, 262800.0, 686.0, 690.0, 700.0, 525000.0, 727.0, 262887.0, 746.0, 750.0, 262900.0, 760.0, 770.0, 800.0, 804.0, 262983.0, 840.0, 850.0, 263000.0, 886.0, 263031.0, 910.0, 929.0, 938.0, 948.0, 950.0, 263100.0, 52296.05, 263124.0, 1000.0, 1008.0, 70377.84, 1029.0, 1050.0, 1061.0, 1066.0, 787500.0, 1100.0, 1104.0, 1158.0, 1165.0, 1166.0, 1574060.0, 1198.0, 1200.0, 1203.0, 52347.75, 263375.0, 263387.0, 1246.0, 52352.0, 263400.0, 52353.0, 263407.0, 263411.0, 1288.78, 1300.0, 1302.0, 1320.0, 263486.0, 52370.0, 1350.0, 263500.0, 1370.0, 1400.0, 52382.0, 1050000.0, 1429.0, 1500.0, 52402.0, 52415.0, 1596.0, 1600.0, 1608.0, 64618.0, 52424.0, 52425.0, 263800.0, 52435.5, 1700.0, 52447.0, 1750.0, 263900.0, 1764.0, 1770.0, 1780.0, 1800.0, 52469.0, 264000.0, 1862.0, 1874.0, 1884.0, 52479.48, 1896.0, 1900.0, 52482.0, 52487.16, 1944.0, 52491.0, 52492.52, 1958.0, 788392.03, 1968.0, 1982.0, 52497.76, 264134.0, 1993.0, 2000.0, 52509.0, 2062.0, 264218.0, 2100.0, 2135.0, 2140.0, 2189.0, 52540.8, 2200.0, 2280.0, 2300.0, 52570.0, 264500.0, 2400.0, 2402.0, 2436.0, 2439.0, 264600.0, 264633.0, 264638.0, 2500.0, 789000.0, 52617.0, 2600.0, 52629.99, 2700.0, 2711.42, 52643.64, 264902.0, 52653.64, 2784.0, 52657.78, 52659.62, 2800.0, 6032121.0, 52665.6, 2100000.0, 265000.0, 52673.0, 265021.0, 2896.0, 52684.5, 265075.0, 2934.0, 265083.0, 2940.0, 2951.0, 3000.0, 3003.0, 52700.96, 265221.72, 3092.0, 3109.34, 3120.0, 3181.0, 3200.0, 3240.0, 265436.0, 265441.0, 3300.0, 52762.8, 265489.0, 3354.0, 265500.0, 52772.0, 52773.12, 3378.0, 3400.0, 3412.0, 265561.0, 265583.96, 52787.04, 3500.0, 265655.0, 3545.0, 790000.0, 3600.0, 3700.0, 528000.0, 52844.0, 3734.0, 3755.96, 3775.2, 3799.0, 3800.0, 3804.0, 3840.0, 266000.0, 3888.0, 3900.0, 3911.0, 3981.0, 4000.0, 4002.0, 52904.0, 4065.0, 266220.0, 4080.0, 266234.0, 4100.0, 4177.0, 266340.0, 4200.0, 266349.6, 4235.0, 4296.0, 4300.0, 4320.0, 266500.0, 52974.0, 4392.0, 4400.0, 4412.0, 52983.44, 4450.0, 4452.0, 4464.0, 4475.0, 4476.0, 266632.0, 4488.0, 4500.0, 4511.0, 53002.8, 4524.0, 4535.0, 266700.0, 4560.0, 4600.0, 4695.0, 53039.0, 529000.0, 266859.0, 4738.8, 4740.0, 529068.0, 4800.0, 4824.0, 267000.0, 4888.0, 267097.0, 4966.0, 4992.0, 5000.0, 5016.0, 1053595.0, 5088.0, 5112.0, 5160.0, 5181.0, 5200.0, 5208.0, 5235.0, 5240.0, 53145.0, 5244.0, 267400.0, 5300.0, 267454.0, 529600.0, 267500.0, 5360.0, 267525.0, 267527.0, 5400.0, 5412.0, 5416.0, 5424.0, 5440.0, 267600.0, 53189.0, 5472.0, 5483.64, 5500.0, 5532.0, 5538.0, 5550.0, 792000.0, 5568.0, 53211.0, 5580.0, 5592.0, 5600.0, 4462056.0, 267765.0, 5628.0, 53224.32, 267791.0, 267800.0, 5664.0, 5674.0, 5694.0, 4200000.0, 5700.0, 530000.0, 5724.0, 267900.0, 5772.0, 5784.0, 5786.0, 5800.0, 5805.0, 5843.0, 268000.0, 5880.0, 5928.0, 5940.0, 53288.59, 5971.0, 268125.0, 6000.0, 530305.0, 6048.0, 53309.0, 6078.0, 53313.04, 6108.0, 6111.84, 6130.0, 53324.96, 6144.0, 6168.0, 53334.16, 53334.0, 6200.0, 53336.76, 6228.0, 6240.0, 6250.0, 6252.0, 6276.0, 6280.35, 6288.0, 6292.0, 53354.0, 6300.0, 268447.0, 6312.0, 6336.0, 53364.0, 53365.0, 268500.0, 6359.0, 6360.0, 6380.0, 6384.0, 53372.08, 6395.4, 268539.0, 53375.28, 6400.0, 6408.0, 6420.0, 6424.0, 53382.6, 268600.0, 6456.0, 6480.0, 6492.0, 6495.0, 6500.0, 53395.68, 6504.0, 268700.0, 6564.0, 793000.0, 6572.0, 6576.0, 6588.0, 6600.0, 268750.0, 6610.0, 268761.0, 6634.0, 6635.0, 59529.96, 6650.0, 268803.0, 6660.0, 6672.0, 268818.0, 6682.8, 6684.0, 6695.0, 6700.0, 268851.0, 531000.0, 6720.0, 268871.0, 6732.0, 6736.0, 53443.92, 6744.0, 6756.0, 53450.58, 6780.0, 6792.0, 6800.0, 268971.0, 53461.0, 6840.0, 531139.0, 6852.0, 268997.0, 269000.0, 6864.0, 53469.0, 6882.0, 6887.0, 6900.0, 6901.0, 6933.0, 6960.0, 7000.0, 53495.78, 269156.0, 7020.0, 53499.48, 7050.0, 7062.0, 7067.0, 53512.0, 53514.0, 7100.0, 7111.0, 7117.0, 53519.0, 269280.0, 269285.0, 7156.0, 7159.4, 7176.0, 7196.0, 53534.0, 7200.0, 7208.0, 53538.0, 7224.0, 7225.0, 269369.0, 7240.0, 7247.0, 7255.0, 7256.0, 7261.0, 7272.0, 53549.88, 1318000.0, 7280.0, 53550.03, 53553.8, 7300.0, 7320.0, 7356.0, 269500.0, 7400.0, 269557.0, 53577.51, 53579.0, 7428.0, 7440.0, 53583.0, 53583.98, 7452.0, 269600.0, 7459.64, 53586.39, 7464.0, 7476.0, 53593.92, 7500.0, 7510.8, 7512.0, 7560.0, 794000.0, 7570.0, 7578.0, 7591.0, 7592.0, 7596.0, 7600.0, 7605.0, 7618.0, 7620.0, 53618.0, 269790.0, 7652.0, 269800.0, 7666.0, 7675.0, 7680.0, 53630.0, 7692.0, 269837.0, 7696.0, 7700.0, 532000.0, 7740.0, 7749.0, 7764.0, 7771.0, 7776.0, 7777.0, 269923.0, 7788.0, 7800.0, 269950.0, 7812.0, 7848.0, 270000.0, 7856.73, 7858.0, 7860.0, 7872.0, 7876.0, 7884.0, 7896.0, 7900.0, 7904.04, 7904.0, 53674.72, 7917.0, 7920.0, 7932.0, 270080.0, 7938.0, 7944.0, 7956.0, 270100.0, 7968.0, 7970.88, 7979.0, 7992.0, 8000.0, 8004.0, 53695.72, 8016.0, 53697.0, 8025.0, 270182.0, 8040.0, 53702.4, 8052.0, 8054.0, 8057.0, 8064.0, 8065.0, 794500.0, 8068.0, 8076.0, 8088.0, 53711.1, 8100.0, 270246.0, 53715.0, 8112.0, 270264.0, 8136.0, 8140.0, 8160.0, 8167.2, 8172.0, 8180.0, 53730.13, 8196.0, 8200.0, 8208.0, 8222.0, 8232.0, 8242.0, 8243.0, 8244.0, 8250.0, 270400.0, 8256.0, 8262.0, 53747.0, 8276.0, 8280.0, 8300.0, 8304.0, 8320.0, 53758.9, 8328.0, 8332.0, 8340.0, 8342.0, 270493.0, 270500.0, 8357.0, 53765.02, 8374.0, 8376.0, 8385.0, 8391.48, 8400.0, 8412.0, 53777.0, 8431.0, 8435.0, 8436.0, 8450.0, 53784.0, 8460.0, 53785.0, 8471.0, 8472.0, 8479.0, 270624.0, 8484.0, 8496.0, 8500.0, 8506.79, 8508.0, 8520.0, 8532.0, 8542.79, 8550.0, 8556.0, 8568.0, 8580.0, 8592.0, 8600.0, 8604.0, 270750.0, 8616.0, 8628.0, 270780.0, 8640.0, 8650.0, 8652.0, 8662.0, 8663.0, 8664.0, 8676.0, 8678.0, 8679.0, 8688.0, 8691.0, 8700.0, 8701.0, 8700.96, 8705.0, 8706.0, 8707.0, 314352.0, 8724.0, 8736.0, 8745.0, 8748.0, 8749.0, 8760.0, 8762.5, 8774.0, 8784.0, 8790.0, 8796.0, 8797.0, 53852.4, 8800.0, 8807.0, 8808.0, 8820.0, 8830.0, 8832.0, 8840.0, 8844.0, 8855.04, 8856.0, 271000.0, 8864.0, 8868.0, 8880.0, 53869.0, 8888.0, 8890.0, 8892.0, 8893.0, 271036.0, 8895.0, 8900.0, 8904.0, 8916.0, 8917.2, 8928.0, 8933.0, 8938.0, 8940.0, 8950.0, 8960.0, 8964.0, 8972.0, 8976.0, 8979.6, 8988.0, 9000.0, 9009.0, 9012.0, 9023.0, 9024.0, 9025.0, 9035.0, 9036.0, 9042.0, 9048.0, 9050.0, 9060.0, 9061.0, 9061.2, 9070.0, 9072.0, 9072.12, 9077.0, 62882.0, 9081.0, 9084.0, 9087.0, 9096.0, 9100.0, 9108.0, 9118.0, 9120.0, 271266.0, 9123.36, 9156.0, 9166.8, 9168.0, 9171.0, 9174.4, 9176.0, 9178.0, 9180.0, 9188.0, 9192.0, 9200.0, 9204.0, 9213.0, 9216.0, 533511.0, 9228.0, 9232.0, 9240.0, 9250.0, 9252.0, 9264.0, 9267.7, 9268.8, 9274.0, 1320000.0, 9283.68, 9286.8, 9288.0, 9300.0, 9301.0, 9301.2, 9310.0, 9312.0, 9318.0, 9320.0, 9324.0, 9325.2, 9325.0, 9336.0, 9348.0, 9360.0, 9374.0, 9380.0, 9384.0, 9392.0, 9396.0, 9400.0, 9408.0, 9428.0, 9432.0, 9444.0, 9456.0, 9468.0, 9469.0, 9479.0, 9480.0, 9492.0, 9500.0, 9504.0, 9507.84, 9518.0, 9519.0, 9520.0, 9525.2, 9526.0, 9528.0, 9536.0, 53999.0, 9540.0, 53999.92, 9549.0, 54001.0, 9552.0, 9562.0, 533850.0, 9570.8, 9576.0, 9588.0, 9600.0, 9612.0, 9622.79, 9623.0, 9624.0, 9626.0, 9630.0, 9635.0, 9636.0, 9641.0, 9648.0, 9660.0, 54023.0, 9665.0, 9669.72, 9672.0, 9684.0, 9690.0, 54030.0, 9696.0, 9700.0, 9704.0, 9708.0, 9720.0, 9729.0, 9732.0, 9736.0, 9737.52, 271883.0, 9744.0, 9745.0, 9750.0, 9756.0, 9763.0, 9765.0, 9766.0, 9768.0, 9780.0, 9783.0, 9789.36, 9790.0, 9792.0, 9798.0, 9800.0, 9801.36, 62911.0, 9804.0, 9816.0, 9828.0, 9832.0, 9840.0, 9850.0, 9852.0, 272000.0, 9858.0, 9862.0, 9864.94, 9865.0, 9864.0, 9876.0, 9888.0, 9898.0, 9899.0, 9900.0, 9906.0, 9912.0, 9924.0, 9931.56, 272076.0, 9936.0, 54078.99, 54080.2, 9948.0, 9952.0, 9960.0, 9964.0, 9972.0, 9977.0, 9980.0, 9984.0, 9996.0, 9998.0, 9999.0, 10000.0, 10001.0, 10003.0, 10008.0, 10014.0, 10016.0, 10018.0, 10020.0, 10026.0, 10032.0, 10040.0, 10043.0, 10044.0, 10045.0, 10050.0, 10056.0, 10060.0, 10062.0, 54103.0, 10065.0, 10068.0, 10078.0, 10080.0, 10092.0, 10098.0, 10100.0, 272244.0, 10104.0, 10106.0, 10112.0, 10114.0, 10116.0, 10120.0, 10124.0, 10128.0, 10140.0, 10150.0, 10151.0, 10152.0, 272300.0, 10160.0, 10164.0, 10167.45, 54125.5, 10176.0, 10180.0, 10187.0, 10188.0, 10192.0, 10200.0, 10206.0, 10208.0, 10210.0, 10212.0, 10221.0, 10224.0, 10236.0, 10240.0, 10244.0, 10248.0, 10250.0, 10254.0, 10255.0, 10254.4, 10260.0, 10264.0, 10270.0, 10272.0, 10276.0, 10277.0, 10280.0, 10284.0, 10291.0, 10294.79, 10296.0, 10300.0, 10302.0, 10308.0, 272453.0, 10312.8, 10313.0, 10320.0, 10324.74, 10329.0, 10331.0, 10332.0, 10334.0, 10336.0, 10341.0, 10344.0, 54159.0, 10349.0, 10350.0, 10354.8, 10358.0, 10362.0, 10368.0, 10372.68, 10380.0, 54167.0, 10386.0, 10390.0, 10392.0, 10398.0, 10400.0, 10404.0, 10409.0, 10410.0, 10416.0, 10428.0, 10440.0, 10447.93, 10452.0, 10464.0, 10467.0, 10471.0, 10476.0, 272621.0, 10488.0, 10492.8, 10500.0, 10512.0, 10520.0, 10524.0, 10525.0, 10528.8, 10528.0, 10535.0, 10536.0, 10540.0, 10541.0, 10548.0, 10550.0, 10556.0, 10560.0, 10563.0, 272708.0, 797000.0, 10572.0, 10575.0, 10584.0, 10595.0, 10596.0, 10599.0, 10600.0, 10606.8, 10608.0, 10612.0, 10620.0, 10624.8, 10631.08, 10632.0, 10631.0, 10635.0, 10636.0, 10642.0, 10644.0, 10650.0, 10652.0, 10668.0, 10672.8, 10673.0, 10672.0, 10680.0, 10681.0, 10686.0, 10690.8, 10692.0, 10700.0, 10704.0, 10708.0, 535000.0, 10716.0, 10720.0, 10728.0, 10730.0, 10731.0, 10740.0, 10744.0, 10744.8, 10746.0, 10748.64, 10750.0, 10752.0, 54242.0, 10764.0, 10768.8, 10768.0, 10776.0, 10780.0, 10788.0, 10794.0, 10800.0, 10810.0, 10812.0, 10816.0, 10824.0, 10830.0, 10836.0, 10848.0, 10850.0, 10852.0, 273000.0, 10858.0, 10860.0, 10871.0, 10872.0, 10874.0, 10880.0, 273035.0, 54268.63, 10896.0, 10896.6, 10900.0, 10902.0, 10908.0, 10909.6, 10910.64, 10910.0, 10920.0, 10926.0, 10930.0, 10932.0, 10935.0, 10939.0, 10940.0, 10944.0, 10947.0, 10956.0, 10960.0, 10965.6, 10968.0, 10975.0, 54285.0, 10980.0, 10986.0, 10988.64, 10988.0, 10992.0, 10993.0, 11000.0, 11004.0, 11008.0, 11016.0, 11028.0, 11029.0, 11036.0, 11038.8, 11040.0, 11049.0, 11050.0, 11052.0, 273200.0, 11061.6, 11064.0, 11075.0, 11076.0, 11080.19, 11084.64, 11088.0, 11092.92, 11100.0, 11111.82, 11112.0, 11111.0, 11115.0, 11118.0, 11124.0, 11128.0, 11130.83, 11132.0, 11134.0, 11136.0, 11140.0, 11145.84, 11148.0, 11150.0, 11151.0, 11152.28, 273300.0, 11160.0, 273307.95, 11164.0, 11168.64, 11170.74, 11170.0, 11172.0, 11180.0, 11184.0, 11188.0, 11193.0, 11196.0, 11199.0, 11200.0, 273348.0, 11208.0, 11214.0, 11220.0, 11232.0, 11236.43, 11244.0, 11245.0, 11246.0, 11250.0, 11252.0, 273400.0, 11256.0, 11262.0, 11263.0, 11265.0, 11268.0, 11270.0, 11271.0, 54344.0, 11278.0, 11280.0, 11292.0, 11295.28, 11295.0, 11297.5, 54348.0, 11300.0, 11304.0, 11305.0, 11310.0, 273458.0, 11314.0, 11316.0, 11323.0, 11326.8, 11328.0, 11340.0, 11350.0, 11352.0, 273500.0, 11360.0, 11364.0, 11376.0, 11383.0, 3681400.0, 11386.0, 11388.0, 11389.0, 8400000.0, 11400.0, 11401.32, 11412.0, 11416.0, 11420.0, 11420.64, 11423.0, 11424.0, 11427.0, 11436.0, 11440.0, 11448.0, 11450.0, 54380.0, 11460.0, 11470.0, 11472.0, 11476.0, 11480.0, 11481.0, 11484.0, 11485.0, 11495.0, 11496.0, 11500.0, 11502.0, 11505.0, 11508.0, 273655.0, 11520.0, 11523.0, 11526.0, 11528.0, 11529.48, 11531.0, 11532.0, 11541.63, 11543.0, 11544.0, 11554.8, 11556.0, 11562.0, 11565.0, 11568.0, 11572.8, 54403.2, 54403.0, 11580.0, 11588.0, 11592.0, 11596.0, 11600.0, 11604.0, 11616.0, 11622.0, 11624.0, 11625.0, 11628.0, 11634.0, 11640.0, 11644.0, 11650.0, 11652.0, 54419.0, 273800.0, 535945.0, 11660.0, 11664.0, 11671.08, 11676.0, 11683.0, 54426.0, 11688.0, 11690.0, 11700.0, 11701.44, 11712.0, 11714.4, 11718.0, 11721.0, 11724.0, 11725.0, 11726.0, 11727.0, 11736.0, 11743.5, 11747.0, 11748.0, 11752.0, 11760.0, 11769.0, 11770.8, 11772.0, 11777.99, 11782.0, 11788.0, 11794.0, 11796.0, 11800.0, 11808.0, 11820.0, 54453.0, 11824.0, 11832.0, 11832.2, 273978.0, 273980.0, 11844.0, 11846.0, 11850.0, 11851.0, 11853.0, 274000.0, 11856.0, 54461.0, 11868.0, 11870.0, 54463.0, 11878.0, 11880.0, 54465.42, 11892.0, 11895.0, 11896.0, 11898.5, 11900.0, 11901.0, 11904.0, 11906.0, 11910.0, 11913.0, 11916.0, 11920.0, 11924.0, 11928.0, 11939.0, 54476.19, 11940.0, 11950.8, 11951.0, 11952.0, 11953.0, 11957.0, 11963.12, 11964.0, 11968.98, 11971.0, 11976.0, 274125.0, 11991.0, 11992.0, 11997.0, 12000.0, 54490.0, 12012.0, 12024.0, 12035.0, 12036.0, 12039.0, 12040.0, 12041.0, 12046.8, 12048.0, 12050.0, 12060.0, 12061.0, 12063.0, 12064.0, 54502.0, 12071.0, 12072.6, 12072.0, 12076.0, 12079.0, 12082.0, 12084.0, 12085.0, 12087.0, 12094.0, 12096.0, 12100.0, 12101.0, 12102.0, 12108.0, 12115.0, 12120.0, 12129.0, 12130.0, 274273.0, 12132.57, 12132.0, 12133.0, 12140.0, 12144.0, 12148.0, 12151.0, 12156.0, 12158.8, 12162.0, 12164.0, 12168.0, 12171.12, 12174.0, 12176.0, 12179.0, 12180.0, 274331.0, 12192.0, 12200.0, 12204.0, 12216.0, 12217.0, 12220.56, 12220.0, 12225.6, 12228.0, 12240.0, 274389.0, 12250.0, 12252.0, 12261.0, 12264.0, 12274.8, 12276.0, 12278.0, 12280.0, 12281.0, 12286.17, 12288.0, 12291.0, 12300.0, 12308.0, 12309.36, 12312.0, 12317.0, 12320.0, 12321.0, 274469.0, 12336.0, 12340.2, 12340.0, 12341.0, 12345.0, 12348.0, 12350.0, 274502.0, 12360.0, 12366.0, 12369.0, 12372.0, 12374.0, 12384.0, 274529.0, 12386.0, 12392.0, 12394.8, 12394.0, 12396.0, 12400.0, 12403.0, 12406.8, 12408.0, 274558.0, 12420.0, 12424.0, 12429.0, 12430.0, 12432.0, 274581.0, 12442.0, 12444.0, 12450.0, 12454.0, 12456.0, 274600.0, 12465.34, 12466.0, 12468.0, 12472.0, 12474.0, 12478.8, 12480.0, 12482.0, 274635.0, 12492.0, 12494.4, 12500.0, 12504.0, 12506.0, 12516.0, 12521.0, 12523.0, 12525.0, 12527.0, 12528.0, 12527.88, 12529.0, 12531.0, 12532.0, 12540.0, 12546.56, 12547.2, 12550.0, 12552.0, 12556.0, 12560.4, 12564.0, 12569.0, 12576.0, 12580.0, 12582.0, 274728.0, 12588.0, 12598.0, 12600.0, 12604.0, 12612.0, 12614.0, 12624.0, 12626.0, 12634.0, 12635.0, 12636.0, 12642.0, 12645.0, 12648.0, 12650.0, 274800.0, 12657.0, 12660.0, 12666.0, 12670.0, 12672.0, 12680.0, 12684.0, 12690.0, 12693.0, 12696.0, 12697.0, 12699.0, 12700.0, 12706.0, 12707.0, 12708.0, 274853.43, 12710.4, 12709.0, 537000.0, 12714.0, 12720.0, 12722.0, 12724.0, 12725.0, 12727.0, 12728.0, 12731.0, 12732.06, 12732.0, 12740.0, 12742.0, 12742.8, 12744.0, 12750.0, 12756.0, 12760.0, 12768.0, 799200.0, 12774.0, 12779.0, 12780.0, 12786.0, 12792.0, 274940.0, 12800.0, 12802.0, 12803.0, 12804.0, 12815.0, 12816.0, 12828.0, 12832.0, 12835.56, 12838.0, 12840.0, 12844.0, 12846.0, 12847.0, 274992.0, 12850.0, 12852.0, 275000.0, 12860.0, 275004.0, 12864.0, 12870.0, 12871.0, 12873.8, 12875.0, 12876.0, 12877.0, 12883.0, 12888.0, 12888.8, 12892.0, 12894.0, 12900.0, 12912.0, 12913.0, 12919.0, 12921.6, 12922.0, 12923.96, 12924.0, 12923.0, 275072.0, 12936.0, 12945.0, 12948.0, 12960.0, 12972.0, 12976.0, 12983.0, 12984.0, 12987.0, 12990.0, 12993.88, 12993.0, 12995.0, 12996.0, 13000.0, 13000.8, 13003.0, 13008.0, 13017.0, 13018.8, 13020.0, 13032.0, 13035.0, 13044.0, 13050.0, 13055.0, 13056.0, 275200.0, 54698.0, 13065.0, 13068.0, 13080.0, 13084.0, 13090.0, 13091.0, 13092.0, 13100.0, 13103.0, 13104.0, 13114.0, 13116.0, 13127.0, 13128.0, 13136.0, 13140.0, 13140.15, 13147.0, 13147.5, 13152.0, 13158.0, 13162.0, 13164.0, 13170.0, 13172.0, 13174.4, 13176.0, 13182.0, 13188.0, 13195.0, 13200.0, 13206.0, 13209.0, 13212.0, 13213.2, 13214.0, 13224.0, 13230.0, 13235.5, 13236.0, 13235.0, 13238.0, 13242.0, 13244.88, 13247.0, 13248.0, 13250.0, 13260.0, 13263.0, 13272.0, 13273.0, 13276.0, 13280.0, 13283.0, 13284.0, 13288.0, 13288.08, 13291.92, 13295.0, 13296.0, 13298.0, 13300.0, 13307.52, 13308.0, 13310.0, 13320.0, 13322.0, 13324.0, 13326.0, 13328.0, 13330.8, 13332.0, 13333.0, 13334.0, 13339.0, 13342.8, 13344.0, 13350.0, 13352.0, 13355.0, 13356.0, 275500.0, 13358.0, 13360.0, 13368.0, 13370.0, 13378.8, 13380.0, 13386.0, 13390.0, 1848400.0, 13393.04, 13392.0, 13396.0, 13400.0, 13404.0, 13410.0, 13411.0, 13416.0, 13419.0, 13421.0, 13422.0, 13425.0, 13428.0, 13430.67, 13430.0, 13432.0, 275579.0, 13440.0, 13444.0, 13448.0, 13450.0, 13452.0, 13455.0, 275600.0, 13459.0, 13460.0, 13462.32, 13464.0, 13467.14, 13468.0, 54780.24, 13472.57, 13474.8, 13476.0, 13478.0, 13483.0, 13488.0, 13496.0, 13498.68, 13499.82, 13500.0, 275645.0, 13512.0, 13520.0, 13524.0, 13525.0, 13530.0, 13533.0, 13534.0, 13536.0, 275680.0, 13542.0, 13548.0, 13554.0, 13560.0, 13563.0, 800000.0, 13568.0, 13572.0, 800006.0, 13576.0, 13584.0, 13586.0, 13591.0, 13592.0, 13594.0, 13596.0, 13600.0, 13602.0, 13606.0, 13608.0, 13610.0, 13618.0, 13618.8, 13620.0, 13622.0, 13623.0, 800060.0, 13629.0, 13632.0, 13634.2, 13635.0, 13636.8, 13640.0, 13644.0, 13649.0, 13650.0, 275800.0, 13656.0, 13656.12, 13661.0, 13665.0, 13668.0, 13670.0, 13671.0, 13672.0, 275820.0, 13680.0, 13682.0, 13685.0, 13688.0, 13692.0, 13695.0, 13700.0, 13704.0, 13705.0, 538000.0, 13716.0, 13720.0, 13725.0, 13728.0, 275873.0, 54832.0, 13736.0, 13739.8, 13740.0, 13742.0, 13743.0, 13750.0, 13752.0, 13764.0, 13766.2, 13768.0, 13774.8, 13775.0, 13776.0, 3421649.0, 13784.0, 13786.0, 13788.0, 13790.4, 13791.0, 13796.0, 13797.0, 13799.0, 13800.0, 54846.0, 13802.0, 13806.0, 13811.0, 13812.0, 13813.0, 13820.0, 13824.0, 275968.0, 13825.0, 13829.0, 13833.0, 13836.0, 54854.0, 13842.0, 13843.0, 13844.0, 13848.0, 13850.0, 54856.0, 13853.0, 13854.0, 276000.0, 13857.0, 13858.44, 13859.0, 13860.0, 13862.0, 13864.8, 13864.0, 13868.0, 13869.0, 13871.8, 13872.0, 13873.8, 13880.8, 538168.0, 13883.0, 13884.0, 13886.0, 13888.0, 13896.0, 13897.0, 13900.0, 13905.72, 13906.68, 13908.0, 13910.0, 13920.0, 13925.0, 13930.0, 13932.0, 13940.0, 13944.0, 13956.0, 13959.0, 13960.0, 13968.0, 13970.0, 13972.68, 54880.92, 13975.0, 13976.0, 13980.0, 13984.0, 13986.0, 13990.0, 13990.8, 13992.0, 13991.28, 54884.4, 54885.39, 14000.0, 14004.0, 14007.0, 14008.0, 54887.0, 14011.8, 14014.0, 14014.8, 14016.0, 14021.0, 14023.0, 54891.0, 14028.0, 14029.0, 14032.0, 14035.0, 14038.0, 14039.0, 14040.0, 14044.0, 14046.0, 14052.0, 14061.5, 14062.8, 54898.0, 14064.0, 14064.96, 14068.0, 14070.0, 14071.0, 14072.0, 54900.5, 14075.0, 14076.0, 14080.0, 54902.16, 14083.0, 14084.0, 63081.0, 14088.0, 14089.0, 14090.0, 14096.0, 14100.0, 14104.0, 14112.0, 14116.0, 14120.0, 54910.0, 14124.0, 54911.0, 14127.0, 14129.0, 14134.0, 14136.0, 14141.0, 14144.0, 14148.0, 54915.0, 14150.0, 14150.8, 14160.0, 14169.0, 14172.0, 14173.92, 14174.0, 14176.0, 14178.0, 14180.0, 1587047.0, 14184.0, 14185.0, 14196.0, 14196.8, 14200.0, 14208.0, 14210.0, 538500.0, 14218.68, 14220.0, 14224.0, 14225.0, 14227.0, 14232.0, 14234.0, 14234.23, 14236.0, 14240.0, 14240.4, 14244.0, 14245.0, 14247.0, 14250.0, 276396.0, 14252.0, 14256.0, 14257.2, 14259.0, 14260.0, 14262.0, 14263.0, 14268.0, 14274.0, 14276.0, 14277.0, 14280.0, 1325000.0, 14283.0, 14286.9, 14288.0, 14290.0, 14292.0, 14295.14, 14300.0, 14304.0, 14310.0, 14314.8, 14315.0, 14316.0, 14317.0, 14322.0, 14324.0, 14328.0, 14340.0, 14344.8, 14348.0, 14350.0, 14352.0, 276500.0, 14362.0, 14363.0, 14364.0, 14365.0, 14366.0, 14369.0, 14370.0, 276516.0, 14373.0, 14374.68, 14376.0, 14378.52, 14379.0, 14380.0, 14382.0, 14384.48, 14388.0, 14397.0, 14400.0, 14402.0, 14404.0, 14405.0, 14412.0, 14422.0, 14424.0, 14425.0, 14430.0, 14431.0, 14436.0, 14440.0, 14441.0, 14442.0, 276587.0, 14444.0, 14446.0, 14447.8, 14448.0, 14450.0, 14452.0, 14454.48, 14458.0, 14460.0, 14462.0, 14464.0, 14470.0, 14472.0, 14480.0, 14484.0, 14496.0, 14497.0, 276640.0, 14500.0, 14504.0, 14506.8, 14508.0, 14512.0, 14516.0, 14518.8, 14520.0, 14521.0, 14523.0, 14525.0, 14532.0, 14534.0, 14540.0, 14542.92, 14542.8, 14544.0, 14550.0, 14556.0, 14557.56, 14558.0, 14560.0, 14566.0, 14567.39, 14568.0, 54999.0, 54999.12, 276716.66, 14572.0, 14573.0, 14579.0, 14580.0, 14584.8, 55002.0, 14587.0, 14588.0, 14592.0, 14597.0, 14599.0, 14600.0, 14604.0, 55006.8, 276750.0, 14614.64, 14616.0, 14620.0, 14621.0, 14623.0, 14628.0, 14629.0, 14636.0, 14640.0, 14641.0, 14644.0, 14646.0, 14648.0, 14650.0, 14652.0, 14653.0, 14657.98, 14661.0, 14663.0, 14664.0, 14668.0, 14672.0, 14676.0, 14677.22, 14686.8, 14686.0, 14688.0, 14690.0, 14695.0, 14696.0, 14697.0, 14700.0, 14701.0, 14704.0, 14705.0, 14707.0, 14709.0, 14712.0, 539000.0, 14722.0, 14724.0, 14726.0, 14730.0, 14734.0, 14736.0, 14742.0, 276888.0, 14748.0, 14750.0, 14753.0, 14754.0, 276900.0, 14760.0, 14761.0, 14763.36, 63108.0, 14765.0, 14768.0, 14770.8, 14771.0, 14772.0, 14784.0, 276933.0, 14790.0, 14794.8, 14795.54, 14796.0, 55044.48, 14798.0, 14798.76, 14800.0, 14801.0, 14804.0, 14808.0, 6568417.0, 14817.6, 14820.0, 14832.0, 14834.0, 14836.8, 14840.0, 14843.0, 14844.0, 14850.0, 14851.0, 277000.0, 14856.0, 14858.4, 14856.48, 14860.7, 14860.0, 14863.2, 14864.0, 14868.0, 14869.8, 14874.0, 14878.0, 14880.0, 14883.76, 14884.0, 14885.0, 14892.0, 14896.0, 14897.0, 14898.0, 55064.88, 14900.0, 14902.0, 14903.0, 14904.0, 14916.0, 277070.0, 14926.0, 14928.0, 14929.56, 14934.0, 14936.28, 14940.0, 14944.0, 14948.0, 14950.0, 14951.0, 14952.0, 14955.0, 14955.6, 14958.0, 14960.0, 277104.0, 14964.0, 14965.0, 14968.0, 14974.0, 14976.0, 14978.0, 14985.0, 14988.0, 14989.0, 14990.0, 14991.0, 14995.0, 14996.0, 14998.0, 14999.0, 15000.0, 15007.2, 15008.0, 15009.0, 15007.92, 15007.0, 15012.0, 15010.0, 15020.0, 15024.0, 15025.0, 15034.0, 15035.0, 15036.0, 15047.52, 15048.0, 15050.0, 15056.0, 15060.0, 15072.0, 15077.0, 15078.0, 15080.0, 15084.0, 15085.0, 15088.0, 15090.0, 15094.0, 15096.0, 15100.0, 15104.0, 15105.84, 15108.0, 15114.0, 15116.0, 15120.0, 15121.2, 15121.0, 277269.0, 15125.0, 15126.0, 15125.4, 15132.0, 15136.32, 15140.0, 15144.0, 15150.0, 15151.0, 15154.0, 15154.56, 15156.0, 15167.0, 15168.0, 15173.0, 15176.0, 15177.0, 15178.8, 15180.0, 15182.0, 15192.0, 277343.0, 15200.0, 55124.0, 15204.0, 15205.0, 15210.0, 15211.0, 15216.0, 15217.55, 15226.0, 15226.8, 15228.0, 15230.8, 15238.0, 15240.0, 15245.0, 15249.0, 15250.0, 55134.0, 15252.0, 15253.0, 15257.88, 15257.84, 15260.0, 15262.8, 15263.0, 15264.0, 15265.0, 55138.8, 15274.8, 15275.0, 15276.0, 15277.44, 15280.0, 15281.0, 55140.96, 15288.0, 15289.0, 15291.0, 15294.0, 15298.8, 15299.0, 15300.0, 15310.0, 15312.0, 15317.0, 15320.0, 15324.0, 15325.0, 15325.8, 55149.0, 15328.0, 15330.0, 15333.6, 277478.0, 15335.04, 15336.0, 15338.0, 15340.0, 15348.0, 15349.2, 15350.0, 15352.5, 277500.0, 15358.0, 15360.0, 15364.0, 15370.0, 15372.0, 15373.2, 15375.0, 15379.0, 15380.0, 15382.0, 15384.0, 15385.8, 55161.6, 15390.65, 55163.0, 15396.0, 15400.0, 15408.0, 15420.0, 55168.0, 15423.36, 15423.17, 15429.6, 15432.0, 15435.0, 15436.8, 15440.0, 15443.0, 15444.0, 15446.0, 15450.0, 15452.0, 15454.0, 15456.0, 15461.0, 15462.71, 15462.0, 15465.0, 15466.0, 15467.0, 15468.0, 63136.0, 15470.0, 15477.0, 15478.8, 15480.0, 15487.0, 15488.0, 15489.0, 15490.8, 15492.0, 15497.0, 15500.0, 15501.0, 15502.0, 15503.0, 15504.0, 15508.0, 15513.0, 15514.0, 15516.0, 15520.0, 15522.0, 15523.0, 15526.8, 15527.0, 15528.0, 15530.04, 15531.0, 15532.0, 15534.96, 15534.0, 277680.0, 15537.0, 15538.8, 15540.0, 15550.0, 15552.0, 15555.0, 15556.0, 15558.0, 15560.0, 15561.0, 15564.0, 15566.0, 15567.0, 15574.44, 15576.0, 15579.0, 15580.0, 15585.36, 15588.0, 15590.0, 15592.0, 15599.0, 15600.0, 15601.0, 15604.0, 15605.0, 15606.0, 15605.64, 15610.0, 15612.0, 15616.0, 15617.0, 15618.0, 15624.0, 15628.0, 15629.0, 15635.0, 15636.0, 15640.0, 15642.0, 15644.0, 15648.0, 15650.0, 15652.0, 15654.0, 15659.0, 15660.0, 15661.0, 15666.6, 15667.0, 15671.0, 15672.0, 15673.0, 15674.0, 15677.5, 15680.0, 15682.5, 15683.0, 15684.0, 15685.44, 15686.0, 15687.0, 15688.0, 15691.0, 15694.4, 15695.0, 15696.0, 15696.8, 15698.0, 15700.0, 15706.01, 15708.0, 540000.0, 15714.0, 15718.0, 15720.0, 15721.0, 15725.0, 15726.24, 15730.8, 15732.0, 15741.96, 15742.0, 15744.0, 277889.0, 15745.0, 15747.0, 15748.0, 15750.0, 55234.0, 15755.0, 15756.0, 15760.0, 15764.0, 15765.12, 15768.0, 15770.0, 277915.0, 15772.0, 15777.0, 15780.0, 15784.0, 15786.0, 15790.0, 15790.8, 15792.0, 15791.28, 15796.72, 15800.0, 15803.16, 15804.0, 55244.08, 15808.0, 15816.0, 15820.0, 15822.0, 15823.2, 15823.87, 15827.0, 15828.0, 15829.0, 15832.0, 15838.8, 15840.0, 15847.0, 15850.0, 15851.9, 15852.0, 15854.64, 15855.0, 278000.0, 15857.0, 15858.0, 15860.0, 15864.0, 15866.88, 15868.0, 15874.0, 15875.0, 15876.0, 55259.03, 15880.0, 15882.0, 15888.0, 15889.2, 15890.0, 55261.91, 15892.0, 15897.0, 15900.0, 278044.0, 15908.0, 15909.96, 15909.0, 15911.0, 15912.0, 15913.0, 15910.0, 15915.0, 55266.0, 15922.0, 15924.0, 15930.0, 15932.0, 15934.0, 15935.0, 15936.0, 15937.0, 15940.0, 15942.96, 15942.0, 15945.98, 15948.0, 15949.44, 15950.0, 55274.0, 15958.0, 15960.0, 15964.0, 15965.0, 15968.0, 15969.0, 55277.0, 15972.0, 15973.0, 55278.0, 15983.0, 15984.0, 15986.0, 15988.0, 15995.0, 15996.0, 15998.4, 15999.0, 16000.0, 16000.53, 16006.0, 16008.0, 16012.0, 16016.0, 16018.8, 55287.96, 16020.0, 16023.0, 16026.0, 16028.0, 16029.0, 16030.0, 278175.0, 16032.0, 16034.0, 16035.24, 16036.0, 16039.0, 16040.0, 16044.0, 16046.0, 16048.0, 16050.0, 16052.0, 16056.0, 16057.0, 16059.01, 16060.0, 16062.0, 16064.0, 16065.0, 16068.0, 16070.0, 16072.0, 16075.28, 16080.0, 55299.0, 16083.0, 16090.39, 16092.0, 16094.0, 16098.0, 16100.0, 16102.0, 16103.0, 16104.0, 16107.0, 16108.0, 16110.0, 16112.0, 16114.0, 16115.0, 16116.0, 16120.0, 16120.92, 55307.16, 16123.0, 16124.0, 16125.0, 16128.0, 16130.0, 278275.0, 16132.0, 16134.0, 16136.0, 16138.0, 16140.0, 16141.0, 16144.0, 16146.0, 16150.0, 55313.0, 16152.0, 16154.0, 16155.0, 16160.0, 16164.0, 55316.0, 16166.0, 16167.0, 16168.0, 55317.33, 16174.8, 16175.0, 16176.0, 16185.0, 16188.0, 16189.2, 16191.0, 63165.0, 16194.0, 16198.0, 16199.0, 16200.0, 16199.4, 16202.0, 16203.0, 16204.0, 16207.0, 16211.0, 16212.0, 16215.0, 16216.0, 16224.0, 16226.0, 16230.0, 16231.0, 16232.0, 16233.24, 16236.0, 16238.0, 16240.0, 16244.0, 16244.93, 16247.0, 16248.0, 16250.0, 16254.0, 16257.0, 16258.56, 16260.0, 16265.0, 16266.0, 16268.44, 16269.6, 55337.87, 16272.0, 16274.0, 16276.0, 16279.0, 16279.2, 16282.44, 16283.0, 16284.0, 16285.0, 55340.92, 16288.0, 16291.23, 16294.0, 16296.0, 16300.0, 16302.0, 16304.0, 16307.0, 16308.0, 16311.0, 16312.0, 16314.0, 16316.0, 16318.0, 16318.8, 16320.0, 16324.0, 16330.0, 16332.0, 16333.0, 16335.0, 16338.0, 16341.8, 16341.0, 16343.28, 16344.0, 16345.0, 16342.8, 16347.0, 16348.0, 16343.0, 16350.0, 16349.0, 16354.8, 16356.0, 278500.0, 16360.0, 278510.0, 16366.8, 16368.0, 16375.56, 16377.0, 16380.0, 16382.0, 16385.0, 16386.0, 16387.71, 16390.0, 16390.8, 16392.0, 16391.0, 16400.0, 16402.0, 16404.0, 278550.0, 16410.0, 16412.0, 16416.0, 16422.0, 16425.0, 16426.8, 16427.0, 16428.0, 16426.0, 16433.0, 16435.0, 16436.0, 16438.0, 16440.0, 16446.0, 55372.0, 16448.0, 16450.0, 16452.0, 16456.0, 16457.0, 16458.6, 16462.8, 16464.0, 16474.8, 16476.0, 16480.0, 16484.52, 16486.8, 16487.0, 16488.0, 16489.0, 16489.2, 16500.0, 16502.0, 16504.0, 16506.88, 16506.48, 16510.0, 16512.0, 16512.23, 55385.64, 16516.0, 16517.0, 16518.0, 16523.06, 16524.0, 16525.0, 16526.0, 16528.8, 16529.0, 16532.0, 16535.0, 16536.0, 16544.0, 16545.0, 16548.0, 16549.0, 16550.0, 16558.0, 16560.0, 16563.0, 16570.0, 16572.0, 55397.76, 16575.0, 16577.0, 16580.0, 16583.0, 16584.0, 16585.0, 16586.41, 16587.0, 16588.0, 16589.0, 16590.0, 16594.0, 16595.0, 278740.0, 16596.0, 16599.0, 16600.0, 16604.0, 16608.0, 16609.0, 16614.0, 16620.0, 16621.91, 16622.0, 16623.0, 16626.0, 16627.2, 16628.0, 16629.0, 16630.8, 16632.0, 16633.0, 16638.0, 16639.0, 16640.0, 16642.08, 16644.0, 16645.0, 16650.0, 16652.0, 16654.0, 16655.0, 16656.0, 16658.0, 16661.0, 16662.0, 16663.0, 16664.0, 16665.0, 16667.0, 16668.0, 16669.0, 16670.0, 16677.7, 16678.9, 278822.0, 16680.0, 16680.48, 16682.0, 16684.0, 16686.57, 16686.0, 16689.0, 16690.0, 16692.0, 16700.0, 16704.0, 16705.56, 16710.0, 16711.0, 541000.0, 16714.0, 16715.0, 16716.0, 16718.0, 16720.0, 16722.0, 16723.53, 16725.0, 16726.8, 16727.0, 16728.0, 16729.0, 16730.76, 16730.55, 16729.8, 16730.0, 16734.0, 16735.0, 16740.0, 16743.84, 16745.0, 16748.0, 16749.0, 16750.0, 16751.0, 16752.0, 16750.8, 278895.0, 16756.0, 16758.0, 16763.0, 16764.0, 16766.0, 16774.0, 16775.72, 16776.0, 16777.0, 16778.0, 16780.0, 16781.0, 16784.0, 16786.8, 16788.0, 16789.0, 16790.0, 16796.0, 63189.0, 16800.0, 16804.0, 16812.0, 16815.0, 16816.8, 16824.0, 16832.0, 16836.0, 16838.0, 16839.0, 16840.0, 16842.0, 16843.0, 16848.0, 16850.0, 16852.0, 16854.39, 279000.0, 16856.0, 16858.0, 16860.0, 16869.0, 16872.0, 16874.0, 16876.0, 16879.0, 16884.0, 16886.0, 16890.0, 16892.0, 16894.6, 16895.0, 16896.0, 16900.0, 16902.0, 16906.8, 16906.0, 16908.0, 16912.5, 16913.0, 16920.0, 16927.0, 16931.0, 16932.0, 16933.0, 16942.0, 16944.0, 16945.2, 16948.0, 16952.0, 16955.0, 16956.0, 16959.0, 16965.0, 16968.0, 16969.0, 16973.0, 16978.0, 16979.0, 16980.0, 16982.0, 16983.0, 16988.0, 16992.0, 16996.0, 16996.43, 16998.3, 16999.0, 17000.0, 17003.0, 17004.0, 17007.0, 17010.0, 17014.8, 17016.0, 17024.51, 17026.0, 17028.0, 17032.0, 17035.0, 17036.0, 17040.0, 17042.0, 17043.0, 17044.0, 17047.0, 17048.0, 17049.6, 17050.0, 17051.0, 17052.0, 17060.55, 17062.8, 17064.0, 17066.0, 17068.0, 17071.2, 17072.0, 17074.8, 17074.0, 17076.0, 17077.0, 17082.0, 17088.0, 17090.0, 17094.0, 17095.0, 17096.75, 17100.0, 55502.0, 17103.0, 17106.0, 17108.0, 17111.0, 17112.0, 17114.0, 17118.0, 17120.0, 17121.0, 17124.0, 17125.0, 17126.0, 17129.0, 17130.0, 17131.0, 17134.8, 17136.0, 17137.0, 17140.0, 17142.0, 17143.44, 17145.0, 17148.0, 17160.0, 279306.0, 17166.0, 17167.0, 17170.8, 17172.0, 279320.0, 17180.0, 17181.0, 17182.0, 17184.0, 17186.0, 17190.0, 17195.0, 17196.0, 17200.0, 17202.51, 17203.0, 17204.0, 17205.0, 17207.4, 17208.0, 17210.0, 17218.0, 17220.0, 17226.0, 17228.0, 17231.0, 17232.0, 55528.0, 279377.0, 17236.46, 17242.0, 17244.0, 17250.0, 17251.0, 17254.0, 17255.0, 17256.0, 17257.0, 17260.0, 17262.0, 17263.0, 17264.0, 17265.0, 17266.0, 17266.8, 17278.0, 63208.0, 17280.0, 279431.0, 17289.0, 17292.0, 17295.0, 17300.0, 17300.4, 17301.0, 17304.0, 17308.01, 17308.0, 17314.0, 17314.8, 17316.0, 17315.0, 17318.76, 17320.0, 17323.18, 17328.0, 17329.72, 17332.0, 17333.0, 17335.8, 17336.8, 17337.6, 17337.0, 17340.0, 17341.0, 17344.8, 17347.0, 17350.0, 17352.0, 17353.0, 17354.76, 279500.0, 17357.0, 17358.0, 17360.0, 17363.0, 17364.0, 17369.0, 55556.0, 17374.0, 17375.0, 17376.0, 17377.08, 55556.55, 17386.8, 17387.0, 17388.0, 17390.0, 17393.88, 17394.0, 17393.0, 17398.8, 17400.0, 17402.0, 17407.8, 17409.0, 17410.0, 17412.0, 17414.0, 17420.0, 17421.6, 17422.8, 17423.0, 17424.0, 17425.0, 17426.0, 17427.0, 17426.09, 17428.0, 17430.0, 17432.8, 17435.0, 17436.0, 279583.0, 17440.0, 17439.0, 17444.88, 17444.0, 17446.8, 17448.48, 17449.0, 17450.0, 17448.0, 17452.8, 17453.0, 55573.0, 17460.0, 17463.0, 17464.35, 17468.4, 17470.0, 17472.0, 17474.0, 17476.0, 17477.0, 17479.0, 17480.0, 17482.8, 17484.0, 17485.0, 17486.88, 17489.0, 17490.0, 17496.0, 17496.8, 55581.0, 17500.0, 17505.0, 17506.8, 17508.0, 17509.6, 17514.0, 17515.0, 17517.0, 17518.95, 17520.0, 17522.0, 17532.0, 17533.44, 17535.0, 17539.0, 17540.0, 17542.0, 17543.0, 17544.0, 17545.0, 17548.0, 17550.0, 17554.8, 17556.0, 17557.0, 17560.0, 17564.0, 17565.0, 17566.8, 17566.0, 17568.0, 17569.0, 17572.0, 17573.0, 17576.0, 17580.0, 17583.0, 17586.0, 17587.0, 279730.0, 17590.0, 17591.0, 17592.0, 17595.0, 17598.0, 17600.0, 17601.15, 17601.0, 17604.0, 17606.4, 279750.0, 17608.52, 17610.0, 17614.0, 17616.0, 17618.0, 17622.0, 17624.0, 17625.0, 17626.0, 17627.0, 17628.0, 17631.0, 17635.0, 17638.0, 17640.0, 17642.0, 17649.0, 17650.0, 17651.0, 17652.0, 17654.0, 17656.37, 17656.0, 17660.0, 17662.0, 17664.0, 17667.0, 17669.0, 17670.0, 17669.52, 17674.8, 17674.0, 17676.0, 17680.0, 17688.0, 17689.68, 17690.52, 17690.0, 17694.0, 17700.0, 541992.0, 17705.0, 17704.0, 17709.0, 17710.0, 17711.0, 542000.0, 17712.0, 17714.0, 17720.09, 17721.8, 17722.8, 17721.0, 17724.0, 17726.28, 17728.0, 17729.0, 63226.0, 17730.0, 279876.0, 17733.0, 17732.0, 17735.0, 17736.0, 17740.0, 17747.0, 17748.0, 17749.92, 17750.0, 17753.0, 17756.0, 17757.0, 17760.0, 17761.0, 17764.0, 17767.0, 17769.0, 17770.8, 17770.0, 17772.0, 17774.0, 17775.0, 17780.0, 17782.0, 17784.0, 17790.0, 17794.48, 17795.0, 17796.0, 17798.0, 17799.0, 17800.0, 17804.0, 17805.5, 17806.8, 17805.0, 17808.0, 17808.5, 17807.4, 17811.0, 17812.0, 17818.0, 17820.0, 17821.0, 17826.0, 17828.0, 17829.6, 17830.0, 17832.0, 17833.0, 17840.0, 17841.0, 17841.58, 17842.68, 17844.0, 17846.5, 17847.0, 17848.0, 17850.0, 17851.0, 17852.0, 279996.0, 280000.0, 17856.0, 17862.0, 17863.0, 17866.8, 17868.0, 17873.0, 55656.48, 17878.0, 17880.0, 17883.0, 17887.0, 17888.0, 17889.62, 17891.0, 17892.0, 17893.0, 17894.52, 17894.0, 17896.0, 17897.0, 280041.0, 17900.0, 17902.34, 17903.0, 17904.0, 17909.0, 17914.0, 17915.0, 17916.0, 17920.0, 55665.83, 17925.0, 17926.0, 17928.0, 17937.6, 17938.8, 17940.0, 17943.41, 17946.0, 17947.0, 17950.8, 17950.0, 17952.0, 55671.0, 17955.0, 17955.72, 17957.0, 17955.35, 17959.0, 17960.0, 17961.0, 17962.5, 17964.0, 17970.0, 17973.0, 17974.8, 17974.0, 17976.0, 17978.0, 17980.0, 17981.0, 55677.12, 17985.0, 280130.0, 17987.0, 17988.0, 17986.0, 17990.0, 17991.0, 17992.0, 17998.0, 18000.0, 18001.0, 18002.0, 18008.0, 18009.0, 18010.0, 18012.0, 18014.0, 18016.0, 18017.0, 18018.6, 18020.0, 18022.9, 55685.0, 18024.0, 18027.0, 18030.0, 18031.0, 18035.0, 18036.0, 18042.0, 18043.0, 18046.0, 18048.0, 18049.0, 55691.0, 18055.0, 280200.0, 18060.0, 18070.0, 18071.0, 18072.0, 18075.0, 18076.5, 18081.0, 18082.0, 18083.28, 18084.0, 18085.0, 18090.0, 18092.0, 18096.0, 18100.0, 18101.85, 18102.6, 18106.8, 18108.0, 18109.0, 18108.96, 18111.72, 18115.0, 18117.75, 18120.0, 18121.0, 18123.0, 18125.0, 18126.0, 18128.0, 18130.8, 18132.0, 18135.0, 18137.0, 18141.6, 18142.8, 18143.0, 18144.0, 18145.0, 18145.56, 18145.8, 18150.0, 18151.0, 18152.0, 18155.0, 18156.0, 18157.0, 18161.0, 18166.0, 18167.0, 18168.0, 18172.0, 18174.0, 18176.0, 18177.0, 18180.0, 18187.0, 18190.8, 18191.0, 18192.0, 18193.0, 18190.0, 18195.0, 18194.0, 55719.0, 18200.0, 18202.0, 18203.0, 18204.0, 18204.72, 18208.0, 18214.0, 18216.0, 18218.28, 18218.0, 18224.0, 18228.0, 18229.0, 18233.0, 18234.45, 18236.0, 18238.0, 18239.0, 18240.0, 18242.0, 18244.0, 18250.8, 18251.0, 18252.0, 18250.0, 18255.0, 18263.0, 18264.0, 18265.0, 18268.8, 18270.0, 18274.8, 18276.0, 280424.0, 18280.0, 18287.0, 18288.0, 18290.0, 18292.0, 18294.0, 18296.0, 18297.16, 18300.0, 18302.4, 18304.0, 18310.8, 18310.0, 18312.0, 18314.0, 18315.0, 18316.0, 18316.8, 18316.56, 18319.0, 18323.0, 18324.0, 18330.0, 18332.89, 18333.0, 18334.0, 18335.0, 18336.0, 18336.96, 18345.0, 18347.0, 18348.0, 18350.0, 280500.0, 18358.0, 18358.8, 18360.0, 18363.0, 18365.0, 18366.0, 18369.0, 18370.0, 18371.0, 18372.0, 18374.0, 18375.0, 18376.0, 18377.0, 18382.8, 18382.0, 18384.0, 18392.0, 18395.0, 18396.0, 18398.0, 18399.0, 18400.0, 18402.0, 18404.0, 18406.0, 18407.0, 18408.0, 18411.0, 18412.0, 18414.0, 18416.0, 18418.8, 18419.0, 18420.0, 18421.0, 18424.44, 18425.0, 55765.7, 18429.0, 18430.0, 18431.0, 18432.0, 18433.56, 18435.0, 18439.0, 18440.0, 18442.0, 18443.0, 18444.0, 18442.8, 18446.79, 18450.0, 18453.6, 18454.8, 18455.0, 280600.0, 18456.0, 18457.0, 18460.0, 18462.0, 18464.0, 18467.0, 18468.0, 18469.0, 18473.0, 18478.0, 18480.0, 18481.0, 18483.84, 18483.0, 18484.0, 18487.2, 18490.0, 18490.8, 18492.0, 18491.0, 18496.0, 18499.2, 18500.0, 18503.64, 18504.0, 280653.0, 18512.0, 18516.0, 18519.0, 18523.28, 18523.05, 18525.0, 18527.0, 18528.0, 18532.0, 18533.0, 18534.0, 18536.0, 18540.0, 18550.0, 18552.0, 18554.0, 18555.18, 18555.0, 18559.0, 18560.0, 18562.0, 18563.0, 18564.0, 18565.0, 18566.0, 805000.0, 280714.0, 18571.0, 18572.0, 18572.85, 18574.8, 18575.0, 18576.0, 18578.0, 18579.0, 18582.0, 18583.0, 18586.0, 18587.59, 18588.0, 18589.0, 18590.0, 18592.0, 18594.0, 18596.98, 18596.46, 18598.8, 18599.0, 18600.0, 18601.0, 18602.0, 18597.0, 18605.15, 18607.2, 18610.0, 18611.0, 18612.0, 18614.27, 18619.61, 18620.0, 18623.0, 18624.0, 18625.0, 18626.99, 18626.0, 18630.0, 55806.04, 18634.0, 18635.0, 18636.0, 18638.4, 18638.0, 18642.0, 18643.0, 18644.0, 18647.0, 18648.0, 18650.0, 18651.0, 18652.44, 18659.0, 18660.0, 18662.0, 18665.0, 18666.0, 18667.0, 18668.0, 18670.0, 18671.0, 18672.0, 18673.2, 18674.0, 18675.0, 18671.16, 18682.8, 18682.0, 18684.0, 18686.0, 18689.0, 18690.0, 18692.0, 18694.8, 18696.0, 18697.0, 18700.0, 18701.0, 18702.0, 18703.0, 18707.0, 18708.0, 18708.48, 18710.0, 543000.0, 18715.0, 18719.0, 18720.0, 18724.0, 18725.9, 18726.0, 18727.0, 18730.0, 18731.16, 18732.0, 18731.0, 18734.0, 18734.73, 18737.0, 55828.0, 18742.0, 18743.0, 18744.0, 18748.0, 18750.0, 18755.0, 18756.0, 55832.42, 18764.0, 18765.0, 18767.0, 18768.0, 18769.0, 18770.0, 18772.0, 18773.0, 18774.82, 18775.0, 18774.0, 18777.8, 18777.0, 18777.81, 18780.0, 18781.0, 18778.0, 18783.0, 18786.0, 18788.0, 18790.0, 18792.0, 18795.0, 18796.0, 18799.0, 18800.0, 18804.0, 18813.0, 18814.0, 18815.0, 18816.0, 18820.0, 18822.0, 18824.0, 18828.0, 18830.0, 18832.0, 18835.2, 18838.0, 18840.0, 18843.0, 18846.0, 2116000.0, 18850.0, 18852.0, 281000.0, 18857.0, 18859.0, 18862.0, 18864.0, 18865.0, 18868.8, 18871.0, 18873.0, 18874.8, 18875.0, 18876.0, 18878.0, 18880.0, 18886.0, 18888.96, 18889.0, 18888.0, 18892.0, 18895.0, 18898.8, 18900.0, 18901.0, 18909.0, 18910.0, 18912.0, 18922.0, 18922.8, 18924.0, 18927.0, 18930.0, 18932.52, 18934.0, 18936.0, 18937.2, 18938.0, 18939.0, 18937.0, 18940.0, 18948.0, 18949.0, 18950.0, 18951.0, 18953.0, 18955.0, 18956.0, 18960.0, 18961.0, 18962.0, 18964.22, 18965.0, 18966.0, 18969.0, 18970.8, 18970.0, 18972.0, 18975.0, 18976.83, 18978.0, 18979.0, 18980.0, 18981.24, 281125.0, 18982.0, 18984.0, 18985.0, 18981.0, 18989.0, 281136.0, 18993.0, 18994.8, 18996.0, 18999.0, 19000.0, 19001.0, 19005.0, 19008.0, 19009.75, 19010.0, 19011.0, 19012.8, 19013.0, 281158.0, 19015.0, 19017.6, 19019.8, 19020.0, 19021.0, 19022.0, 19025.0, 19027.2, 19028.0, 19030.8, 19032.0, 19032.4, 19035.0, 19036.0, 19040.0, 19041.0, 19041.6, 19044.0, 19050.0, 19052.8, 19054.4, 19055.0, 19056.0, 19056.8, 19058.0, 19062.0, 19063.2, 19065.0, 19066.0, 19068.0, 19073.6, 19075.96, 19076.0, 19080.0, 19084.0, 19086.0, 19088.0, 19092.0, 281236.0, 55898.0, 19100.0, 19102.0, 19102.8, 19104.0, 19110.0, 19110.59, 19115.0, 19116.0, 19120.0, 19121.0, 19122.8, 19124.0, 19125.6, 19126.0, 19128.0, 19130.0, 19131.0, 19132.0, 19134.0, 19136.0, 19138.8, 19140.0, 19147.0, 19148.0, 19150.8, 19151.0, 19152.0, 19150.0, 19156.0, 19158.0, 19160.0, 19161.0, 19162.8, 19164.0, 19166.0, 19170.0, 19174.0, 19175.0, 19176.0, 19180.0, 19187.0, 19188.0, 19198.4, 19200.0, 19203.0, 19204.0, 19205.0, 19206.0, 19207.0, 19210.0, 19212.0, 19213.0, 19212.94, 19216.0, 19220.0, 19222.0, 19224.0, 19225.56, 19225.0, 19228.0, 19230.06, 19230.0, 19236.0, 19240.0, 19243.0, 19244.0, 19245.0, 19247.0, 19248.0, 19250.0, 19253.0, 19254.0, 19256.0, 281400.0, 19258.0, 19260.0, 19262.0, 5000000.0, 19266.0, 19267.2, 19268.0, 19272.0, 5000010.0, 19276.0, 19278.0, 19279.0, 19278.24, 19284.0, 19286.0, 19289.0, 19291.0, 19292.0, 19296.0, 19299.0, 19300.0, 19301.31, 19306.0, 19308.0, 19310.44, 19312.0, 19314.0, 19318.0, 19319.0, 19320.0, 19324.0, 19324.8, 19328.0, 19329.6, 19331.0, 19332.0, 19333.0, 19337.0, 19340.0, 19342.8, 19342.0, 19344.0, 19349.0, 19350.0, 19352.0, 19354.0, 19355.0, 19356.0, 19354.79, 19360.0, 19362.0, 19365.0, 19365.6, 19368.0, 19368.73, 19370.0, 19371.0, 19373.0, 19374.0, 19376.0, 19377.34, 19377.0, 19379.0, 19380.0, 19381.0, 19378.8, 19383.0, 19384.0, 19384.8, 19385.6, 19390.8, 19392.0, 19394.0, 281540.0, 19400.0, 19401.6, 19402.0, 19404.0, 19406.0, 19408.0, 19409.0, 19412.0, 19414.55, 19416.0, 19420.0, 1068000.0, 19426.8, 19427.0, 19428.0, 19430.0, 19433.0, 19437.0, 19439.0, 19440.0, 19443.48, 19446.0, 19449.84, 19450.0, 19452.0, 19456.0, 19460.0, 19460.91, 19463.0, 19464.0, 19465.0, 19466.0, 19467.0, 19465.2, 19470.0, 281616.0, 19474.0, 19476.0, 19478.0, 19480.0, 19484.0, 19486.0, 19486.8, 19488.0, 19489.0, 19489.6, 19498.0, 19500.0, 19506.0, 19509.36, 19510.0, 19512.0, 281658.0, 19514.4, 19518.32, 19520.0, 19522.0, 19524.0, 19526.0, 19527.0, 19530.0, 19536.0, 19537.0, 19542.0, 19543.44, 19544.0, 19546.8, 19547.0, 19548.0, 19550.0, 19551.0, 19553.0, 19557.0, 19560.0, 19562.0, 19565.0, 19569.6, 19570.8, 19570.0, 19572.0, 19572.8, 55993.56, 19575.0, 19576.43, 19580.0, 19581.0, 19582.09, 19584.0, 19585.0, 19586.0, 19587.0, 19590.0, 55997.0, 19598.0, 19599.0, 19600.0, 19601.0, 19603.0, 19604.0, 19606.0, 19607.0, 19608.0, 56000.04, 19610.0, 19610.61, 19612.0, 19614.0, 19616.0, 19620.0, 19621.0, 19625.0, 19630.68, 19631.0, 19632.0, 19630.0, 19634.0, 19635.0, 19637.0, 19640.0, 19641.0, 19642.0, 19644.0, 19646.0, 19650.0, 19656.0, 19661.25, 19661.0, 19666.8, 19667.0, 19668.0, 19676.0, 19680.0, 19681.0, 19683.0, 19685.0, 19689.0, 19690.0, 19690.8, 19692.0, 19694.04, 19697.88, 19698.0, 19700.0, 19702.0, 19703.93, 19704.0, 19703.0, 19706.0, 19708.0, 19709.0, 19711.28, 19712.4, 19712.0, 19714.8, 19715.0, 19716.0, 19719.0, 19720.0, 19721.0, 19723.0, 19724.0, 19725.36, 19725.0, 19728.0, 19729.2, 19734.0, 19738.8, 19740.0, 19745.0, 19747.0, 19750.0, 19752.0, 19755.0, 19756.56, 19760.0, 19761.0, 19764.0, 19766.0, 19767.0, 19772.0, 19775.0, 19776.0, 19778.0, 19779.0, 19780.0, 56035.4, 19785.0, 19788.0, 19789.0, 19790.04, 19789.91, 19792.0, 56037.0, 19800.0, 19802.0, 19803.0, 19811.0, 19812.0, 56041.0, 19820.0, 19821.68, 19822.0, 19824.0, 19828.0, 19830.0, 19831.0, 19833.0, 19835.0, 19836.0, 19840.0, 281988.0, 19848.48, 19848.0, 19850.0, 19853.0, 19854.0, 56049.0, 282000.0, 19858.0, 19859.0, 19860.0, 19861.0, 19862.0, 19863.0, 19864.08, 19864.0, 19866.0, 19866.35, 19865.78, 19869.0, 19870.08, 19871.0, 19872.0, 19873.0, 19874.0, 19875.0, 19876.0, 19881.0, 19883.0, 19884.0, 19885.0, 19888.0, 19890.0, 19893.0, 19895.0, 19896.0, 19900.0, 19900.34, 19903.0, 19903.32, 19906.0, 19908.0, 19910.5, 19911.0, 19912.0, 19914.0, 19918.8, 19919.0, 19920.0, 19924.0, 19928.0, 19930.0, 19932.0, 19933.27, 19934.0, 56065.32, 19940.0, 19942.8, 19944.0, 19945.0, 19946.0, 19949.0, 19950.0, 19954.0, 19955.94, 19956.0, 19958.0, 19960.0, 19963.2, 19964.0, 19966.4, 19967.34, 19968.0, 19969.0, 19967.0, 19971.0, 19970.01, 19970.0, 56072.0, 19978.0, 19980.0, 19981.0, 544271.0, 19987.0, 19987.07, 19989.0, 19991.0, 19992.0, 544282.0, 19995.0, 19998.0, 19999.0, 20000.0, 20001.0, 20002.0, 19999.68, 20004.0, 20004.99, 20006.0, 20008.0, 20013.0, 20015.37, 20016.0, 20020.0, 20023.0, 20024.0, 20026.0, 20027.63, 20028.0, 20027.0, 20032.0, 20033.28, 20037.84, 20039.0, 20040.0, 20041.0, 20044.8, 20048.0, 20050.0, 20052.0, 20054.0, 20055.0, 20058.0, 20060.0, 20062.8, 20062.71, 20064.0, 20065.0, 20066.64, 20067.0, 20069.0, 20070.0, 20070.24, 20073.0, 20074.0, 20076.0, 20079.0, 20080.0, 20082.0, 20083.0, 20084.0, 56095.0, 20088.0, 20089.43, 20092.0, 20093.0, 20095.71, 20096.0, 20097.0, 20098.0, 20098.8, 20100.0, 20102.0, 20103.0, 20104.0, 20105.0, 20106.0, 20107.0, 20108.0, 20112.0, 20113.0, 20113.56, 20119.0, 20120.0, 20121.0, 20123.0, 20124.0, 20125.56, 20125.0, 20126.0, 20124.6, 20133.0, 20134.0, 56105.76, 20136.0, 20136.48, 20139.0, 20139.48, 20140.0, 20142.0, 20141.0, 20144.0, 20144.4, 20144.73, 20143.0, 20148.0, 20147.0, 20152.0, 20154.0, 20156.0, 20157.0, 20158.0, 20159.0, 20160.0, 20162.52, 20163.36, 20166.0, 20167.0, 20168.0, 20170.0, 20170.8, 20172.0, 20173.0, 20175.24, 20176.0, 20178.0, 20179.0, 20180.0, 20181.0, 20184.0, 20185.6, 20187.0, 20188.0, 20189.0, 20190.0, 20191.0, 20193.12, 20194.8, 20195.0, 20196.0, 20200.0, 20202.0, 20204.0, 20206.0, 20207.0, 20208.0, 20208.48, 20211.84, 20216.0, 20217.6, 20218.0, 20219.0, 20220.0, 20221.0, 20222.0, 20218.8, 20224.0, 20225.0, 282368.0, 20223.0, 20228.0, 20230.0, 20232.0, 20233.0, 20234.0, 20236.0, 20239.0, 20240.0, 20239.08, 20243.0, 20244.0, 20245.0, 63326.0, 20248.48, 20249.0, 20250.0, 20254.0, 20256.0, 20257.0, 20259.2, 20266.0, 20266.8, 20268.0, 20269.0, 20270.0, 20271.0, 20274.0, 20277.0, 20280.0, 20284.8, 20285.0, 20287.0, 20292.0, 20294.0, 20295.0, 20300.0, 20301.0, 20304.0, 20309.0, 20312.0, 544600.0, 20315.0, 20316.0, 20320.0, 20322.0, 20323.0, 20324.0, 20323.08, 20328.0, 20329.36, 20329.0, 20328.24, 20332.0, 20333.0, 20335.0, 20336.0, 20337.0, 20337.46, 20340.0, 20343.0, 20344.0, 20345.0, 20348.0, 20349.0, 20350.8, 20350.5, 20352.0, 20350.0, 20354.0, 20359.0, 20360.0, 20364.0, 20365.0, 282508.5, 20367.09, 20369.0, 20373.0, 20375.0, 20376.0, 20377.56, 20378.0, 20380.0, 20381.0, 20382.0, 20383.0, 20384.0, 20388.0, 20393.0, 20395.0, 20396.3, 20397.0, 20400.0, 20404.0, 20405.0, 20410.0, 20412.0, 20413.0, 20414.0, 20413.29, 20416.32, 20419.0, 20420.4, 20420.0, 20423.0, 20424.0, 20424.18, 20425.0, 20428.0, 20430.0, 20432.0, 20434.0, 20436.0, 20436.96, 20437.0, 20440.84, 282586.0, 20444.0, 20445.32, 20446.0, 20447.16, 20448.0, 20447.0, 20450.0, 20451.0, 20452.0, 20453.0, 20454.0, 20455.0, 20456.47, 20458.0, 20460.0, 20464.0, 20470.8, 20470.0, 20472.0, 20475.0, 20477.0, 20478.0, 20484.0, 20485.0, 20485.56, 20490.0, 20491.2, 20491.0, 20493.0, 20495.0, 20496.0, 20495.04, 20496.32, 20500.0, 20500.08, 20505.0, 20506.0, 20506.8, 20508.0, 20510.0, 20516.0, 20517.0, 20518.91, 20520.0, 20520.21, 20523.0, 20526.0, 20530.0, 20532.0, 20533.0, 20534.4, 20537.64, 20537.0, 20540.0, 20541.0, 20543.0, 20544.0, 20546.0, 20547.96, 20550.0, 20556.0, 20557.0, 20561.0, 20562.0, 20563.2, 20563.0, 20565.72, 20567.0, 20568.0, 20569.0, 807000.0, 20571.84, 20572.0, 20574.0, 20575.5, 20576.16, 20578.0, 20579.0, 20580.0, 20582.4, 20582.0, 20586.6, 282733.0, 20590.0, 20592.0, 20598.0, 20600.0, 20600.8, 20601.0, 20604.0, 20605.32, 20607.76, 20608.0, 282756.0, 20613.0, 20614.0, 20616.0, 20619.0, 20620.0, 20621.0, 20627.0, 20628.0, 20630.0, 20632.59, 20633.0, 20637.0, 20638.8, 20639.0, 20640.0, 20641.0, 20644.0, 20645.0, 20646.0, 20649.0, 20649.43, 20650.0, 20652.0, 20653.8, 20654.8, 20655.0, 20650.8, 544940.63, 20658.0, 20657.0, 20662.0, 20664.0, 20665.0, 20666.0, 20667.0, 20668.0, 20670.0, 20671.0, 20672.0, 20674.0, 20680.0, 20683.0, 20684.0, 20688.0, 20690.0, 20694.0, 20696.0, 20699.4, 20700.0, 20700.72, 20702.0, 20710.0, 20711.0, 20712.0, 545000.0, 20713.0, 20716.8, 20719.2, 20721.0, 20722.8, 20723.0, 20724.0, 20725.0, 20731.31, 20734.0, 20735.0, 20736.0, 20738.0, 20739.48, 20741.64, 20741.0, 20743.0, 20748.0, 20749.2, 20750.0, 20752.0, 20754.0, 20755.0, 20757.0, 20760.0, 20762.0, 20763.0, 20764.0, 20766.0, 20767.0, 20767.32, 20769.0, 20770.0, 20769.72, 20772.0, 20768.0, 56231.66, 20777.0, 20780.0, 20782.8, 20784.0, 20785.0, 20786.0, 20787.0, 20789.0, 20791.0, 20792.0, 20795.0, 20796.0, 20797.0, 20800.0, 20804.0, 20806.0, 20806.56, 20808.0, 20809.2, 56239.0, 20806.68, 20812.9, 20807.0, 20815.0, 20817.0, 20820.0, 20828.04, 20830.0, 20831.64, 20832.0, 20834.0, 20838.0, 20840.0, 20841.6, 20842.0, 20841.0, 20844.0, 56246.61, 20846.0, 20848.0, 20849.82, 20850.0, 20856.0, 283000.0, 20857.0, 20856.53, 20860.0, 20862.0, 20864.0, 20865.96, 20866.0, 20867.0, 20868.0, 20869.0, 20869.66, 20869.91, 20874.0, 20876.0, 20880.0, 20883.0, 20884.0, 20885.0, 56254.0, 20888.0, 20889.0, 20890.8, 20892.0, 20893.0, 20895.36, 20895.0, 20898.0, 20899.0, 20900.0, 20900.18, 20902.17, 20902.0, 20904.0, 20908.0, 20915.0, 20916.0, 20918.0, 20921.76, 20921.0, 20925.0, 20926.0, 20925.24, 20928.6, 20928.0, 20930.0, 20929.0, 20932.92, 56262.0, 20935.0, 20937.0, 20940.0, 20941.08, 20942.0, 20943.0, 20944.08, 20945.0, 20944.0, 20950.8, 20950.0, 20952.0, 20955.0, 20956.2, 20956.0, 20960.0, 20963.0, 20964.0, 20965.0, 20967.18, 20968.0, 20970.0, 20971.0, 20976.0, 20977.0, 20980.0, 20982.0, 20984.0, 56274.0, 20986.0, 20987.0, 20988.0, 20993.0, 20995.0, 20996.0, 20996.4, 20998.8, 20998.0, 21000.0, 21001.0, 21002.0, 21000.1, 21006.0, 21008.0, 21008.8, 21009.0, 21008.96, 21012.0, 21013.89, 283152.96, 21018.0, 21022.0, 21023.0, 21024.0, 21022.8, 21026.0, 21028.0, 21029.0, 21030.0, 21031.0, 21033.6, 21035.0, 21036.0, 21039.6, 21040.0, 21040.2, 21045.0, 21048.0, 21049.0, 21050.0, 21052.0, 21056.04, 21060.0, 21063.0, 21064.0, 21066.0, 21071.0, 21072.0, 21074.0, 21075.0, 283219.15, 21078.0, 21080.0, 21082.8, 21082.0, 21084.0, 21084.21, 21085.0, 21085.5, 21086.0, 21090.24, 21093.0, 21094.0, 21095.25, 21095.0, 21097.2, 21096.0, 21100.0, 21101.0, 21102.0, 21108.0, 21109.41, 21109.0, 21111.0, 21112.0, 21117.0, 21118.0, 21120.0, 21125.0, 21127.68, 21129.0, 21132.0, 21139.0, 21140.0, 21144.0, 21148.0, 21150.0, 21151.0, 21154.0, 21155.0, 21156.0, 21158.0, 21160.0, 21166.0, 283311.0, 21168.0, 21170.0, 21171.0, 21174.0, 21175.2, 21177.0, 21178.0, 21180.0, 21184.0, 21185.0, 21188.0, 21190.8, 21191.0, 21190.0, 21192.0, 21194.0, 21197.0, 21200.0, 21202.0, 21203.0, 21204.0, 21204.74, 21207.0, 21208.92, 21208.0, 21210.0, 21211.0, 21212.0, 21216.0, 21218.04, 21219.84, 21220.0, 21221.0, 21222.0, 21225.0, 21227.0, 21228.0, 21233.0, 21236.0, 21237.48, 21238.0, 21238.8, 21240.0, 21244.0, 21246.0, 21247.56, 21248.0, 21250.0, 21251.64, 21252.0, 21253.0, 21255.0, 21256.0, 21260.0, 21263.0, 21264.0, 21267.0, 21269.0, 21270.0, 21271.0, 21273.6, 21274.0, 21275.0, 21276.0, 21280.0, 21281.0, 21283.0, 21285.0, 21287.0, 21288.0, 21291.0, 21292.0, 21294.0, 21296.0, 21299.2, 21300.0, 21304.4, 21305.64, 21305.76, 21306.0, 21304.53, 21305.0, 21307.0, 21311.0, 21312.0, 21312.6, 21315.0, 21318.0, 21318.11, 21320.0, 21323.0, 21324.0, 21325.0, 21327.0, 21328.0, 21330.0, 21331.0, 21332.0, 21336.0, 283480.0, 21337.8, 21340.0, 21346.0, 21347.04, 21348.0, 21349.0, 21350.0, 21346.32, 21352.0, 21353.0, 21354.53, 21350.4, 283500.0, 21357.0, 21358.0, 21359.0, 21360.0, 21358.8, 21364.0, 21365.0, 21368.86, 21369.0, 21370.0, 283514.0, 21372.0, 21378.0, 21379.0, 21380.0, 21382.8, 21383.0, 21384.0, 21382.0, 21385.0, 21388.29, 21392.0, 21393.0, 21394.0, 21395.0, 21396.0, 21394.8, 21400.0, 21403.44, 21403.2, 21405.0, 21408.0, 21410.0, 21411.0, 21411.58, 21418.0, 21420.0, 21422.0, 21424.0, 21425.0, 1070000.0, 21428.0, 21429.0, 21430.8, 21430.0, 21432.0, 21435.0, 21437.0, 21438.84, 21440.0, 21441.0, 21442.0, 21444.0, 21445.92, 21446.0, 21448.0, 21449.36, 21450.0, 21451.0, 21452.16, 21454.0, 21455.0, 21456.0, 21460.0, 21462.0, 21464.0, 21465.0, 21466.0, 21465.6, 21468.0, 21469.32, 21470.0, 21471.0, 21473.0, 21474.0, 21476.0, 21476.04, 21477.0, 21480.0, 21488.0, 21490.0, 21492.0, 21495.0, 21496.0, 21500.0, 21501.23, 21501.0, 21504.0, 21509.0, 21513.0, 21514.0, 21515.0, 21516.0, 21521.88, 21524.0, 21527.28, 21528.0, 21529.0, 21530.0, 21527.0, 21531.0, 21536.0, 21538.0, 21538.8, 21540.0, 21541.0, 21542.0, 21544.0, 21546.0, 21548.0, 21550.0, 21552.0, 21555.0, 21559.0, 21560.0, 21562.0, 21563.0, 21564.0, 21562.8, 283711.0, 21568.0, 21569.0, 56390.52, 21571.0, 21572.43, 21576.72, 21576.0, 21580.0, 21582.0, 21585.0, 21586.0, 21585.74, 21588.0, 21588.13, 21590.0, 283731.0, 21587.0, 21593.0, 21594.0, 283739.0, 21598.9, 21598.0, 21600.0, 21602.0, 21605.0, 21609.55, 21610.0, 283755.0, 21612.0, 21614.4, 21615.0, 21619.0, 21620.0, 21621.0, 21622.0, 21624.0, 21626.96, 21628.0, 21629.0, 21632.0, 21636.0, 21638.4, 56404.0, 21640.0, 21646.0, 21648.0, 21650.0, 21651.0, 21651.12, 21652.0, 21658.0, 21659.0, 21660.0, 21661.2, 21658.8, 56408.21, 21664.0, 21667.0, 21669.0, 21670.0, 21672.0, 21676.0, 21678.0, 21679.0, 21680.0, 21681.0, 21682.0, 21684.0, 21685.0, 21687.0, 21689.0, 21696.0, 21700.0, 21704.0, 21706.0, 21708.0, 21709.0, 21710.28, 21711.0, 21712.0, 21710.0, 546000.0, 21712.75, 21714.0, 21718.0, 21719.0, 21720.0, 21723.0, 21726.0, 21727.44, 21730.0, 21732.0, 21734.0, 21736.0, 21742.0, 21744.0, 21745.0, 21746.0, 21750.0, 21752.0, 21753.6, 21755.0, 21756.0, 21758.0, 21759.0, 21760.0, 283905.0, 21762.0, 21764.52, 21766.8, 21767.0, 21768.0, 21769.22, 21770.0, 21772.0, 21772.8, 21773.0, 21776.0, 21778.0, 21779.0, 21780.0, 21781.0, 56432.04, 21784.0, 21791.0, 21792.0, 21798.0, 21800.0, 21804.0, 21805.0, 21805.8, 21808.0, 56438.0, 21811.2, 21811.0, 21812.0, 21814.0, 21815.0, 21816.0, 21819.0, 21821.0, 21823.92, 21825.0, 21827.88, 21828.0, 21830.4, 21833.74, 21834.72, 21835.0, 21836.0, 21837.0, 21838.0, 21833.0, 21840.0, 21838.8, 283986.0, 21848.0, 21850.0, 21852.0, 56447.78, 284000.0, 21856.0, 21858.0, 21860.0, 21864.0, 21865.0, 21868.0, 21869.0, 21871.0, 21872.0, 21872.28, 21874.45, 21871.2, 21876.0, 21877.85, 56451.0, 21880.0, 21881.0, 21884.0, 21885.0, 21888.0, 21888.34, 21889.0, 21893.26, 21893.0, 21896.5, 21898.8, 21899.0, 21900.0, 21898.0, 21902.0, 21903.0, 21905.0, 21909.0, 21910.0, 21912.0, 21914.0, 21916.0, 56459.0, 21918.0, 21920.0, 21924.0, 21925.0, 21926.0, 21927.0, 21930.0, 21931.0, 21932.0, 21934.9, 21934.8, 21936.0, 21935.0, 21938.0, 21940.0, 21942.2, 21943.88, 21945.0, 21948.0, 21950.0, 21951.51, 21951.0, 21954.0, 21956.0, 21958.0, 21960.0, 21961.03, 21961.0, 21963.0, 21964.0, 21965.0, 21970.0, 21971.88, 21972.41, 21972.0, 21974.0, 21975.0, 21976.0, 21977.0, 21978.8, 21980.0, 21981.0, 21983.0, 21984.0, 21986.0, 21986.8, 21988.0, 21990.0, 21991.0, 21992.0, 21992.04, 21994.8, 21994.0, 21996.0, 21996.12, 21999.0, 22000.0, 22002.0, 284147.0, 22004.4, 22004.0, 22008.0, 22009.9, 22009.0, 22017.6, 22018.0, 22020.0, 22023.0, 284168.0, 22025.0, 22026.36, 22027.32, 22030.0, 22032.0, 22032.33, 22036.2, 22037.0, 22038.0, 22038.32, 22040.0, 22036.0, 22043.0, 22044.0, 22046.0, 22048.0, 22050.0, 22051.0, 22056.0, 22057.0, 22058.09, 22060.8, 22060.0, 22062.0, 22063.8, 22066.8, 22066.0, 22068.0, 22070.0, 22072.0, 22074.0, 22076.0, 22080.0, 22082.0, 22084.0, 22087.0, 22088.0, 22090.0, 22092.0, 22092.36, 22096.0, 22097.0, 22098.0, 22097.16, 22100.0, 22102.0, 22102.8, 22104.0, 22106.0, 56497.0, 22110.0, 22114.89, 22114.8, 22116.0, 22114.0, 22118.0, 22119.49, 22119.84, 22120.0, 22122.0, 22124.16, 22125.0, 22126.0, 22128.0, 22130.0, 22131.0, 22132.0, 22135.0, 22140.0, 22144.0, 22146.0, 22147.56, 22148.0, 22149.0, 22150.0, 22149.07, 22152.0, 22153.44, 22154.45, 284300.0, 22157.0, 22156.0, 22160.0, 22164.0, 22165.0, 22167.0, 22169.76, 22175.0, 22176.0, 22178.61, 22179.0, 22187.0, 22188.0, 22189.0, 22190.4, 22191.0, 22192.0, 22195.0, 22198.64, 22199.0, 22200.0, 22201.0, 22200.48, 22203.0, 22206.0, 22208.0, 22210.0, 22212.0, 22214.0, 22215.0, 22220.0, 22221.0, 22222.0, 22224.0, 22225.0, 22226.0, 22228.0, 22230.0, 22231.0, 22232.0, 22233.0, 22234.0, 22235.0, 22236.0, 22234.8, 22240.0, 22244.0, 22246.8, 22246.49, 22248.0, 22246.0, 22250.0, 22252.0, 284396.0, 22254.0, 22256.0, 284400.0, 22259.0, 22260.0, 22265.0, 22266.84, 22267.0, 22271.0, 22272.0, 22272.38, 22274.0, 22275.0, 22276.0, 22272.88, 22278.0, 56531.06, 22280.0, 22281.0, 56530.0, 22283.0, 22284.0, 22285.0, 22288.16, 22289.0, 22291.0, 22292.0, 22294.0, 22296.0, 22297.0, 22298.0, 22298.76, 22300.0, 22303.0, 22306.8, 22307.0, 22308.0, 22309.5, 284455.0, 22317.0, 56539.0, 22320.0, 22322.0, 22325.0, 22329.0, 22330.0, 22331.0, 22332.0, 22333.8, 22329.24, 22334.83, 22336.0, 22337.0, 22338.0, 284481.0, 22340.0, 56543.52, 22342.0, 22343.0, 22344.0, 22345.21, 22342.8, 22347.29, 22348.0, 22349.04, 22350.0, 22351.78, 22346.0, 22353.0, 22354.8, 22354.0, 22356.0, 22357.0, 22358.0, 22359.0, 22360.0, 284500.0, 22362.75, 22361.0, 22364.0, 22365.38, 22366.0, 22367.0, 22368.0, 22370.0, 22371.0, 22373.0, 22374.0, 22375.0, 22376.0, 22377.0, 22380.0, 22380.8, 22382.0, 22383.0, 22387.0, 22388.0, 22390.92, 22390.0, 22392.0, 22392.84, 22394.76, 22395.0, 22398.0, 22399.0, 22400.0, 22401.0, 22402.0, 22404.0, 22406.0, 22409.52, 22410.0, 22409.5, 22409.0, 22414.0, 22416.0, 22418.06, 22418.0, 22420.0, 22421.05, 22422.4, 22425.0, 22426.8, 22425.6, 22428.0, 22432.0, 22435.24, 22436.0, 22437.6, 22437.0, 22440.0, 22443.2, 22444.92, 22444.0, 22448.0, 22449.0, 22450.0, 22451.0, 22452.0, 22450.8, 22454.0, 22453.0, 22460.0, 22462.0, 22464.0, 22467.0, 22467.6, 22468.41, 22471.68, 22472.0, 22474.0, 22476.0, 22480.0, 22483.0, 56572.0, 22487.0, 22488.0, 56573.0, 22492.0, 22497.44, 22498.0, 22500.0, 22501.0, 22502.0, 22505.6, 22507.0, 22508.0, 22509.0, 22510.8, 22511.0, 22512.0, 22517.48, 22519.79, 22520.0, 22521.0, 22522.0, 22521.6, 22524.0, 22526.0, 22528.0, 22536.0, 22538.79, 22538.0, 56583.58, 22541.0, 22544.0, 22546.0, 22547.0, 22548.0, 22548.48, 22550.0, 22552.0, 22555.0, 22555.25, 284700.0, 22556.04, 22558.0, 22560.0, 22561.63, 22562.0, 22563.0, 22564.0, 22561.0, 22565.0, 22568.0, 22569.9, 22571.0, 22572.0, 22574.0, 22575.0, 22579.0, 22580.0, 22582.2, 22584.0, 22585.0, 22587.29, 22589.0, 22592.84, 22593.0, 22594.0, 22595.0, 22596.0, 284740.0, 22598.0, 22599.0, 22600.0, 22601.52, 22603.0, 22604.0, 22605.0, 22606.0, 22607.0, 22606.8, 22608.0, 22611.0, 22612.0, 22614.0, 22615.0, 22617.23, 22618.0, 22618.8, 22620.0, 22618.73, 22622.0, 22626.0, 22627.0, 22629.0, 22630.0, 22632.0, 22633.69, 22634.0, 22636.8, 22638.0, 22639.28, 22640.0, 22641.6, 22643.54, 22644.0, 22643.0, 22644.7, 22645.0, 22648.3, 22648.0, 22650.0, 22651.0, 22652.0, 22653.0, 22654.0, 22654.8, 22656.0, 22657.0, 56606.0, 22661.16, 22662.0, 22664.0, 22665.0, 22668.0, 22668.36, 22670.0, 22671.0, 22672.0, 22672.32, 22674.0, 22676.0, 22677.0, 22678.0, 22680.0, 22682.0, 22684.0, 22686.0, 22688.06, 22689.8, 22689.36, 22688.24, 22692.0, 22693.0, 22694.0, 22700.0, 22704.0, 22705.0, 22706.0, 22708.0, 22710.0, 22711.0, 547000.0, 22713.6, 22714.0, 22713.0, 22716.0, 22714.84, 22718.0, 22720.0, 22722.2, 22724.54, 22724.0, 22726.0, 22726.8, 22728.0, 22730.4, 22732.0, 22740.0, 22742.0, 22744.0, 22746.0, 22750.0, 22750.8, 22752.0, 22753.0, 22754.0, 22755.0, 22756.0, 22751.0, 22759.92, 22761.28, 22762.0, 22763.0, 22764.0, 22765.0, 22768.0, 22771.0, 22772.25, 22773.0, 22774.0, 22776.0, 22777.8, 22782.0, 22783.0, 22784.0, 22788.0, 22790.3, 22790.0, 22796.79, 22800.0, 22804.0, 22807.0, 22809.0, 22810.0, 56637.48, 22812.0, 22812.72, 22814.0, 22815.0, 22817.6, 22817.0, 22819.0, 22820.0, 22822.92, 22822.8, 22824.0, 22828.0, 22835.52, 22836.0, 22836.36, 22838.0, 22839.0, 22840.0, 22837.0, 22842.0, 22844.4, 22844.0, 22846.44, 22847.0, 22848.0, 22848.38, 22846.0, 22846.6, 22852.8, 22849.0, 22850.0, 22855.0, 285000.0, 22860.0, 22861.8, 22864.0, 22870.0, 22871.0, 22872.0, 22875.0, 22876.0, 22878.0, 22879.08, 22880.0, 22882.0, 22883.0, 22884.0, 22887.0, 22888.0, 22890.0, 22892.0, 22894.0, 22896.96, 22896.0, 22897.68, 22899.0, 22900.0, 22897.0, 22907.0, 22908.0, 22910.0, 22911.0, 22914.0, 22915.0, 22917.0, 22918.0, 22920.0, 22921.0, 22922.0, 22926.0, 22927.0, 22928.0, 285075.0, 22932.0, 22932.72, 22931.2, 22934.0, 22937.16, 22938.0, 22940.0, 22942.68, 22942.0, 22944.0, 22950.0, 22951.0, 22952.0, 22956.0, 22958.0, 22958.8, 22960.32, 22960.0, 22964.0, 22965.76, 22965.0, 22967.0, 22968.0, 22974.0, 22977.0, 22980.0, 22981.0, 22984.0, 22986.0, 22989.0, 22990.0, 22991.5, 22992.0, 22994.04, 22996.0, 22998.0, 285143.39, 23000.0, 23004.0, 23005.0, 23007.0, 23012.0, 23016.0, 23020.8, 23023.0, 23024.0, 23025.6, 23028.0, 23028.36, 23030.0, 23033.0, 23038.0, 23039.0, 23040.0, 23041.0, 23043.0, 23046.38, 23050.0, 23051.0, 23052.0, 23054.0, 23056.0, 23056.47, 23058.0, 23059.2, 23060.0, 23062.48, 23063.57, 23064.0, 23063.0, 23062.8, 23068.0, 23070.0, 23072.0, 23075.0, 23076.0, 23077.0, 23078.88, 23078.0, 23080.0, 23082.0, 23085.0, 23086.0, 23087.0, 23088.0, 23088.21, 23090.0, 23088.6, 23094.6, 23096.0, 23098.0, 23100.0, 23103.0, 23108.0, 23111.0, 23112.0, 23113.0, 23113.8, 23117.0, 23118.0, 23120.0, 23121.0, 23123.88, 23124.0, 23126.88, 23126.0, 23128.0, 23129.0, 23130.0, 56700.8, 23133.08, 23133.0, 23134.0, 23136.0, 23140.0, 23141.52, 23144.0, 23146.8, 23148.0, 23150.0, 23150.62, 23153.0, 23155.0, 23156.0, 23157.0, 23158.0, 23160.0, 23164.0, 23165.0, 23166.0, 23165.96, 23168.0, 23169.0, 23172.0, 23173.97, 23174.0, 23175.0, 56709.0, 23178.0, 23179.5, 23180.0, 23184.0, 23189.0, 23192.0, 23193.6, 23194.0, 23195.0, 23196.0, 23197.0, 23197.33, 23199.0, 23200.0, 23198.0, 23201.0, 56714.29, 23204.0, 23205.0, 23208.0, 23209.0, 23210.0, 23211.0, 23212.0, 23213.0, 23216.8, 23217.0, 23217.12, 23219.0, 23220.0, 23221.0, 23223.0, 23224.17, 23225.0, 23227.0, 23230.8, 23232.0, 23233.0, 23236.0, 23238.0, 23239.0, 23240.0, 23241.0, 23242.0, 23243.0, 23244.0, 23245.08, 23242.8, 23249.0, 23250.0, 23254.0, 23255.0, 23256.0, 23258.0, 23259.0, 23260.0, 23263.0, 23265.0, 23266.0, 23267.0, 23268.0, 23270.0, 23272.0, 23275.2, 23275.0, 23278.06, 285424.0, 23280.0, 23283.52, 23284.0, 23285.0, 23285.64, 23283.0, 23288.0, 23289.0, 23290.0, 23290.8, 23292.0, 23293.0, 23295.17, 23296.0, 23295.0, 23298.36, 23300.0, 23301.0, 23302.0, 23304.0, 23305.0, 23305.44, 23307.0, 23308.8, 23306.52, 23314.0, 23314.68, 23316.0, 23317.0, 23320.8, 23320.0, 23323.0, 23323.92, 23325.0, 23325.82, 23327.0, 23328.0, 23329.0, 23330.0, 23333.0, 23334.0, 23335.0, 23336.0, 23337.0, 23338.0, 23340.0, 23342.52, 23348.0, 23349.56, 23350.56, 23351.0, 23352.0, 23350.0, 23353.0, 23355.0, 285500.0, 23356.32, 23358.0, 23359.0, 23360.0, 23361.0, 23362.8, 23361.48, 23364.0, 23364.18, 23366.0, 23358.4, 23368.0, 23369.0, 23370.0, 23374.8, 23374.0, 23376.0, 56749.92, 23378.0, 23380.0, 23382.0, 23385.6, 23386.0, 23387.0, 23388.0, 23390.0, 23392.0, 23395.0, 23396.63, 23398.8, 23399.0, 23400.0, 23402.0, 23403.0, 23404.1, 23406.0, 23407.5, 23408.0, 23410.0, 23412.0, 23413.0, 23414.0, 56757.01, 23417.0, 23420.0, 23421.0, 23422.0, 23424.0, 23426.0, 23427.0, 23430.0, 23434.66, 23435.0, 23436.0, 23439.0, 23440.34, 23440.0, 23442.0, 23444.0, 23445.0, 23446.8, 23448.0, 23449.8, 23450.0, 23449.0, 23452.0, 56765.0, 23456.0, 285600.0, 23459.0, 23460.0, 23462.0, 23463.0, 23465.0, 23468.0, 23469.0, 23470.8, 23471.4, 23472.0, 23473.0, 23474.89, 23475.36, 23475.96, 56769.0, 23480.0, 23483.0, 23484.0, 23488.0, 23494.8, 23496.0, 23499.0, 23500.0, 23501.0, 23502.0, 23503.0, 23504.0, 285646.0, 23508.0, 23509.0, 23511.0, 23513.14, 23514.96, 23513.0, 23516.0, 23520.0, 23521.0, 23520.72, 23523.72, 23523.0, 23527.0, 23528.0, 23529.7, 23530.0, 23531.0, 23532.0, 23533.2, 23533.0, 23533.66, 23536.32, 23533.12, 23536.0, 23539.0, 23540.0, 23541.0, 23543.0, 23544.0, 23545.0, 23546.0, 23547.25, 23550.0, 23551.68, 23555.64, 23556.0, 23557.0, 23558.4, 23559.24, 23560.0, 56786.0, 23562.0, 23562.77, 23558.0, 23565.0, 23566.0, 23567.76, 23568.0, 23566.4, 23570.25, 810000.0, 23573.0, 23575.0, 23576.75, 23576.0, 23578.0, 23579.0, 23580.0, 23581.0, 23579.3, 23579.6, 23584.0, 23585.0, 23577.76, 23587.68, 23587.0, 23589.0, 23590.0, 23591.0, 23592.0, 23593.0, 23594.0, 23591.28, 23597.0, 23598.0, 23599.0, 23600.0, 23604.0, 23606.0, 23607.6, 23608.0, 23612.0, 23614.0, 23616.0, 23617.0, 23623.0, 23624.95, 23626.8, 23626.0, 23628.0, 23630.0, 23631.0, 23632.0, 23633.0, 23635.0, 23640.0, 23642.0, 23645.0, 23647.0, 23649.0, 23650.0, 23651.0, 23652.0, 23654.52, 23654.0, 23656.0, 23655.0, 23658.05, 23659.0, 23660.0, 23658.0, 23662.01, 23663.0, 23664.0, 23666.0, 23666.52, 23670.0, 23671.0, 23674.8, 23675.0, 23676.0, 23678.0, 23679.12, 23680.0, 23682.0, 23684.0, 23685.0, 23685.85, 23687.0, 23688.0, 23692.0, 23694.0, 23699.0, 23700.0, 23701.0, 285850.0, 23707.0, 23706.0, 23710.0, 23712.0, 23713.0, 23716.32, 23717.53, 23716.0, 23721.0, 23722.8, 23724.0, 23726.0, 23729.0, 23731.0, 23734.8, 23735.0, 23736.0, 23734.0, 23738.0, 23740.0, 23743.44, 23745.0, 23746.8, 23746.28, 23748.0, 23749.0, 23750.0, 23747.0, 23753.4, 23754.27, 23760.0, 23763.0, 23764.0, 23763.24, 23766.0, 23768.0, 23770.0, 23771.0, 23772.0, 23772.87, 23774.0, 23773.0, 23776.95, 23775.0, 23780.0, 23780.83, 23782.0, 23782.2, 23784.0, 23785.0, 23787.0, 23788.0, 23789.0, 23788.8, 23792.0, 23793.6, 23794.56, 23795.0, 23796.0, 23795.2, 23797.0, 23799.64, 23800.0, 23794.8, 23795.52, 23805.0, 23807.0, 23808.0, 23809.0, 23816.0, 23818.0, 23820.6, 23820.0, 23823.0, 23824.0, 23826.0, 23830.0, 23831.88, 23832.0, 23834.0, 23836.8, 23837.0, 23839.0, 23840.0, 285984.0, 23842.0, 23843.0, 23844.0, 23845.08, 23846.4, 23848.0, 23849.0, 23850.0, 23851.0, 286000.0, 23856.0, 23857.0, 23860.0, 23861.0, 23864.0, 23865.0, 23866.0, 23868.0, 23869.6, 23870.0, 23872.0, 23873.0, 23874.0, 23875.0, 23876.0, 23874.96, 23878.0, 23878.4, 23880.0, 23881.0, 23882.0, 23884.0, 23885.0, 23889.0, 23890.0, 23892.0, 23893.0, 23895.0, 23896.0, 23898.0, 23899.0, 23900.0, 23902.0, 23903.0, 23904.0, 23907.0, 23908.92, 23910.0, 23911.0, 23914.0, 23914.8, 23916.0, 23917.0, 23915.0, 23919.41, 23920.0, 23923.0, 23923.2, 23928.0, 23929.0, 23930.0, 23934.0, 23936.0, 23937.6, 23938.08, 23939.0, 23940.0, 23942.0, 23947.62, 23948.0, 23950.0, 23952.0, 23954.0, 23955.0, 23960.04, 23960.0, 23962.0, 23963.0, 23964.0, 23965.0, 23970.0, 23971.0, 23972.0, 23974.8, 23976.0, 23980.8, 23982.0, 23985.0, 23986.8, 23988.0, 23991.6, 23993.52, 23994.0, 23994.72, 23998.8, 23999.0, 24000.0, 24001.14, 23998.0, 24002.0, 24004.0, 24006.0, 24008.0, 24010.0, 24012.0, 24015.0, 24016.56, 24016.0, 24019.2, 24020.0, 24024.0, 24025.0, 24024.89, 24027.0, 24028.0, 24029.0, 24033.0, 24034.0, 24036.0, 24040.0, 24044.0, 24045.0, 24048.0, 24050.0, 24051.0, 24052.0, 24053.0, 24053.88, 24055.0, 24056.0, 24060.0, 24061.0, 24065.0, 24066.0, 24070.0, 24071.0, 24072.0, 24075.0, 24076.0, 24077.0, 24078.0, 24080.0, 24082.0, 24083.0, 24084.0, 24086.0, 24088.0, 24089.0, 24089.85, 24091.68, 24090.0, 24096.0, 24097.0, 24100.0, 24108.0, 24111.0, 24115.0, 24116.0, 286260.0, 24120.0, 24121.0, 24122.0, 24123.0, 24124.0, 24125.0, 24128.0, 24130.0, 24131.64, 24132.0, 24132.11, 24131.0, 24134.0, 24136.0, 24140.0, 24141.0, 24142.8, 24144.0, 24146.0, 24148.2, 24149.0, 24150.0, 24148.0, 24151.0, 24154.0, 24155.0, 24156.0, 24158.8, 24160.0, 24163.0, 24165.84, 24166.8, 24166.0, 24168.0, 24172.0, 24175.0, 24176.0, 24178.0, 24180.0, 24183.0, 24184.0, 24186.0, 24188.0, 24189.96, 24189.0, 24190.0, 24192.0, 24195.6, 24198.0, 24199.8, 24200.0, 24201.45, 24202.0, 286347.0, 24204.0, 24206.0, 24208.8, 24210.0, 24211.42, 24212.0, 24216.0, 24217.0, 24219.0, 24220.0, 24221.0, 24222.0, 24223.6, 24224.0, 24226.8, 24227.0, 24228.0, 24230.0, 24230.4, 24232.0, 24234.0, 24235.0, 24236.0, 24237.0, 24240.0, 24242.88, 24243.6, 24244.0, 24244.95, 24247.04, 24249.6, 24250.0, 24251.0, 24252.8, 24253.0, 24254.0, 24252.0, 286400.0, 24256.0, 24260.0, 24261.68, 24264.0, 24265.0, 24273.0, 24275.0, 24276.0, 24277.57, 24278.5, 24279.0, 24280.64, 24281.0, 24280.0, 24282.0, 24284.0, 24284.78, 24287.0, 24288.0, 24290.0, 24290.4, 24292.0, 24293.0, 24294.0, 24295.0, 24296.0, 24297.0, 24298.0, 24290.5, 24300.0, 286440.0, 24305.0, 24306.0, 24310.0, 24312.0, 24314.0, 24316.0, 24317.0, 24317.28, 24320.0, 24322.8, 24323.0, 24324.0, 24322.0, 24326.0, 24326.8, 24327.0, 24326.4, 24330.0, 24334.0, 24336.0, 24337.0, 24340.0, 24340.16, 24342.0, 24343.0, 24341.0, 24345.0, 24346.0, 24348.0, 24349.0, 24350.0, 24351.0, 24353.0, 286500.0, 24357.0, 24360.0, 24363.0, 24364.0, 24365.6, 24370.0, 24372.0, 24374.0, 24375.0, 24376.3, 24377.49, 24377.0, 24379.0, 24380.0, 24376.0, 24384.0, 24387.34, 24389.0, 24390.78, 24391.0, 24392.0, 24390.96, 24394.8, 24395.38, 24396.0, 24395.0, 24398.0, 24400.0, 24402.0, 24404.0, 24406.0, 24407.0, 24408.0, 24409.0, 24410.64, 24408.48, 24412.0, 24414.0, 24419.0, 24420.0, 24422.4, 24423.0, 24424.0, 24422.0, 24429.6, 24431.0, 24432.0, 24433.0, 24433.94, 24436.0, 24439.0, 24440.0, 24441.0, 24442.22, 24443.0, 24444.6, 24444.0, 24446.38, 24446.0, 24448.0, 24449.0, 24450.0, 24449.16, 24450.72, 24448.56, 24455.0, 24456.0, 24456.79, 24459.0, 24460.0, 24463.0, 24466.68, 24468.0, 24468.45, 24470.6, 24471.0, 24472.0, 24471.52, 24474.0, 24475.0, 24475.56, 24478.0, 24480.0, 24481.0, 24482.0, 24483.0, 24484.0, 24482.4, 24487.0, 24488.0, 24490.0, 24492.0, 24493.0, 24495.0, 24497.5, 24498.0, 24499.0, 24500.0, 24501.0, 24502.8, 24502.0, 24504.0, 24506.0, 24507.6, 24507.89, 24509.0, 24510.0, 24511.0, 24513.0, 24514.0, 24515.0, 24516.0, 24518.0, 24522.0, 24523.8, 286668.0, 24525.0, 24524.0, 24523.0, 24528.0, 24530.57, 24530.0, 24532.0, 24538.0, 24540.0, 24541.0, 24543.84, 24544.0, 24545.0, 24548.0, 24550.0, 24552.0, 24553.08, 24555.0, 24556.0, 24557.0, 24556.59, 24555.6, 24560.0, 24556.8, 24563.0, 24564.0, 24567.0, 811000.0, 24569.0, 24570.0, 24571.0, 24572.68, 24573.6, 24573.0, 24575.0, 24576.0, 24577.0, 24578.0, 24576.68, 24580.0, 24578.5, 24582.0, 24583.0, 24578.4, 24585.0, 24586.0, 24587.86, 24588.0, 24589.0, 24592.0, 24593.0, 24595.0, 24595.2, 24597.0, 24599.0, 24600.0, 24599.28, 24602.0, 24603.0, 24604.0, 24605.0, 24606.0, 24608.0, 24609.0, 24611.0, 24612.0, 24615.96, 24615.0, 24617.0, 24618.0, 24619.0, 24620.0, 24624.0, 24625.0, 24627.2, 24627.0, 24631.0, 24632.49, 24636.0, 24638.0, 24639.0, 24640.0, 24641.28, 24642.71, 24642.0, 24643.0, 24645.0, 24648.0, 24650.0, 24651.0, 24653.0, 24655.42, 286800.0, 24660.0, 24662.0, 24663.0, 286806.0, 24665.0, 24666.0, 24669.0, 24672.0, 24674.0, 24674.54, 24678.0, 24679.0, 24680.0, 24681.0, 24682.0, 24684.0, 24686.0, 24687.0, 24688.0, 24690.0, 24691.2, 24691.27, 24692.0, 24694.0, 24696.72, 24696.0, 24698.0, 24700.0, 24701.0, 24702.0, 24704.45, 24705.69, 24708.0, 24709.08, 24710.0, 286854.0, 24715.0, 24716.16, 24716.0, 24720.0, 24721.0, 24724.0, 24725.0, 24727.0, 24728.0, 24729.6, 24730.0, 24731.14, 24732.0, 24733.0, 24731.0, 24735.0, 24736.0, 24737.0, 24739.0, 24740.0, 24744.0, 24745.0, 24747.0, 24748.0, 24749.0, 24750.0, 24747.64, 24752.0, 24753.0, 24754.8, 24755.0, 24756.0, 24757.0, 24758.0, 24759.0, 24760.0, 24761.0, 286900.0, 24766.0, 24768.0, 24771.0, 24772.8, 24773.0, 24775.4, 24776.0, 24778.0, 24779.0, 24780.0, 24785.0, 24786.0, 24788.0, 24790.0, 286935.0, 24790.92, 24792.0, 24793.6, 24793.08, 24795.72, 24796.0, 24794.0, 24797.0, 24799.2, 24800.0, 24798.49, 24803.0, 24804.0, 24807.0, 24808.0, 24809.0, 24812.0, 24814.8, 24815.0, 24816.0, 24817.32, 57036.96, 24820.0, 24822.0, 24824.0, 24825.0, 24826.0, 24828.0, 24830.0, 24831.0, 24832.0, 24835.0, 24836.0, 24837.76, 24838.0, 24840.0, 24841.0, 24843.0, 24844.0, 24845.96, 24845.16, 24849.0, 24850.0, 24850.8, 24852.0, 24852.88, 24854.76, 24855.0, 287000.0, 24856.0, 24859.0, 24860.0, 24861.0, 24864.0, 24865.0, 24870.0, 24875.0, 24876.0, 24877.35, 24878.76, 24880.0, 24881.0, 24883.2, 24888.0, 24891.48, 24892.0, 24891.42, 24897.72, 24897.0, 24900.0, 24902.75, 24903.0, 24904.0, 24906.0, 24907.0, 24908.0, 24910.0, 24911.42, 24912.0, 24913.0, 24911.0, 24915.2, 24915.0, 24917.0, 24918.0, 24916.0, 24920.0, 24922.0, 24923.88, 24924.0, 24928.0, 24930.0, 24933.36, 24933.98, 24934.8, 24936.0, 24937.0, 24934.75, 24939.0, 24940.0, 24935.0, 24943.0, 24944.0, 287089.0, 24947.3, 24948.0, 24950.0, 24954.12, 24956.0, 24959.0, 24960.0, 24961.0, 24965.0, 24968.0, 24969.0, 24970.0, 24972.0, 287116.0, 24972.36, 24975.0, 24976.0, 24976.4, 24979.0, 24980.0, 24981.0, 24982.0, 24984.0, 24985.0, 24988.0, 24989.0, 24990.0, 24992.0, 24994.0, 24995.0, 24996.0, 24997.0, 57071.62, 24999.0, 25000.0, 25001.0, 25002.0, 25004.0, 25005.0, 25008.0, 25009.0, 287154.0, 25012.0, 25015.0, 25015.8, 25017.0, 25018.0, 25019.0, 25020.0, 25021.0, 25022.0, 25023.0, 25024.0, 25025.0, 25024.44, 25030.0, 25031.0, 25032.0, 25037.0, 25038.0, 25039.0, 25040.0, 25039.22, 25042.0, 25043.28, 25044.0, 25041.0, 25050.0, 25052.4, 25056.0, 25058.4, 25060.0, 25061.0, 25062.0, 25062.24, 25064.73, 25064.0, 25066.0, 25067.0, 25068.0, 25072.0, 25073.0, 25075.0, 25076.49, 25078.0, 25080.0, 25081.0, 25082.0, 25084.0, 25087.0, 25089.0, 25092.0, 25094.0, 25098.0, 25099.0, 25100.0, 25101.0, 25102.08, 25103.33, 25104.0, 25102.0, 25108.0, 25110.0, 25111.0, 25112.0, 25113.0, 25116.0, 25117.0, 25118.0, 25120.0, 25120.8, 25122.84, 25123.0, 25125.0, 25126.0, 25126.4, 25128.0, 25130.0, 25131.05, 25132.0, 25133.0, 25135.0, 25136.0, 25137.48, 25138.0, 25139.0, 25140.0, 25141.0, 25137.0, 25143.04, 25144.0, 57100.78, 25146.0, 25147.2, 25147.0, 25148.0, 25150.0, 25151.0, 25152.0, 25154.0, 25155.0, 25156.0, 25154.5, 25160.0, 25164.0, 25164.48, 25167.0, 25168.0, 25168.51, 25170.0, 25171.0, 25171.2, 25173.55, 25174.0, 25175.0, 25176.0, 25178.0, 25179.0, 25180.0, 25182.0, 25183.34, 25184.0, 25185.0, 25184.88, 25188.0, 25189.0, 25190.0, 25190.4, 25192.0, 25193.0, 25194.0, 25196.0, 25199.0, 25200.0, 25201.08, 25201.0, 25203.0, 25200.75, 25206.96, 25206.0, 25207.2, 25209.6, 25210.0, 25209.0, 25212.0, 25213.0, 25211.0, 25210.8, 57115.2, 25217.0, 25220.0, 287364.0, 25222.0, 25224.0, 25228.0, 25230.4, 25230.0, 25232.0, 25231.0, 25234.0, 25235.0, 25236.0, 25236.72, 25238.0, 25237.0, 25240.0, 25239.0, 25242.0, 25243.56, 25241.32, 57119.0, 25246.0, 25240.79, 25248.0, 25247.0, 25250.0, 25251.0, 25252.0, 25252.19, 25254.0, 25255.0, 25256.0, 25258.0, 25260.0, 25264.0, 25265.28, 25268.0, 25269.0, 25270.0, 25272.0, 25275.0, 25277.0, 25279.0, 25280.0, 25279.67, 25284.0, 25285.0, 25288.0, 25290.0, 25291.0, 25292.0, 25294.0, 25296.0, 25300.0, 25301.56, 25303.0, 25304.0, 25305.28, 25305.0, 25306.0, 25308.0, 25310.0, 25313.6, 25314.0, 25318.0, 25320.0, 25323.0, 25324.8, 25324.03, 25328.0, 25329.6, 25332.0, 25333.0, 25334.0, 25334.4, 57139.2, 25339.0, 25340.0, 25343.0, 25344.0, 287488.0, 25350.0, 25352.0, 25355.0, 25356.0, 287500.0, 25358.0, 25360.0, 25361.0, 25363.0, 25364.0, 25365.0, 25363.68, 25367.0, 25368.0, 25370.0, 25371.0, 25373.0, 25374.0, 25375.0, 25376.0, 25376.98, 25380.0, 25381.44, 25384.0, 25388.0, 25391.4, 25392.0, 25396.8, 25396.0, 25400.0, 25402.0, 25403.0, 25404.0, 25405.0, 25407.0, 25408.0, 25410.0, 25411.44, 25414.0, 25415.0, 25416.0, 25417.0, 25417.6, 25415.26, 25420.0, 25422.0, 25423.0, 25425.0, 25426.56, 25428.0, 25430.0, 25432.0, 25434.0, 25434.12, 25437.76, 25438.0, 25439.0, 25440.0, 25441.0, 25442.04, 25440.67, 25442.0, 25445.0, 25448.0, 25450.0, 25451.0, 25452.0, 25453.0, 25454.0, 25457.64, 25459.2, 25460.0, 25459.0, 25462.0, 25464.0, 25465.0, 25466.0, 25467.84, 25468.92, 57165.0, 57165.6, 25472.0, 25473.84, 25475.0, 25475.32, 25476.0, 25478.0, 25477.0, 25480.0, 25480.44, 549769.0, 25478.4, 25484.0, 25485.92, 25485.0, 25487.0, 25486.0, 25487.77, 25488.0, 25490.0, 25492.0, 25493.0, 25494.0, 25496.0, 25497.92, 25498.0, 25497.0, 25500.0, 25500.8, 57172.2, 25505.0, 25506.96, 25509.0, 25510.48, 25510.0, 25512.0, 25513.0, 25514.0, 25509.92, 25516.0, 25519.32, 25520.0, 25523.0, 25524.0, 25527.0, 25527.72, 25530.0, 25533.5, 25534.48, 287678.0, 25536.0, 25539.0, 25540.0, 25542.0, 25544.0, 25545.0, 25546.0, 25547.41, 25548.0, 25547.0, 25550.0, 25552.0, 25554.39, 25555.0, 25554.36, 25555.2, 25558.88, 25559.0, 25560.0, 25558.0, 25562.0, 25563.84, 25563.2, 25565.0, 25567.44, 25568.0, 25568.32, 57185.0, 25572.0, 25573.0, 57186.0, 25576.32, 25577.86, 25576.8, 25577.76, 25580.0, 25581.0, 25584.0, 25585.0, 25587.0, 25588.0, 25589.28, 25590.0, 25591.0, 25593.0, 25595.0, 25596.0, 25596.62, 25597.2, 25600.0, 25601.0, 25602.0, 25603.0, 25604.0, 25605.0, 25606.0, 25608.0, 25612.29, 25613.0, 25615.0, 25615.2, 25616.08, 25618.0, 25619.0, 25620.0, 25621.0, 25621.63, 25623.0, 287765.0, 25625.0, 25626.0, 25628.0, 25630.0, 25631.0, 25632.0, 25633.45, 25634.0, 25633.0, 25635.0, 25637.0, 287777.0, 25640.0, 25642.0, 25644.0, 25645.0, 25645.44, 25647.96, 25648.0, 25647.0, 25650.0, 25651.0, 25652.04, 25653.0, 25652.0, 25654.0, 25656.0, 25655.0, 25660.0, 25662.0, 25663.0, 25664.0, 25665.0, 25666.0, 25667.88, 25668.0, 25669.0, 25670.0, 25671.0, 25672.92, 25672.0, 25674.0, 25675.0, 25676.0, 25677.0, 25678.0, 25679.0, 25680.0, 25681.0, 25682.0, 57207.36, 25685.0, 25686.0, 25688.0, 25690.0, 25691.15, 25692.0, 25693.0, 25694.0, 25695.6, 25691.0, 25698.0, 25699.0, 25700.0, 25703.0, 25707.0, 25708.0, 25709.0, 25710.0, 25711.0, 550000.0, 25713.0, 25714.0, 25715.0, 25716.0, 25716.72, 550006.0, 25719.0, 25720.0, 25721.0, 25722.0, 25724.0, 25725.0, 25726.08, 25728.0, 25730.0, 25732.0, 25735.0, 25738.8, 25740.0, 25741.0, 25742.0, 25743.0, 25744.0, 25747.0, 25748.0, 25749.0, 25750.0, 25751.0, 25752.0, 25753.0, 25754.46, 25754.67, 25756.0, 287900.0, 25758.0, 25759.8, 25760.0, 25762.0, 25763.76, 25764.0, 25765.0, 25766.0, 25767.0, 25768.0, 25762.64, 25770.0, 25772.0, 25774.0, 25775.0, 25776.0, 25777.0, 25778.0, 25774.8, 25780.0, 25781.0, 25782.0, 25781.6, 25784.0, 25785.0, 25786.2, 25785.06, 25788.0, 25789.0, 25788.36, 25791.12, 25792.0, 25791.0, 25793.0, 25788.35, 25796.0, 25798.0, 25800.0, 25801.42, 25803.0, 25804.0, 25804.8, 25805.0, 25810.92, 25811.0, 25812.0, 25813.22, 25814.0, 25810.0, 25816.0, 25818.0, 25819.0, 25820.0, 287965.0, 25821.0, 25824.0, 25825.0, 25826.0, 25827.28, 25831.78, 25832.12, 25833.0, 25832.0, 25835.0, 25836.0, 25837.2, 25838.0, 25834.0, 25840.0, 287985.0, 25841.36, 25840.08, 25844.0, 25845.36, 25845.0, 25847.0, 25848.0, 25850.0, 25853.0, 25855.0, 288000.0, 25857.0, 25855.2, 25860.0, 25862.0, 25863.0, 25865.0, 25866.0, 25868.0, 25870.0, 25872.0, 25874.0, 25875.0, 25875.2, 25877.0, 25880.0, 25881.0, 25882.92, 25883.0, 25884.0, 25882.0, 25886.0, 25887.0, 25888.0, 25889.0, 25890.36, 25888.52, 25892.0, 25890.0, 25895.0, 25896.0, 25897.0, 25899.0, 25900.0, 25905.89, 25906.0, 25905.0, 25908.0, 25909.0, 25915.0, 57254.64, 25917.0, 25917.87, 25920.0, 25922.0, 25922.97, 25925.0, 25926.0, 25930.0, 25932.0, 25932.91, 25934.0, 25935.0, 25935.36, 25937.0, 25939.0, 25940.0, 25944.0, 25945.5, 25946.0, 25947.0, 25948.21, 25949.0, 25950.0, 25950.12, 25952.0, 25947.48, 25954.8, 25956.0, 25957.0, 25958.0, 25959.0, 25960.0, 25961.0, 25962.0, 25963.0, 25965.0, 25966.0, 25968.0, 25974.0, 25976.04, 25977.0, 25978.0, 25979.0, 25980.0, 25979.2, 25982.0, 25983.0, 25984.0, 25985.0, 25986.0, 25984.89, 25988.0, 25990.0, 25991.0, 25992.0, 25994.0, 25995.0, 25996.0, 25999.0, 26000.0, 26001.0, 26002.44, 26002.0, 26004.0, 25999.2, 26006.0, 26009.0, 26010.0, 26011.0, 26012.0, 26013.0, 26016.0, 26019.77, 288164.0, 26020.0, 26022.0, 26024.0, 26026.0, 26026.13, 26028.0, 26029.0, 26030.0, 26035.08, 26035.2, 26036.0, 26038.44, 26039.0, 26040.0, 26041.0, 26042.0, 26043.0, 26038.0, 26046.97, 26047.0, 26050.0, 26051.0, 26052.0, 26052.36, 26054.0, 26055.48, 26053.0, 26056.0, 26058.0, 26054.81, 26060.0, 26062.0, 26064.0, 26065.0, 26066.0, 26067.0, 26067.6, 26067.72, 26071.44, 26073.0, 26074.0, 26075.0, 26076.0, 26074.8, 26078.0, 26078.48, 26080.0, 26081.0, 26082.0, 26083.0, 26085.36, 26086.0, 26087.0, 26088.0, 26089.0, 26090.0, 26092.0, 26095.0, 26095.99, 26098.0, 26100.0, 26101.0, 26102.0, 26104.0, 26106.0, 26107.0, 26108.0, 26109.0, 26112.0, 26120.0, 26120.52, 26122.8, 26123.95, 26124.0, 26124.8, 26127.0, 26128.0, 26132.0, 26133.48, 26135.04, 26136.0, 26138.0, 26143.68, 26143.93, 26145.0, 26147.67, 26148.0, 26147.0, 26150.0, 26151.96, 26152.0, 26152.7, 26158.8, 26159.0, 26160.0, 26158.0, 26161.0, 26163.0, 26166.0, 26167.0, 26168.64, 26169.0, 26167.6, 26170.0, 26172.0, 26173.0, 26171.0, 26173.73, 26176.0, 26177.0, 26178.0, 26179.14, 26180.0, 26182.0, 26183.0, 26184.0, 26185.0, 26186.4, 26188.8, 26189.0, 26191.0, 26192.0, 26196.0, 26198.0, 26200.0, 26201.0, 26202.0, 26203.0, 26204.0, 26205.0, 26206.0, 26208.0, 26213.0, 26214.0, 26215.0, 26216.72, 26217.0, 26218.0, 26220.0, 26222.0, 26223.0, 26222.88, 26224.0, 26225.0, 26222.28, 26227.0, 26227.2, 26229.0, 26226.0, 26231.11, 26232.0, 26234.0, 26228.0, 26236.0, 26237.0, 26238.0, 26239.0, 26240.0, 26237.88, 26244.0, 26245.0, 26246.0, 26246.4, 26248.0, 26250.0, 26251.0, 26254.0, 26254.68, 288400.0, 26256.0, 26258.0, 26259.0, 26260.4, 26260.0, 26261.0, 26263.35, 26263.0, 26265.6, 26266.81, 26266.0, 26268.0, 550555.0, 26270.0, 26271.0, 26274.0, 26276.28, 26278.0, 26280.0, 26281.0, 26283.0, 26284.8, 26285.0, 26286.0, 26284.0, 26288.0, 26290.0, 26291.0, 26292.0, 26293.0, 26297.0, 26299.0, 26300.0, 26302.0, 26303.86, 26304.0, 26306.0, 26307.12, 26307.0, 26312.0, 26313.0, 26314.0, 26316.0, 26318.36, 26320.0, 26320.68, 26324.0, 26325.0, 26326.0, 26328.0, 26329.0, 26330.0, 26331.0, 26332.0, 26333.0, 26334.0, 26335.0, 288478.0, 26338.0, 26340.0, 26342.4, 26342.0, 26345.0, 26348.0, 26349.44, 26348.4, 26350.0, 26351.0, 26352.0, 26353.0, 26354.0, 26355.0, 26351.95, 288500.0, 26356.0, 26363.0, 26364.0, 26363.04, 26366.0, 26367.71, 26368.0, 26369.0, 26371.22, 26371.0, 26371.93, 26372.99, 26374.0, 26374.41, 26375.0, 26376.0, 26374.8, 26379.0, 26380.0, 26382.4, 26383.92, 26387.0, 26387.16, 26388.0, 26390.0, 26389.0, 26390.72, 26391.0, 26393.0, 26395.0, 26396.0, 26400.0, 26402.4, 26406.0, 26407.0, 26408.0, 26409.0, 26410.0, 26412.0, 26416.0, 26418.0, 26420.0, 26425.0, 26426.28, 26426.0, 26428.18, 26429.0, 26434.0, 26435.0, 26436.0, 26438.0, 26438.52, 26439.0, 26440.0, 26439.12, 26441.0, 26445.0, 26445.63, 26447.9, 26448.0, 26450.0, 26453.0, 26455.0, 550744.0, 26457.6, 26456.0, 26458.0, 26459.0, 26460.0, 26462.0, 26461.0, 26464.0, 26465.0, 26466.44, 26466.0, 26468.0, 26469.38, 26470.0, 26472.0, 26473.0, 26476.0, 26478.0, 26479.52, 26480.0, 26478.24, 26481.0, 26484.0, 26485.0, 26488.56, 26488.8, 26490.0, 26491.63, 26493.0, 26494.0, 26495.0, 26496.0, 26499.0, 26500.0, 26499.2, 26502.0, 26504.0, 26505.36, 26505.0, 26507.0, 26508.0, 26509.0, 26510.0, 26515.0, 26516.0, 26517.0, 26518.0, 26520.0, 26522.0, 26524.0, 26525.0, 26526.0, 26529.0, 26532.0, 26533.0, 26532.48, 26535.0, 26536.0, 26537.0, 26538.55, 26538.0, 26540.0, 26534.0, 26542.0, 26543.0, 26544.0, 26545.0, 26546.0, 26550.0, 26551.0, 26552.0, 26553.6, 26554.0, 26555.0, 26556.0, 26555.4, 26553.0, 26559.71, 26560.0, 26561.77, 26562.0, 26568.0, 26569.0, 26572.0, 26574.0, 26576.0, 26578.6, 26578.0, 26580.0, 26581.38, 26583.07, 26583.24, 26585.0, 26586.0, 26590.0, 26590.92, 26592.0, 26592.36, 26594.0, 26595.06, 26596.0, 26595.12, 26598.0, 26591.0, 26600.0, 26600.95, 26602.0, 26603.0, 26604.0, 26601.0, 26605.0, 288750.0, 26608.0, 26604.5, 26610.0, 26612.0, 26616.0, 26617.96, 26618.0, 26618.29, 26620.0, 26621.0, 26623.0, 26624.0, 26625.0, 26627.74, 26628.0, 26630.0, 26631.0, 26632.0, 26633.0, 26633.25, 26635.0, 26636.0, 26638.0, 26640.0, 26640.96, 26642.8, 26643.0, 26644.0, 26645.0, 26646.0, 26647.39, 26648.0, 26649.8, 26650.0, 26651.0, 26652.0, 26653.0, 26652.39, 26655.0, 26656.0, 26657.0, 26658.01, 26657.28, 26660.0, 26661.0, 26662.0, 26664.0, 26665.0, 26666.0, 26667.0, 26668.8, 26668.0, 26670.0, 26673.0, 26674.0, 26675.0, 26676.0, 26677.5, 26678.8, 26679.38, 26678.0, 26680.0, 26677.84, 26684.0, 26685.0, 26686.0, 26687.0, 26688.0, 26690.0, 26692.0, 26693.0, 26694.0, 26695.56, 26695.0, 26698.0, 26699.0, 26700.0, 26704.0, 26705.0, 26707.0, 26708.0, 551000.0, 26713.0, 26712.0, 26714.0, 26714.43, 26713.68, 26718.0, 26719.0, 26720.0, 26721.0, 26720.16, 26724.0, 26726.0, 26727.84, 26727.0, 26729.0, 26730.0, 26728.0, 26732.16, 26729.2, 26734.0, 26732.0, 26736.0, 26739.55, 26740.0, 26741.0, 26743.2, 26744.0, 26745.6, 26745.0, 26743.0, 26748.0, 26749.0, 26750.0, 26753.0, 26754.0, 26755.0, 26757.0, 26758.0, 26759.28, 26760.0, 26761.0, 26761.32, 26763.0, 26766.0, 26769.6, 26770.0, 26771.94, 26772.0, 26773.22, 288916.91, 26775.0, 26769.0, 26779.92, 26780.0, 26781.0, 26782.0, 26783.0, 26784.0, 26785.0, 26788.0, 26790.0, 26795.6, 26796.0, 26797.0, 26800.0, 26804.0, 26804.11, 26806.0, 26807.0, 26808.0, 26808.97, 26811.0, 26812.0, 26813.0, 26811.72, 26815.0, 26816.0, 26818.0, 26819.0, 26820.0, 26818.8, 26822.0, 26822.4, 26822.5, 26825.0, 26826.0, 26831.0, 26832.0, 26833.0, 26835.0, 26835.84, 26838.0, 26839.0, 26840.0, 26841.0, 26842.0, 26843.0, 26844.0, 26845.59, 26845.0, 26847.0, 26848.0, 26850.0, 26851.0, 26852.0, 26853.0, 26854.0, 26856.0, 289000.0, 26859.0, 26860.0, 26859.72, 26862.0, 26863.0, 26864.4, 26864.0, 26866.0, 26867.0, 26868.0, 26869.0, 26871.0, 26872.0, 26873.47, 26874.0, 26871.94, 26876.0, 26877.0, 26878.0, 26878.06, 26874.64, 26878.65, 26879.16, 26880.0, 26881.0, 26882.0, 26883.0, 26882.26, 26886.0, 26886.48, 26888.0, 26884.0, 26891.0, 26892.0, 26895.0, 26896.36, 26895.67, 26898.0, 26899.0, 26900.0, 26901.0, 26900.75, 26899.2, 26904.0, 26905.0, 26903.2, 26902.0, 26909.0, 26910.0, 26912.53, 26912.0, 26915.0, 26916.0, 26919.12, 26920.0, 26919.0, 26922.0, 26924.0, 26926.0, 26927.0, 26928.0, 26930.0, 26930.64, 26934.12, 26936.0, 26938.0, 26939.0, 26940.0, 26943.0, 26944.0, 26948.86, 26949.37, 26950.0, 26952.0, 26954.88, 26956.0, 26958.0, 26958.86, 26960.0, 26962.0, 26964.0, 26966.0, 26967.0, 26971.36, 26973.0, 26976.0, 26978.4, 26980.0, 26983.0, 26984.0, 26986.0, 26988.0, 26989.22, 26990.0, 26991.0, 289139.0, 26996.17, 26995.0, 26998.0, 27000.0, 27003.0, 27007.0, 27008.8, 27009.0, 27012.0, 27013.26, 27014.0, 27016.0, 27018.0, 27019.0, 27020.0, 27024.0, 27024.36, 27026.0, 27027.96, 27028.87, 27029.0, 27030.0, 27025.16, 27031.0, 27033.48, 27033.0, 27028.0, 27036.0, 27036.36, 27035.0, 27040.0, 27042.0, 27043.0, 27045.0, 27046.0, 27048.0, 27049.0, 27050.0, 27052.0, 27054.0, 27058.8, 27060.0, 27060.8, 27062.0, 27063.0, 27064.0, 27065.0, 27066.0, 27067.0, 27069.0, 27070.92, 27071.0, 27072.0, 27073.0, 27080.0, 27080.4, 27081.0, 27083.0, 27081.6, 27085.0, 27086.0, 27087.5, 27091.0, 27091.8, 27093.0, 27096.0, 27097.0, 27099.0, 27100.0, 27102.0, 27103.0, 27106.0, 27108.0, 27109.0, 27110.0, 27111.0, 27112.0, 27117.0, 27118.0, 27119.0, 27120.0, 27121.0, 27122.0, 27123.2, 27124.0, 27123.0, 27124.8, 27126.0, 27125.0, 27128.0, 27130.0, 27131.52, 27132.0, 27133.0, 27135.28, 27135.0, 1600000.0, 1600001.0, 27136.0, 27139.0, 27140.0, 27141.0, 27142.0, 27144.0, 27145.0, 27138.0, 27148.8, 27149.13, 27150.0, 27151.0, 27148.0, 27153.0, 27154.0, 27156.0, 27157.0, 27158.0, 27160.0, 27161.0, 27163.0, 27164.0, 27165.0, 27166.0, 27167.0, 27168.0, 27168.12, 27170.0, 27172.0, 27172.56, 27174.0, 27175.0, 27176.0, 27176.4, 27173.4, 27179.0, 27180.0, 27182.0, 27183.0, 27184.44, 27184.0, 27185.0, 27187.2, 27186.0, 27189.0, 27185.6, 27190.0, 27192.0, 27193.0, 27190.4, 27193.41, 27195.0, 27191.0, 27198.0, 27194.0, 27200.0, 27200.28, 27202.0, 27204.0, 27205.12, 27208.0, 27209.0, 27210.0, 27215.0, 27216.0, 27217.0, 27218.0, 27219.68, 27221.0, 27224.64, 27225.0, 27225.6, 27226.0, 27227.0, 27228.0, 27230.0, 27224.93, 27232.0, 27233.0, 27234.6, 27237.48, 27237.0, 27239.0, 27240.0, 27242.0, 27244.0, 27245.0, 27246.0, 27244.8, 27248.0, 27249.0, 27250.0, 27252.0, 27254.0, 27256.0, 27259.44, 27260.0, 27261.12, 27263.0, 27264.0, 27265.0, 27268.0, 27270.0, 27273.12, 27273.0, 27275.0, 27276.0, 27274.0, 27278.0, 27275.45, 27281.28, 27281.81, 27283.0, 27284.0, 27285.12, 27288.0, 27289.6, 27291.0, 27292.0, 27295.0, 27297.0, 27298.0, 27300.0, 27302.16, 27302.0, 27304.0, 27302.4, 27308.0, 27310.0, 27311.0, 27312.0, 27310.8, 27314.0, 27316.0, 27317.0, 27318.0, 27319.0, 27319.39, 27320.0, 27321.0, 27322.0, 27321.6, 27324.0, 27325.0, 27326.0, 27325.8, 27329.0, 27330.0, 27331.0, 27332.0, 27333.0, 27334.56, 27327.12, 27336.0, 27337.0, 27337.14, 27339.96, 289483.0, 27340.0, 27341.0, 27342.0, 27337.3, 27343.08, 27345.16, 27346.0, 27347.5, 27348.0, 27344.0, 27350.0, 27352.0, 551638.0, 27354.0, 27353.0, 27356.0, 289500.0, 27359.0, 27360.0, 27362.0, 27364.0, 27366.0, 27366.98, 27368.0, 27370.0, 27370.56, 27372.0, 27373.0, 27372.8, 27374.0, 27376.0, 27377.0, 27378.84, 27379.2, 27380.0, 27381.0, 27382.0, 27383.0, 27384.0, 27385.0, 27388.92, 27388.0, 27388.44, 27390.0, 27391.0, 27393.36, 27392.0, 27395.0, 27396.0, 27397.0, 27398.0, 27393.0, 27400.0, 27401.0, 27402.0, 27401.93, 27404.0, 27405.26, 27398.4, 27407.0, 27408.0, 27410.0, 27411.91, 27411.0, 27414.0, 27415.0, 27417.0, 27419.24, 27420.69, 27420.0, 27424.0, 27425.0, 27427.0, 27429.0, 27429.5, 27430.0, 27432.0, 27433.2, 27433.72, 27435.0, 27434.0, 27437.0, 27433.0, 27439.0, 27440.0, 27442.0, 27442.02, 27444.0, 27443.0, 27447.22, 27449.0, 27450.0, 27452.0, 27452.76, 27454.0, 289600.0, 27456.0, 27458.0, 27459.0, 27460.0, 27457.0, 27462.96, 27464.52, 27464.0, 27466.0, 57562.0, 27468.0, 27475.2, 27476.0, 27477.0, 27475.0, 27479.0, 27480.0, 27478.0, 27483.06, 27484.0, 27486.0, 27488.4, 27489.94, 27490.0, 27491.0, 27492.0, 27493.32, 27490.08, 27495.0, 27500.0, 27501.0, 27504.0, 27505.15, 27506.0, 27511.0, 27513.0, 27515.04, 27517.0, 27518.14, 27518.0, 27520.8, 27521.0, 27522.0, 27523.0, 27520.0, 27526.0, 27528.0, 27529.4, 27530.0, 57575.75, 27532.0, 27533.95, 27533.0, 27535.0, 27536.16, 27536.0, 27538.0, 27539.0, 27540.0, 27542.0, 27543.0, 27544.0, 27545.0, 27546.0, 27547.0, 27548.0, 27544.03, 27550.0, 27552.0, 27552.31, 27555.0, 27556.0, 27557.0, 27558.0, 289700.0, 27560.0, 27560.52, 27562.23, 27561.0, 27564.0, 27565.0, 27566.0, 27567.0, 27568.0, 814000.0, 27570.0, 27572.0, 27573.36, 27575.0, 27576.0, 27577.0, 27579.0, 27580.0, 27580.8, 27582.0, 27583.0, 27584.76, 27584.0, 27586.0, 27587.0, 27588.0, 27589.0, 27590.76, 27591.72, 27589.44, 27588.13, 27585.0, 27592.0, 27598.0, 27599.0, 27600.0, 27601.0, 27602.0, 27604.0, 27607.0, 27608.0, 27609.0, 27610.0, 1338330.0, 27612.0, 27613.0, 27614.0, 27615.0, 27616.92, 27620.0, 57593.08, 27622.0, 27623.0, 27624.0, 27625.0, 27627.0, 27629.0, 27630.0, 27634.8, 27636.0, 27638.3, 27639.0, 27640.0, 57597.0, 27642.0, 27643.2, 27643.0, 27645.0, 27646.0, 27647.0, 27648.0, 27649.0, 27650.0, 27651.0, 27655.56, 27656.0, 27655.0, 27658.72, 27655.68, 27660.0, 27662.0, 27663.0, 27664.0, 27666.0, 27667.0, 27666.6, 27670.0, 27671.51, 27672.0, 27673.08, 27674.88, 27674.0, 27676.2, 27676.29, 27671.0, 27679.0, 27680.0, 27681.0, 27682.0, 27678.0, 27683.0, 27684.0, 27685.0, 27686.76, 27686.4, 27686.0, 27690.0, 27690.96, 27692.0, 27692.16, 27694.0, 27692.4, 27696.0, 27698.0, 27699.0, 27700.0, 27701.0, 27703.0, 27704.0, 27705.0, 27705.72, 27707.52, 27707.0, 27708.0, 27710.0, 27711.36, 552000.0, 27713.0, 27714.0, 27712.0, 27716.0, 27718.0, 27720.0, 27720.92, 27724.0, 27725.0, 27726.0, 27725.25, 27729.0, 27730.0, 27732.0, 27733.0, 27734.0, 27735.0, 27736.0, 27739.0, 27740.0, 27742.0, 27743.84, 27744.0, 27745.0, 27747.0, 27750.0, 27754.0, 27755.28, 27756.0, 289900.0, 27760.0, 27762.0, 27763.0, 27764.16, 27765.0, 27764.0, 27768.0, 27770.0, 27771.0, 27772.0, 27774.0, 27775.08, 27775.07, 27777.0, 27775.0, 27778.0, 27780.75, 27780.0, 27782.0, 27783.72, 27783.0, 27785.28, 27788.8, 27788.0, 27790.0, 27791.0, 27792.0, 27796.0, 27797.0, 27799.0, 27800.0, 27801.0, 27802.8, 27804.0, 27805.0, 27806.0, 27808.92, 27809.0, 27810.0, 27810.24, 27811.0, 27812.4, 27814.8, 27815.0, 27809.6, 27816.0, 27817.0, 27818.0, 27814.0, 27820.0, 27820.8, 27822.0, 27823.0, 27824.0, 27826.0, 27825.0, 27821.0, 27827.0, 27828.0, 27830.0, 27829.0, 27832.0, 27833.0, 27834.6, 289979.0, 27834.0, 27837.0, 27838.0, 27834.96, 27840.0, 27835.01, 27843.4, 27844.56, 27845.0, 27846.0, 27850.0, 27851.0, 27851.2, 27852.0, 27854.0, 289999.0, 290000.0, 27857.0, 27855.0, 27856.0, 27860.0, 27861.0, 27856.47, 27863.99, 27864.0, 27859.8, 27866.0, 290004.0, 27865.05, 27870.0, 27871.34, 27872.0, 27873.64, 27874.0, 27875.0, 27876.0, 27879.0, 27880.0, 27881.0, 27884.64, 27885.6, 27885.0, 27886.32, 27887.0, 27888.0, 27887.84, 27886.0, 27890.0, 27892.0, 27894.0, 27886.07, 27887.68, 27889.0, 27898.0, 27897.6, 27898.8, 27895.0, 27900.0, 27899.0, 27903.48, 27905.0, 27906.0, 27903.0, 27908.47, 27911.16, 27912.0, 27913.82, 27914.0, 27911.52, 27916.0, 27919.2, 27919.0, 27920.0, 27923.0, 27924.0, 27924.84, 27925.0, 27926.0, 27928.0, 27930.49, 27931.0, 27932.0, 27934.0, 27935.0, 27936.0, 27937.0, 27938.0, 27939.24, 27940.0, 27941.0, 27944.0, 27945.0, 27946.0, 27948.0, 27949.0, 27950.0, 27953.0, 27954.0, 27955.0, 27954.48, 27957.96, 27958.0, 290100.0, 27960.0, 27961.0, 27962.87, 27962.0, 27955.67, 27956.0, 27966.0, 27969.82, 27969.0, 27970.0, 27972.0, 27974.0, 27976.0, 27976.68, 27978.0, 27977.0, 27980.0, 27980.58, 27982.0, 27983.0, 27984.0, 27985.0, 27986.0, 27988.0, 27988.94, 27988.2, 27990.0, 27992.0, 27993.0, 27994.0, 27991.0, 27996.0, 27999.0, 28000.0, 28000.62, 28002.33, 28005.36, 28006.0, 28008.0, 28009.58, 28011.0, 28012.0, 28013.0, 28014.0, 28017.0, 28019.52, 28020.0, 28024.0, 28025.28, 28026.0, 28025.0, 28028.0, 28030.0, 28032.0, 28033.0, 28034.0, 28036.56, 28037.0, 28038.0, 28038.4, 28040.0, 28042.0, 28043.04, 28044.0, 28048.0, 28049.0, 28050.0, 28052.0, 28053.0, 28053.6, 28055.0, 28056.0, 28059.0, 28060.0, 28061.4, 28062.0, 28061.0, 28064.0, 28065.0, 28070.0, 28071.0, 28072.0, 28075.0, 28077.84, 28079.0, 28080.0, 28081.0, 28083.6, 28084.0, 28087.0, 28088.0, 28089.0, 28090.0, 28091.0, 28092.0, 28098.0, 28099.0, 28100.0, 28103.0, 28104.0, 28105.0, 28106.0, 28109.35, 28110.0, 28111.0, 28112.0, 28113.6, 28114.92, 28112.1, 28115.8, 28116.0, 290260.0, 28119.0, 28114.32, 28120.0, 28121.6, 28121.0, 28124.0, 28125.0, 28126.0, 28127.0, 28128.0, 28130.0, 28133.76, 28134.0, 28138.0, 28139.92, 28140.0, 28141.0, 28142.0, 28142.4, 28145.0, 28148.0, 28150.92, 28151.52, 28150.0, 28152.0, 28154.0, 28152.46, 28156.0, 28157.0, 28158.0, 28159.0, 28160.0, 28161.0, 28161.37, 28163.0, 28164.0, 28164.36, 28162.08, 28167.0, 28160.5, 28170.0, 28174.0, 28176.0, 28178.0, 28180.0, 28182.12, 28182.0, 28184.0, 28186.0, 28188.0, 28191.0, 28193.0, 28195.0, 28195.67, 28198.0, 28200.0, 28201.0, 28201.32, 28203.84, 28204.2, 28204.0, 28200.96, 28207.6, 28205.0, 28210.8, 28210.0, 28211.0, 28213.0, 28213.92, 28215.0, 28212.0, 28216.8, 28216.0, 28218.0, 28219.0, 28219.2, 28221.0, 28220.0, 28223.0, 28224.0, 28225.0, 28226.0, 28230.0, 28233.0, 28234.0, 28236.0, 28237.08, 28238.0, 28240.0, 28241.0, 28242.0, 28245.0, 28246.4, 28248.0, 28250.0, 28251.0, 28251.12, 28252.0, 28254.0, 28255.0, 28260.0, 28261.48, 28260.19, 28262.0, 28263.0, 28264.0, 28265.0, 28261.0, 28266.0, 28268.0, 28269.0, 28271.0, 28269.41, 28270.8, 28272.0, 28273.0, 28274.0, 28275.0, 28276.84, 28275.36, 28278.0, 28280.0, 28281.75, 28279.0, 28276.59, 28284.0, 28288.0, 28289.0, 28290.0, 28291.0, 28292.0, 28294.0, 28296.0, 28297.0, 28298.0, 28299.24, 28300.0, 28303.19, 28305.0, 28306.0, 28308.0, 28308.8, 28310.4, 28311.45, 28312.0, 28310.0, 28313.0, 28315.0, 28316.0, 28317.0, 28320.0, 28321.0, 28323.0, 28324.0, 28325.0, 28326.0, 28327.0, 28328.0, 28329.6, 28330.0, 28331.0, 28332.0, 28333.0, 28334.0, 28329.0, 28336.0, 28339.0, 28340.0, 28341.0, 28342.0, 28340.5, 28344.0, 28346.95, 28350.0, 28350.4, 28352.0, 28354.8, 28354.0, 28355.0, 28356.0, 28358.21, 28358.0, 28360.37, 28359.0, 28360.0, 28364.09, 28365.0, 28366.0, 28368.0, 28370.0, 28371.0, 28373.0, 28375.59, 28376.0, 28377.0, 4484824.0, 28379.05, 28380.0, 28378.0, 28379.0, 28383.84, 28384.8, 28385.0, 28384.0, 28387.08, 28389.0, 28390.0, 28392.0, 28394.0, 28394.16, 28394.5, 28396.0, 28399.0, 28400.0, 28401.0, 28402.86, 28404.0, 28405.0, 28408.0, 28409.0, 28412.0, 28413.0, 28414.0, 28415.0, 28416.0, 28416.84, 28418.0, 28420.0, 28422.0, 28425.0, 28427.49, 28427.0, 28428.0, 28430.4, 28429.0, 28431.0, 28433.0, 28430.0, 28432.0, 28434.0, 28436.0, 28437.0, 28438.0, 28439.0, 28440.0, 28441.0, 28442.0, 28444.0, 28445.0, 28446.0, 28448.0, 28449.0, 28450.0, 28451.0, 28452.48, 28452.0, 28453.0, 28454.0, 28456.0, 28455.0, 28452.99, 28454.4, 28457.0, 28459.0, 28454.2, 28461.0, 28462.0, 28456.2, 28464.0, 28468.5, 28469.8, 28470.0, 28473.6, 28473.0, 28475.2, 28476.0, 28475.0, 28474.8, 28477.0, 28479.12, 28475.21, 28480.0, 28481.0, 28484.28, 28484.0, 28485.0, 28487.0, 28488.0, 28491.48, 28492.0, 28493.0, 28494.96, 28494.0, 28496.0, 28497.0, 28500.0, 28504.0, 28505.0, 28506.0, 28508.0, 28509.0, 28511.0, 28512.0, 28513.0, 28514.0, 28515.0, 28517.0, 28518.0, 28520.0, 28521.0, 28522.0, 28523.0, 28524.0, 28525.0, 28526.0, 28527.0, 28528.0, 28530.0, 28531.0, 28532.0, 28533.0, 28536.0, 28537.0, 28538.0, 28540.0, 28541.0, 28543.0, 28544.0, 28545.36, 28548.0, 28549.0, 28550.0, 28549.44, 28552.0, 28553.0, 28554.0, 290700.0, 28558.0, 28560.0, 28564.0, 28565.0, 28566.0, 28567.2, 28568.0, 28569.6, 28570.0, 28571.84, 28572.0, 28573.0, 28574.0, 28575.0, 28576.0, 28578.71, 28579.2, 28579.0, 28580.0, 28581.0, 28581.12, 28583.0, 28584.0, 28585.0, 28587.0, 28587.02, 28589.4, 28588.0, 28591.0, 28592.0, 28590.0, 28595.0, 28596.0, 28598.0, 28599.0, 28600.0, 28601.0, 28598.4, 28603.0, 28604.67, 28605.24, 28606.0, 28605.0, 28608.0, 28609.0, 28610.0, 28604.0, 28613.0, 28614.0, 28616.0, 28618.0, 28620.0, 28621.0, 28622.0, 28624.0, 28624.86, 28627.2, 28628.0, 28628.83, 28631.0, 28632.0, 28632.96, 28636.8, 28636.0, 28638.0, 28639.0, 28640.0, 28641.0, 28643.0, 28644.0, 28645.0, 28646.0, 28649.0, 28650.0, 28651.0, 28652.0, 28654.0, 28655.0, 28656.0, 28657.0, 28659.0, 28660.0, 28660.12, 28662.0, 28661.36, 28664.0, 28665.0, 28666.0, 28667.0, 28668.0, 28671.0, 28672.8, 28675.92, 28675.17, 28677.0, 28679.0, 28680.0, 28681.0, 28683.0, 28684.0, 28685.0, 28683.2, 28686.0, 28687.0, 28687.4, 28690.0, 28688.0, 28692.0, 28693.0, 28694.0, 28696.0, 28697.0, 28698.0, 28700.0, 28700.1, 28702.0, 28701.36, 28704.0, 28703.28, 28706.09, 28709.0, 28710.72, 28711.0, 28712.0, 28713.12, 28713.0, 28714.0, 28716.0, 28717.71, 28710.0, 28719.0, 28720.0, 28721.4, 28721.0, 28722.0, 28723.2, 28724.0, 28723.0, 28726.0, 28725.84, 28728.0, 28730.0, 28725.0, 28732.92, 28733.0, 28732.0, 28727.0, 28736.0, 28739.0, 28740.0, 28741.0, 28742.0, 28743.0, 28744.0, 28745.0, 28747.0, 28748.0, 28749.0, 28750.0, 28751.0, 28752.0, 28755.0, 28756.0, 28757.0, 28758.0, 28760.0, 28761.0, 28761.6, 28763.0, 28764.0, 28765.0, 28766.4, 28766.0, 28768.0, 28764.84, 28770.0, 28771.6, 28767.0, 28773.0, 28774.0, 28776.0, 28777.0, 28779.33, 28780.0, 28782.0, 28783.0, 28785.0, 28786.0, 28787.2, 28788.0, 28787.0, 28790.0, 28791.0, 28790.28, 28793.2, 28794.0, 28792.0, 28791.28, 28795.0, 28798.0, 28799.0, 28800.0, 28801.0, 28793.44, 28807.0, 28808.0, 28810.0, 28812.0, 28816.0, 28817.28, 28818.0, 28819.2, 28820.0, 28823.0, 28824.0, 28825.0, 28826.0, 28824.64, 28828.0, 28829.0, 28828.8, 28831.0, 28832.0, 28830.0, 28834.0, 28835.0, 28836.0, 28838.0, 28838.4, 28840.0, 28842.0, 28846.0, 28847.0, 28847.52, 28849.0, 28850.0, 28848.0, 28852.0, 28854.0, 28855.0, 291000.0, 28857.0, 28859.0, 28860.0, 28862.78, 28863.0, 28862.0, 28865.0, 28864.0, 28867.32, 28863.49, 28869.36, 28868.0, 28869.0, 28870.0, 28871.39, 28873.0, 28872.0, 28875.0, 28876.0, 28877.0, 28879.2, 28874.0, 28880.0, 28882.0, 28883.0, 28884.0, 28885.4, 28885.0, 28887.13, 28887.0, 28889.0, 28886.0, 28882.8, 28888.0, 28890.0, 28891.0, 28892.0, 28893.75, 28894.0, 28895.0, 28896.0, 28891.2, 28900.0, 28897.16, 28904.0, 28905.6, 28905.0, 28905.5, 28908.0, 28907.0, 28906.0, 28911.0, 28912.0, 28913.0, 28912.28, 28915.0, 28917.84, 28918.6, 28920.0, 28921.0, 28924.0, 28925.0, 28926.2, 28926.0, 28928.0, 28929.0, 28930.0, 28931.16, 28932.0, 28933.0, 28938.0, 28939.0, 28940.0, 28940.68, 28942.0, 28944.0, 28945.0, 28945.42, 28946.0, 28948.92, 28948.0, 28950.0, 57857.88, 28952.0, 28953.0, 28955.0, 28956.0, 28957.0, 28958.0, 28959.0, 28960.0, 28961.0, 28963.0, 28965.0, 28966.0, 28967.4, 28968.0, 28969.28, 28967.0, 57860.25, 57861.86, 28974.0, 28975.0, 28974.68, 28974.4, 28976.0, 28979.0, 28980.0, 28981.39, 28982.0, 28981.0, 28983.0, 28978.08, 28988.4, 28989.0, 28990.0, 28990.08, 28991.0, 28992.0, 28994.0, 28993.0, 28995.0, 28995.2, 28996.0, 28999.0, 29000.0, 29001.0, 29002.8, 29002.0, 29004.0, 29005.0, 29010.0, 57869.0, 29012.88, 29013.21, 29012.0, 29015.0, 29016.0, 29017.0, 29018.0, 29014.0, 29020.0, 29015.62, 29022.0, 29025.0, 29028.0, 29028.48, 29030.4, 29031.0, 29030.0, 29033.0, 29033.88, 29036.0, 29037.0, 29036.8, 29039.0, 29040.0, 29041.0, 29044.0, 29047.0, 29047.68, 29049.0, 29050.0, 29051.88, 29051.0, 29053.36, 29054.4, 29052.0, 29055.0, 29054.0, 29058.41, 29057.0, 29058.0, 29056.87, 29060.25, 29060.0, 29061.0, 29062.0, 29064.0, 29064.84, 29065.0, 29067.0, 29068.0, 29068.08, 29070.0, 29066.18, 29072.0, 29073.36, 29074.0, 29069.0, 29078.0, 29080.0, 29081.0, 29082.0, 29083.0, 29084.0, 29085.0, 29087.0, 29088.0, 29089.0, 29090.88, 29090.0, 29096.0, 29099.0, 29100.0, 29099.16, 29102.88, 29103.5, 29104.0, 29103.0, 29100.6, 29102.76, 29108.0, 29110.0, 29112.0, 29113.0, 29117.0, 29119.0, 29120.0, 29121.0, 29123.97, 29124.0, 29125.0, 29126.0, 29128.0, 29129.0, 29130.0, 29132.04, 29132.0, 29133.0, 29135.0, 29136.0, 29137.0, 29138.0, 29139.12, 29140.0, 29140.8, 29141.06, 29134.23, 29142.0, 29145.0, 29145.69, 29148.0, 29149.0, 29150.0, 29151.33, 29152.0, 29152.7, 29154.0, 29151.0, 29158.0, 29158.08, 29160.0, 29161.0, 29162.0, 29159.0, 29159.88, 29165.0, 29161.6, 29162.92, 29168.69, 29169.0, 29162.42, 29171.0, 29172.0, 29164.0, 29174.0, 29176.0, 29177.0, 29180.0, 29181.0, 29182.4, 29182.0, 29184.0, 29185.0, 29187.84, 29189.0, 29190.0, 29191.2, 29192.0, 29193.0, 29196.0, 29197.0, 29198.0, 29199.96, 29200.0, 29202.0, 29202.04, 29203.0, 29204.0, 29206.0, 29205.0, 29208.0, 29210.0, 29211.08, 29212.0, 29212.99, 29213.0, 29215.0, 29216.0, 29214.0, 29218.8, 29218.0, 29220.0, 29221.92, 29219.0, 29222.0, 29224.0, 29225.06, 29227.0, 29228.0, 29229.0, 29230.0, 29231.92, 29232.0, 29230.68, 29231.0, 29236.0, 29238.0, 29240.0, 29241.0, 29242.0, 29243.0, 29244.0, 29245.0, 29247.0, 29247.4, 29248.0, 29250.0, 29252.0, 29256.0, 29259.0, 29260.0, 29261.0, 29264.0, 29265.0, 29265.6, 29267.0, 29268.0, 29270.0, 29272.0, 29274.0, 29275.0, 29276.0, 29277.0, 29278.0, 29280.0, 29281.79, 1340000.0, 29282.0, 29284.0, 29285.0, 29286.0, 29287.8, 29286.4, 29289.0, 29290.0, 29292.0, 29294.0, 29297.0, 29299.0, 29300.0, 29301.0, 29303.34, 29304.0, 29305.0, 29306.0, 29307.2, 29307.0, 29309.0, 29310.0, 29311.0, 29312.0, 29313.0, 29314.0, 29315.0, 29316.0, 29320.0, 29321.0, 29322.0, 29323.0, 29322.78, 29324.73, 29325.93, 29326.0, 29327.27, 29329.6, 29328.0, 29327.0, 29332.0, 29330.6, 29325.0, 29329.0, 29330.0, 29335.0, 29338.0, 29332.2, 29340.0, 29340.8, 29341.0, 29341.32, 29344.0, 29339.28, 29347.0, 29348.0, 29350.0, 29350.92, 29352.81, 29352.0, 29353.0, 57937.0, 29356.0, 29359.0, 29360.0, 29361.0, 29363.0, 29364.0, 29366.0, 29367.0, 29369.0, 29369.54, 29369.6, 29372.0, 29373.0, 29374.0, 29375.0, 29376.0, 29377.0, 29376.1, 29379.0, 29380.0, 29382.0, 29383.24, 29384.77, 29385.84, 29385.0, 29384.28, 29387.0, 29388.0, 29384.0, 29390.4, 29391.0, 29390.0, 29393.2, 29394.36, 29395.0, 29394.0, 29398.0, 29399.0, 29400.0, 29401.0, 29396.0, 29406.0, 29408.0, 29409.0, 29411.2, 29411.0, 29412.0, 29413.0, 29415.96, 29415.0, 29416.0, 29418.0, 29419.2, 29417.0, 29422.0, 29423.52, 29424.0, 29423.0, 29425.0, 29426.0, 29428.0, 29429.0, 29431.0, 29432.0, 29433.0, 29434.05, 29435.0, 29436.0, 29439.0, 29440.0, 29442.79, 29442.0, 29444.0, 29446.0, 29447.0, 29448.0, 29449.0, 29450.0, 29452.0, 29453.0, 29454.0, 29452.8, 29456.0, 29458.0, 29460.0, 29461.0, 29462.4, 29463.06, 29464.0, 29465.0, 29463.0, 29468.0, 29470.0, 29472.0, 29473.0, 29475.0, 29476.0, 29478.0, 29480.0, 29481.0, 29482.08, 29483.0, 29484.0, 29486.2, 29487.0, 29490.0, 29491.28, 29492.0, 29491.2, 29494.0, 29495.0, 29496.0, 29491.0, 29498.0, 29499.0, 29500.0, 29497.0, 29504.0, 29508.0, 29510.0, 29512.0, 29513.0, 29514.0, 29515.0, 29516.0, 29516.88, 29520.0, 29522.76, 29524.32, 29524.0, 29525.0, 29527.0, 29529.0, 29530.0, 29531.0, 29532.0, 29534.0, 29536.0, 29537.0, 29538.0, 29539.0, 29540.0, 29541.0, 29539.8, 57975.0, 29544.0, 29545.0, 29543.0, 29548.0, 29549.0, 29550.0, 29552.0, 29552.76, 29553.0, 29554.0, 29555.0, 29556.0, 29556.8, 29558.26, 29560.0, 29559.0, 29563.0, 29567.0, 29568.0, 29569.08, 29569.28, 29570.0, 816000.0, 29572.43, 29574.0, 29575.0, 29576.0, 29577.0, 29577.6, 29578.0, 29580.0, 29584.0, 29585.0, 29586.0, 29587.37, 29588.0, 29587.2, 29587.0, 29592.0, 29593.0, 29595.0, 29596.0, 29595.26, 29598.0, 291742.0, 29600.0, 29599.0, 29602.0, 29604.0, 29606.4, 29606.0, 29608.0, 29610.0, 29611.0, 29614.0, 29616.0, 29619.0, 29620.0, 29620.86, 29622.4, 29623.0, 29624.0, 29625.0, 29627.0, 29628.0, 29629.0, 29630.0, 29632.0, 29634.48, 29635.0, 29636.28, 29638.0, 29639.0, 29640.0, 29641.0, 29643.0, 29644.0, 29645.0, 29646.0, 29649.0, 29650.0, 29652.0, 29653.0, 29654.0, 29654.58, 29656.0, 29658.0, 29660.0, 29661.0, 29662.0, 29663.0, 29664.0, 29660.8, 29666.0, 29665.0, 29670.0, 29673.6, 29675.0, 29676.0, 29679.0, 29680.0, 29681.0, 29682.0, 29683.0, 29687.3, 29688.0, 29689.0, 29687.0, 29691.0, 29692.0, 29693.0, 29694.0, 29695.0, 29692.06, 29697.0, 29699.0, 29700.0, 29701.0, 29702.4, 29702.0, 29703.0, 29705.0, 29701.92, 29710.0, 29712.0, 29712.51, 29714.88, 29715.0, 29716.67, 29716.0, 29717.28, 29717.0, 29719.0, 29720.0, 29721.36, 29721.0, 29723.2, 29724.0, 29725.0, 29725.08, 29727.0, 29730.38, 29731.0, 29732.0, 29733.0, 29730.0, 29735.0, 29736.0, 29737.0, 29740.0, 29742.0, 29743.0, 29744.0, 29744.96, 29747.0, 29748.0, 29749.0, 29750.0, 29750.44, 29756.0, 29757.0, 29759.0, 29760.0, 29761.0, 29762.0, 29764.8, 29765.0, 29766.84, 29766.0, 29764.0, 29767.0, 29769.0, 29770.0, 29772.96, 29770.8, 29772.0, 29771.0, 29774.0, 29775.0, 29778.36, 29779.0, 29780.0, 29773.0, 29782.8, 29783.0, 29784.0, 29785.0, 29786.0, 29789.65, 29790.0, 29789.0, 29795.0, 29796.0, 29796.58, 29798.0, 29798.86, 29800.8, 29799.0, 29800.0, 29803.0, 29801.0, 29797.0, 29804.0, 29805.0, 29806.0, 29807.0, 29808.0, 29810.0, 29809.8, 29812.08, 29812.0, 29814.0, 29816.0, 29817.0, 29818.0, 29820.0, 29821.0, 29822.0, 29823.0, 29825.0, 29826.0, 29827.0, 29828.0, 29831.0, 29832.0, 29836.0, 29837.0, 29838.0, 29839.24, 29840.0, 29842.0, 29843.0, 29843.73, 29844.0, 29845.0, 29846.0, 29846.5, 29848.0, 29849.0, 29850.0, 29851.0, 291996.0, 29852.0, 29855.0, 292000.0, 29856.0, 29858.16, 29859.0, 29857.0, 29861.0, 29858.0, 29863.0, 29860.0, 29864.0, 29866.0, 29865.0, 29868.2, 29864.11, 29867.0, 29868.0, 29869.0, 29870.0, 29867.28, 29872.0, 29873.0, 29874.0, 29875.0, 29876.0, 29875.2, 29878.0, 29877.0, 29880.0, 29881.32, 29881.0, 29879.0, 29885.76, 29886.0, 29885.0, 29888.0, 29889.0, 29890.0, 29891.0, 29892.0, 29893.0, 29894.0, 29895.0, 29896.0, 29896.16, 29899.0, 29900.0, 29898.0, 29904.0, 29909.76, 29910.0, 29914.0, 29916.0, 29919.0, 29920.8, 29921.0, 29920.0, 29922.0, 29922.5, 29924.0, 29925.0, 292070.0, 29926.0, 29929.0, 29930.0, 29931.0, 29931.12, 29933.0, 29932.0, 29935.0, 29936.0, 29928.0, 29940.0, 29942.0, 29943.0, 29944.0, 29946.48, 29946.0, 29947.0, 29948.0, 29950.0, 29952.0, 29955.0, 29960.0, 29962.0, 29964.0, 29965.0, 29965.32, 29967.0, 29968.0, 29970.0, 29972.0, 29973.0, 29974.0, 29975.0, 29976.0, 29972.8, 29978.0, 29979.4, 29980.0, 29981.0, 29982.0, 29983.0, 29984.0, 29985.0, 29986.0, 29986.2, 29988.0, 29989.0, 29990.0, 29991.0, 29992.0, 29993.28, 29994.24, 29993.6, 29991.8, 29993.0, 29994.0, 29995.0, 29996.0, 29998.0, 29999.0, 30000.0, 30001.0, 30002.0, 30003.0, 30004.0, 30005.0, 30007.0, 30008.0, 30009.6, 30009.0, 30012.0, 30013.25, 30014.0, 30016.2, 30014.28, 30016.32, 30017.0, 30019.0, 30018.0, 30020.0, 30015.0, 30024.0, 30025.0, 3437892.0, 30028.0, 30028.8, 30030.0, 30031.0, 30032.0, 30035.0, 30036.0, 30037.0, 30040.0, 30042.0, 30043.0, 30044.0, 30046.0, 30048.0, 30049.0, 30050.0, 30052.0, 30056.0, 30057.12, 30057.0, 30059.64, 30060.0, 292200.0, 30058.0, 30066.0, 30067.0, 30068.0, 30067.2, 30071.0, 30072.0, 30073.0, 30074.0, 30076.0, 30077.0, 30079.0, 30080.0, 30081.0, 30082.88, 30082.0, 30084.0, 30085.0, 30086.0, 30087.0, 30088.0, 30089.0, 30085.72, 30096.0, 30097.0, 30098.0, 30099.0, 30100.0, 30101.0, 30102.0, 30097.6, 30104.0, 30105.0, 30106.0, 30108.0, 30109.81, 30110.0, 30111.0, 30111.76, 30109.0, 30108.6, 30114.12, 30115.0, 30116.0, 30117.0, 30118.0, 30119.0, 30120.0, 30118.4, 30123.0, 30124.0, 30125.0, 30126.0, 30128.0, 30129.0, 30130.0, 30131.0, 30132.0, 30134.0, 30138.0, 30139.0, 30140.0, 30141.0, 30142.0, 30143.0, 30144.0, 30145.0, 30140.16, 30139.2, 30148.34, 30150.0, 30151.0, 30153.0, 30155.0, 30156.6, 30156.0, 30158.0, 30159.96, 30159.0, 30160.0, 30161.0, 30162.84, 30163.0, 30165.0, 30166.0, 30166.8, 30168.0, 30162.0, 30164.0, 30172.0, 30175.0, 292320.0, 30177.0, 30178.92, 30178.0, 30180.8, 30179.0, 30182.4, 30180.0, 30181.0, 30185.0, 30182.0, 30183.0, 30188.0, 30184.0, 30183.24, 30186.0, 30187.0, 30188.4, 30189.0, 30190.0, 30192.0, 30192.22, 30191.0, 30194.0, 30197.07, 30200.0, 30201.0, 30202.0, 30203.0, 30202.8, 30205.5, 30204.0, 30207.0, 30208.0, 30209.0, 30210.0, 30204.7, 30211.2, 30213.0, 30216.0, 30216.49, 30218.0, 30219.0, 30220.0, 30222.0, 30223.0, 30224.4, 30225.0, 30226.0, 30226.8, 30227.0, 30228.0, 30229.08, 30224.0, 30232.0, 30233.0, 30231.71, 30235.08, 30235.0, 30235.44, 30230.0, 30237.52, 30240.0, 30239.0, 30237.24, 30243.0, 58114.0, 30246.0, 30247.0, 30248.0, 30250.0, 30251.0, 30252.0, 30253.0, 30254.4, 30254.0, 30255.13, 30256.0, 30254.76, 30255.0, 30256.23, 30253.8, 292400.0, 30259.0, 30260.0, 30263.0, 30264.0, 30265.0, 30266.0, 30268.0, 30262.8, 30270.0, 30267.0, 30273.0, 30272.0, 30275.0, 30276.12, 30276.0, 30278.0, 30277.0, 30280.0, 30281.0, 30282.0, 30284.8, 30285.0, 30286.0, 30284.0, 30288.0, 30286.68, 30290.0, 30291.0, 30288.24, 30292.0, 30285.37, 30291.54, 30296.0, 30299.13, 30300.0, 30299.0, 30302.0, 30303.0, 30305.0, 30306.0, 30305.6, 30307.0, 30309.96, 30310.0, 30312.0, 30313.68, 30313.0, 30316.0, 30319.0, 30320.0, 30321.0, 30320.88, 30323.0, 30324.0, 30325.2, 30326.0, 30325.0, 30328.8, 30326.4, 30327.76, 30331.21, 30330.0, 30333.0, 30330.84, 30332.0, 30336.0, 30334.0, 30338.0, 30339.0, 30340.0, 30341.0, 30342.0, 292478.11, 30344.9, 30345.0, 30337.0, 30347.0, 30348.5, 30348.6, 30349.05, 30350.0, 30352.0, 30348.0, 30355.0, 30355.2, 30356.59, 30356.0, 30358.0, 292500.0, 30360.0, 30357.0, 30363.0, 30366.0, 30367.0, 30368.0, 30369.19, 30370.0, 30371.0, 30372.0, 30374.0, 30377.0, 30378.0, 30379.0, 30380.0, 30383.0, 30384.0, 30385.0, 30387.0, 30388.8, 30388.0, 30390.0, 30393.0, 30394.8, 30394.0, 30396.0, 30398.0, 30399.64, 30400.0, 30399.0, 30402.0, 30404.0, 30405.0, 30407.0, 30408.0, 30409.0, 30410.38, 30410.0, 30412.8, 30410.64, 30409.6, 30413.0, 30414.0, 30415.0, 30412.0, 30418.0, 30411.97, 30420.0, 30420.14, 30422.0, 30422.5, 30425.0, 30426.0, 30423.0, 30429.0, 30430.0, 30432.0, 30434.0, 30435.0, 30440.0, 30441.0, 30442.8, 30444.0, 30444.72, 30446.0, 30447.0, 30449.0, 30450.0, 30451.0, 30452.0, 30455.04, 30456.0, 30457.0, 30458.0, 292600.0, 30459.0, 30461.6, 30462.0, 30461.0, 30463.0, 30466.4, 30467.0, 30468.0, 30466.0, 30470.0, 30471.0, 30472.0, 30472.56, 30474.0, 30475.79, 30475.0, 30476.0, 30478.0, 30480.0, 30483.0, 30485.0, 30487.0, 30488.0, 30489.0, 30489.6, 30491.0, 30492.0, 30492.96, 30495.0, 30496.0, 30498.63, 30498.87, 30500.0, 30499.0, 30504.0, 30505.0, 30506.56, 30508.8, 30509.49, 30510.0, 30511.0, 30512.04, 30512.0, 30512.52, 30513.0, 30516.0, 30517.0, 292660.0, 30514.0, 30521.0, 30524.0, 30526.08, 30526.0, 30528.57, 30528.0, 30530.0, 292675.0, 30533.52, 30534.0, 30535.0, 30535.08, 30536.0, 30537.84, 30540.0, 30542.0, 30542.29, 30544.0, 30545.0, 30546.0, 30547.0, 30548.0, 30549.0, 30550.0, 30552.0, 30554.0, 30555.0, 30555.2, 30556.0, 30558.0, 30559.0, 30560.0, 30561.0, 30562.0, 30564.0, 58178.08, 30566.0, 817000.0, 30568.0, 30570.0, 30571.0, 30572.0, 30573.7, 30574.0, 30575.57, 30576.0, 30577.0, 30578.0, 30579.65, 30580.0, 30585.0, 30586.0, 30588.0, 30589.0, 30591.8, 30592.0, 30593.0, 30596.0, 30597.0, 30596.8, 30600.0, 30603.0, 30604.0, 30605.0, 30607.0, 30609.0, 30612.0, 30614.0, 30616.0, 30617.0, 30618.0, 30617.6, 30620.0, 30621.0, 30619.0, 30624.0, 30625.0, 30626.0, 30627.0, 30628.0, 30626.28, 30630.0, 30632.0, 30635.0, 30636.0, 30638.0, 30639.0, 30640.0, 30642.81, 30643.0, 30642.0, 30644.0, 30648.0, 30649.84, 30650.0, 30651.27, 30652.0, 30653.0, 30654.0, 30655.0, 30656.0, 30657.0, 30658.0, 30659.2, 30660.0, 30661.31, 30661.0, 30661.68, 30664.56, 30662.0, 30666.0, 30661.92, 30663.0, 30669.24, 30670.0, 30672.24, 30672.0, 30674.0, 30675.0, 30676.0, 30678.0, 30680.0, 30681.0, 30683.0, 30684.0, 30685.32, 30686.0, 30688.0, 30691.0, 30692.0, 30693.0, 30694.0, 30695.88, 30696.0, 30696.62, 30697.0, 30699.0, 30700.0, 30701.0, 30702.0, 30708.0, 30709.07, 30710.0, 555000.0, 30714.0, 30716.0, 30717.28, 30718.0, 30719.0, 30720.0, 30721.0, 30721.6, 30723.32, 30724.0, 30725.0, 30726.56, 30726.0, 30727.0, 30729.0, 30730.0, 30731.0, 30732.0, 30733.0, 30734.0, 30735.0, 30738.0, 30740.0, 30740.44, 30741.0, 30742.0, 30743.0, 30744.0, 30742.4, 30745.0, 30747.0, 30748.0, 30750.0, 30752.0, 30754.0, 30755.0, 30756.0, 30756.56, 30758.0, 30758.61, 30760.0, 30763.0, 30763.68, 30765.0, 30766.0, 30768.0, 30771.0, 30772.0, 30773.0, 30775.0, 30776.0, 30779.0, 30780.0, 30781.21, 30781.0, 30784.0, 30785.0, 30786.0, 30785.28, 30789.0, 30790.0, 30791.0, 30792.29, 30792.0, 30791.29, 30795.0, 30796.8, 30796.66, 30798.0, 30791.7, 30800.0, 30793.0, 30797.15, 30797.0, 30804.0, 30805.0, 30799.0, 30803.0, 30806.4, 30809.0, 30810.0, 30810.94, 30812.9, 30813.0, 30813.13, 30814.0, 30816.0, 30817.0, 30812.0, 30819.0, 30820.0, 30819.12, 30822.0, 30824.0, 30825.96, 30825.0, 30827.4, 30826.0, 30827.6, 30828.0, 30830.98, 30830.0, 30827.0, 30833.0, 30834.0, 30835.0, 30831.0, 30837.0, 30838.0, 30840.0, 30841.0, 30834.84, 30843.0, 30844.8, 30844.0, 30846.0, 30847.0, 30848.48, 30848.0, 30850.0, 30845.0, 30852.0, 30849.6, 30854.4, 30854.0, 293000.0, 30857.0, 30858.0, 30860.0, 30860.77, 30862.11, 30861.29, 30862.0, 30861.0, 30864.0, 30866.0, 30867.62, 30868.0, 30867.0, 30870.24, 30870.0, 30867.2, 30873.0, 30874.0, 30875.0, 30876.23, 30876.0, 30877.0, 30873.6, 30880.0, 30883.0, 30884.0, 30885.0, 30886.0, 30887.0, 30888.0, 30885.96, 30890.0, 30891.43, 30892.63, 30893.0, 30892.8, 30889.96, 30900.0, 30901.08, 30903.0, 30904.0, 30905.0, 30907.0, 30908.0, 30908.8, 30910.0, 30909.0, 30912.0, 30913.0, 30914.0, 30915.0, 30917.0, 30918.0, 30919.0, 30920.0, 30921.0, 30923.0, 30924.0, 30927.0, 30928.0, 30929.6, 30930.0, 30931.2, 30931.0, 30932.0, 30933.0, 30934.0, 30935.0, 30936.0, 30937.08, 30938.32, 30940.0, 30938.66, 30933.76, 30939.0, 30944.24, 30945.0, 30946.35, 30947.0, 30948.0, 30947.52, 30950.4, 30950.0, 30952.0, 30953.72, 30953.0, 30956.0, 30959.0, 30960.0, 30961.0, 30961.4, 30964.0, 30965.0, 30967.2, 30969.0, 30970.0, 30971.0, 30972.0, 30971.2, 30973.0, 30975.02, 30976.0, 30972.27, 30974.0, 30975.0, 30982.0, 30984.0, 30985.0, 30987.0, 30988.62, 30988.8, 30990.0, 30989.0, 30988.0, 30992.0, 30993.0, 30995.0, 30996.0, 30997.0, 30998.0, 30999.0, 31000.0, 31001.0, 31003.56, 31004.0, 31005.2, 31006.0, 31008.0, 31009.0, 31008.59, 31012.0, 31013.0, 31015.0, 31016.0, 31017.0, 31020.0, 31023.2, 31025.0, 31027.0, 31028.0, 31032.0, 31033.0, 31034.0, 31035.0, 31040.0, 31041.0, 31042.0, 31043.9, 31044.0, 31045.0, 31046.53, 31046.4, 31042.8, 31048.0, 31050.0, 31051.0, 31049.0, 31052.0, 31054.0, 31055.0, 31056.0, 31058.0, 31059.0, 31060.0, 31061.0, 31064.0, 31065.0, 31067.0, 31068.0, 31069.0, 31072.0, 31073.4, 31074.0, 31073.0, 31075.0, 31080.0, 31082.0, 31082.76, 31084.0, 31085.0, 31086.0, 31087.0, 31090.0, 31090.56, 31091.0, 31093.0, 31090.32, 31095.0, 31096.0, 31097.0, 31099.32, 31100.0, 31099.0, 31102.0, 31103.0, 31101.0, 31104.0, 31105.0, 31106.0, 31107.18, 31108.0, 31107.0, 31110.0, 31111.08, 31112.0, 31114.0, 31111.87, 31113.0, 31115.0, 31116.0, 31116.8, 31118.0, 31119.0, 31120.69, 31120.0, 31121.0, 31123.0, 31122.0, 31125.0, 31126.0, 31127.0, 31128.0, 31128.76, 31131.0, 31133.2, 31129.32, 31136.0, 31138.0, 31140.0, 31141.0, 31144.0, 31146.0, 31148.0, 31149.51, 31150.0, 31151.0, 31152.0, 31150.32, 31154.0, 31155.0, 31156.0, 31157.85, 293300.0, 31156.58, 31158.4, 31158.49, 31160.0, 31158.0, 31162.0, 31163.0, 31164.0, 31165.0, 31162.62, 31167.0, 31169.54, 31168.0, 31171.0, 31170.0, 31176.0, 31176.88, 31179.0, 31180.0, 31179.2, 31182.0, 31181.76, 31184.0, 31185.0, 31185.24, 31187.0, 31188.0, 31190.5, 31190.0, 31191.0, 31193.0, 31192.0, 293337.0, 31194.0, 31193.35, 293340.0, 31197.6, 31197.0, 31199.0, 31200.0, 31201.59, 31201.0, 31204.0, 31205.0, 31206.0, 31208.0, 31209.0, 31210.0, 31211.29, 31212.13, 31212.0, 31214.0, 31215.96, 31216.0, 31217.56, 31218.36, 31219.0, 31220.0, 31221.0, 31222.0, 31223.01, 31224.0, 31225.0, 31226.0, 31226.52, 31228.0, 31231.0, 31232.0, 31233.0, 31233.8, 31235.0, 31236.0, 31237.0, 31236.98, 31240.45, 31240.0, 31242.24, 31243.0, 31241.0, 31243.92, 31246.0, 31246.62, 31242.0, 31245.0, 31241.6, 31248.0, 31249.0, 31250.0, 31252.0, 31254.0, 31255.0, 31256.0, 31258.0, 31257.0, 31260.0, 31261.0, 31262.4, 31262.0, 31264.0, 31265.0, 31266.8, 31267.51, 31268.0, 31267.0, 31270.0, 31266.0, 31272.0, 31273.0, 31269.51, 31275.0, 31276.0, 31277.0, 31278.0, 31279.0, 31280.64, 31280.04, 31282.0, 31280.0, 31283.0, 31284.0, 31285.0, 31286.0, 31288.0, 31285.74, 31289.0, 31287.0, 31290.0, 31293.0, 31294.0, 31295.0, 31296.0, 31297.0, 31297.5, 31299.0, 31300.0, 31302.0, 31303.0, 31304.0, 31306.56, 31308.4, 31308.0, 31310.0, 31312.84, 31313.0, 31314.0, 31315.0, 31315.08, 31318.98, 31320.0, 31322.0, 31323.0, 31325.0, 31328.0, 31329.0, 31330.0, 31332.0, 31333.0, 31334.0, 31334.4, 31340.0, 31343.0, 31344.0, 31345.0, 31345.56, 31345.6, 58333.0, 31350.0, 31351.0, 31352.0, 31353.12, 31354.0, 31356.0, 31357.0, 293500.0, 31358.0, 31359.8, 31360.0, 31361.0, 31357.45, 31363.0, 31365.0, 31366.0, 31366.92, 31367.0, 31368.0, 31369.0, 31370.0, 31371.0, 31372.12, 31373.0, 31374.0, 31374.72, 31373.88, 31380.22, 31380.0, 31380.24, 31383.12, 31382.0, 31383.85, 31385.0, 31386.0, 31387.0, 31389.0, 31390.0, 31391.04, 31392.0, 31393.0, 31395.0, 31396.0, 31397.8, 31398.66, 31400.0, 31401.0, 31402.0, 31404.0, 31405.44, 31406.0, 31407.0, 31408.0, 31409.0, 31410.0, 31411.0, 31411.2, 31412.0, 31413.6, 31415.0, 31416.0, 31417.0, 31418.0, 31419.0, 31420.0, 31422.0, 31423.0, 31424.58, 1080000.0, 31426.0, 58349.52, 31428.8, 31428.0, 31430.4, 31430.0, 31432.0, 31431.0, 31433.0, 31435.0, 31436.0, 31434.0, 31437.75, 31439.56, 31440.0, 31441.56, 31442.0, 31443.0, 31445.0, 31448.88, 31448.0, 31449.6, 31450.0, 31451.0, 31452.0, 31453.0, 31454.4, 31451.84, 31457.0, 31458.0, 31459.0, 31460.0, 31461.0, 31462.0, 31456.0, 31464.94, 31463.0, 31466.22, 31464.0, 31465.0, 31467.0, 31468.92, 31470.0, 31471.21, 31471.0, 31466.0, 31475.0, 31476.0, 31469.0, 31480.0, 31482.0, 31483.0, 31485.0, 31485.84, 31485.12, 31488.0, 31489.56, 31486.0, 31491.2, 31491.0, 31493.0, 31494.39, 31495.0, 31496.0, 31497.0, 31498.0, 31499.0, 31500.0, 31494.0, 31502.0, 31504.27, 31505.0, 31506.0, 31507.0, 31511.64, 31512.0, 31513.0, 31511.0, 31515.0, 31515.48, 31513.07, 31518.0, 31516.92, 31520.0, 31519.0, 31523.48, 31524.0, 31525.0, 31526.4, 31527.0, 58369.0, 31529.0, 31530.0, 293675.66, 31532.0, 31533.0, 31534.0, 31535.0, 31536.0, 31537.8, 31538.0, 58371.0, 31540.0, 31541.0, 31542.0, 31543.2, 31544.0, 31545.0, 31547.0, 31548.0, 31547.3, 31550.0, 31551.84, 31552.0, 31553.0, 31550.07, 31555.0, 31554.24, 31557.98, 31558.0, 31557.12, 31560.0, 31562.0, 31563.0, 31564.0, 31565.91, 31566.0, 31567.0, 31568.0, 31568.42, 31570.2, 31565.0, 31564.8, 31572.0, 31574.4, 31574.0, 31575.0, 31576.0, 31577.0, 31577.6, 31580.0, 31582.0, 31583.0, 31584.0, 31585.0, 31586.0, 31587.0, 31586.41, 31589.0, 31590.0, 31591.92, 31588.0, 31592.0, 31594.0, 31595.0, 31596.0, 31595.2, 31599.0, 31600.0, 31601.69, 31599.84, 31603.0, 31605.0, 293750.0, 31607.0, 31608.0, 31609.52, 31611.44, 31616.0, 31617.0, 31618.0, 31619.0, 31620.0, 31620.9, 31622.0, 31621.0, 31624.0, 31625.0, 31626.0, 31628.0, 31629.79, 31629.0, 31630.0, 31632.0, 31633.0, 31634.64, 31635.64, 31636.75, 31636.0, 31637.0, 31635.0, 31640.0, 31641.99, 31641.0, 31642.8, 31644.24, 31642.0, 31646.0, 31644.0, 31643.0, 31645.12, 31648.0, 31650.0, 31647.46, 31652.8, 31653.0, 31654.0, 31655.0, 31656.0, 31657.0, 31658.0, 31659.24, 31659.0, 31660.0, 31657.6, 31663.32, 31665.71, 31664.0, 31662.0, 31666.55, 31667.0, 31668.0, 31669.0, 31670.0, 31666.0, 31674.0, 31674.61, 31676.48, 31677.0, 31678.0, 31678.4, 31680.0, 31679.0, 31681.0, 31679.77, 31684.0, 31687.0, 31688.0, 31691.0, 31692.0, 31693.0, 31695.0, 31696.0, 31698.0, 31699.0, 31700.0, 31700.8, 31699.55, 31704.0, 31705.0, 31706.48, 31708.0, 31710.0, 31711.0, 556000.0, 31713.0, 31714.0, 31716.0, 31716.72, 31717.0, 31719.0, 31720.99, 31721.0, 31718.0, 31719.48, 31720.0, 31721.88, 31722.0, 31727.16, 31719.99, 31724.0, 31725.0, 31726.9, 31726.0, 31728.0, 31730.0, 31729.0, 31733.0, 31732.0, 31736.0, 31737.72, 31737.0, 31739.0, 31740.0, 31741.0, 31742.0, 31740.98, 31746.0, 31749.0, 31750.0, 31752.12, 31753.0, 31754.0, 31752.0, 31756.0, 31757.0, 31758.0, 31752.25, 31759.0, 31760.0, 31761.0, 31762.0, 31764.0, 31763.64, 31763.0, 31766.0, 31767.0, 31766.4, 31769.0, 31770.0, 31768.0, 31772.0, 31769.4, 31775.0, 31776.0, 31778.0, 31779.0, 31780.0, 31781.0, 31782.0, 31782.4, 31784.0, 31786.0, 31788.0, 31789.0, 31790.0, 31791.0, 31789.88, 31793.71, 31794.0, 31795.2, 31791.84, 31799.44, 31799.0, 31800.0, 31800.12, 31803.0, 31804.0, 31805.0, 31806.0, 31800.04, 31808.81, 31808.0, 31811.0, 31812.0, 31814.0, 31815.0, 31816.0, 31817.0, 31817.39, 31818.0, 31820.0, 31822.0, 31824.0, 31825.0, 31826.0, 31827.0, 31828.0, 31830.0, 31830.96, 31833.0, 31834.0, 31835.96, 31836.0, 31838.45, 31840.0, 31841.0, 31844.0, 31845.0, 31846.0, 31847.0, 31848.0, 31849.8, 31850.0, 31844.8, 31852.0, 31853.64, 31849.0, 31854.0, 31855.0, 294000.0, 31855.56, 31858.0, 31856.0, 31860.0, 31859.0, 31862.0, 31863.0, 31865.6, 31867.68, 31868.0, 31867.32, 31870.03, 31870.0, 31872.0, 31873.0, 31867.0, 31875.0, 31876.0, 31877.16, 31877.0, 31878.0, 31880.64, 31881.0, 31875.72, 31880.28, 31884.0, 31885.0, 31886.4, 31887.0, 31888.0, 31886.0, 31890.0, 31891.08, 31889.0, 31888.34, 31893.65, 31893.0, 31891.0, 31896.11, 31896.5, 31899.96, 31896.0, 31899.0, 31900.0, 31898.21, 31902.0, 31898.0, 31897.0, 31904.4, 31906.32, 31907.0, 31908.0, 31904.0, 31910.0, 31906.0, 31905.12, 31913.0, 31915.0, 31917.0, 31918.0, 31917.12, 31920.0, 31921.06, 31922.0, 31924.0, 31924.8, 31926.0, 31927.21, 31928.0, 31930.0, 31932.0, 31934.0, 31935.0, 31937.0, 31939.0, 31940.0, 31941.0, 31942.63, 31944.0, 31946.0, 31948.8, 31949.0, 31948.0, 31951.0, 31952.0, 31950.0, 31954.0, 31955.0, 31956.0, 31958.88, 31960.0, 31961.0, 31962.0, 31963.0, 31966.82, 31968.0, 31969.0, 31969.6, 31971.0, 31972.0, 31973.0, 31970.0, 31975.0, 31976.52, 31976.0, 31977.6, 31979.0, 31980.0, 31974.0, 31982.0, 31981.1, 31986.96, 31987.0, 31988.0, 31989.12, 31990.0, 31991.0, 31992.0, 31993.0, 31992.24, 31990.39, 31996.0, 31990.4, 31999.0, 32000.0, 32001.0, 32002.0, 32001.53, 32004.0, 32003.4, 32006.0, 32007.0, 32008.0, 32010.0, 32011.0, 32014.0, 32015.0, 32016.0, 32017.0, 32018.0, 32019.8, 32017.68, 32021.0, 32021.46, 32020.0, 32022.76, 32023.0, 32022.0, 32025.0, 32019.0, 32027.0, 32028.0, 32029.0, 32029.08, 32032.0, 32034.6, 32034.0, 32036.0, 32038.0, 32039.0, 32040.0, 32041.54, 32042.0, 32043.0, 32043.18, 32045.0, 58472.0, 32047.0, 32048.0, 32050.0, 32052.0, 32054.0, 32055.0, 32056.96, 32056.0, 32059.0, 32062.0, 32062.8, 32064.0, 32065.0, 32066.27, 32067.0, 32069.0, 32070.0, 32071.0, 32072.4, 32072.0, 32074.0, 32073.0, 32075.0, 32077.0, 32078.0, 32076.0, 32080.0, 32082.0, 32084.36, 32085.72, 32086.0, 32084.0, 32088.0, 32089.08, 32090.0, 32085.0, 32093.0, 32094.0, 32095.0, 32096.0, 32097.0, 32098.0, 32099.0, 32100.0, 32101.08, 32102.0, 32103.0, 32101.0, 294247.0, 32106.0, 32104.44, 32108.0, 32110.0, 32111.0, 32112.0, 32110.9, 32113.0, 32115.0, 32115.2, 32116.06, 32118.0, 32116.0, 3964280.0, 32120.0, 32122.0, 32122.17, 32124.0, 32125.0, 32126.0, 32127.0, 32121.0, 32131.0, 32132.0, 32134.95, 32136.0, 32137.0, 32139.0, 32140.8, 32140.0, 32142.0, 32143.0, 32144.0, 32145.0, 32141.0, 32148.0, 32149.52, 32150.0, 32151.0, 32152.0, 32148.68, 32154.0, 32155.0, 32156.0, 32157.0, 32158.0, 32159.0, 32160.0, 32156.8, 32162.0, 32158.8, 32161.0, 32165.0, 32168.0, 32170.0, 32171.0, 32172.0, 32173.0, 32174.76, 32175.0, 32176.0, 32177.6, 32178.0, 32173.32, 32174.64, 32179.0, 32182.0, 32183.0, 32184.0, 32180.0, 32186.0, 32188.0, 32190.0, 32192.0, 32193.0, 32194.8, 32195.0, 32196.0, 32197.0, 32198.4, 32198.0, 32200.0, 32199.0, 32201.0, 32204.0, 32205.09, 32206.0, 32207.0, 32205.0, 32208.0, 32208.02, 32208.84, 32211.0, 32213.0, 32212.0, 32213.4, 32216.0, 32217.0, 32218.0, 32219.0, 32220.0, 32221.08, 32222.0, 32219.2, 32224.0, 32225.0, 32226.0, 32227.0, 32228.0, 58507.8, 32230.0, 32231.76, 32231.0, 32232.0, 32234.0, 32238.0, 32239.0, 32240.0, 32241.0, 32239.96, 32241.68, 32244.5, 32244.0, 32246.0, 32245.0, 32248.0, 32249.0, 32250.0, 32248.24, 32252.0, 58513.0, 32254.44, 32254.0, 32256.0, 32257.0, 32258.0, 32259.0, 32256.16, 32260.8, 32260.0, 32263.0, 32261.4, 32264.0, 32266.0, 32265.0, 32262.0, 32267.0, 32268.0, 32269.16, 32270.0, 32271.0, 32272.0, 32271.24, 32274.0, 32277.92, 32275.0, 32277.15, 32278.0, 32281.83, 32279.0, 32280.0, 32281.6, 32282.0, 32277.0, 32287.2, 32284.0, 32281.0, 32287.0, 32288.0, 32289.0, 32290.0, 32286.0, 32295.0, 32292.0, 32294.0, 32293.0, 32296.0, 32297.0, 32299.0, 32300.0, 32301.0, 32302.0, 32303.0, 32304.0, 32300.33, 32307.0, 32308.0, 32307.34, 32310.0, 32311.0, 32311.28, 32314.2, 32313.0, 32314.0, 32312.0, 32316.0, 32317.56, 32318.0, 32317.8, 32320.0, 32321.0, 32322.0, 32323.0, 32323.2, 32325.0, 32325.96, 32328.0, 32330.0, 32331.0, 32332.68, 32333.0, 32334.0, 32335.0, 32335.2, 32336.8, 32334.6, 32332.0, 32340.0, 32336.0, 32342.0, 32343.0, 32344.0, 32345.0, 32346.0, 32339.0, 32347.0, 32350.0, 32352.0, 32353.0, 32354.0, 32355.0, 32356.0, 294500.0, 32357.0, 32358.0, 32360.0, 32360.56, 32361.87, 32363.52, 32364.0, 32365.0, 32366.0, 32359.0, 32368.0, 32369.75, 294507.0, 32371.0, 32372.0, 294517.0, 32374.0, 32375.0, 32376.0, 32376.8, 32378.0, 58538.0, 32380.0, 32381.11, 32382.0, 32381.0, 32384.0, 32385.6, 32385.0, 32386.0, 32388.6, 32388.0, 32390.0, 32391.0, 32392.56, 32393.0, 32394.0, 32395.0, 32396.0, 32397.0, 32398.06, 32399.0, 32400.0, 32394.08, 32403.86, 32404.0, 32406.0, 32407.0, 32406.4, 32409.0, 32410.29, 32411.0, 32412.0, 32414.0, 32415.0, 32417.0, 32419.0, 32420.0, 32421.0, 32422.0, 32421.96, 32424.0, 32425.0, 32427.0, 32428.0, 32429.0, 32430.0, 32432.4, 32434.0, 32435.0, 32436.0, 32438.0, 32439.0, 32440.0, 32439.6, 32443.0, 32444.0, 32445.0, 32446.0, 32448.0, 32449.0, 32450.0, 32456.0, 32457.0, 32460.0, 32461.0, 32463.0, 32464.0, 32465.0, 32466.0, 32467.0, 32468.0, 32468.8, 32469.0, 32470.0, 32472.0, 32470.36, 32474.0, 32475.0, 32476.0, 32476.22, 32477.62, 32478.28, 32478.06, 32481.0, 32479.0, 32480.0, 32478.0, 32484.0, 32486.0, 32487.0, 32488.0, 32489.0, 32490.0, 32489.6, 32492.0, 32495.3, 32496.0, 32498.0, 32498.67, 32500.0, 32502.62, 32503.0, 32504.0, 32505.0, 32503.64, 32507.0, 32508.0, 32510.0, 32511.0, 32512.0, 32510.4, 32520.0, 32521.05, 32522.0, 32523.0, 32524.0, 32525.0, 32526.0, 32527.32, 32521.84, 32528.0, 32521.0, 32531.2, 32532.0, 32529.0, 32534.0, 32535.41, 32536.0, 32535.0, 32531.0, 32539.0, 32539.99, 32541.0, 32540.0, 32539.64, 32540.4, 32545.5, 32543.6, 32544.0, 32545.0, 32540.97, 32547.0, 32548.0, 32550.0, 32550.07, 32552.0, 32553.56, 32547.96, 32555.0, 32556.0, 294700.0, 32558.0, 32557.0, 32560.0, 32562.0, 32563.0, 32564.0, 32565.0, 32566.0, 32567.0, 32568.0, 32569.0, 32571.84, 32572.0, 32570.0, 32574.0, 32571.0, 32576.0, 32577.0, 32571.36, 32574.4, 32580.0, 32581.0, 32582.0, 32578.5, 32584.0, 32578.0, 32586.34, 32586.0, 32587.0, 32588.0, 32590.0, 32589.6, 32592.0, 32593.0, 32594.0, 32596.0, 32599.0, 32600.0, 32602.0, 32602.04, 32604.0, 32606.0, 32607.0, 32608.0, 32607.43, 32607.6, 32611.0, 32612.0, 32613.24, 32614.0, 32615.0, 32616.0, 32614.4, 32618.0, 32618.56, 32620.0, 32621.0, 32622.0, 32619.0, 32618.1, 32625.0, 32626.0, 32627.0, 32628.0, 32629.0, 556913.0, 32634.0, 32635.0, 32636.0, 32637.0, 32635.2, 32639.0, 32640.0, 32642.0, 32642.64, 32645.0, 32646.0, 32647.0, 32646.24, 32649.0, 32650.0, 32651.0, 32652.0, 32653.0, 32654.0, 32655.0, 32656.0, 32655.72, 32658.0, 32659.0, 32660.0, 32661.0, 32653.92, 32663.0, 32664.0, 32666.0, 32668.0, 32669.0, 32670.0, 32671.0, 32669.72, 32669.61, 32674.0, 32675.0, 32676.0, 32677.0, 32678.4, 32679.04, 32680.0, 32678.0, 32682.0, 32683.0, 32684.0, 32685.0, 32688.0, 32690.0, 32692.0, 32693.0, 32694.0, 32695.0, 32696.0, 32697.0, 32698.0, 32700.0, 32701.0, 32702.0, 32703.0, 32704.0, 32705.32, 32706.0, 32707.0, 32708.0, 32710.65, 32711.0, 32712.0, 32713.2, 557000.0, 32712.72, 32716.0, 32715.0, 32718.0, 32718.01, 32720.0, 32721.56, 32722.0, 32723.0, 32724.8, 32719.0, 32726.0, 32728.0, 32730.0, 32732.0, 32733.0, 58609.0, 32735.16, 32736.0, 32736.57, 32735.0, 32739.0, 32740.0, 32741.0, 32742.0, 32738.0, 32744.0, 32737.0, 32747.0, 32748.0, 32750.0, 32751.0, 32752.0, 32753.0, 32754.0, 32756.0, 32758.0, 32759.17, 32760.0, 32761.0, 32759.0, 32763.0, 32764.17, 32766.0, 32767.0, 32768.0, 32769.0, 32770.0, 32771.0, 32772.0, 32770.8, 32774.4, 32775.0, 32776.0, 32777.0, 32778.0, 32778.72, 32780.0, 32780.8, 32782.0, 32783.0, 32784.0, 32785.0, 32786.0, 32787.0, 32788.0, 32789.76, 32790.0, 32791.5, 32792.0, 32789.72, 32794.2, 32795.0, 32796.0, 32797.0, 32798.0, 32799.0, 32800.0, 32801.0, 32804.0, 32805.0, 32806.0, 32807.0, 32808.0, 32809.0, 32808.02, 32811.0, 32812.0, 32813.0, 32812.8, 294956.0, 32810.0, 32817.0, 32818.0, 32819.0, 32820.0, 32821.0, 32822.0, 32824.0, 32826.0, 32827.0, 32828.0, 32829.0, 32830.0, 32831.0, 32832.0, 32833.0, 32836.0, 32838.0, 32839.0, 32840.0, 32842.0, 32843.19, 32844.0, 32842.8, 32843.0, 32847.0, 32848.0, 32845.0, 32850.0, 32846.0, 32852.0, 58632.0, 32854.0, 58632.75, 295000.0, 32856.0, 32857.0, 32858.0, 32860.0, 32859.0, 32861.0, 32863.0, 32864.0, 32865.0, 32862.0, 32867.0, 32868.0, 32869.0, 32872.0, 32875.0, 32877.0, 32878.0, 32879.0, 32880.0, 58638.84, 32882.0, 32884.0, 32885.0, 32884.8, 32887.0, 32888.0, 32889.0, 32890.0, 32891.0, 32892.0, 32886.0, 32894.0, 32895.0, 32896.92, 32897.0, 32896.0, 32899.0, 32900.0, 32901.0, 32902.4, 32903.0, 32904.0, 32905.6, 32906.0, 32905.0, 32908.0, 32908.44, 32910.25, 32910.0, 32910.81, 32912.0, 32911.0, 32914.0, 32916.0, 32917.0, 32918.64, 819348.0, 32920.0, 32921.0, 32922.0, 32923.91, 32919.0, 32925.0, 32926.0, 32927.0, 32928.0, 32929.0, 32933.0, 32934.0, 32935.19, 32936.0, 32934.83, 32939.0, 32940.0, 32944.0, 32945.8, 32946.0, 32947.0, 32947.19, 32949.0, 32950.0, 32947.2, 32952.0, 32952.5, 32951.0, 32953.0, 295100.0, 32956.0, 58652.88, 32959.0, 32960.0, 32960.22, 32962.0, 32963.0, 32964.0, 32965.0, 32966.04, 32966.0, 32968.26, 32969.0, 32970.0, 32971.0, 32968.0, 32973.0, 32974.0, 32975.0, 32976.0, 32977.0, 32976.84, 32978.0, 32980.0, 32981.0, 32984.0, 32985.0, 32985.8, 32986.0, 32988.0, 32989.0, 32988.87, 32990.0, 32992.0, 32993.73, 32994.0, 32995.0, 32996.0, 32997.6, 32999.0, 33000.0, 33001.0, 33003.0, 33004.0, 33005.0, 33006.8, 33006.0, 33009.0, 33010.0, 33009.6, 33012.0, 33013.2, 33013.24, 33015.0, 33017.0, 33020.0, 33022.0, 33024.0, 33025.0, 33028.08, 33030.0, 33031.0, 33033.08, 33033.0, 33034.53, 33036.0, 33035.0, 33033.96, 33035.64, 33040.0, 33041.0, 33042.0, 33043.2, 33044.0, 33046.0, 33048.0, 33048.25, 33050.0, 33051.14, 33051.0, 33052.8, 33053.0, 33052.0, 33055.52, 33055.0, 33057.0, 33058.0, 33060.0, 33058.2, 33062.0, 33055.68, 33065.53, 33065.0, 33067.0, 33068.0, 33069.0, 33070.0, 33071.0, 33072.0, 58675.14, 33074.53, 33075.0, 33076.0, 33077.06, 33078.0, 33079.0, 33080.0, 33077.0, 33082.0, 33083.0, 33084.0, 33085.0, 33081.0, 33078.66, 33088.0, 33089.0, 33090.24, 33091.0, 33092.0, 33093.0, 33092.8, 33090.0, 33096.0, 33098.0, 33098.88, 33100.0, 33099.0, 33102.0, 33103.0, 33100.04, 33102.12, 33107.0, 33108.0, 33110.0, 33111.0, 33112.19, 33113.0, 33114.0, 33115.0, 33116.52, 33116.25, 33118.0, 33117.0, 33120.0, 33120.1, 33122.0, 33123.0, 33124.0, 33125.0, 33123.94, 33120.96, 33128.0, 33130.0, 33132.0, 33133.0, 33134.4, 33135.0, 33134.0, 33137.0, 33138.0, 33139.0, 33140.0, 33141.6, 33142.0, 33141.0, 33144.0, 33144.34, 33146.0, 33145.0, 33147.0, 33149.0, 33150.0, 33151.32, 33152.0, 33153.0, 33154.0, 33155.19, 33156.0, 33155.0, 33158.0, 33159.0, 33160.0, 33157.0, 33165.0, 33167.0, 33168.0, 33169.0, 33170.4, 33171.0, 33172.0, 33173.0, 33170.0, 33175.0, 33176.0, 33177.0, 33178.0, 33180.0, 33181.0, 33182.0, 33184.0, 33185.0, 33186.0, 33189.0, 33190.0, 33192.0, 33193.0, 33195.0, 33196.8, 33196.0, 33197.0, 33199.0, 33200.0, 33201.0, 33198.12, 33203.0, 33204.0, 33205.0, 33198.0, 33207.96, 33208.0, 33209.0, 33210.0, 33205.44, 33211.0, 33213.0, 33214.44, 33214.0, 33216.0, 33217.0, 33215.0, 33219.0, 33220.0, 33221.0, 33222.0, 33223.0, 33225.0, 33226.0, 33227.0, 33228.0, 33230.0, 33232.52, 33232.44, 33234.0, 33232.2, 33236.0, 33235.0, 33238.4, 33238.0, 33240.0, 33241.19, 33233.76, 33245.0, 33246.0, 33248.0, 33250.0, 33252.0, 33254.56, 33254.0, 33256.27, 33257.0, 33255.0, 33259.0, 33260.0, 33258.76, 33262.0, 33254.4, 33264.0, 33265.0, 33266.0, 33267.0, 33268.0, 33266.35, 33270.0, 33271.93, 33272.0, 33275.0, 33276.0, 33277.0, 33278.0, 33279.0, 33280.0, 33281.64, 33282.08, 1344000.0, 33280.2, 33285.0, 33286.0, 33284.0, 33288.0, 33289.0, 33290.0, 33283.0, 33292.0, 33292.6, 33293.0, 33295.0, 33292.84, 33296.0, 33295.56, 33299.87, 33300.0, 33301.0, 33300.8, 33302.0, 33304.04, 33305.0, 33306.0, 33307.0, 33308.0, 33307.82, 33310.56, 33310.0, 33312.0, 33313.0, 33312.34, 33315.0, 33316.0, 33319.0, 33320.0, 33321.0, 33322.0, 33321.6, 33324.0, 33325.0, 33326.79, 33320.74, 33328.44, 33329.0, 33330.0, 33331.2, 33329.08, 33330.61, 33333.0, 33333.33, 33333.3, 33334.0, 33336.0, 33338.0, 33339.0, 33341.0, 33342.0, 33343.0, 33335.0, 33345.0, 33342.4, 33346.0, 33348.0, 33349.0, 33350.0, 33353.0, 33354.0, 33355.0, 295500.0, 33356.0, 33358.0, 33357.07, 33360.0, 33361.0, 33353.64, 33363.0, 33364.4, 33365.0, 33366.96, 33367.0, 33364.41, 33366.82, 33370.0, 295515.0, 33372.0, 33370.88, 33371.0, 33375.0, 33377.0, 33378.0, 33380.0, 33381.0, 33382.8, 33384.0, 33386.0, 33388.0, 33389.2, 33390.0, 33389.0, 33392.0, 33394.0, 33395.0, 33396.0, 33398.0, 33400.0, 33401.0, 33402.23, 33404.0, 33405.0, 33406.0, 33407.0, 33408.0, 33409.0, 33410.0, 33404.8, 295556.8, 33413.0, 33414.0, 33415.0, 33416.44, 33414.57, 33416.0, 33417.0, 33420.0, 33416.36, 33417.12, 33423.0, 33425.0, 33425.6, 33427.94, 33428.72, 33428.0, 33426.0, 33431.4, 33432.0, 33433.2, 33427.0, 33430.0, 33436.0, 33435.0, 33438.0, 33439.0, 33440.0, 33443.0, 33444.0, 33446.0, 33446.4, 33448.0, 33449.0, 33450.0, 33451.0, 33452.0, 33453.0, 33455.0, 33456.0, 33458.0, 33459.0, 33458.16, 33460.0, 33462.0, 33463.0, 33464.0, 33465.0, 33465.19, 33467.0, 33468.0, 33470.0, 33471.0, 33475.0, 33475.32, 33477.0, 33479.0, 33480.0, 33481.0, 33483.0, 33486.0, 33488.0, 33490.0, 33491.4, 33492.75, 33492.0, 33490.92, 33495.96, 33496.0, 33497.4, 33498.0, 33497.0, 33500.0, 33498.12, 58760.16, 33503.0, 33504.0, 33504.96, 33504.72, 33508.0, 33508.8, 33508.55, 33511.0, 33510.0, 33513.0, 33514.0, 33509.63, 33516.0, 58763.0, 33520.0, 33520.32, 33523.0, 33523.2, 33523.52, 33525.0, 33527.0, 33528.0, 33529.0, 33530.0, 33531.0, 33529.6, 33533.0, 33533.63, 33535.0, 33532.44, 33537.0, 33534.0, 33539.0, 33540.0, 33541.69, 33540.83, 33543.0, 33544.0, 33545.28, 33545.87, 33547.0, 33545.51, 33549.0, 33550.0, 33551.0, 33552.0, 33550.4, 33546.0, 33556.0, 33557.0, 33559.0, 33560.0, 33561.0, 33562.0, 33563.0, 33564.0, 33565.92, 33565.0, 33567.62, 820000.0, 33565.15, 33570.0, 33571.0, 33572.72, 33573.15, 33573.6, 33575.0, 33573.87, 33576.0, 33577.0, 33579.0, 33578.74, 33574.0, 33580.0, 33582.1, 33582.0, 33584.0, 33585.0, 33578.87, 33580.78, 33588.67, 33588.0, 33590.03, 33590.0, 33592.0, 33593.16, 33594.0, 33596.0, 33598.0, 33599.0, 33600.0, 33601.0, 33602.0, 33607.0, 33608.0, 33609.0, 33610.0, 33612.0, 33613.0, 33615.0, 33616.0, 33616.8, 33617.0, 33619.0, 33620.0, 33621.0, 33622.0, 33619.2, 33624.0, 33625.0, 33626.0, 33626.04, 33628.0, 33629.0, 33630.0, 33631.0, 33632.04, 33633.0, 33632.0, 33635.0, 33636.0, 33634.0, 58789.52, 33640.0, 33641.0, 33644.0, 33648.0, 33650.0, 33651.0, 33652.0, 33654.0, 33655.0, 33654.4, 33657.13, 33657.0, 33659.0, 33660.0, 33661.32, 33654.6, 33662.0, 33656.0, 33659.76, 33666.0, 33667.0, 33664.44, 33664.0, 33670.56, 33670.0, 33672.0, 33671.0, 33674.64, 33675.19, 33676.0, 33674.0, 33678.0, 33677.0, 33680.0, 33681.0, 33679.0, 33683.0, 33683.11, 33685.08, 33684.0, 33685.0, 33688.0, 33689.0, 33690.0, 33694.44, 33695.23, 33696.0, 33697.0, 33698.0, 33699.9, 33700.0, 33701.0, 33695.0, 33703.0, 33704.72, 33705.0, 33706.0, 33708.0, 33710.0, 33711.53, 33712.0, 33713.64, 558000.0, 33715.08, 33716.0, 33716.8, 33717.0, 33715.59, 33720.0, 33721.0, 33715.0, 33719.0, 33724.28, 33718.0, 33726.0, 33727.61, 33727.0, 33729.72, 33729.36, 33722.0, 33732.0, 33733.0, 33734.0, 33734.05, 33734.99, 33737.0, 33737.6, 33738.0, 33740.0, 33741.0, 33739.0, 33743.88, 33744.8, 33745.94, 33746.0, 33745.0, 33748.0, 33743.0, 33750.0, 33751.0, 33752.0, 33753.0, 33754.95, 33755.0, 33756.0, 33754.0, 33758.4, 33759.96, 33760.0, 33758.0, 33762.0, 33763.0, 33761.0, 33765.0, 33767.0, 33768.0, 33769.0, 33770.0, 33772.0, 33773.0, 33774.0, 33775.0, 33776.0, 33777.0, 33778.0, 33779.0, 33780.0, 33782.28, 33783.0, 33786.0, 33787.0, 33788.0, 33789.12, 33790.0, 33790.2, 33792.0, 33793.0, 33794.65, 33795.0, 33796.0, 33797.0, 33794.0, 33800.0, 33800.16, 33800.13, 33804.0, 33805.0, 33808.0, 33809.88, 33810.0, 33809.0, 33812.0, 33814.0, 33814.21, 33816.0, 33817.0, 33815.0, 33819.0, 33820.0, 33821.0, 33824.0, 33824.78, 33826.0, 33827.0, 33828.0, 33829.0, 33828.55, 33825.0, 33832.0, 33833.0, 33830.0, 33835.0, 33836.0, 33838.4, 33838.0, 33839.0, 33840.0, 33841.0, 558128.0, 33840.63, 33845.36, 33842.0, 33845.0, 33841.6, 33849.0, 33850.0, 33851.0, 33852.0, 33850.36, 33845.48, 33855.0, 296000.0, 33856.0, 33859.0, 33860.0, 33860.23, 33862.0, 33862.4, 33864.0, 33863.0, 33865.68, 33862.19, 33868.0, 33869.0, 33870.0, 33871.0, 33873.0, 33874.0, 33875.0, 33876.0, 33877.6, 33878.27, 33879.0, 33880.0, 33881.0, 33882.0, 33883.2, 33884.0, 33885.0, 33886.9, 33887.0, 33888.0, 33888.72, 33890.28, 33891.0, 33891.83, 33889.0, 33894.0, 33895.98, 33896.0, 33897.0, 33898.0, 33893.0, 33900.0, 33902.0, 33903.0, 33904.0, 33905.0, 33906.0, 33908.0, 33909.0, 33912.0, 33913.0, 33915.0, 33917.0, 33918.0, 33920.0, 33921.83, 33922.44, 33923.45, 33924.48, 33924.8, 33926.4, 33924.0, 33928.0, 33929.52, 33930.0, 33929.0, 33927.0, 33933.0, 33934.0, 33935.0, 33936.0, 33937.0, 33937.56, 33939.0, 33940.0, 33943.0, 33945.0, 33946.0, 33945.6, 33948.0, 33947.0, 33950.0, 33951.12, 33951.0, 33952.0, 33954.0, 33955.0, 33956.0, 33958.0, 296102.0, 33960.0, 33961.0, 33964.0, 33965.0, 33966.0, 33964.77, 33968.0, 33966.4, 33969.0, 33971.0, 33972.0, 33974.64, 33975.0, 33976.0, 33977.0, 33974.0, 33976.94, 33980.0, 33977.52, 33982.0, 33979.0, 33984.0, 33986.0, 33987.0, 33988.0, 33990.0, 33991.0, 33994.2, 33995.0, 33996.0, 33998.0, 33999.92, 34000.0, 33999.0, 34001.0, 296144.0, 34002.0, 34005.0, 34006.0, 34008.0, 34009.0, 34009.12, 34008.01, 34012.0, 34010.0, 34014.0, 34017.0, 34020.0, 34022.0, 34023.0, 34024.0, 34022.4, 34026.0, 34027.0, 34028.0, 34029.0, 34030.0, 34031.0, 34032.0, 34033.0, 34034.52, 34035.0, 34032.48, 34034.0, 34038.0, 34039.0, 34040.0, 34039.08, 34043.0, 34044.0, 34047.0, 34048.0, 34049.0, 34050.0, 34049.6, 34052.0, 34053.15, 34054.8, 34055.0, 34056.0, 34055.4, 34058.0, 34059.0, 34060.0, 34062.0, 34063.0, 34064.0, 34065.0, 34066.0, 34067.0, 34068.0, 34068.04, 34070.4, 34071.0, 34069.0, 34070.0, 34074.0, 34075.0, 34076.0, 34072.0, 34077.06, 34072.44, 34080.0, 34081.32, 34080.84, 34081.0, 34084.0, 34086.96, 34087.0, 34088.4, 34086.0, 34089.0, 34087.56, 34091.2, 34091.0, 34092.48, 34095.0, 34096.0, 34097.0, 34092.0, 34099.2, 34100.0, 34101.09, 34104.0, 34105.0, 34107.0, 34108.0, 34109.0, 34110.0, 34112.0, 296257.0, 34113.0, 34115.0, 34116.0, 34117.0, 34118.0, 34120.0, 34121.0, 34122.0, 34123.0, 34124.0, 34125.0, 34120.04, 34120.32, 34128.0, 34129.0, 34124.36, 34126.68, 34132.8, 34133.0, 34132.0, 34134.0, 34136.0, 34137.6, 34138.0, 34139.68, 34140.0, 34139.0, 34142.0, 34143.0, 34137.0, 34145.0, 34146.0, 34148.0, 34150.0, 34152.0, 34153.0, 34154.0, 34156.0, 34157.0, 34156.93, 34159.35, 34160.0, 34162.0, 34163.45, 34164.0, 34165.0, 34163.0, 34168.0, 34171.0, 34172.0, 34171.92, 34174.8, 34174.0, 34176.0, 34177.0, 34175.39, 34179.0, 34180.0, 34175.0, 34182.0, 34185.0, 34186.0, 34188.0, 34189.64, 34190.0, 34191.6, 34192.0, 34193.58, 34193.0, 34195.0, 34196.0, 34192.92, 34198.0, 34199.0, 34200.0, 34203.0, 34206.0, 34207.92, 34208.0, 34207.0, 34210.0, 34211.12, 34212.0, 34213.0, 34214.0, 34215.0, 34216.0, 34213.32, 34210.44, 34220.0, 34221.0, 34221.43, 34222.0, 34224.0, 34225.0, 34226.0, 34227.0, 34228.0, 34229.0, 34230.0, 34231.0, 34233.0, 34234.0, 34235.0, 34236.0, 34236.8, 34237.0, 34233.6, 34240.0, 34241.66, 34242.0, 34244.0, 34247.0, 34248.0, 34248.09, 34250.0, 34250.28, 34252.0, 34253.0, 34255.0, 34256.0, 34257.0, 34258.0, 34259.83, 34260.0, 34260.96, 34257.6, 34264.0, 34265.0, 34266.0, 34268.0, 34270.0, 34271.0, 34272.0, 34274.0, 34275.59, 34275.0, 34276.0, 34277.0, 34278.4, 34278.0, 34280.0, 34281.84, 34281.31, 34283.0, 34284.0, 34282.0, 34285.0, 34288.0, 34281.0, 34290.0, 34291.2, 34292.0, 34293.6, 34291.0, 34295.0, 34296.0, 34297.0, 34298.0, 34299.0, 34300.0, 34300.56, 34300.5, 34303.0, 34304.0, 34305.0, 34306.8, 34307.0, 34308.0, 34307.28, 34310.0, 34311.0, 34312.0, 34312.17, 34314.0, 34315.0, 34317.0, 34320.0, 34322.0, 34323.0, 34324.0, 34322.98, 34325.0, 34327.0, 34326.0, 34329.0, 34328.0, 34330.0, 34332.0, 34332.62, 34334.0, 34335.0, 34336.0, 34335.6, 34338.0, 34339.4, 34340.0, 58927.55, 34343.0, 34344.0, 34346.76, 34347.0, 34348.0, 34349.0, 34350.0, 34351.0, 34352.0, 34353.0, 34348.8, 34355.0, 34356.0, 34356.83, 34359.0, 34360.0, 34361.0, 34361.6, 34362.0, 34363.0, 34365.0, 34364.0, 34366.0, 34368.0, 34369.0, 34368.36, 34369.71, 34372.8, 34373.0, 34371.0, 34375.31, 34376.99, 34377.98, 34378.0, 34375.0, 34380.0, 34381.0, 34382.4, 34383.0, 34382.0, 34385.0, 34384.0, 34386.0, 34388.0, 34390.0, 34391.0, 34392.0, 34393.0, 34394.4, 34395.0, 34396.0, 34399.0, 34400.0, 34400.08, 34399.32, 34403.0, 34404.0, 34403.02, 34406.35, 34407.0, 34408.0, 34406.8, 34406.0, 34405.0, 34412.0, 34413.0, 34409.43, 34415.0, 34416.0, 34415.16, 34417.0, 34419.0, 34420.0, 34418.0, 34424.0, 34425.0, 34425.6, 34427.0, 34428.0, 34429.56, 34429.44, 34431.0, 34429.0, 34433.0, 34434.0, 34435.0, 34436.0, 34437.48, 34438.0, 34437.0, 34440.0, 34438.4, 34442.0, 34443.0, 34444.65, 34444.0, 34445.82, 34445.0, 34444.8, 34446.0, 296586.0, 34450.0, 34451.0, 34449.0, 34448.0, 34454.0, 34447.0, 34456.0, 34457.76, 34452.0, 34455.29, 34460.0, 34461.0, 34462.0, 34463.0, 34464.0, 34465.6, 34465.0, 34466.0, 34468.0, 34470.0, 34474.0, 34476.0, 34478.0, 34479.05, 34480.0, 34481.29, 34482.0, 34481.88, 34484.0, 34485.0, 34486.0, 34486.4, 34488.0, 34485.72, 34490.0, 34494.0, 34495.0, 34496.0, 34497.72, 34498.0, 34499.0, 34500.0, 34501.48, 34502.0, 34503.0, 34504.0, 34505.01, 34506.0, 34507.0, 34508.0, 34507.44, 34510.0, 34512.0, 34514.0, 34515.01, 34515.87, 34517.0, 34518.0, 34519.0, 34520.0, 34521.0, 34522.0, 34523.0, 34524.0, 34525.0, 34526.28, 34527.0, 34528.0, 34528.61, 34530.0, 34526.0, 34532.83, 34535.0, 34536.0, 34538.0, 34540.0, 34540.8, 34542.0, 34543.0, 34544.0, 34545.0, 34546.0, 34546.24, 34548.0, 34548.8, 34550.0, 34549.0, 34552.0, 34551.0, 34554.6, 34555.0, 34556.0, 34549.5, 34558.8, 34553.0, 34560.0, 34561.8, 34561.2, 34561.0, 34564.0, 34565.0, 34566.0, 34567.32, 34567.0, 34568.0, 34569.6, 34570.0, 34571.0, 34572.0, 34573.0, 34573.03, 34576.0, 34577.4, 34577.0, 34579.2, 34580.0, 34579.0, 34580.4, 34583.0, 34584.0, 34585.0, 34586.0, 34587.0, 34588.32, 296732.0, 34590.0, 34591.7, 34589.27, 34594.0, 34596.0, 34597.0, 34600.0, 34601.0, 34602.0, 34603.05, 34604.0, 34605.0, 34606.0, 34603.92, 34608.0, 34610.0, 34611.0, 34611.2, 34612.0, 34614.0, 34615.0, 34613.64, 34617.0, 34618.0, 34620.0, 34621.0, 34623.0, 34624.0, 34624.24, 34626.0, 34627.0, 34625.64, 34629.0, 34628.0, 34631.0, 34632.0, 34636.8, 34636.0, 34637.0, 34639.53, 34640.0, 34642.0, 34643.0, 34644.0, 34645.0, 34646.16, 34646.0, 34648.0, 34646.27, 34650.0, 34651.0, 34652.0, 34650.24, 34654.0, 34655.0, 34656.0, 34657.2, 34658.0, 34659.84, 34659.0, 34661.0, 34657.0, 34663.0, 34660.0, 34665.6, 34666.0, 34667.0, 34668.0, 34669.0, 34670.0, 34666.56, 34672.0, 34673.0, 34674.0, 34675.0, 34676.52, 34677.84, 34676.0, 34679.0, 34680.0, 58995.3, 34682.76, 34682.0, 34684.0, 34685.0, 34686.57, 34687.0, 34688.0, 34685.52, 34689.25, 34691.0, 34692.0, 34689.6, 34694.0, 34686.0, 34693.0, 34694.5, 34698.0, 34699.11, 34700.0, 34701.0, 34699.0, 34703.0, 34704.0, 34705.0, 34705.08, 34707.0, 34709.0, 34710.0, 34711.0, 559000.0, 34712.0, 34714.8, 34715.0, 34716.0, 34717.32, 34715.2, 34719.0, 34720.0, 34713.0, 34722.0, 34723.0, 34725.0, 34726.0, 34727.0, 34728.0, 34726.03, 34729.0, 34731.72, 34730.0, 34733.0, 34734.0, 34733.5, 34736.0, 34737.83, 34737.0, 34731.0, 34740.0, 34741.98, 34741.73, 34742.4, 34743.0, 34745.0, 34746.0, 34747.0, 34747.68, 34749.44, 34750.0, 34749.0, 34752.0, 34753.0, 34755.0, 34756.0, 34757.0, 34759.0, 34760.0, 34761.12, 34762.0, 296907.0, 34764.0, 34765.0, 34766.87, 34762.61, 34768.0, 34770.0, 34771.2, 34772.0, 34771.0, 34771.04, 34775.0, 34776.0, 34777.0, 34778.0, 34777.6, 34780.0, 34774.46, 34782.0, 59015.0, 34784.0, 34785.0, 34786.0, 34786.25, 34788.0, 34789.0, 34790.0, 34791.0, 34792.0, 34793.0, 34797.0, 34798.0, 34798.32, 34800.0, 34802.0, 34806.0, 34807.0, 34809.0, 34811.0, 34812.27, 34813.0, 34812.0, 34813.45, 34815.0, 34818.0, 34819.2, 34820.0, 34819.0, 34822.04, 34823.4, 34823.0, 34824.0, 34825.0, 34826.76, 34827.0, 34828.0, 34830.0, 34826.0, 34832.0, 34833.0, 34834.56, 34835.55, 34836.0, 34838.0, 34839.0, 34840.0, 34840.32, 34842.0, 34843.0, 59028.96, 34845.0, 34846.0, 34847.52, 34848.0, 34847.0, 34850.0, 34851.0, 34852.0, 34853.0, 34850.58, 59029.0, 297000.0, 34856.0, 34860.0, 34861.0, 34863.0, 34864.0, 34865.0, 297009.0, 34866.0, 34868.83, 34869.0, 34868.0, 34871.44, 34872.0, 34873.0, 34874.0, 34875.0, 34876.0, 34874.98, 34876.56, 34879.0, 34880.0, 34881.96, 34882.0, 34883.0, 34881.0, 34885.0, 34884.0, 34887.0, 34888.0, 34881.16, 34890.0, 34892.0, 34893.0, 34895.0, 34896.0, 34896.6, 34899.0, 34900.0, 34902.0, 34902.4, 34904.0, 34905.0, 34906.0, 297050.0, 34908.0, 34907.0, 34910.0, 34911.0, 34914.0, 34917.0, 34918.0, 34919.0, 34920.0, 34921.0, 34921.93, 34923.0, 34924.0, 34925.44, 34925.0, 34927.0, 34928.12, 34929.0, 34931.0, 34932.0, 34931.44, 34934.2, 34934.0, 34936.0, 34937.0, 34938.0, 34939.0, 34940.0, 34942.0, 34944.0, 34946.0, 34947.82, 34946.5, 34949.0, 34950.0, 34951.0, 297093.0, 34953.36, 34952.0, 34955.0, 34956.0, 34957.68, 34953.0, 34959.0, 34960.0, 34961.0, 34962.0, 34959.24, 34964.0, 34959.72, 34966.0, 34967.0, 34968.0, 34969.92, 34970.0, 34971.0, 34972.0, 34965.0, 34974.0, 34975.0, 34976.0, 34976.87, 34977.0, 34979.32, 34980.0, 34977.84, 34982.21, 34983.0, 34984.8, 34984.0, 34985.0, 34987.0, 34986.6, 34982.0, 34990.0, 34986.0, 34992.0, 34985.6, 34994.0, 34995.21, 34996.08, 34995.0, 34996.0, 34999.0, 35000.0, 35001.0, 35002.0, 35000.99, 35004.0, 35005.0, 35006.0, 35006.4, 35008.0, 35009.0, 35010.0, 35011.0, 35013.51, 35013.0, 35015.0, 35016.0, 35017.68, 35018.0, 35019.0, 35020.0, 35021.0, 35022.0, 35023.0, 35024.0, 35019.6, 35023.8, 35027.0, 35027.62, 35027.2, 35028.0, 35031.0, 35025.0, 35033.0, 35027.28, 35035.0, 35036.16, 35036.0, 35037.0, 35038.0, 35039.04, 35040.0, 35041.44, 35041.52, 35043.0, 35045.0, 297188.0, 35041.0, 35048.0, 35049.0, 35050.0, 35051.04, 35051.0, 35053.0, 35052.0, 35055.0, 297200.0, 35054.0, 35058.52, 35058.0, 35060.0, 35059.0, 35062.0, 35063.0, 35064.0, 35065.0, 35066.0, 35067.6, 35068.8, 35069.0, 35070.0, 35067.48, 35073.0, 35073.69, 35075.0, 35076.0, 35077.0, 35078.4, 35079.0, 35080.0, 35081.0, 35084.0, 35084.63, 35086.0, 35087.0, 35088.0, 35089.0, 35089.43, 35090.0, 35092.0, 35091.72, 35094.84, 35094.0, 35096.0, 35095.0, 35098.0, 35096.26, 35100.0, 35102.0, 35103.0, 35105.0, 35106.0, 35107.8, 35109.0, 35110.0, 35111.0, 35112.0, 35111.28, 35115.48, 35116.0, 35118.0, 35120.0, 35121.0, 35122.0, 35123.0, 35124.0, 35125.0, 35126.0, 59084.4, 35128.0, 35129.59, 35130.0, 35131.0, 35132.0, 35133.0, 35134.0, 35135.0, 35136.0, 35137.0, 35138.4, 35140.0, 35141.0, 35142.0, 35143.0, 35144.24, 35144.0, 35145.0, 35147.4, 35148.0, 35148.32, 35150.0, 35144.4, 35152.0, 35153.0, 35154.0, 35146.0, 35156.0, 35157.0, 35158.0, 35155.0, 35160.0, 35162.0, 35163.0, 35164.0, 59092.8, 35167.0, 35168.0, 35170.01, 35170.0, 35172.0, 35172.8, 35172.08, 35175.0, 35175.96, 35173.0, 35179.0, 35180.0, 35181.0, 35182.08, 35179.15, 35184.0, 35190.0, 35190.48, 35192.4, 35193.0, 35192.0, 35194.0, 35196.0, 35197.0, 35198.16, 35199.0, 35200.0, 35193.6, 35195.0, 35204.0, 35205.0, 35206.0, 35206.96, 35208.0, 35209.0, 35210.0, 35211.36, 35211.0, 35212.8, 35214.0, 35214.4, 35214.08, 35217.0, 35212.0, 35211.96, 35220.0, 35221.0, 35216.0, 35215.0, 35219.0, 35220.82, 35226.0, 35226.71, 35228.0, 35229.0, 35230.08, 35231.0, 35232.0, 35230.33, 35230.0, 35235.0, 35236.0, 35231.04, 35238.0, 35239.0, 35240.0, 297385.0, 35241.0, 35243.0, 35244.0, 35245.0, 35246.0, 35238.48, 35248.86, 35250.0, 35251.2, 35251.0, 35253.0, 35251.32, 35255.0, 35256.0, 35257.0, 35254.0, 35259.0, 35260.32, 35260.0, 35262.84, 35263.0, 35264.0, 35265.0, 35261.0, 35267.0, 35268.0, 35269.0, 35270.0, 35271.0, 35272.0, 35271.38, 35274.0, 35275.0, 35276.0, 35277.0, 35276.8, 35280.0, 35281.0, 35282.0, 35283.0, 35284.0, 35285.12, 35286.0, 35287.0, 35287.2, 35289.0, 35290.0, 35289.6, 35292.0, 35291.0, 35294.0, 35296.0, 35297.6, 35297.0, 35298.0, 35300.0, 35302.0, 35303.15, 35304.0, 35305.0, 35306.0, 35303.0, 35308.0, 35309.06, 35310.0, 35312.0, 35316.0, 35317.0, 35318.0, 35318.4, 35320.0, 35322.0, 35323.0, 35324.64, 35325.0, 35324.0, 35326.0, 35328.0, 35329.68, 35330.0, 35323.55, 35333.0, 35334.0, 35339.0, 35340.0, 35341.0, 35342.0, 35343.0, 35339.2, 35345.6, 35344.92, 35344.0, 35348.0, 35341.19, 35350.0, 35351.64, 35352.0, 35353.0, 35354.0, 35351.07, 297500.0, 35356.0, 35358.0, 35351.28, 35360.0, 35362.72, 35363.0, 35362.0, 35365.0, 35364.0, 35366.0, 35367.0, 35369.0, 35370.0, 35371.0, 35372.0, 35373.0, 35374.0, 35375.0, 35376.0, 35377.0, 35379.0, 35380.0, 35381.0, 35382.0, 35383.08, 35384.0, 35386.0, 35387.0, 35388.0, 35389.12, 35389.0, 35391.79, 35392.0, 35392.32, 35394.0, 35395.0, 35396.0, 35397.0, 35398.0, 35399.0, 35400.0, 35401.6, 35402.4, 35403.0, 35401.0, 35405.0, 35406.67, 35404.65, 35408.0, 35402.72, 35404.0, 297555.0, 35412.0, 35413.0, 35414.0, 35406.0, 35416.8, 35416.0, 35418.0, 35417.0, 35420.0, 35421.0, 35422.0, 35422.4, 35424.56, 35425.0, 35424.0, 35426.21, 35428.0, 35429.0, 35430.0, 35430.8, 35432.0, 35433.0, 35433.5, 35435.0, 35436.0, 35437.0, 35431.0, 35439.0, 35440.0, 35441.0, 35442.0, 35443.0, 35443.2, 35444.81, 35443.19, 35444.0, 35448.0, 35449.0, 35450.0, 35448.19, 35452.0, 35451.0, 35447.0, 35450.58, 35456.0, 35453.0, 35458.56, 35460.0, 35461.34, 35462.51, 35463.0, 35464.0, 35464.06, 35465.0, 35467.0, 35462.0, 35469.0, 35470.0, 35471.0, 35472.0, 35473.0, 35474.0, 35475.0, 35476.0, 35472.48, 35476.4, 35479.0, 35480.0, 35481.6, 35482.0, 35477.0, 35484.0, 35485.0, 35486.0, 35487.0, 35488.0, 35484.8, 35490.0, 35491.62, 35483.0, 35493.0, 35493.97, 35496.0, 35498.0, 35499.0, 35500.0, 35500.2, 35502.07, 35501.0, 35504.0, 35505.6, 35505.0, 35502.0, 35508.0, 35509.0, 35510.76, 35511.0, 35512.0, 35510.4, 35512.99, 35515.0, 35516.0, 35517.0, 35517.39, 35510.0, 35520.0, 35521.0, 35523.0, 35525.0, 35526.4, 35526.0, 35528.0, 35529.6, 35529.0, 35531.34, 35532.0, 35531.0, 35534.4, 35535.12, 35535.0, 35536.0, 35537.0, 35539.0, 35540.0, 35538.19, 35542.32, 35543.0, 35544.0, 35545.0, 35538.0, 35547.19, 35548.0, 35547.0, 35550.0, 35549.0, 35552.68, 35553.0, 35554.0, 35555.0, 35556.0, 35557.24, 35558.0, 35559.0, 35560.0, 35556.84, 35562.0, 35556.6, 35563.11, 35565.0, 35566.0, 35568.0, 35569.6, 35570.0, 59173.43, 35572.0, 35572.8, 35574.0, 35576.0, 35577.0, 35578.0, 35579.0, 35580.0, 35581.08, 35580.53, 35583.0, 35584.68, 35585.0, 35584.25, 35584.0, 35588.0, 35589.25, 35590.0, 35589.0, 35592.0, 35591.17, 35594.0, 35595.0, 35588.16, 35596.0, 35598.0, 35599.2, 35600.0, 35601.0, 35599.0, 35604.0, 35604.46, 35607.0, 35608.2, 35609.0, 35610.0, 35611.0, 35609.6, 35615.0, 35616.0, 35617.0, 35620.0, 35621.0, 35620.2, 35624.0, 35625.0, 35627.04, 35628.0, 35627.0, 35630.0, 35631.96, 35632.0, 35629.0, 35631.0, 35635.2, 35636.52, 35635.0, 35639.76, 35640.0, 35642.64, 35642.0, 35644.96, 35643.0, 35646.0, 35645.0, 35648.0, 35650.0, 35651.0, 35650.9, 35653.56, 35654.0, 35653.0, 35656.0, 35657.0, 35658.0, 35652.0, 35659.2, 35659.68, 35662.0, 35655.0, 35664.0, 35665.0, 35660.0, 35667.41, 35667.0, 35669.0, 35670.0, 559951.96, 35672.0, 35673.0, 35673.6, 35675.0, 35676.0, 35677.0, 35678.4, 35679.0, 35680.0, 35682.0, 35684.0, 35685.0, 35686.32, 35686.0, 35688.0, 35689.0, 35690.0, 35688.48, 35692.0, 35692.8, 35691.0, 35696.28, 35697.0, 35698.0, 35699.0, 35700.0, 35696.0, 35702.4, 35704.0, 35706.0, 35707.0, 35709.0, 35710.0, 35711.0, 35712.0, 35713.0, 560000.0, 35713.6, 35717.0, 35720.0, 35722.0, 35724.0, 35725.0, 35728.0, 35729.0, 35730.0, 35731.0, 35733.0, 35734.85, 35734.0, 35736.0, 35734.4, 35735.0, 35737.0, 35740.0, 35741.0, 35742.0, 35745.92, 35746.0, 35747.28, 35748.0, 35749.0, 35750.0, 35750.4, 35747.0, 35749.2, 35754.0, 35755.0, 35756.0, 35760.0, 35761.0, 35762.0, 35765.0, 35766.0, 35767.14, 35768.0, 35769.0, 35770.0, 35771.0, 35772.0, 35773.0, 35775.0, 35776.0, 35775.63, 35778.0, 35779.0, 35780.0, 35781.0, 35781.6, 35782.0, 35784.0, 35785.0, 35786.0, 35786.4, 35787.0, 35789.0, 35790.0, 35791.0, 35792.0, 35789.92, 35794.41, 35794.64, 35796.8, 35794.0, 35796.0, 35799.0, 35800.0, 35801.3, 35797.0, 35802.0, 35803.0, 35804.0, 35801.6, 35798.0, 35807.0, 35808.12, 35808.0, 35810.0, 35809.0, 35812.89, 35813.44, 35814.0, 35815.0, 35812.0, 35817.6, 35818.0, 35819.68, 35820.0, 35817.36, 35817.0, 35818.8, 35824.5, 35825.0, 35826.0, 35825.54, 35828.0, 35830.0, 35829.0, 35832.0, 35827.2, 35834.0, 35835.0, 35836.8, 35835.6, 35838.0, 35839.0, 35840.0, 35844.0, 35845.87, 35846.0, 35845.0, 35848.0, 35849.2, 35850.0, 35852.0, 35853.0, 35854.8, 35855.0, 298000.0, 35856.0, 35856.8, 35859.24, 35859.0, 35860.0, 35857.0, 35859.2, 35865.0, 35866.8, 35867.0, 35868.84, 35868.0, 35870.0, 35871.67, 35872.0, 35873.0, 35872.99, 35875.0, 35876.0, 35877.0, 35871.0, 35879.0, 35880.0, 35877.6, 35878.8, 35883.0, 35884.0, 35885.0, 35882.0, 35887.0, 35888.0, 35890.0, 35891.0, 35892.0, 35893.8, 35893.0, 35894.0, 35896.0, 35898.0, 35900.0, 35901.4, 35902.0, 35904.0, 35905.0, 35909.0, 35910.0, 35911.29, 35912.0, 35911.24, 35915.0, 35916.0, 35918.0, 35918.4, 35920.0, 35921.0, 35922.0, 35925.0, 35928.84, 35929.0, 35930.0, 35928.48, 35932.0, 35931.0, 35928.0, 35935.0, 35935.76, 35937.0, 35938.0, 35939.21, 35940.92, 35940.0, 35942.0, 35943.6, 35938.8, 35941.13, 35946.0, 35947.0, 35948.0, 35949.0, 35950.0, 35951.0, 35952.0, 35957.0, 35960.0, 35962.0, 35963.0, 35964.0, 35965.0, 35966.4, 35963.12, 35968.0, 35969.0, 35970.34, 35971.0, 35973.0, 35975.0, 35976.0, 35975.96, 35978.0, 35978.25, 35980.0, 35978.61, 35982.65, 35981.0, 35984.0, 35982.0, 35986.0, 35985.0, 35988.72, 35988.0, 35990.0, 35991.0, 35992.0, 35993.0, 35994.0, 35995.07, 35995.0, 35997.0, 35998.0, 35999.0, 36000.0, 36001.0, 36000.03, 36003.0, 36004.0, 36005.0, 36004.8, 36008.76, 36008.0, 36010.8, 36010.32, 36012.0, 36010.42, 36011.0, 36015.0, 36016.55, 36017.86, 36018.0, 36018.49, 36017.64, 36021.0, 36022.0, 36021.96, 36024.0, 36025.0, 36026.0, 36026.61, 36020.0, 36029.21, 36030.96, 36031.0, 36032.0, 36031.32, 36034.0, 36035.0, 36036.0, 36037.0, 560323.0, 36039.0, 36040.0, 36041.0, 36044.0, 36045.0, 36046.0, 36046.44, 36048.0, 36047.0, 36050.0, 36051.0, 36052.0, 36053.0, 36054.0, 36055.0, 36056.0, 36060.0, 36063.36, 36063.0, 36064.0, 36065.0, 36067.75, 36067.0, 36067.2, 36070.74, 36071.0, 36072.0, 36068.0, 36070.0, 36068.87, 36076.8, 36076.0, 36078.0, 36079.0, 36080.0, 36077.28, 36075.0, 36084.0, 36085.0, 36084.36, 36088.0, 36090.0, 36092.0, 36093.0, 36094.0, 36096.0, 36097.0, 36097.35, 36099.0, 36100.0, 36101.13, 5541120.0, 36103.0, 36104.24, 36105.0, 36106.26, 36107.5, 36108.0, 36109.0, 36110.0, 36111.0, 36112.0, 36113.0, 36109.44, 36115.0, 36116.0, 36118.0, 36120.0, 36120.72, 36122.0, 36123.0, 36124.8, 36125.0, 36124.0, 36126.0, 36128.0, 36129.0, 36130.0, 298275.7, 36132.0, 36133.0, 36134.37, 36134.0, 36135.58, 36136.0, 36137.0, 36136.79, 36139.22, 36140.0, 36142.0, 36144.0, 36145.0, 36144.36, 36148.0, 36148.7, 36150.0, 36152.64, 36153.0, 36154.44, 36155.0, 36156.0, 36157.93, 36154.0, 36159.0, 36160.0, 36162.0, 36163.0, 36163.4, 36164.0, 36166.0, 36167.0, 36168.0, 36169.16, 36170.0, 36171.0, 36172.0, 36171.2, 36174.0, 36175.0, 36176.0, 36177.0, 36178.0, 36173.0, 36180.0, 36176.4, 36182.0, 36183.0, 36184.8, 36185.0, 36186.0, 36187.0, 36188.0, 36189.0, 36190.0, 36184.0, 36192.0, 36193.0, 36194.0, 36195.0, 36196.0, 1084773.0, 36198.0, 36199.0, 36200.0, 298343.0, 36202.0, 36203.67, 36204.0, 36205.8, 36206.0, 36201.2, 36203.0, 36209.0, 36210.0, 36211.0, 36212.0, 36213.0, 36214.0, 36215.0, 36216.0, 36212.8, 36219.0, 36220.8, 36220.0, 36222.0, 36223.0, 36224.0, 36225.0, 36226.0, 36221.0, 36228.0, 36229.0, 36230.4, 36231.0, 36232.0, 36233.6, 36233.0, 36232.14, 36236.0, 36232.38, 36229.44, 36239.0, 36240.0, 36240.3, 36242.0, 36243.92, 36244.0, 36244.56, 36245.76, 36247.26, 36243.0, 36241.0, 36250.0, 36251.23, 36252.0, 36252.4, 36254.0, 36254.4, 36256.0, 36257.0, 36258.0, 36253.0, 36260.0, 36261.0, 36262.0, 36263.0, 36264.0, 36259.0, 36266.0, 36267.0, 36261.6, 36265.0, 36270.0, 36270.7, 36272.6, 36266.19, 36274.0, 36275.0, 36276.0, 36275.2, 36272.0, 36279.0, 36280.0, 36281.0, 36282.0, 36285.0, 36288.0, 36289.0, 36290.0, 36292.0, 36293.0, 36294.0, 36294.72, 36296.0, 36297.88, 36298.0, 36299.0, 36300.0, 36297.0, 36301.62, 36306.0, 36307.0, 36308.8, 36309.36, 36310.0, 36311.0, 36312.0, 36313.0, 36313.56, 36314.0, 36316.0, 36316.8, 36320.0, 36321.0, 36322.0, 36323.0, 36324.0, 36326.0, 36328.0, 36328.99, 36330.0, 36331.0, 36329.0, 36333.0, 36334.0, 36333.51, 36336.0, 36337.0, 36337.6, 36335.0, 36340.0, 36341.0, 36339.0, 36343.0, 36344.0, 36345.0, 36345.6, 36347.0, 36348.0, 36349.0, 36350.0, 36350.16, 36352.0, 36351.0, 36353.0, 36355.0, 36356.0, 36353.88, 36358.0, 36358.4, 36360.0, 36359.0, 36362.38, 36363.0, 36364.0, 36365.0, 36364.51, 36366.0, 36368.0, 36369.0, 36370.0, 36369.96, 36372.0, 36374.0, 36375.0, 36376.0, 36377.0, 36379.0, 36380.0, 36379.2, 36382.0, 36384.0, 36385.0, 36386.0, 36385.5, 36388.0, 36389.0, 36387.0, 36390.82, 36392.87, 36393.0, 36395.0, 36396.0, 36398.0, 36399.0, 36400.0, 36400.1, 36402.0, 36403.0, 36401.0, 36405.0, 36406.0, 36406.5, 36408.0, 36409.0, 36410.0, 36411.0, 36412.0, 36413.0, 36414.0, 36415.0, 36416.0, 36416.84, 36413.69, 36419.5, 36420.67, 36420.8, 36421.0, 36420.0, 36424.0, 36423.0, 36425.0, 36422.0, 36428.0, 36426.0, 36430.8, 36430.0, 36431.0, 36432.0, 36433.0, 36430.18, 36436.0, 36437.0, 36435.0, 36434.0, 36440.0, 36441.0, 36441.24, 36443.0, 36444.0, 36441.16, 36446.0, 36442.0, 36441.6, 36449.4, 36450.0, 36451.0, 36452.0, 36453.0, 36454.0, 36455.0, 36456.0, 36453.36, 36458.19, 36460.0, 36461.0, 36462.0, 36464.0, 36465.72, 36466.0, 36467.0, 36468.0, 36465.0, 36470.0, 36466.23, 36472.56, 36471.83, 36474.0, 36475.0, 36476.85, 36476.0, 36477.0, 36478.0, 36479.0, 36480.0, 36481.0, 36482.0, 36483.0, 36485.49, 36484.0, 36485.0, 36486.0, 36487.0, 36488.0, 36489.0, 36490.0, 36489.6, 36492.0, 36493.92, 36493.0, 36495.0, 36494.0, 36499.2, 36500.0, 36501.0, 36502.0, 36497.0, 36504.0, 36505.0, 36506.0, 36504.96, 36499.0, 36508.85, 36510.0, 36512.0, 36513.0, 36516.0, 36516.12, 36518.4, 36520.0, 36521.0, 36522.0, 36523.0, 36524.0, 36525.0, 36526.0, 36527.0, 36528.96, 36528.0, 36530.0, 36524.8, 36531.12, 36535.0, 36536.0, 36537.28, 36538.32, 36538.0, 36540.0, 36543.0, 36545.0, 36546.0, 36545.6, 36546.06, 36549.12, 36550.0, 36552.0, 36552.12, 36554.0, 36554.66, 36556.0, 36554.72, 36555.0, 36559.0, 36560.0, 36556.8, 36562.0, 36563.0, 36564.0, 36565.0, 36566.0, 36567.0, 36565.51, 36569.0, 36568.0, 36571.0, 36572.88, 36566.4, 36575.0, 36576.0, 36577.0, 36578.0, 36579.0, 36580.0, 36582.0, 36584.0, 36585.0, 36586.0, 36587.0, 36588.0, 36589.0, 36590.0, 36584.83, 36592.05, 36593.0, 36594.0, 36595.0, 36596.0, 36593.28, 36598.0, 36599.0, 36600.0, 36597.0, 36601.0, 36604.0, 36604.48, 36606.18, 36606.0, 36607.0, 36609.37, 36608.0, 36611.0, 36609.0, 36610.68, 36611.24, 36610.0, 36612.0, 36614.4, 36615.0, 36619.0, 36620.0, 36621.0, 36622.0, 36623.0, 36624.0, 36625.91, 36626.0, 36627.0, 36628.0, 36629.0, 36630.0, 36631.0, 36625.0, 36628.8, 36634.82, 36634.92, 36636.0, 36636.22, 36638.0, 36639.0, 36640.0, 36641.0, 36633.0, 36643.68, 36643.0, 36644.0, 36646.0, 36647.0, 36648.0, 36649.0, 36650.0, 36645.0, 36654.0, 36655.0, 36656.0, 36657.0, 298800.0, 36659.0, 36660.0, 36658.0, 36662.0, 36664.0, 36665.0, 36666.0, 36667.0, 36666.56, 36668.0, 36670.66, 36670.0, 36672.11, 36672.0, 36671.0, 36669.0, 36676.0, 36675.0, 36669.2, 36679.0, 36680.0, 36681.36, 36678.0, 36683.0, 36683.83, 36684.0, 36685.0, 36686.0, 36687.0, 36688.0, 36681.0, 36690.0, 36691.0, 36692.0, 36693.0, 36694.21, 36696.0, 36697.0, 36698.0, 36700.0, 36702.12, 36702.0, 36705.0, 36706.0, 36707.0, 36706.8, 36708.0, 36710.0, 36712.0, 561000.0, 36713.52, 36715.0, 36716.77, 36716.0, 36717.0, 36719.0, 36720.0, 36721.0, 36719.54, 36722.0, 36721.65, 36725.0, 36726.0, 36723.0, 36719.64, 36729.0, 36729.6, 36730.0, 36732.0, 36732.8, 36734.0, 36735.14, 36736.0, 36735.41, 36738.0, 36739.0, 36740.0, 36742.0, 36743.0, 36744.0, 36744.71, 36746.0, 36747.43, 36748.0, 36748.8, 36750.0, 36751.0, 36747.0, 36753.0, 36754.12, 36754.0, 298900.0, 36757.0, 36758.0, 36756.0, 36760.0, 36752.0, 36762.0, 36763.0, 36764.0, 36765.6, 36765.66, 36761.0, 36768.0, 36770.4, 36771.0, 36772.0, 36770.0, 36774.0, 36775.0, 36775.56, 36777.0, 36776.53, 36778.1, 36779.0, 36780.0, 36781.0, 36782.0, 36784.22, 36785.0, 36786.0, 36784.0, 36788.0, 36789.24, 36789.0, 36791.0, 36792.0, 36790.0, 36795.2, 36796.0, 36795.0, 36798.46, 36799.0, 36800.0, 36801.0, 36797.0, 36803.52, 36804.0, 36805.0, 36806.0, 36807.0, 36803.0, 36809.87, 36810.0, 36811.56, 36812.0, 36812.47, 36814.0, 36813.0, 36816.0, 36817.0, 36818.0, 36819.0, 36820.0, 36821.76, 36822.0, 36823.0, 36815.0, 36825.0, 36826.8, 36827.0, 36828.0, 36829.31, 36829.0, 36831.0, 36830.0, 36833.0, 36832.0, 36834.0, 36836.0, 36837.0, 36838.0, 36836.8, 36840.0, 36835.0, 36842.0, 36839.0, 36844.0, 36845.0, 36846.0, 36850.0, 36851.0, 36852.0, 36854.0, 36855.0, 299000.0, 36856.0, 36858.48, 36858.0, 36860.0, 36861.0, 36857.0, 36859.0, 36863.0, 36864.0, 36865.0, 36865.61, 36868.0, 36862.0, 36870.0, 36867.0, 36875.0, 36876.0, 36877.17, 36878.4, 36878.0, 36880.0, 36881.0, 36882.0, 36877.0, 36884.0, 36885.0, 36886.32, 36887.0, 36888.0, 36889.16, 299030.0, 36887.64, 36892.0, 36893.0, 36894.12, 36895.0, 36896.0, 36894.0, 36898.0, 36899.0, 36900.0, 36898.55, 36902.0, 36903.0, 36897.0, 36905.0, 36906.0, 36907.0, 36908.0, 36909.0, 36901.0, 36911.0, 36912.0, 36912.24, 36910.56, 36915.0, 36916.0, 36917.0, 36918.0, 36919.0, 36920.0, 36921.6, 36922.0, 36923.81, 36924.96, 36924.0, 36925.0, 36923.0, 36928.0, 36928.26, 36930.0, 36931.0, 36932.0, 36933.0, 36933.72, 36935.0, 36936.0, 36937.0, 36939.6, 36940.8, 36941.0, 36941.84, 36940.0, 36939.24, 36943.0, 36945.48, 36947.0, 36948.0, 36947.35, 36950.0, 36945.0, 36952.0, 36949.08, 36956.0, 36958.0, 36958.76, 36960.0, 36961.0, 36961.42, 36963.0, 36959.0, 36958.4, 36966.0, 36968.0, 36969.0, 36970.0, 36971.0, 36972.0, 36973.0, 36973.97, 36973.32, 36976.0, 36977.0, 36978.0, 36975.0, 36980.0, 36981.0, 36982.0, 36982.56, 36984.0, 36983.0, 36985.0, 36987.6, 36987.0, 36989.0, 36990.0, 36991.0, 36992.75, 36993.0, 36994.0, 36995.0, 36996.0, 36997.0, 36992.0, 36999.0, 37000.0, 36998.0, 37003.2, 37003.0, 37005.0, 37003.19, 37007.52, 37008.0, 37007.0, 37011.0, 37012.0, 37013.0, 37014.0, 37016.0, 37017.0, 37020.0, 37021.0, 37022.0, 37023.0, 37024.0, 37025.0, 37024.8, 37028.0, 37030.0, 37032.0, 37035.0, 37036.92, 37037.0, 37040.0, 37041.0, 37041.7, 37043.0, 37044.8, 37045.0, 37046.0, 37047.0, 37048.0, 37049.0, 37050.0, 37046.63, 37052.0, 37053.0, 37054.84, 37055.0, 37056.0, 299200.0, 299202.0, 37057.0, 37060.0, 37061.0, 37062.0, 37058.0, 37065.0, 37066.0, 37067.0, 37068.0, 37069.0, 37070.0, 37070.39, 37072.0, 37073.0, 37075.0, 37076.0, 37078.8, 37079.0, 37080.0, 37081.0, 37078.0, 37083.0, 37084.67, 37085.64, 37085.0, 37086.0, 37086.4, 37087.0, 37090.0, 37086.72, 37092.0, 37092.96, 37094.0, 37096.0, 37096.92, 37098.0, 37099.0, 37100.0, 37101.0, 37101.36, 37103.0, 37104.87, 37104.0, 37105.44, 37107.0, 37108.0, 37107.19, 37109.0, 37110.0, 37111.0, 37113.0, 37112.0, 37107.2, 37116.0, 37117.8, 37117.0, 37119.5, 37120.0, 37118.0, 37115.0, 37114.0, 37122.02, 37125.0, 37121.0, 37126.8, 37122.0, 37128.0, 37124.44, 37131.5, 299276.0, 37131.19, 37134.08, 37134.0, 37136.0, 37137.6, 37138.0, 37139.0, 37140.0, 37141.0, 37142.94, 37143.0, 37142.0, 37144.0, 37137.46, 37146.0, 37147.0, 37148.0, 37148.8, 37150.0, 37149.0, 37152.0, 37151.0, 37154.0, 37151.66, 37156.0, 37157.0, 37158.0, 37160.0, 37158.24, 37158.48, 37161.0, 37163.0, 37164.0, 37163.64, 37166.0, 37167.12, 37169.0, 37170.0, 37171.0, 37168.0, 37172.0, 37174.0, 37175.0, 37176.0, 37177.0, 37178.0, 37179.32, 37180.0, 37181.0, 37183.56, 37184.0, 561472.0, 37184.4, 37186.0, 37188.0, 37189.56, 37189.0, 37190.0, 37191.0, 37192.0, 37190.4, 37195.2, 37191.36, 37197.0, 37200.0, 37202.0, 37203.36, 37204.0, 37203.24, 37206.0, 37207.0, 37209.0, 37209.69, 37211.0, 37212.0, 561500.0, 37209.6, 37215.0, 37215.51, 37217.76, 37213.0, 37217.64, 37220.0, 37221.0, 37222.0, 37223.0, 37224.0, 37225.0, 37226.0, 37227.8, 37228.8, 37229.0, 37230.0, 37231.0, 37232.0, 37233.0, 37234.0, 37235.0, 37236.96, 37238.0, 37239.0, 37240.08, 37240.0, 37242.0, 37244.0, 37245.0, 37248.0, 37248.14, 37250.0, 37252.0, 37253.0, 37254.0, 37252.8, 37255.0, 37257.0, 37258.0, 37260.0, 37261.0, 37262.0, 37264.0, 37265.0, 37266.0, 37267.0, 37270.5, 37270.0, 37270.98, 37272.0, 37273.6, 37273.0, 37270.69, 37275.0, 37277.76, 37277.0, 37279.0, 37276.0, 37281.38, 37281.0, 37280.0, 37284.0, 37282.0, 37286.39, 37287.0, 37283.0, 37289.0, 37290.0, 37291.0, 37286.0, 37293.0, 37294.4, 37294.0, 37296.0, 37295.0, 37297.78, 37299.0, 37300.0, 37297.0, 37303.82, 37302.0, 37304.3, 37304.0, 37305.0, 37303.0, 37308.0, 37309.8, 37310.0, 37311.0, 37313.0, 37313.16, 37314.0, 37315.0, 37316.0, 37312.0, 37310.54, 37316.64, 37320.0, 37321.0, 37322.0, 37323.0, 37325.0, 37326.0, 37327.0, 37328.0, 37330.56, 37330.0, 37332.0, 37333.0, 37334.0, 37331.0, 37336.0, 37337.0, 37330.32, 37339.0, 37340.0, 37341.0, 37342.0, 37343.0, 37344.0, 37342.5, 37346.0, 37345.0, 37348.0, 37349.88, 37350.0, 37343.04, 37352.0, 37353.0, 37355.0, 37356.0, 37357.0, 299500.0, 37358.0, 37360.0, 37361.0, 37362.0, 37363.87, 37363.0, 37365.0, 37366.0, 37364.0, 37368.0, 37369.0, 37370.0, 37367.87, 37372.0, 37373.0, 37374.0, 37375.0, 299520.0, 37377.0, 37377.6, 37378.0, 37380.0, 37381.0, 37382.0, 37383.0, 37384.0, 37379.0, 37387.0, 37389.0, 37390.0, 37389.6, 37392.0, 37391.0, 37394.0, 37395.0, 37397.36, 37398.0, 37399.0, 37400.0, 37397.0, 37398.4, 37397.37, 37404.0, 37405.0, 37406.94, 37407.24, 37408.0, 37407.0, 37410.0, 37411.0, 37412.0, 37409.0, 37408.36, 37415.51, 37416.0, 37417.0, 37415.0, 37419.0, 37420.0, 37421.0, 37423.0, 37424.0, 37425.0, 37423.99, 37427.0, 299572.0, 37428.0, 37429.0, 37431.0, 37432.0, 37431.24, 37429.2, 37435.44, 37435.0, 37430.0, 37438.97, 37439.0, 37440.0, 37441.0, 37442.0, 37441.68, 37444.0, 37443.0, 37446.0, 37447.0, 37448.0, 37449.0, 37450.0, 37451.0, 37452.0, 37453.0, 37452.96, 37454.56, 37454.0, 37450.43, 37458.0, 37459.0, 37460.8, 37460.0, 37462.0, 37461.0, 37464.0, 37460.81, 37466.0, 37465.0, 37468.0, 37469.0, 37472.0, 37474.0, 37475.0, 37476.0, 37476.25, 37477.0, 37480.0, 37481.0, 37482.0, 37483.0, 37484.0, 37481.6, 37488.0, 37489.0, 37490.0, 37488.87, 37492.08, 37492.0, 37493.0, 37495.0, 37494.0, 37492.6, 37498.0, 37499.0, 37500.0, 37501.0, 37502.0, 37502.4, 37504.0, 37503.0, 37506.0, 37506.48, 37508.4, 37509.0, 37508.0, 37511.4, 37512.13, 37511.64, 37514.0, 37512.0, 37516.0, 37515.0, 37518.0, 37516.8, 37520.0, 37520.52, 37521.12, 37523.0, 37524.0, 37525.0, 37526.0, 37521.3, 37528.4, 37529.0, 37530.0, 37528.0, 37532.0, 37534.44, 37535.16, 37536.0, 37534.66, 37537.5, 37538.34, 37540.08, 37540.0, 37542.0, 37535.54, 37544.0, 37542.83, 37543.0, 37545.0, 37548.0, 37549.4, 37550.0, 37551.0, 37549.0, 37552.0, 37554.69, 37555.0, 37556.0, 37557.0, 37558.0, 37554.0, 37560.0, 37553.0, 37562.0, 37563.0, 37564.0, 37565.0, 37565.16, 37567.0, 37568.0, 37564.8, 37570.0, 37566.0, 37572.0, 37573.0, 37571.0, 37575.0, 37576.0, 37577.0, 37578.0, 37574.0, 37580.0, 37581.0, 37582.0, 37583.0, 37584.0, 37585.0, 37586.0, 37587.0, 37588.0, 37589.0, 37590.0, 37591.0, 37585.6, 37592.0, 37593.0, 37594.0, 37595.0, 37596.0, 37592.4, 37598.0, 37599.0, 37600.0, 37601.0, 37602.0, 37604.0, 37605.0, 37606.0, 37607.0, 37608.0, 37609.36, 37610.0, 37603.56, 37611.0, 37609.0, 37614.95, 37614.0, 37616.0, 37615.36, 37618.0, 37615.0, 37620.0, 37621.0, 37622.0, 37622.69, 37624.0, 37625.0, 37626.0, 37627.0, 37628.0, 37629.5, 37630.0, 37630.16, 37632.68, 37632.0, 37629.0, 37635.0, 37636.0, 37636.59, 37638.94, 37639.0, 37640.0, 37633.0, 37643.0, 37644.0, 37646.64, 37647.0, 37648.0, 37649.0, 37650.0, 37651.0, 37652.0, 37653.0, 37654.0, 37655.0, 37656.0, 37654.5, 37658.0, 37657.0, 37660.0, 37661.36, 37661.0, 37663.0, 37664.0, 37665.0, 37666.0, 37667.0, 37668.0, 37669.0, 37670.0, 37671.0, 37672.3, 37668.8, 37674.0, 37675.0, 37670.4, 37676.0, 37676.04, 37677.0, 37680.0, 37681.0, 37679.76, 37682.0, 37678.48, 37685.0, 37686.0, 37687.0, 37688.0, 37689.0, 37690.0, 37690.4, 37692.0, 37692.6, 37691.0, 37695.0, 37696.0, 37697.0, 37700.0, 37701.36, 37702.0, 37703.0, 37704.0, 37705.0, 37703.37, 37708.0, 37710.0, 37710.4, 562000.0, 37712.0, 37713.0, 37716.0, 37717.2, 37718.0, 37719.0, 37720.0, 37721.0, 37722.0, 37724.0, 37725.0, 37726.0, 37727.0, 37728.0, 37729.0, 37728.17, 37731.96, 37731.0, 37733.6, 37734.0, 37733.0, 37736.0, 37737.0, 37738.0, 37739.0, 37732.83, 37740.0, 37741.15, 37742.0, 37743.0, 37744.0, 37745.4, 37745.0, 37747.0, 37748.0, 37741.0, 37750.0, 37752.0, 37754.0, 37755.0, 37754.16, 37757.0, 37758.0, 37759.0, 37760.0, 37760.48, 37761.0, 37763.59, 37764.0, 37765.0, 37766.0, 37767.0, 37768.0, 37769.0, 37770.0, 37769.76, 37772.0, 37773.0, 37774.0, 37775.0, 37776.0, 37777.0, 37776.19, 37772.8, 37780.0, 37781.0, 37782.12, 37782.0, 37785.0, 37786.0, 37788.0, 37790.0, 37791.0, 37792.0, 37792.39, 37793.0, 37793.6, 37794.0, 37794.39, 37795.0, 37799.0, 37800.0, 37797.0, 37802.0, 37803.0, 37804.64, 37804.0, 37805.0, 37807.0, 37808.0, 37809.0, 37810.0, 37811.0, 37812.0, 37812.32, 37814.0, 37815.0, 37816.0, 37815.12, 37818.0, 37813.12, 37820.0, 37817.0, 37822.2, 37823.0, 37824.0, 37825.0, 37826.38, 37822.0, 37823.25, 37827.72, 37829.0, 37832.0, 37832.88, 299978.0, 37835.0, 37836.0, 37838.0, 37839.0, 37840.0, 37842.0, 37843.0, 37845.0, 37846.0, 37847.64, 37848.0, 37847.0, 37850.0, 37851.0, 37852.0, 37853.14, 37849.0, 299999.0, 300000.0, 37856.0, 37858.0, 37858.86, 37860.0, 37861.0, 37862.0, 300002.0, 37864.04, 300009.0, 37866.0, 37867.5, 37867.0, 37869.9, 37869.0, 37870.0, 37867.92, 37873.68, 37874.0, 37875.0, 37876.0, 37877.0, 37873.0, 37872.0, 37878.96, 37876.8, 37882.0, 37884.0, 37885.0, 37886.0, 37888.0, 37889.0, 37890.0, 37891.0, 37892.0, 37893.0, 37894.0, 37895.0, 37896.0, 37897.0, 37898.0, 37899.0, 37900.0, 37896.44, 37895.44, 37903.78, 37903.0, 37898.64, 37906.0, 37907.0, 37908.0, 37909.0, 37910.0, 37911.0, 37908.36, 37912.0, 37915.89, 37916.0, 37916.82, 37918.0, 37918.4, 37920.0, 37921.0, 37919.0, 37918.56, 37923.17, 37925.0, 37926.84, 37922.0, 37928.0, 37931.0, 37932.0, 37936.0, 37938.24, 37939.08, 37939.0, 37941.0, 37940.0, 37943.0, 37944.0, 37942.0, 37947.0, 37948.0, 37949.76, 37950.0, 37951.0, 37947.84, 37952.0, 37954.0, 37955.83, 37956.0, 37957.0, 37958.04, 37959.49, 37960.0, 37959.0, 37961.0, 37963.0, 37963.38, 37965.0, 37966.0, 37967.0, 37968.0, 37968.72, 37970.0, 37970.01, 37972.0, 37965.24, 37975.39, 37976.0, 37975.0, 37978.0, 37978.65, 37980.8, 37981.0, 37980.0, 37983.0, 37984.76, 37984.0, 37984.49, 37986.0, 37987.0, 37988.0, 37989.0, 37990.0, 37992.65, 37992.0, 37993.17, 37993.87, 37994.0, 37997.0, 37996.0, 37999.0, 38000.0, 38001.0, 38001.6, 38002.0, 38004.0, 37998.0, 38006.0, 38003.0, 38008.0, 38009.76, 38010.0, 38012.48, 38013.0, 38014.0, 38015.0, 38016.0, 38017.0, 38019.0, 38020.0, 38021.0, 38022.4, 38022.0, 38024.0, 38025.0, 38023.44, 38027.0, 38028.0, 38029.0, 38025.6, 38033.0, 38034.0, 38035.0, 38038.0, 38039.0, 38040.0, 38041.0, 38042.0, 38043.2, 38043.0, 38044.94, 38046.0, 38048.0, 38049.0, 38050.0, 38051.0, 38052.0, 38054.0, 38055.0, 38056.0, 38060.0, 38064.0, 38069.0, 38070.0, 38071.28, 38073.0, 38075.0, 38076.0, 38081.88, 38081.0, 38083.0, 38084.8, 38085.0, 38085.12, 38086.65, 38088.0, 38089.2, 38090.0, 38091.0, 38092.0, 38094.0, 38096.0, 38097.0, 38098.0, 38096.76, 38100.0, 38101.0, 38102.0, 38100.12, 38104.0, 38105.0, 38105.6, 38107.0, 38109.0, 38110.0, 38111.0, 38112.0, 38110.5, 38115.0, 38116.0, 38117.0, 38118.0, 38119.0, 38120.0, 38116.9, 38123.0, 38124.0, 38125.0, 38126.0, 38127.0, 38128.0, 38129.0, 38130.0, 38131.0, 38132.0, 38133.0, 38134.8, 38134.0, 38136.0, 38138.0, 38140.0, 38142.0, 38143.0, 38145.0, 38146.0, 38147.32, 38148.0, 38147.0, 38150.0, 38147.2, 38152.0, 38150.4, 38151.0, 38154.0, 300300.0, 38157.0, 38158.0, 38159.0, 38160.0, 38161.0, 38163.0, 38164.0, 38166.0, 38168.0, 38169.6, 38170.0, 38171.0, 38172.0, 38174.21, 38175.0, 38174.0, 38177.46, 38176.0, 38177.0, 38180.0, 38178.0, 38181.0, 38183.0, 38184.0, 38179.0, 38186.0, 38187.64, 38188.0, 38188.8, 38190.0, 38191.0, 38192.4, 38193.83, 38192.0, 38193.0, 38196.0, 38194.62, 38197.0, 38191.44, 38200.0, 38200.08, 38202.91, 38199.0, 38204.0, 38203.0, 38206.48, 38207.0, 38208.0, 38209.0, 38210.0, 38211.0, 38208.23, 38209.6, 38214.44, 38215.0, 38216.0, 38217.0, 38218.0, 38213.0, 38220.0, 38221.15, 38222.0, 38219.0, 38224.0, 38226.0, 38227.0, 38228.0, 38230.0, 38231.0, 38232.0, 38233.0, 38234.0, 38235.6, 38236.68, 38237.0, 38238.48, 38238.0, 38240.0, 38241.0, 38242.0, 38242.87, 38244.0, 38245.0, 38245.21, 38241.84, 38248.0, 38249.59, 38250.0, 38251.2, 38251.0, 38252.0, 38254.0, 38255.0, 38256.0, 38257.0, 38258.0, 38258.99, 38260.0, 38261.0, 38262.99, 38259.0, 38264.0, 38265.0, 38266.0, 38267.0, 38268.0, 38269.0, 38270.0, 38271.0, 38272.0, 38269.33, 38274.0, 38275.0, 38276.0, 38274.08, 38277.0, 38279.56, 38280.0, 38279.0, 38282.0, 38281.41, 38284.02, 38285.0, 38284.86, 38288.0, 38289.79, 38290.0, 38291.28, 38292.8, 38293.0, 38293.95, 38292.0, 38296.0, 38297.0, 38295.6, 38299.92, 38300.0, 38299.0, 38295.0, 38303.0, 38304.0, 38305.0, 38302.0, 38307.6, 38309.0, 38310.0, 38311.0, 38312.0, 38313.0, 38314.0, 38313.6, 38316.0, 38315.0, 38318.0, 38319.0, 38320.0, 38321.0, 38322.0, 38323.0, 38324.0, 38323.74, 38325.0, 38327.0, 38328.0, 38330.0, 38331.0, 38332.0, 38334.0, 38334.4, 38335.0, 38337.0, 38336.0, 38339.33, 38340.0, 38341.0, 38338.8, 38343.72, 38338.0, 38345.0, 38346.0, 38347.92, 38348.0, 38349.48, 38350.0, 38350.8, 38352.0, 38353.0, 38347.0, 38355.0, 38356.0, 38357.81, 38358.0, 38358.11, 38360.0, 38355.2, 38362.0, 38363.0, 38364.92, 38365.0, 38364.0, 38362.5, 38368.0, 38369.0, 38370.0, 38363.12, 38372.0, 38373.26, 38373.0, 38375.0, 38376.0, 38377.0, 38376.12, 38379.36, 38380.0, 38381.76, 38382.0, 38383.0, 38384.0, 38385.0, 38386.0, 38387.0, 38388.0, 38389.0, 38390.0, 38393.0, 38395.0, 38396.0, 38397.0, 38396.19, 38399.0, 38400.0, 300544.8, 38402.0, 38400.83, 38404.0, 38405.0, 38406.0, 38407.0, 38408.0, 38409.0, 38410.0, 38411.0, 38412.0, 38412.67, 38413.0, 38415.0, 38416.56, 38417.0, 38418.0, 38416.0, 38420.0, 38421.0, 38422.0, 38417.6, 38424.0, 38425.0, 38426.0, 38427.0, 38428.06, 38429.0, 38430.0, 38431.0, 38428.0, 38433.0, 38434.0, 38436.0, 38437.43, 38438.0, 38438.4, 38440.0, 38439.0, 38442.0, 38443.56, 38444.0, 38443.36, 38446.08, 38445.0, 38448.0, 38446.0, 38450.0, 38451.96, 38452.96, 38453.0, 38455.0, 38456.0, 38459.0, 38460.0, 38459.2, 38461.0, 38463.0, 38464.0, 38465.0, 38466.0, 38468.88, 38468.0, 38469.12, 38470.0, 38471.0, 38472.0, 38473.0, 38474.0, 38471.74, 38476.0, 38476.36, 38479.0, 38480.0, 300625.0, 38481.0, 38483.0, 38484.0, 38482.39, 38478.0, 38487.0, 38488.8, 38489.0, 38490.0, 38491.0, 38492.0, 38493.25, 38494.0, 38493.06, 38496.0, 38495.0, 38498.0, 38499.0, 38500.0, 38496.6, 38502.0, 38496.88, 38504.59, 38504.0, 38506.0, 38508.0, 38510.0, 38513.0, 38515.0, 38516.0, 38517.6, 38519.0, 38520.0, 38521.6, 38522.0, 38523.0, 38521.0, 38525.0, 38527.0, 38528.0, 38530.0, 38531.0, 38532.0, 38533.0, 38534.0, 38535.0, 38536.0, 38530.59, 38538.0, 300683.0, 38540.0, 38541.0, 38542.0, 38540.52, 38544.0, 38542.4, 38546.0, 38548.0, 38549.64, 38550.0, 38549.0, 38552.0, 38549.6, 38554.0, 38555.0, 38556.0, 38558.0, 38559.57, 38559.8, 38560.0, 38561.0, 38562.0, 38563.0, 38564.0, 38559.0, 38566.0, 825000.0, 38568.0, 38570.0, 38571.0, 38572.0, 38573.79, 38574.0, 38575.0, 38576.0, 38577.0, 38576.88, 38579.0, 38580.0, 38581.0, 38581.52, 38583.0, 38584.0, 38585.0, 38586.0, 38578.0, 38589.0, 38591.0, 38592.0, 38592.28, 38594.0, 38595.0, 38596.8, 38597.0, 38596.0, 38599.0, 38600.0, 38594.4, 38598.0, 38599.8, 38604.0, 38605.0, 38606.28, 38606.0, 38608.0, 38609.0, 38610.0, 38611.19, 38611.0, 38613.21, 38613.0, 38615.0, 38616.0, 38617.0, 38612.0, 38612.03, 38620.0, 38621.0, 38622.0, 38623.0, 38624.0, 38625.0, 38626.0, 38625.83, 38628.0, 38628.96, 38630.0, 38630.4, 38632.0, 38633.0, 38634.0, 38635.0, 38636.0, 38634.96, 38638.0, 38639.37, 38640.0, 38642.37, 38642.0, 38644.0, 38642.76, 38646.0, 38646.4, 38648.0, 38647.0, 38650.0, 38651.0, 38645.17, 38645.0, 38654.0, 38655.5, 38656.0, 38656.21, 38657.0, 38652.0, 38660.0, 38660.97, 38661.0, 38663.52, 38663.0, 38664.0, 38666.0, 38667.0, 38665.0, 38668.0, 38670.0, 38671.0, 38672.0, 38674.88, 38675.0, 38676.0, 38677.0, 38678.0, 38678.64, 38680.0, 38683.0, 38684.0, 38685.0, 38686.2, 38687.0, 38688.0, 38689.0, 38690.0, 38691.0, 38689.44, 38692.0, 38693.0, 38694.0, 38695.0, 38696.4, 38697.0, 38699.0, 38700.0, 38696.0, 38702.78, 38703.39, 38702.0, 38698.0, 38706.0, 38707.0, 38708.0, 38709.0, 38710.0, 38705.0, 38712.0, 563000.0, 38714.0, 38715.0, 38716.0, 38717.0, 38718.0, 38719.0, 38720.0, 38720.59, 38721.0, 38722.0, 38724.0, 38720.64, 38725.0, 38727.0, 38727.12, 38729.64, 38729.0, 38731.0, 38732.0, 38733.6, 38734.0, 38735.0, 38736.0, 38730.0, 38738.0, 38732.92, 38740.0, 38743.0, 38744.0, 38745.0, 38746.0, 38748.0, 38750.0, 38751.0, 38752.0, 38750.4, 38754.0, 38755.0, 38756.0, 300900.0, 38753.0, 38759.04, 38760.0, 38754.94, 38762.0, 38764.0, 38766.0, 38767.0, 38768.0, 38769.89, 38770.0, 38771.2, 38772.0, 38773.0, 38769.12, 38768.04, 38771.0, 38777.0, 38778.0, 38774.14, 38780.0, 38780.04, 38782.0, 38781.24, 38784.0, 38785.0, 38779.0, 38787.96, 38788.0, 38789.0, 38790.0, 38786.0, 38792.0, 38791.0, 38794.0, 38795.4, 38796.0, 38797.0, 38798.0, 38799.0, 38800.0, 38801.0, 38802.0, 38803.0, 38806.0, 38808.0, 38808.43, 38811.2, 38812.0, 38813.0, 38814.95, 38814.0, 38813.46, 38816.0, 38817.0, 38819.0, 38820.0, 38821.0, 38822.0, 38824.0, 38825.0, 38826.24, 38826.0, 38826.12, 38829.0, 38829.77, 38827.0, 38832.0, 38833.92, 38834.0, 38833.0, 38835.0, 38836.0, 38838.0, 38837.0, 38840.4, 38840.0, 38841.0, 38843.77, 38844.0, 38845.0, 38846.0, 38847.0, 38848.0, 38849.98, 38850.0, 38849.0, 38851.92, 38853.0, 38854.4, 38854.0, 301000.0, 38856.0, 38858.69, 38859.0, 38860.0, 38861.0, 38862.0, 38858.0, 38855.0, 38865.0, 38865.24, 38866.0, 38868.0, 38869.0, 38870.0, 38871.0, 38872.0, 38873.0, 38868.12, 38875.08, 38875.0, 38876.0, 38878.0, 38879.87, 38880.0, 38881.0, 38875.2, 38883.0, 38884.15, 38884.0, 38885.0, 38886.0, 38887.16, 38888.76, 38888.0, 38889.96, 38892.0, 38893.0, 38894.04, 38894.0, 38896.0, 38891.0, 38898.0, 38890.0, 38900.0, 38901.0, 38902.0, 38903.0, 38904.0, 38905.0, 38906.0, 38902.5, 38908.0, 38910.0, 38912.0, 38913.0, 38914.0, 38915.0, 38916.11, 38916.0, 38916.8, 38919.0, 38920.0, 38921.0, 38917.0, 38918.4, 38924.0, 38925.0, 38926.0, 38927.0, 38928.0, 38929.0, 38930.0, 38931.0, 38934.0, 38935.0, 38937.0, 38938.0, 38939.04, 38940.0, 38941.0, 38942.0, 38943.0, 38937.6, 38945.0, 38946.0, 38939.0, 38948.0, 38949.84, 38950.0, 38951.0, 38952.0, 38953.0, 38954.0, 38954.91, 38956.0, 38951.4, 38958.85, 38959.0, 38960.0, 38958.0, 38962.32, 38958.4, 38964.0, 38965.0, 38963.9, 38963.0, 38968.0, 38969.0, 38970.81, 38968.32, 38972.96, 38972.0, 38974.0, 38975.0, 38976.0, 38977.42, 38977.75, 38979.0, 38980.0, 38981.0, 38979.19, 38978.0, 38984.16, 38985.0, 38986.0, 38987.67, 38988.0, 38987.0, 38990.0, 38990.47, 38991.0, 38993.16, 38994.47, 38995.0, 38996.16, 38997.2, 38993.0, 38998.0, 38992.0, 39000.0, 39000.31, 39001.0, 39002.0, 39003.0, 38997.98, 39007.0, 38999.0, 39009.0, 39010.0, 39011.76, 39012.0, 39013.0, 39014.0, 39015.0, 39016.0, 39014.4, 39020.0, 39021.0, 39020.08, 39022.8, 39024.0, 39028.0, 39029.8, 39030.94, 39031.0, 39032.0, 39033.66, 39035.0, 39036.0, 39036.96, 39039.46, 39040.0, 39041.0, 39042.0, 39043.0, 39044.0, 39047.5, 39048.0, 39050.0, 39051.0, 39052.0, 39053.76, 39053.0, 39055.0, 39054.0, 39057.93, 39058.92, 59865.6, 39060.0, 39061.0, 39062.0, 39062.4, 39063.0, 39064.0, 39063.29, 39067.0, 39068.0, 39068.66, 39070.0, 39068.76, 39072.0, 39072.33, 39074.0, 39075.0, 39076.0, 39077.0, 39080.0, 39081.6, 39082.0, 39083.0, 39084.0, 39083.2, 39085.0, 39086.92, 39088.0, 39086.0, 39090.0, 39087.0, 39091.0, 39093.0, 39093.9, 39089.21, 39096.0, 39097.0, 39099.0, 39100.0, 39102.0, 39103.0, 39104.0, 39103.37, 39106.1, 39107.33, 39108.0, 39107.0, 39110.0, 39107.28, 39112.4, 39113.0, 39114.0, 39115.0, 39116.0, 39116.3, 39112.0, 301260.0, 39120.0, 39121.0, 39119.22, 39122.0, 39124.0, 39125.0, 39126.4, 39125.25, 39126.0, 39128.16, 39129.0, 39130.0, 39131.0, 39132.0, 39128.0, 39134.0, 39133.0, 39133.2, 39138.0, 39140.0, 39141.0, 39142.0, 39144.0, 39145.0, 39146.0, 39147.0, 39148.0, 39149.0, 39150.0, 39151.0, 39148.8, 39145.6, 39153.0, 39152.0, 39156.0, 39157.34, 39157.0, 39158.0, 39160.0, 39154.0, 39162.0, 39161.2, 39164.0, 39161.0, 39166.64, 39167.0, 39168.0, 39166.0, 39170.0, 39171.0, 39168.52, 39173.0, 39174.88, 39175.16, 39176.0, 39175.17, 39178.0, 39179.16, 39180.0, 39179.0, 39182.0, 39183.0, 39177.0, 39185.0, 39184.44, 39187.2, 39187.0, 39181.0, 39190.0, 39191.0, 39192.0, 39193.0, 39190.26, 39195.0, 39196.0, 39188.0, 301342.0, 39200.0, 39202.0, 39204.0, 39205.0, 39206.0, 39208.0, 39209.0, 39210.0, 39212.0, 39213.0, 39215.0, 39216.0, 39217.0, 39218.0, 39217.44, 39220.0, 39221.0, 39223.0, 39224.0, 39225.0, 39226.07, 39227.0, 39228.0, 39229.0, 39230.28, 39231.0, 39230.0, 39233.0, 39234.0, 39233.16, 39236.0, 39237.0, 39235.0, 39239.33, 39240.0, 39241.0, 39239.0, 39243.0, 39246.0, 39247.0, 39248.0, 39249.0, 39250.0, 39251.0, 39252.0, 39253.0, 39254.0, 39252.68, 39256.0, 301400.0, 39257.0, 39258.0, 39260.0, 39259.0, 39263.0, 39264.0, 39266.0, 39267.0, 39268.0, 39270.0, 39270.4, 39272.3, 39272.37, 39274.0, 39275.0, 39276.0, 39270.24, 39278.0, 39279.0, 1350000.0, 39280.0, 1350002.0, 39283.0, 39284.0, 39285.0, 39286.0, 39287.5, 39288.0, 39289.0, 39290.4, 39291.0, 39290.0, 39293.0, 39292.5, 39295.0, 39295.8, 39297.0, 39295.32, 39298.35, 39300.0, 39301.0, 39302.64, 39301.5, 39303.0, 39296.0, 39306.96, 39306.0, 39309.0, 39310.0, 39311.0, 39312.0, 39313.0, 39315.0, 39315.14, 39317.0, 39318.0, 39320.0, 39320.64, 39321.48, 39321.0, 39324.0, 39325.0, 39326.3, 39327.0, 39329.0, 39330.0, 39331.0, 39332.0, 39332.8, 39334.55, 39335.0, 39336.0, 39333.0, 39338.0, 39339.0, 39340.8, 39340.0, 39342.48, 39343.0, 39347.2, 39348.0, 39347.0, 39350.0, 39351.25, 39352.0, 39353.0, 39351.0, 39354.0, 39351.4, 39356.0, 39349.0, 39359.0, 39360.0, 39361.0, 39362.0, 39355.0, 39364.0, 39365.76, 39366.88, 39367.44, 39367.0, 39366.0, 39370.0, 39372.04, 39372.0, 39374.0, 39375.0, 39374.4, 39375.52, 39378.0, 39380.0, 39382.0, 39383.0, 39384.0, 39385.0, 39386.0, 39387.0, 39386.39, 39388.25, 39390.0, 39391.0, 39392.0, 39393.64, 39388.0, 39395.0, 39396.0, 39397.35, 39398.0, 39399.0, 39400.0, 39399.22, 39397.0, 39403.0, 39404.0, 39405.0, 39402.0, 39407.0, 39408.0, 39409.0, 39410.0, 39411.0, 39412.0, 39413.0, 39414.0, 39409.96, 39416.0, 39416.64, 39418.8, 39419.0, 39420.0, 39417.6, 39422.0, 39423.8, 1088000.0, 39425.0, 39424.3, 301571.0, 39422.57, 39429.0, 39430.0, 39431.88, 39432.0, 39433.0, 39431.0, 39435.0, 39436.0, 39437.0, 39438.0, 39439.0, 39440.0, 39433.55, 39442.0, 39443.88, 39444.4, 39445.0, 39443.0, 39447.0, 39448.0, 39444.0, 39450.0, 39452.0, 39453.84, 39454.0, 39455.0, 39456.0, 39457.0, 39458.4, 39457.6, 39459.0, 39458.28, 39460.0, 39461.0, 39462.11, 301600.0, 39465.0, 39458.0, 39468.0, 39469.6, 39470.0, 39471.9, 39472.0, 39473.83, 39474.0, 39475.0, 39476.0, 39474.45, 39478.0, 39479.0, 39480.0, 39481.0, 39482.0, 39484.0, 39487.0, 39489.0, 39490.0, 39491.0, 39492.0, 39494.0, 39495.0, 39498.0, 39499.0, 39500.0, 39501.0, 39502.0, 39503.0, 39504.0, 39505.0, 39507.0, 39508.0, 39509.0, 39513.0, 39514.0, 39515.0, 39516.0, 39518.0, 39519.96, 39519.0, 39520.0, 39520.92, 39521.0, 39523.0, 39525.0, 39526.0, 39527.0, 39524.0, 39528.0, 39530.07, 39530.0, 39524.88, 39533.4, 39533.0, 39536.4, 39539.36, 39540.8, 39540.0, 39541.0, 39543.0, 39542.0, 39544.0, 39546.0, 39545.0, 39548.0, 39549.0, 39550.0, 39551.64, 39552.0, 39551.0, 39554.0, 39555.0, 39558.0, 39559.0, 39560.0, 39561.0, 39562.0, 39562.02, 39564.0, 39561.6, 39566.0, 39558.96, 39568.0, 39569.04, 39570.0, 39574.0, 39575.05, 39576.0, 39577.86, 39578.0, 39579.0, 39580.0, 39580.63, 39582.0, 39582.4, 39583.0, 39585.0, 39586.0, 39587.0, 39588.0, 39589.56, 39590.0, 39593.0, 39594.0, 39595.0, 39596.0, 39597.0, 39598.0, 39599.0, 39600.0, 39600.11, 39602.0, 39603.0, 39603.2, 39605.0, 39602.88, 39603.19, 39608.11, 39609.6, 39608.0, 39609.0, 39612.0, 39604.0, 39614.0, 39615.0, 39616.0, 39610.0, 39619.0, 39620.0, 39623.0, 39624.0, 39625.08, 39626.0, 39627.0, 39630.0, 39631.0, 39632.96, 39633.0, 39634.0, 39635.0, 39632.0, 39637.08, 39638.0, 39639.6, 39640.0, 39641.0, 39642.0, 39641.76, 39644.0, 39645.0, 39644.8, 39644.23, 39648.0, 39646.2, 39650.0, 39651.82, 39649.0, 39644.04, 39654.0, 39655.0, 39656.55, 39656.0, 39658.0, 39659.0, 39660.0, 39661.0, 39662.0, 39663.0, 39664.08, 39665.0, 39666.0, 39665.6, 39668.0, 39669.0, 39670.0, 39671.0, 39672.0, 39673.0, 59988.0, 39675.0, 39676.0, 39677.0, 39678.0, 39680.0, 39681.0, 39683.0, 39684.0, 39685.0, 39686.0, 39686.4, 39686.3, 39689.0, 39690.0, 39691.0, 39692.0, 39688.0, 39694.0, 39695.0, 39696.0, 39698.0, 39700.0, 39700.4, 39702.0, 39704.0, 39705.92, 39706.0, 39707.0, 39707.19, 39709.8, 39710.0, 39711.19, 39710.76, 39713.0, 39708.0, 39715.0, 39714.0, 564000.0, 39709.0, 301860.0, 39720.0, 39721.06, 39722.0, 39723.28, 39716.0, 39725.0, 39726.0, 39727.0, 39728.0, 39729.0, 39730.0, 39731.94, 39732.0, 39724.0, 39735.0, 39736.0, 39739.44, 39740.88, 39741.0, 39739.0, 39742.75, 39744.0, 39745.28, 39745.0, 39747.0, 39748.0, 39749.0, 39750.0, 39749.92, 39752.0, 39748.8, 39754.0, 39755.0, 39756.0, 39757.0, 39758.0, 39759.0, 39760.0, 39761.0, 39762.0, 39763.0, 39765.0, 39766.44, 39766.0, 39768.0, 39769.0, 39770.0, 39771.0, 39772.0, 39765.68, 39774.0, 39775.0, 39776.0, 39777.0, 39779.0, 39780.0, 39779.04, 39782.0, 39783.46, 39784.0, 39785.71, 39785.0, 39787.0, 39788.0, 39785.14, 39790.0, 39789.24, 39792.0, 39793.25, 39793.0, 39795.0, 39796.0, 39796.64, 39798.0, 39799.63, 39800.0, 39801.6, 39800.26, 39801.3, 39804.0, 39796.63, 39808.0, 39811.2, 39811.0, 39812.0, 39813.84, 301959.0, 39814.0, 39816.0, 39818.0, 39814.3, 39820.0, 39819.0, 39822.0, 39823.0, 39824.0, 39825.0, 39823.67, 39827.0, 39828.0, 39829.0, 39830.88, 39831.0, 39832.0, 39833.16, 39835.0, 39836.0, 39839.0, 39840.0, 301984.0, 39839.1, 39841.0, 39842.0, 39845.0, 39846.0, 39846.66, 39847.0, 39849.0, 39850.0, 39851.0, 39852.0, 39852.99, 39854.0, 39852.8, 302000.0, 39856.0, 39858.6, 39858.0, 39860.0, 39861.6, 39862.0, 39857.0, 39864.0, 39860.32, 39862.92, 39867.84, 39868.0, 39869.0, 39870.0, 60026.0, 39872.71, 39873.6, 39872.0, 39872.48, 39874.0, 39875.0, 39876.0, 39873.0, 39878.0, 39879.0, 39877.0, 39881.0, 39882.0, 39880.0, 39884.0, 39886.0, 39887.0, 39888.0, 302032.0, 39890.0, 39890.88, 39892.85, 39893.84, 39894.0, 39895.0, 39896.0, 39897.0, 39899.0, 39900.0, 39892.8, 39902.1, 39902.0, 39902.72, 39903.0, 39906.0, 39907.0, 39907.38, 39909.0, 39910.0, 39911.28, 39912.0, 39913.0, 39914.87, 39915.0, 39916.0, 39914.82, 39918.0, 39914.0, 39915.2, 39921.0, 39922.0, 39923.0, 39924.0, 39924.47, 39926.0, 39920.0, 60038.0, 39928.0, 39930.0, 39930.96, 39932.0, 39934.27, 39935.14, 39936.0, 39939.0, 39940.0, 39939.57, 39942.0, 39943.0, 39944.0, 39947.0, 39948.0, 39949.0, 39950.0, 39951.0, 39952.0, 39951.11, 39953.0, 39955.0, 39956.0, 39949.28, 39956.8, 39957.0, 39960.0, 39962.0, 39964.0, 39967.69, 39968.0, 39969.0, 39970.0, 39971.0, 39972.0, 39973.0, 39974.16, 39975.0, 39976.0, 39977.0, 39978.0, 39979.48, 39980.0, 39981.0, 39977.6, 39983.0, 39984.0, 39985.0, 39982.0, 39987.0, 39988.0, 39989.0, 39990.0, 60050.8, 39992.0, 39993.0, 39994.0, 39995.0, 39996.0, 39997.0, 39998.0, 39999.0, 40000.0, 40001.0, 40000.08, 40003.0, 40004.0, 40005.0, 40006.0, 39998.4, 40008.0, 40003.2, 40010.0, 40011.0, 40012.0, 40007.0, 40014.0, 40015.0, 40016.0, 40015.42, 40018.0, 40019.2, 40020.0, 40019.0, 40021.0, 40017.0, 60057.0, 40025.0, 40026.0, 40025.64, 40028.0, 40029.0, 40030.0, 40023.0, 40032.0, 40033.42, 40032.56, 40033.0, 40032.27, 40038.0, 40040.0, 40041.96, 40044.0, 40045.0, 40046.0, 40048.0, 40050.0, 40051.0, 40051.75, 40053.0, 40054.0, 40052.0, 40056.0, 40056.64, 40058.5, 40059.0, 40060.8, 40060.0, 40059.96, 40063.0, 302205.0, 40065.0, 40064.0, 60066.0, 40068.0, 40068.48, 40069.0, 40071.0, 40070.0, 40072.0, 40074.0, 40073.0, 40076.0, 40077.0, 40078.0, 40076.24, 40080.0, 40081.0, 40082.0, 40081.6, 40084.8, 40085.0, 40085.59, 40087.0, 40086.0, 40084.0, 40083.0, 40089.0, 302236.0, 40092.0, 40094.0, 40088.04, 40096.2, 40091.0, 40098.0, 40099.0, 40100.0, 40102.4, 40103.0, 40104.0, 40102.0, 40106.64, 40106.0, 40108.75, 40108.52, 40110.0, 40111.0, 40112.0, 40107.0, 40109.0, 40115.0, 40116.0, 40108.0, 40118.37, 40118.0, 40120.0, 40121.0, 40122.0, 40123.0, 40124.0, 40125.0, 40126.0, 40127.0, 40128.0, 40129.0, 40124.76, 40130.0, 40132.0, 40133.0, 40135.0, 40136.0, 40137.0, 40138.0, 40139.32, 40140.0, 40141.0, 40142.0, 40143.53, 40144.0, 40145.0, 40144.94, 40147.0, 40148.44, 40148.0, 40150.0, 40149.0, 40152.0, 40153.0, 40148.72, 40155.0, 40156.0, 40157.0, 40158.0, 40159.0, 40160.0, 40161.0, 40163.44, 40164.0, 40164.5, 40166.0, 40165.0, 40165.2, 40167.0, 40170.0, 40171.0, 40170.52, 40173.0, 40174.0, 40175.0, 40176.0, 40168.0, 40178.0, 40179.73, 40180.68, 40178.29, 40182.0, 40180.0, 40184.4, 40185.0, 40184.0, 40185.6, 40188.0, 40183.0, 40190.0, 40191.31, 40192.96, 40193.0, 40194.0, 40195.0, 40196.0, 40197.64, 40198.0, 40194.56, 40200.0, 40197.0, 40202.0, 40203.8, 40204.0, 40204.86, 40206.0, 40205.0, 40208.0, 40203.0, 40210.0, 40210.64, 40212.0, 40213.0, 40214.0, 40215.0, 40216.0, 40217.0, 40218.0, 40220.0, 40221.0, 40224.0, 40225.0, 40226.0, 40227.0, 40227.28, 40229.0, 40230.0, 40230.96, 40232.0, 40232.16, 40234.0, 40234.8, 40236.0, 40237.0, 40233.0, 40239.0, 40240.0, 40233.05, 40242.0, 40243.0, 40245.0, 40246.8, 40246.0, 40248.0, 40249.0, 40250.0, 40251.0, 60102.0, 40253.0, 40251.6, 40255.23, 40256.0, 40257.0, 40258.0, 40259.0, 40260.0, 40261.0, 40262.0, 40259.96, 40260.35, 40264.6, 40263.0, 40267.0, 40268.0, 40269.0, 40265.0, 40270.0, 40272.0, 40266.28, 40274.0, 40275.0, 40276.0, 40277.0, 40278.0, 40279.92, 40280.0, 40281.0, 320625.0, 40283.4, 40284.0, 40283.0, 40286.0, 40285.0, 40288.0, 40289.0, 40290.0, 40290.4, 40292.0, 40293.0, 40294.0, 40295.0, 40296.0, 40298.0, 40299.0, 40300.0, 40302.0, 40304.0, 40306.0, 40307.0, 40308.0, 40309.0, 40310.0, 40310.4, 40307.28, 40313.0, 40314.0, 40311.0, 40316.0, 40312.0, 40318.0, 40320.0, 40321.0, 40322.0, 40323.0, 40324.0, 40325.0, 40326.0, 40327.22, 40327.0, 40329.53, 40330.0, 40331.2, 40332.0, 40331.0, 40329.72, 40335.0, 40329.0, 40330.68, 40338.0, 40339.0, 40340.0, 40341.0, 40342.0, 40343.0, 40344.0, 40345.0, 40346.76, 40346.0, 40350.0, 40351.56, 40352.0, 40351.0, 40354.0, 40355.76, 40356.0, 40355.0, 40358.0, 40359.0, 40360.0, 40361.0, 302500.0, 40363.0, 40358.4, 40365.0, 40364.0, 40367.0, 40368.0, 40366.0, 40370.0, 40372.0, 40373.0, 40374.0, 40375.0, 40376.0, 40377.6, 40378.0, 40372.8, 40380.0, 40377.0, 40383.0, 40384.8, 40385.0, 40386.0, 40387.0, 40388.0, 40389.0, 40390.0, 40391.0, 40392.0, 40393.6, 40394.0, 40393.0, 40396.0, 40397.0, 40399.0, 40400.0, 40402.0, 40404.0, 40405.0, 40406.0, 40407.0, 40410.0, 40411.0, 40412.0, 40413.0, 40414.0, 40414.07, 40416.0, 40417.0, 40415.0, 40419.0, 40420.0, 40414.4, 40422.65, 40423.08, 40424.0, 40425.0, 40426.0, 40427.0, 40428.0, 40422.0, 40430.0, 40431.04, 40432.0, 40430.88, 40434.0, 40435.0, 40435.74, 40435.2, 40438.0, 40437.0, 40440.0, 40441.0, 40442.0, 40443.0, 40441.41, 40445.0, 40446.72, 40445.04, 40448.0, 40449.0, 40450.0, 40450.92, 40452.0, 40447.0, 40454.04, 40454.0, 40456.0, 40455.0, 40458.0, 40459.0, 40460.0, 40461.24, 40461.0, 40462.15, 40464.0, 40465.0, 40466.0, 40467.36, 40467.44, 40468.0, 40469.0, 40470.0, 40466.53, 40472.0, 40473.6, 40474.0, 40475.0, 40476.8, 40476.0, 40478.0, 40479.6, 40480.0, 40481.0, 40482.0, 40483.0, 40484.0, 40477.0, 40485.0, 40487.0, 40488.0, 40487.76, 40490.0, 40490.8, 40492.0, 40493.0, 40486.0, 40495.0, 40496.0, 40497.6, 40498.0, 40499.0, 40500.0, 40502.0, 40500.2, 40497.0, 40507.0, 40507.92, 40508.0, 40509.0, 40511.0, 40512.0, 40513.0, 40514.64, 40515.0, 40516.8, 40517.0, 40518.0, 40518.4, 40520.0, 40521.0, 40522.83, 40523.0, 40524.0, 40521.83, 40526.0, 40527.0, 40526.94, 40528.0, 60158.4, 40530.72, 40532.0, 40533.0, 40535.0, 40536.0, 40536.85, 40538.0, 40539.0, 40540.0, 40542.69, 40542.0, 40544.0, 40545.0, 40548.0, 40549.0, 40550.0, 40550.4, 40552.0, 40551.0, 40554.96, 40555.06, 40554.0, 40555.0, 40558.0, 40553.0, 40560.0, 40561.9, 40562.0, 40556.0, 40564.0, 40565.0, 40566.36, 40567.0, 40564.08, 40569.0, 40570.0, 40571.0, 40572.0, 40573.0, 40574.5, 40575.0, 40574.0, 40578.0, 40579.66, 40580.0, 40581.0, 40582.0, 40583.0, 40584.0, 40584.47, 40587.0, 40588.58, 40589.0, 40590.0, 40588.0, 40592.0, 40593.0, 40594.0, 40595.0, 40596.0, 40597.0, 40594.72, 40599.0, 40600.0, 40601.0, 40602.0, 40601.5, 40604.0, 40604.96, 40606.0, 40598.0, 40608.0, 40609.0, 40610.0, 40611.0, 40612.0, 40613.2, 40610.88, 40614.93, 40616.0, 40616.04, 40618.0, 40620.0, 40620.35, 40622.0, 40623.14, 40621.0, 40624.0, 40627.04, 40628.0, 40629.0, 40630.0, 40629.83, 40632.0, 40627.0, 40634.0, 40635.0, 40636.0, 40637.96, 40638.26, 40638.44, 40640.0, 40641.0, 40642.56, 40642.0, 40643.0, 40644.0, 40646.0, 40647.48, 40647.0, 40648.0, 40650.0, 40645.0, 40652.0, 40653.0, 40654.0, 40655.0, 40656.0, 40657.0, 40658.0, 40659.0, 40660.0, 40661.0, 40663.0, 40664.0, 40665.0, 40665.6, 40667.0, 40668.0, 40669.44, 40670.0, 40671.0, 40672.0, 40674.0, 40675.44, 40675.0, 40675.02, 40678.32, 40679.0, 40680.0, 40678.0, 40679.95, 40682.0, 40684.0, 40685.0, 40686.0, 40687.0, 40687.68, 40688.0, 40690.0, 40684.8, 40692.0, 40687.78, 40695.0, 40696.0, 40700.0, 40701.0, 40702.0, 40701.97, 40704.0, 40705.6, 40705.0, 40706.0, 2400000.0, 40707.24, 40710.0, 40708.0, 565000.0, 40711.0, 40714.0, 60194.0, 40716.0, 40708.8, 40718.0, 40719.0, 40720.0, 40721.0, 40722.0, 40723.0, 60197.0, 40724.69, 40726.0, 40726.4, 40728.0, 40729.0, 40730.0, 40726.19, 40727.0, 40733.0, 40734.0, 40731.0, 40731.25, 40737.0, 40738.0, 40739.0, 40740.0, 40737.84, 40742.0, 40743.25, 40744.0, 40745.8, 40745.0, 40747.2, 40748.0, 40747.0, 40750.0, 40751.88, 40751.0, 40753.0, 40752.22, 40752.0, 40755.0, 40749.0, 40758.0, 40759.0, 40760.0, 40761.0, 40762.0, 40763.0, 40764.0, 40765.0, 40766.0, 60205.0, 40768.0, 40768.56, 40770.0, 40771.0, 40770.35, 40773.0, 40774.0, 40775.0, 40776.0, 40777.2, 40777.0, 40780.0, 40780.8, 40782.0, 40783.0, 40784.0, 40785.0, 60209.68, 40783.29, 40788.93, 40788.0, 40790.0, 40789.0, 40788.8, 40792.0, 40794.0, 40795.0, 40795.04, 40793.0, 40798.0, 40799.96, 40800.0, 40799.51, 40801.0, 40803.0, 40799.64, 40797.95, 40797.96, 40804.0, 40808.0, 40809.6, 40810.0, 40809.5, 40808.28, 40813.0, 40814.0, 40815.0, 40816.0, 40817.0, 40818.0, 40819.0, 40820.0, 40816.3, 40821.75, 40823.0, 40824.0, 40825.0, 40822.0, 40828.0, 40829.0, 40830.4, 40830.0, 40831.0, 40832.0, 40834.0, 40835.0, 40836.0, 40833.0, 40839.0, 40840.0, 40841.28, 40842.0, 40841.0, 40844.0, 40845.0, 40843.0, 40846.0, 40848.0, 40850.0, 40851.2, 40851.0, 40852.0, 40854.0, 40855.9, 303000.0, 40857.0, 40858.0, 40856.0, 40860.0, 40861.0, 40862.0, 40863.0, 40864.0, 40865.0, 40866.0, 40859.0, 40870.0, 40871.58, 40872.0, 40873.0, 40872.4, 40875.0, 40876.0, 40877.0, 40878.0, 40879.0, 40880.0, 40881.0, 40882.0, 40883.0, 40884.0, 40885.0, 40886.0, 40884.36, 40888.0, 40887.0, 40890.0, 40892.0, 40893.0, 40893.43, 40895.0, 40896.0, 40894.0, 40899.0, 40900.0, 40903.0, 40904.0, 40905.0, 40906.0, 40907.5, 40908.0, 40909.09, 40910.0, 40911.0, 40912.83, 40913.6, 40914.0, 40913.0, 40915.0, 40917.0, 40918.0, 40919.0, 40920.0, 40921.0, 40923.0, 40924.0, 40923.72, 40925.0, 40927.0, 40928.0, 40929.0, 40930.0, 40931.0, 40932.96, 40933.56, 40934.76, 40932.0, 40936.0, 40937.0, 40934.4, 40935.0, 40943.0, 40944.96, 40944.0, 40945.0, 40947.0, 40948.0, 40949.0, 40950.0, 40951.0, 40952.0, 40948.68, 40948.2, 40955.0, 40956.0, 40957.8, 40958.0, 40959.0, 40958.4, 40961.0, 40960.0, 40963.0, 40962.96, 40965.0, 40958.28, 40962.0, 40968.72, 40968.0, 40970.0, 40971.0, 40972.0, 40973.0, 40974.0, 40975.0, 40976.0, 40976.21, 40978.0, 40977.0, 40980.0, 40982.0, 40983.96, 40986.0, 40987.44, 40988.76, 40989.0, 40990.0, 40991.75, 40992.0, 40994.0, 40995.0, 40996.0, 40998.0, 40999.0, 41000.0, 41001.0, 41000.92, 41000.5, 41004.0, 41005.56, 41005.0, 41007.0, 41008.5, 41009.96, 41004.6, 41011.44, 41012.0, 41013.0, 41008.0, 41016.0, 41017.0, 41018.0, 41019.0, 41020.0, 41021.0, 41017.6, 41023.0, 41017.56, 41025.0, 41022.48, 41027.0, 41028.0, 41029.0, 41030.0, 41028.36, 41032.0, 41033.53, 41034.0, 41035.0, 41036.44, 41037.9, 41038.0, 41039.0, 41040.0, 41038.4, 41041.78, 41044.0, 41045.0, 41046.0, 41048.0, 41049.6, 41050.0, 41052.0, 41053.0, 41055.0, 41056.0, 41057.0, 303202.0, 41059.2, 41059.0, 41061.0, 41060.0, 41061.28, 41064.0, 41065.0, 41066.74, 41058.0, 41068.0, 41068.8, 41070.0, 41071.66, 41072.0, 41067.0, 41074.0, 41075.0, 41076.0, 41075.73, 41077.0, 41076.27, 41080.0, 41081.0, 41078.0, 41083.64, 41084.0, 41085.0, 41083.0, 41086.0, 41088.0, 41088.65, 41090.0, 41087.0, 41095.0, 41096.0, 41097.0, 41098.0, 41099.0, 41100.0, 41101.0, 41102.0, 41103.0, 41104.0, 41096.87, 41106.0, 41109.0, 41110.0, 41111.0, 41112.0, 41113.74, 41114.0, 41116.0, 41117.0, 41118.0, 41119.0, 41120.0, 41121.0, 41121.6, 41123.11, 41122.0, 41124.0, 41126.0, 41127.0, 41128.0, 41129.0, 41127.9, 41125.0, 41132.0, 41133.6, 41132.76, 41135.0, 41136.0, 41139.0, 41140.0, 41141.0, 41142.0, 41142.4, 41144.0, 41145.0, 41145.6, 41147.6, 41148.0, 41149.0, 41150.0, 41151.0, 41152.11, 41152.0, 41154.0, 41155.0, 41156.56, 41156.0, 41158.4, 41157.12, 41160.0, 41161.45, 41159.0, 41163.0, 41164.0, 41165.0, 41166.44, 303311.0, 41168.0, 41166.0, 41167.0, 41171.0, 41172.0, 41173.0, 41174.0, 41175.0, 41171.76, 41177.0, 41178.24, 41178.0, 41180.0, 41176.0, 41183.0, 41184.0, 41186.0, 41187.0, 41188.0, 41190.0, 41191.0, 41192.0, 41192.29, 41194.0, 41196.0, 41196.8, 41198.0, 41199.0, 41200.0, 41197.89, 41202.0, 41201.06, 41204.0, 41205.0, 60293.0, 41207.0, 41208.0, 41209.0, 41210.22, 41211.0, 41210.0, 41212.0, 41215.0, 41217.0, 41218.0, 41219.0, 41220.0, 41222.4, 41222.0, 41225.0, 41226.0, 41227.16, 41228.0, 41229.0, 41230.0, 41231.0, 41232.0, 41233.0, 41234.0, 41235.0, 41236.0, 41237.0, 41238.0, 41239.0, 41240.0, 41241.0, 41242.0, 41243.0, 41244.0, 41245.0, 41246.0, 41247.0, 41246.4, 41243.97, 41250.0, 41251.0, 41245.1, 41253.0, 41254.0, 41255.0, 41256.0, 303400.0, 41258.0, 41259.0, 41260.0, 41261.08, 41258.68, 41263.0, 41264.0, 41265.0, 41266.78, 41267.2, 41268.0, 41262.0, 41270.0, 41266.0, 41267.0, 41273.0, 41274.0, 41275.0, 41276.0, 41277.0, 41278.0, 41280.0, 41281.0, 41282.0, 41283.0, 41283.36, 41286.0, 41288.0, 41289.0, 41290.0, 41291.0, 41292.0, 41289.86, 41294.73, 41295.0, 41296.0, 41297.13, 41295.72, 41295.96, 41300.0, 41301.0, 41302.0, 41303.0, 41304.0, 41305.0, 41306.0, 41307.0, 41308.8, 41308.0, 41310.0, 41311.0, 41309.0, 41313.0, 41315.0, 41316.0, 41317.0, 41318.0, 41319.4, 41320.0, 41319.0, 41322.03, 41322.0, 41323.0, 41324.0, 41325.0, 41325.27, 41328.98, 41321.0, 41330.54, 41328.0, 41329.6, 41329.0, 41332.0, 41333.0, 41332.52, 41335.0, 41336.0, 41335.5, 41335.68, 41338.78, 41340.0, 41337.0, 41342.0, 41341.2, 41341.0, 41345.0, 41339.0, 41347.0, 41347.2, 41349.0, 41350.0, 41351.0, 41352.0, 41350.4, 41354.0, 41355.48, 41356.0, 41353.0, 41358.0, 41359.5, 41360.0, 41361.96, 41362.0, 41361.0, 41364.0, 41365.0, 41366.0, 41367.0, 41368.0, 41367.09, 41370.0, 41371.0, 41372.0, 41371.2, 41374.0, 41375.0, 41376.0, 41377.0, 41378.0, 41379.0, 41380.0, 41382.0, 41383.0, 303529.0, 41386.0, 41383.68, 41388.0, 41389.04, 41390.0, 41385.0, 41392.0, 41392.56, 41394.0, 41391.0, 41396.0, 41397.0, 41398.0, 41395.08, 41400.0, 41401.0, 41402.0, 41403.0, 41404.0, 41405.0, 41408.0, 41409.0, 41410.0, 41412.0, 41413.32, 41414.66, 41415.33, 41415.0, 41414.0, 41416.0, 41418.4, 41420.0, 41421.0, 41422.0, 41423.0, 41424.0, 41425.0, 41423.92, 41421.19, 41428.0, 41429.0, 41430.0, 41426.0, 41432.0, 41433.0, 41434.0, 41435.0, 41436.0, 41437.0, 41438.0, 41433.6, 41440.0, 41441.4, 41442.0, 41443.0, 41444.0, 41445.0, 41446.0, 41448.0, 41450.0, 41451.0, 41452.0, 41453.64, 41454.0, 41455.0, 41453.0, 41457.0, 41458.0, 41457.49, 41460.0, 41459.77, 41462.0, 41459.0, 41464.0, 41465.0, 41466.39, 41467.0, 41468.0, 41469.0, 41470.0, 41471.0, 41472.0, 41473.0, 41466.31, 41475.0, 41467.68, 41477.0, 41478.0, 41474.0, 41480.0, 41481.0, 41476.0, 41484.0, 41485.0, 41486.0, 41487.0, 41488.99, 41488.0, 41490.0, 41491.0, 41492.0, 41489.0, 41494.0, 41495.52, 41496.0, 41497.0, 41498.6, 41499.0, 41500.0, 41501.0, 41502.0, 41503.2, 41504.0, 41505.0, 41498.0, 41507.0, 41508.0, 41509.0, 41502.5, 41511.0, 41504.4, 41513.76, 41510.0, 41510.4, 41516.0, 41517.0, 41516.94, 41516.8, 41520.0, 41521.0, 41522.0, 41521.56, 41524.0, 41520.96, 41526.96, 41526.0, 41519.4, 41529.35, 41530.0, 41531.76, 41532.0, 41531.0, 41534.0, 41535.0, 41536.0, 41530.68, 41531.68, 41539.0, 41540.0, 41537.0, 41542.87, 41538.0, 41544.0, 41545.0, 41545.4, 41548.32, 41548.8, 41550.0, 41551.0, 41552.0, 41553.0, 41554.0, 41555.0, 41556.0, 41548.0, 41558.0, 41560.0, 41561.0, 41562.0, 41563.8, 41562.48, 41562.03, 41566.0, 41567.0, 41568.0, 41569.4, 41570.0, 41563.0, 41572.0, 41573.58, 41574.0, 41573.0, 41575.0, 41577.6, 41576.0, 41579.0, 41580.0, 41579.76, 41581.88, 41583.0, 41584.69, 41585.0, 41586.0, 41587.0, 41588.0, 41589.0, 41590.0, 41591.25, 41592.0, 41592.76, 41594.0, 41591.5, 41596.0, 41597.0, 41595.0, 41600.0, 41601.6, 41601.0, 41601.06, 41604.0, 41605.0, 303750.0, 41607.0, 41608.0, 41603.0, 41606.0, 41611.0, 41612.0, 41615.0, 41616.0, 303760.0, 41617.0, 41619.0, 41620.0, 41621.0, 41622.0, 41623.2, 41624.0, 41625.0, 41626.0, 41625.44, 41628.0, 41628.19, 41630.0, 41631.0, 41632.0, 41627.56, 41634.0, 41635.0, 41637.96, 41638.0, 41638.44, 41640.0, 41640.62, 41641.0, 41638.92, 41642.0, 41646.0, 41647.0, 41648.0, 41649.31, 41650.0, 41652.0, 41653.0, 41654.0, 41655.0, 41656.0, 41657.46, 41657.51, 41659.0, 41660.0, 41658.0, 41662.0, 41662.4, 41664.0, 41664.48, 41660.32, 41667.0, 41668.0, 41665.0, 41670.0, 41671.1, 41672.64, 41673.0, 41666.0, 41675.0, 41676.0, 41677.0, 41678.0, 41679.0, 41680.0, 41681.0, 41682.0, 41683.0, 41680.68, 41685.0, 41688.0, 41689.0, 41690.0, 41691.0, 41688.52, 41693.0, 41694.0, 41695.0, 41696.04, 41692.0, 41698.0, 41699.0, 41700.0, 41702.0, 41702.95, 41704.0, 41705.0, 41706.0, 41707.0, 41708.16, 41710.0, 41711.0, 41712.0, 41713.0, 566000.0, 41715.0, 41716.0, 41717.0, 41714.0, 41719.0, 41720.0, 41721.0, 41722.08, 41721.6, 41724.0, 41724.96, 41725.0, 41727.0, 41728.0, 41725.19, 41730.0, 41729.87, 41726.0, 41731.0, 41734.0, 41736.0, 41737.0, 41738.0, 41739.84, 41740.0, 41743.0, 41744.0, 41745.0, 41746.0, 41747.0, 41748.0, 41749.0, 41750.0, 41751.0, 41749.92, 41753.0, 41754.4, 41754.0, 41756.0, 41755.0, 41758.41, 41758.0, 41760.0, 41761.25, 41761.0, 41764.03, 41765.0, 41766.0, 41766.96, 41768.0, 41769.0, 41764.0, 41770.0, 41772.0, 41772.48, 41774.67, 41773.0, 41776.8, 41776.0, 41777.69, 41779.0, 41780.0, 41777.76, 41773.2, 41775.0, 41784.0, 41785.0, 41786.0, 41787.0, 41788.0, 41789.0, 41790.0, 41791.0, 41792.0, 41792.33, 41794.0, 41792.5, 41796.0, 41795.0, 41798.64, 41798.4, 41800.0, 41800.44, 41802.0, 41803.0, 41804.0, 41805.0, 41806.0, 41798.0, 41808.0, 41809.0, 41810.0, 41811.25, 41812.12, 41811.0, 41814.0, 41816.52, 41817.0, 41816.0, 41818.0, 41820.0, 41817.9, 41822.0, 41823.0, 41823.6, 41824.0, 41827.5, 41828.0, 41829.0, 41830.0, 41831.0, 41832.0, 303971.0, 41834.0, 41830.08, 41835.0, 41837.0, 41836.2, 41839.0, 41840.0, 41841.0, 41842.0, 41843.55, 41843.0, 41844.0, 41846.0, 41847.0, 41848.0, 41849.0, 41850.0, 41849.6, 41852.0, 41852.16, 304000.0, 41857.71, 41856.0, 41859.0, 41860.0, 41861.0, 41862.8, 41858.0, 41864.0, 41865.0, 41860.64, 41867.0, 41868.0, 41869.0, 41870.3, 41870.0, 41872.0, 41870.4, 41874.0, 41875.0, 304011.0, 1090450.0, 41878.57, 41879.0, 41880.0, 41881.0, 41882.0, 41883.0, 41885.0, 41886.0, 41887.0, 41888.0, 41889.0, 41890.0, 41891.0, 41892.0, 41891.2, 41894.4, 41894.0, 41891.87, 41895.0, 41898.0, 41899.0, 41900.0, 41901.15, 41902.8, 41902.0, 41904.0, 41905.0, 41903.0, 41907.0, 41908.0, 41909.0, 41910.21, 41910.0, 41912.0, 41913.0, 41914.0, 41916.3, 41917.0, 41916.0, 41919.83, 41918.0, 41920.0, 41921.0, 41923.44, 41924.0, 41923.0, 41925.0, 41919.0, 41928.0, 41925.13, 41930.2, 41931.0, 41932.0, 41930.0, 41934.0, 41932.8, 41936.0, 41936.94, 41938.83, 41938.0, 41940.0, 41941.0, 41942.0, 41943.0, 41943.36, 41945.0, 41947.0, 41948.92, 41950.0, 41950.8, 41952.0, 41953.6, 41954.0, 41955.0, 41956.0, 41953.0, 41951.0, 41957.0, 41960.0, 41958.06, 41963.0, 41964.0, 41965.0, 41967.0, 828400.0, 41969.0, 41970.0, 41968.0, 41971.0, 41973.0, 41974.44, 41974.0, 41976.0, 41977.32, 41978.0, 41974.4, 41977.0, 41980.0, 41982.0, 41981.0, 41984.0, 41979.36, 41986.49, 41979.6, 41988.0, 41985.0, 41990.0, 41986.0, 41992.0, 41993.51, 41994.0, 41995.2, 41996.0, 41997.0, 41995.0, 41999.0, 42000.0, 41999.2, 41997.84, 42002.0, 42004.0, 42005.0, 42008.0, 42009.0, 42009.12, 42011.0, 42012.0, 42010.48, 42010.0, 42016.0, 42018.0, 42019.0, 42020.0, 42021.0, 42021.29, 42024.0, 42025.0, 42025.04, 42028.0, 42029.0, 42030.0, 42031.0, 42034.0, 42035.0, 42036.8, 42036.0, 42039.22, 42040.0, 42041.0, 42042.0, 42043.4, 42044.0, 42045.0, 42047.0, 42048.0, 42050.0, 42051.0, 42052.0, 42054.56, 42055.44, 304200.0, 42057.6, 42058.0, 42057.0, 42060.0, 42061.22, 42062.0, 42063.0, 42061.0, 42065.0, 42066.0, 42059.0, 42070.0, 42072.0, 42073.0, 42074.0, 42075.0, 42078.4, 42078.0, 42079.0, 42081.96, 42082.0, 42083.0, 42084.0, 42085.0, 42081.0, 42087.0, 42080.0, 42089.92, 42090.0, 42091.0, 42092.95, 42093.0, 42092.0, 42086.0, 42090.48, 42096.0, 42097.0, 42098.0, 42099.48, 42100.0, 42099.0, 42101.0, 42102.24, 42103.0, 42106.0, 42107.0, 42108.48, 42108.0, 42110.16, 42110.0, 42112.0, 42113.8, 42114.0, 42115.0, 42116.0, 42117.0, 42118.47, 42119.0, 42120.0, 42121.0, 42122.0, 42122.26, 42124.0, 42125.0, 42126.0, 42127.6, 42127.0, 42129.0, 42130.0, 42131.7, 42132.0, 42133.0, 42134.0, 42135.84, 42136.0, 42138.0, 42139.0, 42140.0, 42143.24, 42144.0, 42143.0, 42147.0, 42148.0, 42150.0, 42151.0, 42152.0, 42153.0, 42154.0, 42155.0, 42156.0, 320997.0, 42159.0, 42160.0, 42161.0, 42162.0, 42163.0, 42165.0, 42166.23, 42167.0, 42168.63, 42168.0, 42166.65, 42171.16, 42172.0, 42173.0, 42174.0, 42168.89, 42175.0, 42176.53, 42178.0, 42177.0, 42180.0, 42181.0, 42182.0, 42182.4, 42184.0, 42185.0, 42186.0, 42187.44, 42179.04, 42189.0, 42190.0, 42191.71, 42192.0, 42194.0, 42197.0, 42198.0, 42199.0, 42200.0, 42201.0, 42202.0, 42203.0, 42204.0, 42205.0, 42206.0, 42207.0, 42208.0, 42209.0, 42210.0, 42211.0, 42212.0, 42213.0, 42214.0, 60492.0, 42216.0, 42217.0, 42218.0, 42220.0, 42221.0, 42222.0, 42223.0, 42224.0, 42225.0, 42226.0, 42227.0, 42225.82, 42228.0, 42230.0, 42230.04, 42227.79, 42233.0, 42234.0, 42235.0, 42236.0, 42238.0, 42238.32, 42240.0, 42241.0, 42242.0, 42243.96, 42244.0, 42245.0, 42239.0, 42247.0, 42248.0, 42249.0, 42250.0, 42251.0, 42252.0, 42253.71, 42250.75, 42255.2, 42256.0, 42254.0, 42250.08, 42259.0, 42260.0, 42260.36, 42262.8, 42262.0, 42264.0, 42265.0, 42266.0, 42267.96, 42268.0, 42267.0, 42270.0, 42265.6, 42272.0, 42268.2, 42266.4, 42275.0, 42276.0, 42277.28, 42278.84, 42279.0, 42280.0, 42277.0, 42282.0, 42281.0, 42284.0, 42285.0, 42286.0, 42287.76, 42287.18, 42288.0, 42290.0, 42289.0, 42292.0, 42291.0, 42293.0, 42295.0, 42296.19, 42296.61, 42297.0, 42295.2, 42300.0, 42301.0, 42296.8, 42303.0, 42304.0, 42305.0, 42306.0, 42307.0, 42308.88, 42309.0, 42310.8, 42311.0, 42312.0, 42313.8, 42314.0, 42313.0, 42316.0, 42308.0, 42318.0, 42319.0, 42320.0, 42321.0, 42322.0, 42323.0, 42324.0, 42325.0, 42326.0, 42326.52, 42328.0, 42328.84, 42330.0, 42331.0, 42328.83, 42333.0, 42332.04, 42335.0, 42336.0, 42331.92, 42338.0, 42338.45, 42340.0, 42341.0, 42340.6, 42343.0, 42340.17, 42345.0, 42346.32, 42347.0, 42348.0, 42349.0, 42350.0, 42348.48, 42352.76, 42353.0, 42348.8, 42355.0, 42356.0, 42357.0, 42359.0, 42360.24, 42360.0, 42362.28, 42363.0, 42364.8, 42365.0, 42366.05, 42364.0, 42368.0, 42369.0, 42370.0, 42371.0, 42372.0, 42373.0, 42374.05, 42375.0, 42369.6, 42373.93, 42378.0, 42379.0, 42380.0, 42381.0, 42382.9, 42383.88, 42384.0, 42385.0, 42386.75, 42387.18, 42387.0, 42389.0, 42390.0, 42388.28, 42392.0, 42393.12, 42393.0, 42395.0, 42396.0, 42397.0, 42397.44, 42399.0, 42400.0, 42398.0, 42399.86, 42403.8, 42404.0, 42405.0, 42401.89, 42407.0, 42408.0, 42409.0, 42410.0, 42411.0, 42412.0, 42411.2, 42414.0, 42415.0, 42416.0, 42417.0, 42418.77, 42418.0, 42420.0, 42421.0, 42422.0, 42423.0, 42424.0, 42424.54, 42426.24, 42427.0, 42426.72, 42429.0, 42430.0, 42431.0, 42432.0, 42433.0, 42434.0, 42435.0, 42436.0, 42437.12, 42438.5, 42438.6, 42440.0, 42441.0, 42442.0, 42443.0, 42444.0, 42445.0, 42438.48, 42442.8, 42448.0, 42449.0, 42450.0, 304596.0, 42453.0, 42452.8, 42452.0, 42456.0, 42457.0, 42458.0, 42454.0, 42460.0, 42461.28, 566748.0, 42464.0, 42466.0, 42468.0, 42469.51, 42470.0, 42469.0, 42472.0, 42473.6, 42471.0, 42475.0, 42476.5, 42477.0, 42478.0, 42479.0, 42480.0, 42480.1, 42482.0, 42483.0, 42484.0, 42485.31, 42485.0, 42487.0, 42487.63, 42489.0, 42490.0, 42491.0, 42486.0, 42493.0, 42494.4, 42494.0, 42496.62, 42497.0, 42496.0, 42497.76, 42500.0, 42496.8, 42499.99, 42500.12, 42504.0, 42498.0, 42506.0, 42507.31, 42508.0, 42507.64, 42509.0, 42511.0, 42512.04, 42513.0, 42510.0, 42515.0, 42516.96, 42517.0, 42516.0, 42518.71, 42520.0, 42518.0, 42522.0, 42522.84, 42524.0, 42525.0, 42526.8, 42526.0, 42528.0, 42522.44, 42530.0, 42531.0, 42532.0, 42533.0, 42534.0, 42526.2, 42536.0, 42537.0, 42535.0, 42539.0, 42540.0, 42541.0, 42542.88, 42546.0, 42547.0, 42548.0, 42549.0, 42550.0, 42551.66, 42552.0, 42553.0, 42554.0, 42555.0, 42556.0, 42557.0, 42558.0, 42559.0, 42560.0, 42561.0, 42560.72, 42563.0, 42564.0, 42565.0, 42566.6, 42567.0, 42568.0, 42569.0, 42570.0, 42565.66, 42572.0, 42569.46, 42575.0, 42576.0, 42577.0, 42576.9, 42579.0, 42580.0, 42581.0, 42582.0, 42583.88, 42584.0, 42585.0, 42585.6, 42584.64, 42588.0, 42589.0, 42590.0, 42584.85, 42592.0, 42593.0, 42594.0, 42595.0, 42596.97, 42597.0, 42598.0, 42599.0, 42600.0, 42598.4, 42602.0, 42601.0, 42599.69, 42605.88, 42606.0, 42607.0, 42608.0, 42609.0, 42610.0, 42611.0, 42612.0, 42605.0, 42614.0, 42615.84, 42615.0, 42617.0, 42616.0, 42619.0, 42620.0, 42615.04, 42622.0, 42619.25, 42624.0, 42619.22, 42626.0, 42627.0, 42628.0, 42629.28, 42630.0, 42631.0, 42632.83, 42627.1, 42629.0, 42635.0, 42636.0, 42639.0, 42640.0, 42642.0, 42642.71, 42644.0, 42645.0, 42646.0, 42647.0, 42648.0, 42644.04, 42650.0, 42651.0, 42652.0, 42654.0, 42655.99, 42656.0, 42657.0, 42656.4, 42659.64, 42660.0, 42660.8, 42662.0, 42659.0, 42661.0, 42663.0, 42658.0, 42667.0, 42668.0, 42667.04, 42670.0, 42669.0, 42672.0, 42671.0, 42672.48, 42675.0, 42676.0, 42678.0, 42679.0, 42680.0, 42681.0, 42682.0, 42683.52, 42684.0, 42685.0, 42681.6, 42687.0, 42686.4, 42691.0, 42692.0, 42693.0, 42694.0, 42694.32, 42696.0, 42695.0, 42699.0, 42700.0, 42701.0, 42702.0, 42703.08, 42704.0, 42704.4, 42705.0, 304846.0, 42702.4, 42709.0, 42703.0, 42711.0, 42712.0, 567000.0, 42707.0, 42715.0, 42710.0, 42717.0, 42718.0, 42713.0, 42720.0, 42721.0, 42722.05, 42723.0, 42724.0, 42725.0, 42725.53, 42722.16, 42723.2, 42730.0, 42731.3, 42732.0, 42733.0, 42733.18, 42735.0, 42736.0, 42737.0, 42738.0, 42739.0, 42740.0, 42741.0, 42743.0, 42744.0, 42745.0, 42743.75, 42747.0, 42748.38, 42749.0, 42750.0, 42748.33, 42752.0, 42748.0, 42754.0, 42755.0, 42756.0, 42757.0, 42758.0, 42759.0, 42760.0, 42761.0, 42762.0, 42760.86, 42764.8, 42764.0, 42765.0, 42767.0, 42768.0, 42766.16, 42770.0, 42771.12, 42772.0, 42766.0, 42774.24, 42775.0, 42776.0, 42774.0, 42769.0, 42778.68, 42780.0, 42781.0, 42782.0, 42784.0, 42785.6, 42786.0, 42785.0, 42787.44, 42789.0, 42789.6, 42790.0, 42792.0, 42793.0, 304936.0, 42795.0, 42797.0, 42799.0, 42800.0, 42801.96, 42802.71, 42803.0, 42804.0, 42806.0, 42807.0, 42809.69, 42810.96, 42810.0, 42812.0, 42809.0, 42814.0, 42815.0, 42816.0, 42810.36, 42818.0, 42819.45, 42820.0, 42821.0, 42822.14, 42819.0, 42824.0, 42825.72, 42825.0, 42827.0, 42828.0, 42827.2, 42830.0, 42831.21, 42826.0, 42829.0, 42834.0, 42835.0, 42836.0, 42837.0, 42838.66, 42840.0, 42841.18, 42842.0, 42843.0, 42841.0, 42845.0, 42843.49, 42847.0, 42848.0, 42849.0, 42850.0, 42851.0, 42852.0, 42854.0, 42855.0, 305000.0, 42856.0, 42857.0, 42859.0, 42860.0, 42859.6, 42863.34, 42864.0, 42865.0, 42866.0, 42867.0, 42868.0, 42868.8, 42870.0, 42869.0, 42872.0, 42871.0, 42874.0, 42875.0, 42876.0, 42877.0, 42873.0, 42873.6, 42880.0, 42873.36, 42883.0, 42884.0, 42885.0, 42886.0, 42887.0, 42888.0, 42889.0, 42890.0, 42891.0, 42890.3, 42892.0, 42894.31, 42895.0, 42889.6, 42897.0, 42898.0, 42900.0, 42901.0, 42901.73, 42903.0, 42903.75, 42905.0, 42906.0, 42907.0, 42908.16, 42908.0, 42910.0, 42911.28, 42911.0, 42912.0, 42914.4, 42916.0, 42917.0, 42918.6, 42919.0, 42920.0, 42920.64, 42922.0, 42923.0, 42922.7, 42916.68, 42925.0, 42926.0, 42921.0, 42928.0, 42928.25, 42931.2, 42931.0, 42930.0, 42934.0, 42932.0, 42936.0, 42935.0, 42938.89, 42939.0, 42940.0, 42941.0, 42942.0, 42943.0, 42943.15, 42942.47, 42937.0, 42944.0, 42948.0, 42949.0, 42950.0, 42951.0, 42952.0, 42953.0, 42955.0, 42956.9, 42957.0, 42958.0, 42959.16, 42960.0, 42961.0, 42962.0, 42962.4, 42962.76, 42965.0, 42966.5, 42967.0, 305112.0, 42969.0, 42970.0, 42968.0, 42972.0, 42972.8, 42966.0, 42975.0, 42976.0, 42977.74, 42978.0, 42973.0, 42980.0, 42981.0, 42981.24, 42983.0, 42984.0, 42985.0, 42986.0, 42987.0, 42988.0, 42982.0, 42990.0, 42991.0, 42992.0, 42993.0, 42994.0, 42995.0, 42996.0, 42997.0, 42998.0, 42999.0, 43000.0, 43000.77, 43001.0, 43003.0, 43004.0, 43005.0, 43000.99, 43007.0, 43008.0, 43009.0, 43010.0, 43011.66, 43012.0, 43013.0, 43014.0, 43015.0, 43015.44, 43017.0, 43020.0, 43022.0, 43023.24, 43022.96, 43025.4, 43026.0, 43025.0, 43026.22, 43027.0, 43030.0, 43028.0, 43032.0, 43033.0, 43034.94, 43035.0, 43033.68, 43028.82, 43038.0, 43041.0, 43042.0, 43043.0, 43044.0, 43045.0, 43046.0, 43047.96, 43048.2, 43049.0, 43050.0, 43045.56, 43052.88, 43052.65, 43054.0, 43053.0, 43056.0, 43057.0, 43057.82, 43055.0, 43060.0, 43052.0, 43062.0, 43065.0, 43066.0, 43068.0, 43069.0, 43072.0, 43073.16, 43073.0, 43075.0, 43076.4, 43077.0, 43076.0, 43074.0, 43080.0, 43081.0, 43079.0, 43083.6, 43082.0, 43085.0, 43087.2, 43088.0, 43087.0, 43090.0, 43092.0, 43093.0, 43094.0, 43095.0, 43096.0, 43097.6, 43097.0, 43098.0, 43100.0, 43101.0, 43102.0, 43103.0, 43104.0, 43100.18, 43106.14, 43106.0, 43108.0, 43105.0, 43110.0, 43112.0, 43113.0, 43112.16, 43112.49, 43115.0, 43116.0, 43118.0, 43119.0, 43120.0, 43118.4, 43122.0, 43123.0, 43124.0, 43125.0, 43126.0, 43127.0, 43128.0, 43129.07, 43130.0, 43130.59, 43130.73, 43133.0, 43134.0, 43134.6, 43135.0, 43137.0, 60676.29, 43139.0, 43140.0, 43142.0, 43143.0, 43144.0, 43146.25, 43147.0, 43150.0, 43151.0, 43152.0, 43151.16, 43154.0, 43155.0, 43156.0, 43157.67, 43158.0, 43158.9, 43160.0, 43161.0, 43162.0, 43158.36, 43164.0, 43165.0, 43163.0, 43161.6, 43168.0, 43169.0, 43170.0, 43171.0, 43172.0, 43173.0, 43173.6, 43175.0, 43176.0, 43176.65, 43178.0, 43179.0, 43180.0, 43181.0, 43182.0, 43180.8, 43184.0, 43184.54, 43185.96, 43185.0, 43188.0, 43188.48, 43190.0, 43185.79, 43192.0, 43193.0, 43187.0, 43195.0, 43196.0, 43197.12, 43198.0, 43199.0, 43200.0, 43201.0, 43202.0, 43203.0, 43204.0, 43204.8, 43206.95, 43207.12, 43208.0, 43209.0, 43210.0, 43207.0, 43212.0, 43211.0, 43214.0, 43213.0, 43216.0, 43215.54, 43218.0, 43219.0, 43220.0, 43221.74, 43222.0, 43222.92, 43224.0, 43225.0, 43223.0, 43221.42, 43226.0, 43228.0, 43230.0, 43230.16, 43232.0, 43233.0, 43234.0, 43236.0, 43237.0, 43238.0, 43240.0, 43241.77, 43241.0, 43243.2, 43244.0, 43245.45, 43243.0, 43246.32, 43248.0, 43249.0, 43250.0, 43246.0, 43247.0, 43253.0, 43254.0, 43254.72, 43256.0, 43257.6, 43258.0, 43260.0, 43262.0, 43263.0, 43264.0, 43265.0, 43266.8, 43268.39, 43269.0, 43270.4, 43271.0, 43272.0, 43273.0, 43268.0, 43275.0, 43276.0, 43277.0, 43270.0, 43279.0, 43280.0, 305425.0, 43283.0, 43284.0, 43285.0, 43287.0, 43288.0, 43289.0, 43290.0, 43292.81, 43293.0, 43294.0, 43295.0, 43296.0, 43295.28, 43298.0, 43299.0, 43300.0, 43301.0, 43302.0, 43303.0, 43304.0, 43299.14, 43306.0, 43307.0, 43308.0, 43307.74, 43308.68, 43310.0, 43311.0, 43313.46, 43314.48, 43315.0, 43309.0, 43316.0, 43314.0, 43319.0, 43320.0, 43321.0, 43314.6, 43323.7, 43324.0, 43325.4, 43326.0, 43327.25, 43328.0, 43329.0, 43330.0, 43331.0, 43332.0, 43333.0, 43334.16, 43335.0, 43334.0, 43337.39, 43336.0, 43339.0, 43338.0, 43339.2, 43342.0, 43340.0, 43344.0, 43345.0, 43346.0, 43347.2, 43347.0, 43349.0, 43350.0, 43351.0, 43348.0, 43353.77, 43354.2, 43355.0, 43356.0, 43356.19, 43358.88, 43359.0, 43360.0, 43361.0, 43359.68, 43363.0, 43364.0, 43365.6, 43365.0, 43366.0, 43368.0, 829800.0, 43370.0, 43369.0, 43372.0, 43373.0, 43373.17, 43375.0, 43376.81, 43377.0, 43378.0, 43372.8, 43380.96, 43380.0, 43381.0, 43383.0, 43384.0, 43385.0, 43382.0, 43387.0, 43387.5, 43388.0, 43390.0, 43389.0, 43392.0, 43393.0, 43393.72, 43395.0, 43394.0, 43397.0, 43398.0, 43399.5, 43400.0, 43399.0, 43402.39, 43403.0, 43404.0, 43405.0, 43401.6, 43407.0, 43408.0, 43407.12, 43410.0, 43411.0, 43412.0, 43406.48, 43414.0, 43415.28, 43416.0, 43415.0, 43415.16, 43415.35, 43420.0, 43421.0, 43413.0, 43423.0, 43424.0, 43425.84, 43425.0, 43427.0, 43428.0, 43429.82, 43430.0, 43431.0, 43432.0, 43433.0, 43429.0, 43435.0, 43429.92, 43437.0, 43430.4, 43439.0, 43440.0, 43441.0, 43442.0, 43443.0, 43444.0, 43445.0, 43446.0, 43447.13, 43447.0, 43449.6, 43450.0, 43451.0, 43452.0, 43453.0, 43455.0, 43456.64, 43457.0, 43456.0, 43459.0, 43460.0, 43461.0, 43462.62, 43463.0, 43464.0, 43466.0, 43467.88, 43467.0, 43468.0, 43470.0, 43466.4, 43472.0, 43473.67, 43471.0, 43475.2, 43476.0, 43477.0, 43470.96, 43479.28, 43480.0, 43481.0, 43474.0, 43483.0, 43475.0, 43485.0, 43486.12, 43478.92, 43488.0, 43489.0, 43488.72, 43491.0, 43492.88, 43492.8, 43492.0, 43495.0, 43496.44, 43497.96, 43498.0, 43499.0, 43500.0, 43500.24, 43501.0, 43502.0, 43504.0, 43505.73, 43506.0, 43507.2, 43501.34, 43507.0, 43504.32, 43508.0, 43512.0, 43513.0, 43514.0, 43513.6, 43516.0, 43512.73, 43513.34, 43514.64, 43520.0, 43520.32, 43522.0, 43521.0, 43524.0, 60753.0, 43526.0, 43527.0, 43528.54, 43528.0, 43530.0, 43531.0, 43532.0, 43533.25, 43534.0, 43534.4, 43536.0, 43537.0, 43538.0, 43539.0, 43540.0, 43541.0, 43542.0, 43543.0, 43544.0, 43545.0, 43546.0, 43545.6, 43548.0, 43550.0, 43551.0, 43552.0, 4500000.0, 43554.0, 43555.0, 43555.2, 43557.0, 43558.0, 43559.0, 43560.0, 43561.0, 43562.88, 43563.0, 43564.0, 43565.0, 43566.12, 43566.0, 43568.0, 830000.0, 43570.0, 43567.0, 43572.0, 43573.0, 43574.58, 43575.0, 43576.0, 43577.0, 43578.0, 43573.2, 43580.0, 43579.0, 43582.0, 43583.0, 43584.0, 43585.0, 43586.0, 43585.8, 43588.0, 43586.76, 43589.0, 43587.0, 43592.0, 43593.23, 43594.64, 43595.28, 43596.8, 43596.0, 43598.0, 43599.0, 43600.0, 305744.0, 43602.96, 43602.0, 43604.0, 43597.0, 43605.0, 43607.28, 43608.0, 43609.46, 43609.0, 43611.0, 43612.0, 43613.64, 43614.0, 43612.12, 43616.0, 43617.6, 43618.0, 43615.0, 43620.0, 43616.75, 43622.0, 567911.0, 43617.0, 43625.0, 43626.0, 43621.0, 43628.0, 43629.0, 43630.0, 43624.49, 43632.0, 43633.44, 43634.0, 43635.0, 43633.0, 43637.0, 43630.08, 43639.0, 43640.0, 43641.0, 43642.0, 43643.0, 43644.12, 43646.0, 43647.48, 43646.75, 305790.0, 43650.0, 43651.2, 43649.08, 43653.0, 43654.0, 43655.0, 43656.0, 43657.0, 43658.13, 43659.0, 43660.0, 43653.38, 43662.0, 43658.0, 43664.87, 43665.0, 43666.98, 43666.0, 43667.0, 43669.0, 43664.0, 43671.23, 43672.0, 43670.0, 43674.0, 43675.0, 43676.0, 43670.88, 43678.0, 43677.0, 43680.0, 43680.6, 43681.68, 43683.12, 43684.0, 43679.0, 43686.0, 43687.0, 43688.0, 43689.0, 43690.0, 43691.0, 43692.0, 43689.16, 43692.37, 43694.88, 43696.0, 43697.0, 43698.0, 43699.0, 43700.0, 43701.0, 43700.8, 43702.0, 43704.0, 43705.0, 43706.0, 43707.0, 43703.0, 43709.0, 568000.0, 43712.0, 43714.53, 43715.0, 43716.0, 43717.0, 43718.4, 43719.2, 43720.0, 43721.0, 43714.0, 43723.0, 43724.0, 43725.0, 43726.0, 43721.52, 43728.0, 43729.0, 43730.0, 43731.0, 43732.0, 43733.0, 43734.0, 43730.48, 43736.0, 43737.6, 43736.27, 43738.0, 43740.0, 43741.0, 43742.4, 43739.0, 43742.0, 43745.0, 43737.0, 43747.0, 43748.0, 43749.0, 43750.0, 43751.0, 43752.0, 43753.0, 43754.0, 43755.0, 43755.24, 43757.0, 43758.0, 43757.04, 43760.0, 43759.08, 43762.2, 43763.0, 43764.0, 43765.0, 43762.5, 43767.0, 43768.0, 43767.1, 43770.0, 43771.0, 43769.0, 43773.4, 43774.0, 43775.0, 43776.0, 43779.37, 43780.0, 43780.46, 43783.0, 43784.0, 43786.34, 43787.0, 43788.0, 43789.0, 43786.8, 43791.45, 43792.0, 43791.0, 43794.75, 43795.0, 43796.0, 43790.0, 43799.0, 43800.0, 43802.0, 43802.03, 43804.0, 43804.8, 43805.0, 43807.31, 43807.0, 43809.0, 43810.0, 43808.0, 43812.0, 43812.95, 43814.88, 43814.0, 43816.0, 43815.0, 43811.0, 43819.98, 43819.0, 43820.0, 43821.0, 43823.0, 43824.0, 43825.6, 43826.0, 43827.68, 43827.36, 43829.0, 43830.0, 43825.0, 43832.0, 43827.0, 43830.64, 43835.0, 43836.0, 43837.0, 43838.0, 43838.52, 43840.0, 43841.0, 43843.0, 43844.0, 43845.0, 43846.4, 43846.0, 43848.0, 43849.0, 43850.0, 43847.0, 43852.0, 43853.0, 43854.0, 43855.0, 306000.0, 43856.64, 43857.84, 43858.0, 43860.0, 43861.0, 43857.0, 43858.56, 43864.0, 43861.85, 43865.0, 43867.0, 43868.0, 43869.0, 43867.08, 43871.0, 43872.0, 43867.2, 43866.0, 43875.0, 43870.0, 43877.0, 43878.0, 43879.0, 43880.0, 43880.67, 43882.0, 43883.0, 43884.0, 43881.12, 43881.0, 43887.0, 43888.0, 43889.52, 43890.0, 43891.0, 43892.0, 43892.28, 43893.0, 43895.0, 43896.0, 43894.0, 43898.0, 43899.2, 43900.0, 43901.0, 43897.0, 43904.13, 43907.52, 43908.0, 43909.0, 43910.4, 43911.0, 43912.0, 43913.0, 43914.0, 43915.0, 43916.0, 43917.0, 43919.2, 43920.0, 43921.0, 43919.0, 43924.0, 43926.1, 43927.0, 43928.0, 43929.0, 43930.0, 43929.6, 43932.0, 43935.65, 43936.0, 43935.0, 43938.0, 43937.92, 43940.0, 43941.0, 43943.0, 43944.0, 43945.0, 43946.0, 43947.0, 43948.0, 43949.0, 43950.0, 43950.5, 43952.0, 43953.0, 43954.0, 43955.0, 43955.48, 43957.0, 43958.0, 43959.0, 43960.0, 43961.0, 43962.0, 43961.26, 43964.52, 43965.0, 43966.0, 43967.04, 43968.0, 43964.0, 43970.0, 43971.0, 43972.0, 43972.5, 43974.0, 43972.8, 43975.0, 43976.0, 43977.0, 43975.16, 43971.2, 43980.0, 43981.0, 43982.88, 43983.0, 43981.92, 43985.0, 43986.0, 43986.02, 43988.0, 43981.32, 43990.0, 43991.0, 43992.0, 43987.0, 43986.48, 43996.0, 43996.76, 43999.0, 44000.0, 43999.8, 44004.0, 44005.0, 44006.62, 44006.12, 44006.4, 44007.0, 44010.0, 44011.0, 44012.0, 44013.0, 44009.88, 44015.0, 44016.0, 44017.0, 44016.48, 44015.28, 44020.0, 44022.0, 44023.0, 44024.01, 44025.0, 44026.0, 44027.0, 44028.0, 44024.0, 44024.16, 44032.0, 44033.0, 44033.6, 44035.0, 44036.2, 44038.0, 44039.0, 44040.0, 44041.0, 44042.0, 44043.0, 44044.0, 44047.0, 44048.0, 44047.08, 44050.0, 44049.0, 44052.0, 44051.88, 44054.0, 44049.6, 44056.0, 44057.0, 44051.0, 44060.0, 44061.0, 44063.0, 44064.0, 44063.76, 44066.0, 44065.0, 44068.0, 44069.76, 44070.0, 44071.0, 44072.0, 44073.0, 44074.0, 44075.0, 44076.0, 44077.0, 44078.0, 44079.0, 44080.4, 44080.0, 44082.0, 44083.0, 44084.0, 44085.0, 44086.0, 44087.0, 44088.0, 44090.0, 44091.0, 44092.0, 44093.0, 44094.0, 44095.0, 44096.0, 44096.3, 44098.0, 44099.0, 44100.0, 44093.43, 44097.0, 44103.0, 44104.0, 44105.0, 44106.0, 44107.0, 44102.0, 44109.0, 44110.0, 44111.0, 44112.0, 44115.0, 44116.0, 44116.8, 44117.0, 44119.0, 44120.0, 44121.0, 44122.0, 44123.0, 44124.0, 44125.0, 44121.92, 44127.35, 44120.52, 44127.0, 44130.0, 44124.33, 44132.4, 44133.44, 44134.0, 44133.0, 44136.0, 44137.0, 44137.6, 44139.68, 44140.0, 44141.0, 44138.0, 44139.0, 44140.8, 44145.0, 44146.88, 44146.0, 44148.0, 44149.0, 44150.0, 44142.0, 44152.0, 44153.0, 44154.0, 44155.0, 44156.0, 44156.56, 44158.4, 44159.08, 44160.0, 44161.0, 44162.0, 44163.0, 44158.0, 6597760.0, 44166.0, 44167.0, 44168.0, 44169.0, 44170.0, 44171.5, 44172.0, 44173.0, 44165.0, 44175.0, 44176.95, 44171.0, 44178.4, 44179.0, 44180.62, 44180.0, 44182.0, 44183.0, 44184.0, 44184.24, 44181.0, 44187.0, 44188.0, 44188.6, 44190.0, 44185.0, 44192.0, 44193.0, 44195.76, 44196.0, 44195.0, 44197.0, 44199.0, 44200.0, 44201.0, 44195.13, 44204.0, 44206.0, 44207.0, 44208.0, 44209.0, 44210.0, 44211.0, 44211.12, 44213.0, 44214.14, 44215.0, 44216.0, 44217.0, 44211.77, 44217.84, 44220.0, 44221.0, 44222.0, 44214.0, 44224.0, 44225.0, 44226.0, 44227.0, 44228.0, 44229.0, 44230.0, 44223.0, 44232.0, 44233.0, 44234.28, 44235.0, 44236.0, 44236.8, 44231.0, 44239.68, 44240.0, 44241.0, 44241.6, 44242.0, 44244.0, 44245.0, 44248.0, 44249.14, 44250.0, 44249.0, 44252.64, 44252.0, 44253.0, 44255.0, 44256.0, 44257.0, 44258.0, 44259.42, 44260.0, 44261.0, 44262.0, 44262.4, 44264.0, 44265.0, 44263.87, 44267.52, 44268.0, 44267.0, 44270.0, 44271.0, 44272.8, 44273.0, 44274.0, 44275.0, 44272.0, 44275.82, 44278.0, 44279.66, 44280.0, 44279.0, 44282.0, 44282.16, 44284.68, 44284.44, 44285.0, 44281.0, 44288.0, 44289.0, 44290.0, 44291.0, 44292.0, 44284.0, 44294.0, 44295.0, 44287.36, 44293.0, 44298.0, 44299.8, 44300.0, 44299.0, 44302.0, 44301.0, 44304.82, 44304.0, 44306.0, 44305.0, 44301.6, 44309.0, 44310.0, 44311.0, 44312.0, 44313.0, 44315.28, 44316.0, 44316.18, 44318.67, 44319.0, 44320.0, 44321.0, 44319.64, 44323.0, 44324.0, 44325.0, 44326.0, 44327.0, 44328.0, 44329.0, 44330.0, 44331.0, 44334.0, 44336.0, 44337.0, 44338.0, 44339.0, 44340.0, 44341.0, 44342.0, 44343.0, 44344.0, 44345.0, 44346.0, 44345.6, 44348.0, 44347.0, 44350.0, 44351.0, 44352.0, 44353.0, 44350.4, 44355.29, 44356.0, 306501.0, 44357.0, 44359.0, 44360.38, 44361.0, 44362.0, 44362.5, 44364.0, 44365.0, 44366.99, 44360.0, 44367.0, 44369.0, 44370.0, 44371.08, 44372.0, 44371.0, 44374.0, 44375.0, 44376.0, 44375.46, 44374.42, 44378.0, 44380.0, 44377.0, 44380.45, 44381.87, 44384.0, 44385.0, 44386.0, 44387.0, 44388.0, 44386.08, 44390.0, 44391.0, 44392.0, 44393.0, 44394.0, 44395.81, 44390.4, 44397.0, 44398.0, 44395.0, 44400.0, 44401.0, 44402.0, 44403.15, 44404.0, 44403.0, 44406.0, 44407.0, 44408.0, 44405.0, 44410.0, 44403.84, 44412.42, 44412.0, 44413.12, 44415.0, 44416.0, 44417.0, 44418.0, 44419.0, 44420.0, 44417.61, 44422.0, 44423.0, 44424.0, 44425.0, 44426.0, 44427.0, 44428.0, 44429.0, 44430.6, 44430.0, 44432.0, 44433.0, 44434.0, 44435.29, 44436.0, 44435.0, 44436.15, 44431.0, 44440.0, 44441.0, 44433.4, 44443.22, 44444.0, 44445.0, 44446.0, 44442.0, 44448.0, 44449.0, 44450.69, 44451.0, 44452.0, 44452.2, 44450.0, 44453.0, 44454.0, 44456.0, 44458.0, 44459.22, 44460.0, 44457.0, 44462.0, 44455.0, 44463.0, 44465.0, 44466.0, 44467.0, 44468.0, 44469.0, 44470.4, 44470.0, 44472.0, 44473.0, 44473.52, 44475.0, 44476.0, 44477.0, 44474.0, 44478.0, 44480.0, 44477.79, 44482.14, 44482.0, 44484.0, 44479.0, 44486.0, 60945.0, 44488.0, 44487.72, 44490.0, 44491.0, 44492.0, 44492.4, 44493.0, 44495.0, 44496.0, 44499.0, 44500.0, 44501.0, 44503.0, 44504.0, 44505.0, 44506.85, 44506.0, 44508.0, 44508.96, 44510.0, 44511.12, 44512.0, 44511.0, 44514.0, 44515.0, 44515.01, 44516.0, 306662.0, 44517.0, 44520.0, 44521.0, 44522.0, 44523.4, 44524.0, 44525.0, 44526.0, 44527.21, 44528.0, 44530.0, 44531.76, 44532.0, 44533.0, 44534.21, 44535.0, 44532.8, 44537.0, 44538.0, 44538.23, 44540.0, 44541.38, 44534.0, 44543.0, 44544.0, 44545.58, 44546.88, 44545.0, 44548.0, 44549.0, 44550.0, 44549.4, 44552.0, 44553.0, 44554.0, 44555.42, 44556.0, 44557.76, 44557.0, 44553.6, 44560.0, 44555.0, 44562.0, 44563.0, 44564.0, 44565.0, 44562.62, 44567.0, 44568.0, 44569.0, 44570.0, 831000.0, 44572.0, 44573.0, 44574.4, 44575.0, 44575.73, 44574.0, 44578.0, 44579.0, 44580.0, 44581.0, 44582.29, 44583.72, 44584.0, 44582.0, 44586.51, 60964.0, 44586.0, 44589.0, 44590.0, 44591.0, 44592.0, 44592.63, 44594.0, 44594.16, 44596.0, 44595.0, 44598.96, 44595.2, 44600.0, 44601.8, 44601.0, 44599.0, 44604.0, 44605.0, 44604.55, 44607.0, 44608.0, 44602.4, 44610.0, 44611.0, 44608.8, 44614.0, 44615.0, 44616.0, 44617.92, 44618.0, 44619.0, 44620.0, 44621.0, 44622.0, 44617.0, 44624.0, 44625.0, 44626.0, 44627.0, 44628.33, 44629.0, 44630.28, 44628.0, 44632.0, 44634.0, 44635.83, 44636.8, 44636.0, 44637.0, 44639.0, 44640.0, 44641.0, 44642.0, 44638.0, 44644.17, 44644.0, 44646.0, 44648.0, 44650.0, 44651.52, 44652.0, 44653.0, 44651.2, 44655.0, 44656.68, 44657.6, 44658.0, 44657.0, 44660.0, 44659.0, 44654.23, 44654.0, 44664.0, 44665.0, 44667.0, 44668.0, 306813.0, 44670.0, 44671.0, 44673.0, 44674.0, 44675.86, 44676.0, 44677.0, 44678.4, 44678.0, 44680.0, 44679.0, 44682.0, 44680.56, 44675.0, 44685.0, 44686.8, 44687.0, 44688.0, 44688.8, 44690.0, 44684.66, 44692.96, 44693.0, 44694.0, 44695.0, 44696.0, 44697.0, 44689.0, 44699.0, 44700.0, 44699.2, 44702.0, 44703.0, 44707.32, 44708.0, 44709.24, 44709.0, 44711.0, 44712.0, 44713.77, 44714.2, 44714.0, 44715.0, 569000.0, 44718.0, 44719.0, 44720.0, 44721.24, 44722.0, 44722.29, 44724.0, 44725.0, 44721.0, 44727.0, 44728.0, 44730.0, 44731.0, 44732.68, 44733.0, 44734.0, 44735.0, 44736.0, 44737.0, 44738.0, 44739.24, 44739.0, 44741.0, 44741.3, 44742.0, 44744.0, 44745.0, 44740.0, 44747.0, 44748.0, 44747.4, 44750.0, 44751.0, 44752.0, 44749.9, 44754.0, 44755.0, 44756.0, 44753.0, 44758.0, 44758.82, 44760.0, 44761.0, 44762.0, 44763.0, 44764.8, 44764.0, 44765.0, 44766.0, 44768.0, 44765.76, 44770.0, 44771.0, 44772.96, 44772.0, 44767.0, 44775.0, 44776.0, 44777.0, 44778.0, 44774.0, 44780.0, 44774.4, 44782.0, 44782.4, 44784.0, 44783.0, 44786.0, 44787.0, 44786.88, 44789.0, 44790.89, 44791.0, 44790.0, 44793.0, 44794.0, 44788.0, 44796.0, 44797.0, 44789.16, 44799.0, 44800.0, 44803.0, 44804.0, 44805.0, 44807.4, 44808.0, 44809.0, 44810.0, 44811.61, 44812.0, 44813.0, 44813.65, 44815.0, 44816.0, 44817.0, 44811.0, 44819.0, 44820.0, 44821.0, 44824.0, 44825.0, 44826.89, 306970.0, 44828.0, 44829.0, 44830.0, 44827.0, 44832.0, 44833.0, 44834.4, 44835.0, 44834.52, 44828.88, 44829.36, 44839.0, 44840.0, 44834.0, 44842.0, 44843.0, 44844.0, 44845.0, 44844.8, 44847.0, 44848.41, 44848.0, 44850.0, 44851.82, 44851.0, 44853.0, 44854.0, 44847.67, 307000.0, 44856.0, 44858.0, 44856.68, 44860.0, 44857.0, 44862.0, 44863.0, 44862.84, 44865.0, 307008.0, 44867.0, 44868.0, 44869.0, 44870.0, 44871.0, 44865.6, 44873.0, 44872.0, 44875.0, 44876.0, 44877.0, 44878.0, 44879.0, 44880.0, 44881.08, 44882.0, 44882.88, 44884.0, 44881.0, 44886.4, 44886.0, 44888.0, 44889.0, 44890.0, 44891.0, 44892.0, 44894.0, 44895.88, 44896.31, 44896.0, 307042.0, 44897.16, 44900.0, 44901.0, 44901.48, 44900.18, 44904.0, 44902.0, 44903.0, 44907.0, 44908.0, 44907.2, 44909.0, 44908.27, 44912.0, 44913.0, 44914.0, 44915.0, 44915.16, 44916.0, 44917.0, 44919.6, 44920.0, 44921.0, 44919.55, 44918.27, 44924.0, 44924.22, 44926.85, 44925.0, 44928.0, 44928.6, 44929.0, 44931.0, 44932.92, 44926.8, 44934.0, 44935.8, 44935.32, 44935.0, 44936.0, 44939.0, 44939.21, 44940.0, 44941.0, 44945.0, 44946.0, 44948.8, 44948.0, 44950.0, 44950.1, 44952.0, 44951.4, 44955.0, 44956.0, 44957.0, 44958.0, 44959.0, 44960.0, 44961.0, 44962.0, 44963.0, 44964.0, 44966.44, 44966.0, 44967.0, 44969.6, 44969.0, 44970.0, 44972.0, 44973.0, 44974.0, 44975.0, 44976.0, 44976.91, 44978.0, 44977.0, 44980.0, 44975.52, 44982.0, 44983.0, 44984.0, 44985.0, 44987.0, 44988.0, 44989.0, 44990.0, 44991.0, 44990.4, 1880000.0, 44994.0, 44995.0, 44996.0, 44994.56, 44998.0, 44999.0, 45000.0, 45001.0, 45002.0, 45003.12, 45004.0, 45005.8, 45006.0, 45007.0, 45008.0, 45003.0, 45010.0, 45011.0, 45012.0, 45012.72, 45013.55, 45013.8, 45011.2, 45016.0, 45018.0, 45019.0, 45020.0, 45021.0, 45022.11, 45023.0, 45024.0, 45022.38, 45025.75, 45027.0, 45028.2, 45030.0, 45032.0, 45034.0, 45034.08, 45036.0, 45036.91, 45037.0, 45039.0, 45040.0, 45041.0, 45038.66, 45038.0, 45035.0, 45045.0, 45044.0, 45047.66, 45048.0, 45048.48, 45050.0, 45051.0, 45052.0, 45053.0, 45052.8, 45055.0, 307200.0, 45057.72, 45058.0, 45052.32, 45060.0, 45057.6, 45062.0, 45063.0, 45064.0, 45056.0, 45066.0, 45067.0, 45069.0, 45070.0, 45071.0, 45072.0, 45074.0, 45076.0, 45078.0, 45080.0, 45080.6, 45082.0, 45084.0, 45085.0, 45086.92, 45088.0, 45090.0, 45091.0, 45093.0, 45094.0, 45095.0, 45096.0, 45099.0, 45100.0, 45099.4, 45100.38, 45103.0, 45104.0, 45100.8, 45106.0, 45107.0, 45108.84, 45109.56, 45109.0, 45111.0, 45108.8, 45113.0, 45108.0, 45115.2, 45115.0, 45116.0, 45118.0, 45118.26, 45120.0, 45121.0, 45122.0, 45123.2, 45118.48, 45123.0, 45126.0, 45127.0, 45125.0, 45129.0, 45130.0, 45131.0, 45132.0, 45133.0, 45134.0, 45128.0, 45136.0, 45137.44, 45139.0, 45140.0, 45141.0, 45143.0, 45144.0, 45145.0, 45146.0, 45148.0, 45149.0, 45150.0, 45153.0, 45154.0, 45155.0, 45156.0, 45157.0, 45159.84, 45159.72, 45159.0, 45160.96, 45162.0, 45164.0, 45165.78, 45166.0, 45167.0, 45168.0, 45160.0, 45161.0, 45172.0, 45173.32, 45172.49, 45175.0, 45176.0, 45177.0, 45178.0, 45179.0, 45180.0, 45179.68, 45182.0, 45181.0, 45184.0, 45185.0, 45186.0, 45188.0, 45190.0, 45191.0, 45192.0, 45192.5, 45194.0, 45195.0, 45196.8, 45193.0, 45198.0, 45198.4, 45200.0, 45201.0, 45202.8, 45203.0, 45204.0, 45205.0, 45206.0, 45207.0, 45208.8, 45209.0, 45210.0, 45211.0, 45209.46, 45213.0, 45212.0, 45215.0, 45216.0, 45215.88, 45218.0, 45219.0, 45220.0, 45221.0, 45222.0, 45223.0, 45224.0, 45222.6, 45226.0, 45227.0, 45228.0, 45229.0, 45230.0, 45231.0, 45232.0, 45233.0, 45234.0, 45235.0, 45235.96, 45237.0, 45232.59, 45236.0, 45240.0, 45241.0, 45242.88, 45243.12, 45244.0, 45245.0, 45246.0, 45247.12, 45248.0, 45249.0, 45250.0, 45251.0, 45252.0, 45251.96, 45247.0, 45253.0, 45256.0, 307400.0, 45258.0, 45260.0, 45261.0, 45262.0, 45260.4, 45264.0, 45265.0, 45266.49, 45267.0, 45268.36, 45269.04, 45270.0, 45263.0, 45272.0, 45273.37, 45273.0, 45275.0, 45276.0, 45277.0, 45278.0, 45279.0, 45280.0, 45281.0, 45282.0, 45283.0, 45284.0, 45285.0, 45286.0, 45287.0, 45288.0, 45289.56, 45290.0, 45291.0, 45289.0, 45293.0, 45294.0, 45295.0, 45296.0, 45297.96, 45292.0, 45292.8, 45300.0, 45302.4, 45303.0, 45304.0, 45302.0, 45306.0, 45307.0, 45308.88, 45308.0, 45310.0, 45309.48, 45312.0, 45313.0, 45314.0, 45311.0, 45316.0, 45317.0, 45318.0, 45319.0, 45320.0, 45321.0, 45322.86, 45322.88, 45324.0, 45323.0, 45325.0, 307471.0, 45324.12, 45326.0, 45327.0, 45323.2, 45332.36, 45333.0, 45334.0, 45335.76, 45336.0, 45337.0, 61114.0, 45339.36, 45340.0, 45341.0, 45335.16, 45343.0, 45344.0, 45345.0, 45343.9, 45347.6, 45348.0, 45349.78, 45350.0, 45349.0, 45352.0, 45351.0, 45354.0, 45355.0, 45356.0, 45357.0, 45357.96, 45359.0, 45360.0, 307500.0, 45355.8, 45363.0, 45364.0, 45365.0, 45364.8, 45364.68, 45368.0, 45369.0, 45370.0, 45371.0, 45372.0, 45373.0, 45374.0, 45375.2, 45376.0, 45375.0, 45378.0, 45378.79, 45380.0, 45381.0, 45379.0, 45383.0, 45384.0, 45385.0, 45385.6, 45386.0, 45388.0, 45389.0, 45390.0, 45392.0, 45393.0, 45395.32, 45396.0, 45397.0, 45395.0, 45399.72, 45400.0, 45399.0, 45403.0, 45404.0, 45405.0, 45406.0, 45405.36, 45408.0, 45409.0, 45406.56, 45407.0, 45412.03, 45405.96, 45414.22, 45412.6, 45416.0, 45410.0, 45418.0, 45419.0, 45420.0, 45420.3, 45422.64, 45423.0, 45415.0, 45425.0, 45426.0, 45427.0, 45428.0, 45427.2, 45430.0, 45432.0, 45434.0, 45435.0, 45434.28, 45437.0, 45440.0, 45443.0, 45444.84, 45444.0, 45445.0, 45447.0, 45448.0, 45449.04, 45450.0, 45451.0, 45452.0, 45453.0, 45454.0, 45450.45, 45456.0, 45456.64, 45458.0, 45459.48, 45460.0, 45461.0, 45459.2, 45463.0, 45464.0, 45465.0, 45462.0, 45467.0, 45468.0, 45469.0, 45470.0, 45471.0, 45472.0, 45466.0, 45474.0, 45475.0, 45474.84, 45476.0, 45476.83, 45479.0, 45480.0, 45481.0, 45483.0, 45484.0, 45485.0, 45486.0, 45487.0, 45488.12, 45489.6, 45490.5, 45491.0, 45492.0, 45493.0, 45489.0, 45495.0, 45496.92, 45490.0, 45498.0, 45499.0, 45500.0, 45501.0, 45500.52, 6599100.0, 45504.0, 45505.0, 45497.0, 45508.0, 45510.0, 45510.4, 45512.0, 45513.0, 45514.0, 45512.11, 45516.0, 45517.0, 45518.64, 45515.0, 45520.0, 45521.0, 45517.03, 45523.0, 45524.0, 45525.0, 45526.0, 45527.0, 45528.0, 45529.6, 45530.0, 45531.0, 45531.24, 45532.13, 45534.0, 45532.0, 45536.0, 61154.0, 45539.0, 45540.0, 45541.97, 45542.4, 45543.24, 45544.87, 45545.0, 45546.0, 45543.74, 45548.0, 45549.0, 45550.0, 45551.0, 45552.0, 45553.0, 45554.0, 45555.0, 45555.73, 45557.0, 45558.73, 45558.0, 45560.17, 45560.0, 45562.0, 45563.0, 45564.0, 45565.0, 45566.0, 45567.96, 832000.0, 45567.0, 45570.0, 45571.0, 45572.0, 45573.0, 45574.0, 45575.0, 45576.0, 45577.0, 45578.0, 45572.8, 45580.0, 45581.0, 45582.0, 45581.95, 45584.0, 45583.0, 45585.0, 45587.0, 45588.0, 45589.0, 45590.0, 45585.28, 45592.0, 45593.6, 45594.27, 45595.0, 45596.66, 45596.0, 45594.0, 45599.0, 45600.0, 45593.0, 45602.0, 45602.46, 45604.0, 45605.0, 45607.0, 45608.0, 45609.0, 45610.0, 45611.0, 45612.0, 45613.0, 45614.0, 45615.0, 45614.4, 45611.8, 45618.0, 45619.0, 45620.0, 45621.0, 45622.68, 45623.0, 45624.0, 45622.0, 45626.0, 45627.0, 45628.0, 45625.0, 45630.0, 45629.0, 45632.0, 45633.27, 45633.92, 45635.0, 45636.0, 45637.0, 45635.2, 45637.56, 45635.34, 45641.0, 45642.0, 45643.0, 45643.68, 45645.0, 45646.0, 45644.0, 45648.0, 45648.84, 45650.0, 45651.0, 45652.0, 45653.0, 45654.73, 45654.0, 45656.0, 307800.0, 45649.0, 45659.04, 45660.05, 45660.0, 45661.0, 45662.0, 45663.44, 45664.0, 45663.0, 45666.0, 45667.0, 45667.94, 45665.0, 45670.0, 45671.04, 45672.0, 45673.0, 45674.0, 45675.0, 45676.0, 45677.0, 45678.0, 45680.0, 45681.0, 45679.0, 45682.8, 45684.0, 45685.56, 45686.0, 45687.0, 45682.0, 45683.0, 45690.0, 45689.0, 45692.0, 45693.0, 45694.0, 45695.0, 45696.0, 45697.0, 45698.0, 45699.0, 45700.0, 45701.0, 45704.0, 45705.0, 45706.0, 45708.0, 45709.0, 570000.0, 45712.0, 45715.0, 45716.0, 45718.4, 45719.37, 45720.0, 45719.0, 45722.0, 45723.0, 45724.0, 45725.0, 45726.0, 45727.0, 45727.84, 45729.01, 45730.0, 45728.4, 45732.0, 45733.0, 45733.32, 45734.0, 45736.75, 45731.0, 45738.0, 45739.0, 45739.2, 45740.29, 45742.02, 45741.0, 45744.57, 45744.0, 45743.0, 45747.0, 45740.0, 45742.5, 45750.0, 45750.33, 45752.0, 45747.96, 45753.6, 45755.0, 45756.0, 45750.48, 45750.12, 45759.96, 45760.0, 45761.0, 45762.0, 45763.56, 45759.0, 45761.9, 45765.0, 45767.0, 45768.57, 45769.0, 45770.0, 45771.0, 45772.01, 45772.0, 45774.4, 45775.0, 45774.56, 45777.0, 45776.0, 45780.0, 45780.08, 45782.0, 45783.0, 45784.0, 45785.0, 45785.4, 45785.06, 45788.0, 45789.96, 45790.0, 45789.0, 45792.0, 45791.0, 45794.11, 45795.0, 45796.8, 45797.0, 45797.43, 45799.0, 45800.0, 45801.0, 45802.08, 45803.0, 45804.0, 45801.6, 45806.0, 45806.16, 45808.0, 45810.0, 45811.0, 45812.0, 45813.24, 45814.0, 45816.0, 45817.08, 45817.0, 45817.8, 45820.0, 45821.0, 45822.0, 45823.18, 45823.0, 45825.0, 45824.0, 45827.0, 45828.0, 45829.0, 45830.0, 45831.0, 45832.0, 45833.0, 45827.76, 45835.0, 45836.0, 45837.0, 45838.0, 45839.0, 45840.0, 45839.64, 45842.04, 45843.2, 45843.0, 45845.0, 45846.0, 45848.0, 45849.0, 45850.0, 45851.0, 45852.0, 45854.0, 45855.0, 308000.0, 45857.0, 45856.32, 45856.0, 45860.0, 45860.61, 45862.0, 45859.2, 45864.0, 45865.0, 45866.6, 45866.0, 45868.08, 45869.0, 45868.0, 45870.0, 45870.22, 45873.0, 45874.0, 45875.0, 45876.0, 45877.0, 45878.0, 45879.0, 45880.0, 45881.0, 45882.0, 45883.0, 45884.0, 45884.8, 45885.12, 45887.0, 45888.0, 45889.0, 45890.0, 45891.0, 45892.0, 45890.88, 45886.0, 45895.0, 45896.0, 45897.0, 45898.0, 45898.2, 45900.0, 45899.0, 45893.0, 45903.0, 45904.0, 45905.0, 45905.73, 45907.0, 45908.0, 45906.0, 45905.6, 45907.5, 45912.0, 45913.0, 45914.0, 45915.0, 45916.0, 45917.0, 45918.0, 45916.06, 45920.0, 45921.36, 45920.2, 45922.0, 45923.0, 45924.0, 45925.0, 45926.0, 45926.4, 45921.45, 45930.0, 45931.0, 45932.0, 45934.0, 45936.0, 45938.0, 45939.0, 45941.93, 45942.0, 45943.0, 45944.0, 45945.0, 45946.0, 45947.0, 45948.0, 45949.9, 45950.0, 45951.0, 45953.0, 45954.0, 45955.0, 45956.0, 45957.0, 45958.12, 45960.0, 45961.0, 45962.0, 45961.2, 45964.0, 45965.92, 45964.11, 45967.0, 45968.0, 45960.24, 45970.0, 45971.04, 45972.0, 45973.0, 45974.0, 45975.25, 45971.0, 45977.0, 45978.0, 45979.0, 45980.0, 45981.0, 45981.26, 45982.0, 45984.0, 45985.0, 45986.0, 45987.0, 45988.88, 45988.0, 45990.0, 45989.0, 45992.51, 45992.0, 45994.0, 45995.0, 45996.0, 45988.8, 45999.0, 46000.0, 45999.96, 46002.0, 46003.56, 46004.0, 46005.0, 46006.0, 46007.0, 46008.0, 46009.0, 46010.0, 46007.15, 46012.0, 46013.0, 46014.0, 46015.0, 46011.0, 46017.0, 46018.8, 46018.0, 46020.0, 46021.0, 46022.0, 46023.0, 46025.0, 46026.24, 46027.0, 46028.0, 46028.1, 46030.0, 46030.4, 46032.0, 46031.0, 46028.97, 46035.0, 46034.0, 46037.55, 46038.0, 46040.0, 46041.0, 46041.23, 46043.0, 46044.0, 46045.0, 46046.0, 46048.0, 46049.0, 46049.52, 46051.2, 46050.53, 46053.0, 46051.0, 46050.0, 46056.0, 46057.0, 46057.5, 46058.0, 46060.0, 46060.56, 46062.0, 46059.0, 46061.75, 46065.12, 46066.0, 46065.0, 46070.0, 46071.55, 46072.0, 46071.0, 46073.0, 46075.0, 46076.0, 46076.76, 46078.0, 46079.0, 46080.0, 46081.0, 46082.0, 46080.64, 46084.0, 46077.78, 46086.0, 46087.0, 46088.0, 46089.0, 46090.98, 46090.0, 46092.0, 570380.0, 46091.0, 46094.0, 46096.0, 46097.0, 46098.0, 46099.0, 46100.0, 46101.0, 46102.55, 46102.0, 46104.0, 46105.0, 46106.16, 46104.55, 46108.0, 46109.76, 46110.0, 46111.0, 46112.0, 46113.56, 46114.0, 46115.0, 46116.0, 46117.0, 46118.0, 46113.0, 46120.0, 46121.0, 46115.5, 46118.4, 46124.0, 46125.0, 46126.24, 46125.75, 46128.0, 46126.8, 46130.0, 46125.12, 46126.0, 46132.0, 46134.0, 46135.0, 46134.4, 46137.6, 46138.0, 46139.0, 46140.0, 46141.0, 46142.0, 46143.13, 46144.0, 46145.04, 46146.0, 46145.0, 46148.0, 46149.0, 46150.0, 46151.0, 46151.04, 46153.0, 46154.0, 46155.0, 46156.35, 46157.0, 46152.0, 46159.88, 46160.0, 46159.0, 46155.86, 46163.0, 46164.0, 46163.4, 46166.0, 46167.0, 46168.66, 46169.0, 46167.07, 46168.0, 46172.0, 46173.0, 46170.0, 46175.0, 46176.0, 61281.0, 46178.0, 46178.74, 46180.42, 46179.62, 46174.0, 46183.0, 46184.0, 46185.6, 46185.0, 46187.63, 46188.0, 46189.0, 46190.0, 46191.0, 46187.0, 46193.0, 46193.1, 46195.31, 46196.8, 46195.0, 46198.0, 46196.0, 46200.0, 46197.0, 46202.0, 46199.0, 46204.0, 308349.0, 46203.48, 46203.0, 46208.0, 46209.0, 46210.0, 46206.0, 46212.0, 46211.0, 46214.4, 46215.0, 46216.0, 46217.0, 46217.6, 46219.0, 46220.0, 46214.0, 46222.0, 46223.08, 46224.0, 46225.0, 46223.6, 46223.0, 46228.0, 46229.0, 46230.0, 46231.88, 46232.0, 46233.0, 46234.0, 46231.0, 46236.0, 46237.0, 46238.0, 46240.0, 46241.0, 46242.0, 46243.0, 46242.4, 46245.0, 46246.0, 46244.0, 46248.0, 46249.0, 46250.0, 46251.0, 46251.4, 46252.0, 46254.0, 46255.0, 46256.0, 46257.0, 46256.64, 46259.0, 46260.0, 46258.0, 46262.0, 46261.0, 46264.0, 46265.0, 46266.0, 46267.2, 46268.0, 46270.0, 46271.4, 46272.0, 46273.49, 46274.0, 46275.0, 46276.0, 46277.0, 46278.0, 46280.0, 46282.0, 46283.0, 46284.0, 46285.0, 46287.0, 46288.0, 46289.0, 46290.0, 46291.0, 46292.0, 46293.0, 46294.0, 46294.8, 46296.0, 46298.0, 46298.64, 46300.0, 46301.0, 46301.52, 46302.0, 46304.0, 46305.0, 46303.0, 46308.0, 46309.0, 46310.0, 46311.0, 46312.08, 46313.0, 46312.87, 46315.0, 46312.0, 46318.0, 46319.0, 46320.0, 46321.0, 46321.6, 46323.42, 46322.26, 46325.0, 46326.16, 46327.0, 46328.0, 46329.48, 46329.0, 46331.0, 46332.0, 46333.0, 308474.5, 46335.0, 46336.0, 46337.0, 46330.0, 46339.8, 46340.0, 46340.11, 46342.0, 46342.24, 46344.0, 46343.0, 46346.9, 46347.0, 46342.4, 46349.97, 46350.0, 46349.26, 46346.0, 46348.0, 46354.0, 46349.96, 46356.0, 46357.0, 46358.0, 46358.28, 46360.0, 46360.08, 308500.0, 46359.0, 46356.5, 46365.0, 46366.0, 46367.0, 46368.0, 46369.0, 46370.0, 46369.2, 46372.0, 46373.0, 46371.0, 46375.0, 46376.0, 46377.0, 46378.0, 46376.98, 46380.0, 46381.0, 46381.4, 46383.0, 46384.0, 46385.0, 46386.0, 46383.96, 46382.0, 46387.0, 46390.1, 46391.0, 46392.0, 46393.0, 46394.0, 46395.0, 46395.36, 46397.0, 46398.0, 46399.0, 46400.0, 46396.0, 46402.8, 46403.69, 46404.8, 46405.0, 46406.0, 46407.92, 46403.27, 46408.0, 46410.0, 46411.0, 46412.1, 46412.0, 46413.0, 46415.4, 46416.0, 46416.31, 46415.0, 46419.0, 46420.0, 46413.9, 46422.0, 46421.59, 46424.0, 46425.0, 46426.0, 308571.0, 46428.0, 46423.0, 46430.43, 46431.47, 46431.0, 46433.0, 46434.0, 46432.0, 46436.0, 46436.16, 46434.72, 46439.0, 46440.0, 46437.0, 46438.0, 46443.36, 46444.41, 46445.0, 46446.0, 46447.0, 46448.69, 46448.0, 46450.0, 46446.4, 46452.0, 46447.92, 46453.0, 46449.0, 46456.0, 61336.0, 46458.0, 46458.96, 46460.0, 46461.0, 46462.8, 46463.0, 46464.0, 46465.0, 46467.0, 46471.0, 46472.0, 46473.0, 46475.0, 46476.0, 46477.0, 46478.0, 46479.0, 46480.0, 46481.0, 46484.0, 46485.0, 46487.0, 46488.0, 46492.0, 46493.0, 46494.0, 46495.0, 46497.0, 46498.0, 46499.0, 46500.0, 46504.0, 46505.0, 46507.5, 46508.0, 46509.0, 46507.08, 46511.91, 46512.0, 46512.76, 46509.33, 46515.0, 46516.0, 46517.0, 46517.6, 46518.25, 46520.0, 46520.88, 46518.0, 46523.0, 46524.0, 46525.0, 46526.0, 46519.0, 46521.0, 46529.0, 46530.96, 46530.0, 46532.0, 46533.0, 46531.68, 46535.0, 46536.0, 46537.0, 46536.9, 46531.0, 46540.0, 46541.87, 46541.94, 46543.0, 46542.0, 46545.0, 46546.08, 46547.0, 46548.0, 46549.0, 46550.0, 46546.92, 46552.0, 46553.0, 46554.0, 46546.0, 46556.0, 46557.0, 46556.16, 46558.0, 46560.0, 46557.96, 46562.0, 46563.0, 46561.0, 46565.0, 46566.0, 46567.13, 833000.0, 46561.21, 46570.0, 46571.0, 46572.0, 46573.0, 46574.0, 46575.0, 46576.0, 46577.81, 46578.0, 46579.0, 46580.0, 46582.0, 46583.0, 46584.0, 46585.0, 46585.95, 46587.0, 46588.0, 46586.0, 46590.0, 46590.44, 46592.0, 46593.0, 46594.0, 46595.0, 46596.66, 46596.0, 46597.0, 46599.0, 46600.0, 46602.0, 46604.0, 46605.0, 308751.0, 46608.0, 46610.0, 46612.0, 46612.8, 46613.0, 46615.0, 46615.13, 46614.0, 46618.0, 46616.86, 46620.0, 46616.0, 46623.0, 46625.75, 46625.0, 46627.0, 46626.56, 46629.0, 46630.0, 46628.0, 46632.0, 46633.0, 46634.0, 46635.0, 46634.04, 46631.0, 46638.0, 46640.0, 46641.19, 46642.08, 46644.0, 46645.0, 46646.0, 46645.92, 46648.0, 46647.0, 46650.0, 46651.94, 46651.0, 46652.0, 46654.0, 46655.0, 46656.0, 46654.4, 46658.0, 46657.0, 46660.0, 46658.88, 46662.0, 46663.0, 46663.85, 46665.0, 46666.0, 46664.64, 46668.0, 46663.2, 46670.0, 46671.0, 46664.0, 46673.0, 46674.0, 46675.0, 46675.2, 46675.46, 46678.0, 46679.0, 46680.0, 46676.0, 46683.0, 46684.0, 46685.0, 46684.44, 46687.0, 46686.0, 46688.0, 46690.52, 46690.0, 46691.0, 46692.0, 46694.0, 46689.0, 46696.0, 46696.8, 46698.96, 46698.0, 46700.0, 308843.0, 46702.0, 46703.88, 46704.0, 46705.0, 46706.0, 46703.0, 46709.0, 46710.0, 46711.0, 571000.0, 46713.22, 46713.0, 46715.0, 46716.0, 46709.04, 46718.0, 46719.0, 46720.0, 46721.4, 46722.0, 46723.0, 46724.88, 46725.0, 46724.0, 46727.16, 46728.0, 46720.28, 46730.0, 46729.0, 46733.0, 46735.0, 46736.0, 46737.0, 46738.0, 46739.0, 46740.0, 46741.0, 46742.0, 46742.52, 46743.7, 46745.0, 46746.0, 46746.98, 46748.0, 46749.0, 46750.0, 46749.3, 46752.0, 46753.0, 46754.0, 46755.0, 46756.0, 46757.0, 46758.0, 46759.0, 46760.0, 46761.0, 46758.92, 46763.0, 46764.4, 46764.0, 46766.24, 46766.46, 46767.0, 46769.0, 46770.0, 46766.0, 46772.44, 46771.0, 46774.0, 46775.0, 46776.0, 46777.0, 46776.48, 46779.2, 46780.0, 46781.0, 46779.22, 46782.0, 46784.0, 46785.0, 46786.0, 46787.0, 46788.0, 46789.0, 46790.0, 46791.0, 46792.0, 46789.44, 46794.0, 46795.0, 46796.0, 46797.0, 46798.0, 46799.2, 46800.0, 46801.0, 46799.0, 46803.0, 46804.0, 46805.0, 46806.12, 46807.0, 46808.0, 46809.36, 46809.8, 46811.0, 46812.0, 46805.67, 46814.0, 46815.0, 46816.0, 46809.24, 46818.0, 46819.0, 46820.8, 46820.0, 46822.0, 46821.0, 46824.0, 46825.0, 46826.0, 46827.2, 46828.0, 46828.82, 46830.0, 46831.0, 46829.0, 46832.0, 46834.0, 46826.4, 46836.0, 46837.0, 46838.0, 46834.2, 46840.0, 46841.0, 46841.6, 46842.0, 46843.0, 46845.0, 46844.0, 46847.01, 46848.0, 46849.0, 46850.0, 46852.0, 46853.0, 46854.0, 309000.0, 46856.88, 46858.0, 46859.0, 46860.0, 46856.0, 46862.0, 46863.0, 46857.0, 46864.0, 46866.0, 46867.0, 46868.0, 46865.0, 46871.0, 46872.0, 46873.2, 46874.0, 46875.0, 46876.0, 46877.0, 46878.0, 46879.0, 46880.0, 46881.0, 46877.88, 46883.0, 46884.29, 46884.0, 46886.0, 46885.76, 46885.0, 46887.6, 46890.0, 46891.0, 46883.2, 46893.0, 46894.0, 46895.0, 46896.0, 46898.0, 46899.0, 46900.0, 46902.0, 46903.0, 46904.0, 46905.6, 46906.0, 46907.16, 46908.0, 46907.0, 46910.0, 46911.84, 46907.64, 46909.69, 46914.0, 46915.0, 46912.0, 46917.0, 46917.54, 46918.0, 46920.0, 46913.0, 46922.0, 46923.0, 46924.8, 46925.0, 46924.9, 46924.0, 46927.0, 46930.0, 46931.0, 46932.0, 46935.0, 46936.0, 46937.29, 46940.83, 46941.0, 46943.0, 46944.0, 46945.6, 46945.0, 46946.0, 46948.0, 46949.0, 46950.0, 46952.0, 46953.0, 46954.0, 46955.0, 46956.0, 46957.0, 46960.0, 46961.0, 46962.0, 46963.0, 46965.0, 46966.0, 46967.89, 46968.0, 46966.34, 46970.0, 46971.0, 46972.0, 46973.0, 46974.0, 46975.0, 46976.16, 46977.0, 46976.18, 46979.0, 46980.0, 46974.8, 46982.0, 46983.0, 46984.0, 46981.0, 46986.0, 46987.0, 46985.0, 46989.0, 46990.0, 46991.0, 46992.0, 46984.4, 46994.0, 46995.0, 46996.0, 46997.0, 46998.0, 46999.94, 47000.0, 47001.0, 47002.0, 47003.0, 47004.0, 47005.0, 46999.0, 47007.0, 47008.0, 47009.0, 47010.0, 47012.0, 47014.0, 47015.0, 47016.0, 47017.91, 47018.5, 47018.88, 47020.0, 47018.0, 47022.0, 47023.6, 47015.88, 47025.0, 47024.0, 47027.3, 47028.0, 47029.0, 47028.8, 47031.0, 47032.0, 47033.0, 47034.0, 47032.73, 47030.0, 47037.34, 47036.0, 47040.0, 47041.84, 47042.0, 47041.0, 47044.03, 47045.0, 47041.2, 47047.0, 47048.0, 47049.0, 47049.6, 47050.0, 47052.0, 47049.24, 47054.0, 47055.0, 309200.0, 47057.29, 47058.0, 47059.0, 47060.0, 47061.0, 47061.8, 47062.0, 47064.0, 47065.0, 47057.0, 47067.0, 47068.08, 47069.0, 47070.0, 47070.4, 47072.0, 47071.0, 47074.0, 47071.45, 47076.0, 47069.16, 47078.88, 47078.0, 47080.8, 47080.0, 47084.0, 47086.0, 47087.0, 47088.0, 47089.0, 47090.0, 47091.2, 47092.0, 47093.0, 47093.28, 47095.0, 47091.0, 47091.72, 47098.8, 47099.0, 47100.0, 47092.71, 47094.0, 47104.0, 47105.28, 47106.0, 47107.0, 47108.0, 47109.44, 47110.0, 47111.0, 47112.0, 47113.08, 47110.78, 47114.71, 47116.0, 47117.0, 47118.0, 47113.56, 47120.0, 47122.0, 47122.78, 47124.0, 47125.0, 47126.64, 47126.0, 47128.0, 47126.16, 47127.0, 47131.5, 47132.8, 47133.0, 47132.0, 47135.0, 47136.0, 47137.44, 47138.0, 47139.12, 47140.0, 47141.0, 571429.0, 47143.0, 47143.06, 47138.1, 47146.0, 47147.0, 47148.0, 47149.0, 47150.0, 47150.88, 47152.0, 47153.6, 47154.0, 47155.0, 47156.0, 47157.0, 47158.0, 47157.93, 47160.0, 47158.56, 47153.0, 47164.0, 47165.0, 47166.0, 47167.0, 47168.0, 47170.91, 47171.0, 47172.0, 47170.0, 47174.4, 47175.0, 47176.0, 47177.0, 47174.0, 47179.92, 47180.0, 47181.0, 47179.0, 47183.28, 47184.0, 47184.48, 47186.96, 47184.6, 47187.0, 47186.12, 47190.0, 47191.0, 47192.0, 47193.0, 47194.0, 47195.2, 47196.0, 47195.0, 47199.0, 47200.0, 47201.0, 47201.88, 47203.0, 47204.0, 47205.0, 47206.8, 47207.0, 47208.0, 47209.0, 47210.0, 47211.0, 47212.0, 47211.84, 47214.0, 47206.0, 47216.0, 47216.16, 47218.08, 47219.04, 47220.0, 47218.12, 47222.0, 47223.0, 47224.36, 47224.0, 47227.89, 47227.2, 47229.0, 47230.44, 47231.69, 47231.0, 47232.0, 47234.0, 47233.0, 47236.0, 47237.0, 47230.0, 47235.72, 47240.0, 47241.0, 47242.0, 47243.0, 47244.0, 47245.0, 47239.0, 47248.0, 47249.0, 47250.0, 47251.91, 47252.0, 47253.0, 47251.0, 47254.0, 47256.0, 47257.6, 47257.0, 47258.0, 47260.0, 47260.82, 47262.0, 47261.0, 47265.0, 47266.0, 47267.0, 47268.0, 47269.0, 47270.0, 47272.0, 47273.0, 47274.0, 47275.0, 47276.0, 47277.0, 47278.0, 47279.0, 47280.0, 47280.22, 47282.0, 47283.0, 47284.0, 47281.0, 47286.0, 47285.0, 47288.0, 47289.0, 47290.0, 47291.0, 47292.0, 47290.87, 47292.96, 47295.0, 47295.75, 47295.48, 47297.28, 47299.0, 47300.0, 47299.2, 47301.0, 47303.0, 47304.0, 47297.46, 47300.87, 47307.2, 47308.0, 47309.0, 47310.0, 47311.0, 47312.0, 47313.0, 47314.0, 47315.0, 47316.0, 47318.0, 47320.0, 47321.0, 47322.36, 47323.0, 47324.0, 47325.0, 47326.0, 47328.0, 47329.0, 47330.0, 47332.0, 47333.0, 47335.0, 47336.0, 47337.0, 47338.0, 47339.6, 47340.0, 47340.07, 47342.0, 47339.0, 47343.0, 47345.0, 47346.0, 47347.0, 47346.66, 47348.0, 47350.0, 47351.0, 47352.0, 47353.0, 47352.12, 47353.41, 47356.0, 47357.0, 47358.0, 47354.0, 47360.0, 47361.0, 47362.0, 47363.94, 47364.0, 47365.0, 47366.0, 47367.0, 47368.0, 47369.0, 47370.0, 47363.0, 47372.0, 47373.0, 47374.32, 47375.0, 47376.0, 47371.0, 47379.0, 47380.0, 47380.64, 47381.0, 47383.0, 47382.4, 47385.0, 47385.99, 47387.0, 47388.0, 47389.85, 47390.0, 47391.0, 47392.0, 47391.59, 47393.0, 47395.0, 47397.0, 47399.0, 47400.0, 47402.04, 47403.0, 47404.0, 47403.2, 47406.0, 47402.0, 47405.0, 47409.96, 47410.0, 47412.0, 47413.0, 47414.0, 47416.15, 47416.0, 47418.0, 47417.0, 47420.0, 47422.0, 47423.48, 1096000.0, 47424.0, 47426.0, 47423.0, 47428.0, 47429.0, 47430.0, 47431.8, 47431.0, 47427.0, 47434.02, 47435.0, 47436.0, 47437.0, 47436.33, 47439.0, 47440.0, 47435.23, 47442.0, 47443.0, 47444.0, 47445.0, 47446.0, 47444.8, 47448.0, 47449.0, 47450.0, 47451.0, 47452.0, 47453.0, 47450.1, 47455.43, 309600.0, 47457.96, 47456.0, 47456.24, 47460.0, 47454.0, 47462.0, 47463.0, 47464.0, 47465.0, 47466.75, 47467.0, 47468.0, 47467.19, 47470.0, 47471.06, 47472.0, 47473.2, 47474.0, 47475.0, 47476.0, 47476.08, 47476.76, 47479.0, 47480.0, 47481.0, 47482.0, 47480.04, 47484.0, 47485.0, 47486.0, 47487.0, 47488.0, 47486.4, 47486.97, 47491.0, 47492.0, 47490.0, 47493.06, 47495.0, 47496.0, 47497.0, 47498.0, 47499.0, 47500.0, 47500.08, 47502.0, 47499.65, 47504.0, 47505.0, 47502.15, 47507.0, 47508.0, 47507.2, 47503.32, 47511.0, 47512.0, 47513.64, 47513.0, 47515.0, 47516.0, 47516.12, 47520.0, 47521.0, 47522.0, 47523.0, 47524.0, 47525.0, 47526.24, 47527.0, 47528.0, 47529.0, 47530.0, 47526.0, 47532.0, 47529.97, 47534.0, 47535.0, 47536.0, 47532.24, 47538.0, 47539.2, 47540.0, 47541.0, 47542.0, 47544.0, 47545.0, 47546.0, 47548.0, 47548.8, 47550.0, 47552.0, 47553.0, 47554.0, 47556.0, 47558.0, 47560.0, 47561.0, 47562.0, 47563.0, 47564.4, 47564.0, 47565.0, 47567.0, 47568.0, 47569.0, 47570.0, 47567.53, 47572.0, 47573.0, 47566.0, 47575.0, 47572.96, 47571.0, 47578.47, 47579.0, 47580.0, 47581.0, 47582.88, 47580.14, 47584.0, 47585.0, 47586.0, 47586.65, 47588.16, 47589.68, 47590.0, 47587.0, 47592.0, 47593.0, 47588.0, 47595.0, 47596.0, 47597.0, 47597.15, 47599.0, 47600.0, 47598.0, 47601.0, 47603.0, 47604.0, 47605.0, 47606.0, 47610.0, 47611.0, 47611.2, 47610.94, 47614.0, 47613.0, 47616.0, 47616.72, 47617.6, 47619.0, 47620.0, 47615.0, 47622.0, 47621.0, 47624.0, 47625.0, 47623.0, 47627.28, 47628.0, 47629.0, 47630.0, 47631.0, 47632.0, 47633.0, 47634.0, 47635.0, 47633.78, 47636.0, 47638.0, 47639.0, 47640.08, 47641.0, 47640.0, 47643.0, 47644.0, 47645.0, 47646.0, 47640.48, 47648.0, 47649.0, 47650.0, 47651.9, 47652.0, 47651.0, 47654.88, 47655.36, 47656.0, 47657.0, 47658.0, 47659.0, 47660.0, 47661.32, 47661.97, 47661.0, 47664.0, 47665.0, 47666.68, 47667.0, 47666.0, 47669.0, 47670.0, 47671.0, 47672.0, 47673.0, 47665.77, 47675.0, 47676.0, 47677.0, 47678.0, 47679.0, 47680.0, 47681.0, 47682.0, 47683.0, 47684.0, 47685.0, 47687.0, 47688.0, 47689.0, 47690.0, 47691.0, 47692.0, 47692.8, 47694.4, 47694.0, 47693.0, 47697.87, 47697.0, 47698.0, 47700.0, 47701.0, 47695.0, 47703.0, 47704.0, 47705.0, 47704.56, 47707.0, 47699.0, 47709.54, 47710.44, 47710.0, 572000.0, 47712.7, 47712.0, 47715.0, 47708.0, 47717.0, 47711.0, 47719.0, 47720.0, 47721.0, 47722.0, 47722.48, 47724.0, 47721.13, 47726.0, 47727.08, 47728.0, 47729.0, 47730.0, 47731.0, 47732.0, 47733.0, 47734.0, 47735.0, 47736.0, 47737.0, 47739.12, 47740.0, 47740.5, 47741.0, 47743.0, 47742.0, 47745.0, 47746.0, 47747.0, 47748.0, 47749.0, 47750.0, 47751.0, 47752.0, 47750.3, 47751.69, 47755.0, 47756.0, 47757.6, 47758.0, 47759.0, 47760.0, 47757.0, 47759.92, 47761.0, 47756.8, 47765.0, 47764.0, 47767.0, 47768.0, 47762.0, 47770.0, 47772.0, 47773.0, 47775.0, 47776.0, 47777.0, 47777.6, 47779.5, 47780.0, 47779.0, 47782.0, 47783.0, 47784.0, 47785.0, 47786.08, 47788.0, 47789.85, 47790.0, 47792.0, 47796.0, 47797.0, 47798.0, 47800.0, 47802.0, 47803.0, 47804.47, 47804.0, 47806.0, 47804.24, 47808.0, 47808.55, 47810.0, 47807.0, 47811.05, 47813.0, 47814.0, 47811.0, 47816.0, 47817.0, 47818.0, 47819.0, 47820.0, 47821.0, 47819.2, 47822.0, 47819.15, 47825.0, 47826.0, 47827.0, 47828.0, 47829.96, 47830.0, 47831.0, 47832.0, 47833.0, 47828.75, 47835.0, 309981.42, 47838.0, 47839.44, 47840.0, 47841.0, 47839.0, 47843.0, 47844.0, 47842.0, 47846.0, 47847.0, 47848.0, 47849.58, 47850.0, 47851.0, 309996.0, 47845.0, 47851.46, 47855.58, 310000.0, 47856.0, 47858.12, 47855.0, 47860.8, 47860.0, 47862.0, 47863.0, 310006.0, 47865.08, 47866.0, 47867.0, 47868.0, 47869.44, 47870.0, 47871.0, 47870.04, 47874.0, 47876.0, 47879.0, 47880.0, 47881.16, 47882.0, 47883.0, 47881.0, 47885.0, 47886.0, 47881.47, 47888.97, 47889.96, 47890.0, 47891.0, 47892.0, 47893.0, 47888.0, 47895.0, 47896.0, 47897.0, 47889.0, 47899.0, 47900.0, 47901.0, 47902.0, 47903.4, 47904.0, 47904.24, 47901.6, 47907.0, 47908.0, 47909.0, 47910.0, 47903.0, 47912.0, 47913.0, 47914.0, 47915.0, 47916.0, 47917.0, 47918.98, 47919.0, 47920.0, 47921.0, 47922.16, 47923.0, 47923.2, 47923.05, 47926.0, 47927.0, 47928.0, 47923.32, 47921.96, 47932.0, 47933.0, 47932.82, 47935.0, 47936.0, 47937.0, 47938.0, 47939.0, 47940.0, 47942.0, 47943.0, 47944.0, 572230.0, 47946.0, 47947.0, 47948.0, 47946.36, 47950.0, 47950.4, 47952.0, 47953.0, 47954.0, 47951.0, 47956.0, 47956.56, 47955.0, 47952.67, 47960.0, 47961.0, 47960.41, 47964.24, 47964.0, 47965.0, 47964.8, 47968.0, 47969.0, 47970.0, 47971.0, 47972.0, 47973.0, 47974.0, 47975.0, 47976.0, 47977.0, 47978.28, 47979.0, 47978.0, 47980.0, 47982.0, 47983.0, 47984.0, 47985.0, 47985.02, 47987.0, 47988.0, 47989.0, 47990.0, 47991.0, 47992.0, 47990.76, 47994.0, 47995.0, 47996.0, 47990.28, 47998.2, 47999.0, 48000.0, 48001.0, 48000.01, 48003.0, 48003.36, 48004.0, 48006.0, 48007.0, 48008.0, 48009.0, 48006.4, 48005.0, 48012.0, 48013.0, 48012.81, 48015.0, 48017.0, 48019.0, 48020.0, 48023.56, 48024.0, 48025.0, 48027.0, 48028.0, 48030.0, 48033.0, 48034.0, 48034.51, 48035.0, 48037.44, 48038.0, 48036.0, 48040.0, 48041.0, 48043.92, 48044.0, 48045.0, 48046.0, 48048.0, 48050.0, 48051.0, 48054.0, 48056.0, 48057.0, 48057.9, 48058.0, 48060.0, 48057.5, 48059.0, 48063.0, 48062.0, 48065.0, 48066.0, 48067.0, 48068.0, 48068.8, 48072.0, 48073.0, 48074.0, 48075.0, 48076.2, 48075.36, 48078.0, 48079.0, 48080.0, 48078.96, 48081.0, 48083.0, 48084.0, 48081.07, 48086.64, 48082.0, 48087.0, 48082.13, 48089.0, 48086.0, 48090.07, 48093.0, 48094.65, 48095.71, 48096.0, 48092.0, 48098.0, 48099.0, 48100.0, 48101.0, 48102.0, 48103.0, 48104.0, 48101.04, 310250.0, 48106.0, 48108.0, 48105.0, 48110.0, 48111.0, 48112.0, 48113.0, 48114.0, 48114.42, 48116.0, 48115.0, 572400.0, 48119.0, 48120.0, 48117.0, 48120.84, 48123.0, 48124.0, 48125.0, 48126.0, 48125.67, 48128.0, 48127.0, 48122.0, 48131.2, 48132.0, 48131.06, 48134.0, 48135.62, 48135.0, 48137.0, 48137.31, 48137.88, 48136.0, 48133.0, 48138.0, 48143.0, 48144.0, 48142.0, 48137.52, 48147.0, 48148.88, 48149.96, 48150.0, 48148.58, 48152.0, 48153.0, 48154.0, 48155.0, 48156.0, 48157.0, 48154.35, 310296.0, 48160.0, 48162.0, 48163.8, 48164.0, 48165.0, 48168.0, 48169.06, 48170.0, 48171.0, 48172.8, 48173.0, 48172.0, 48175.0, 48176.0, 48177.84, 48178.0, 48179.0, 48180.0, 48177.0, 48181.0, 48183.0, 48184.0, 48185.0, 48186.0, 48185.06, 48188.0, 48189.0, 48190.0, 48182.0, 48192.0, 48193.6, 48194.0, 48195.0, 48193.0, 48197.0, 48198.0, 48199.0, 48200.0, 48200.1, 48197.21, 48203.0, 48204.0, 48205.0, 48206.0, 48205.2, 48208.52, 48208.0, 48210.0, 48211.0, 48212.0, 48213.0, 48214.0, 48215.0, 48216.0, 48214.4, 48217.0, 48219.4, 48219.0, 48221.0, 48222.0, 48223.0, 48220.0, 48225.0, 48226.0, 48227.71, 48228.24, 48228.0, 48230.0, 48224.0, 48232.0, 48233.0, 48233.38, 48235.0, 48235.2, 48231.0, 48234.0, 48237.76, 48240.0, 48238.58, 48242.0, 48243.0, 48244.0, 48245.0, 48246.0, 6863991.0, 48248.0, 48249.6, 48250.0, 48249.0, 48252.0, 48253.68, 48254.0, 48255.0, 48256.0, 48253.0, 48258.0, 48259.0, 48260.0, 48261.97, 48261.96, 48262.0, 48264.0, 48261.7, 48266.0, 48267.48, 48268.0, 48269.28, 48270.0, 48271.0, 48272.0, 48271.89, 48274.0, 48275.0, 48276.61, 48276.0, 48277.0, 48279.0, 48280.0, 48281.0, 48282.0, 48283.0, 48284.82, 48285.0, 48286.53, 48284.0, 48288.0, 48289.0, 48288.63, 48291.0, 48292.74, 48290.0, 48292.0, 48295.0, 48296.64, 48297.0, 48298.0, 48299.0, 48300.0, 48300.41, 48293.76, 48294.0, 48304.0, 48305.0, 48297.6, 48307.0, 48308.0, 48309.0, 48310.01, 48311.0, 48312.0, 48314.0, 48315.0, 48318.0, 48318.4, 48319.0, 48320.0, 48321.0, 48323.0, 48324.0, 48322.56, 48326.4, 48326.0, 48328.0, 48327.0, 48330.0, 48331.0, 48332.75, 48333.0, 48334.95, 48334.0, 48336.0, 48337.0, 48335.0, 48339.0, 48340.0, 48341.0, 48342.0, 48343.0, 61710.0, 48345.0, 48346.0, 48347.0, 48348.0, 48349.0, 48350.0, 48351.0, 48352.0, 48353.76, 48354.0, 48356.0, 48358.0, 48360.0, 48362.0, 48363.0, 48362.4, 48367.17, 48367.92, 48368.0, 48370.0, 48371.0, 48372.0, 48371.64, 48373.0, 48375.0, 48376.0, 48377.23, 48378.0, 48376.44, 48380.0, 48380.33, 48380.4, 48383.0, 48384.0, 48385.0, 48382.0, 48379.0, 48388.0, 48389.6, 48390.0, 48391.0, 48392.0, 48393.0, 48389.73, 48394.0, 48396.0, 48397.0, 48389.0, 48399.0, 48400.0, 48401.6, 48402.0, 48403.0, 48401.0, 48406.0, 48407.0, 48408.0, 48409.0, 48410.0, 48411.0, 48412.0, 48409.6, 48414.0, 48415.68, 48416.0, 48417.0, 48418.0, 48418.5, 48420.0, 48421.0, 48422.0, 48420.72, 48424.0, 48425.0, 48425.52, 48427.0, 48426.0, 48422.08, 48430.32, 48431.0, 48432.0, 48433.0, 48434.0, 48435.0, 48436.0, 48430.0, 48438.0, 48433.93, 48440.0, 48441.0, 48442.9, 48443.0, 48442.0, 48445.0, 48446.0, 48447.88, 48444.0, 48449.0, 48450.0, 48450.59, 48452.0, 48447.0, 48448.0, 48455.0, 48456.0, 48457.0, 48458.0, 48459.0, 48460.8, 48460.0, 48462.0, 48463.97, 48464.0, 48465.0, 48464.28, 48467.0, 48468.0, 48464.04, 48470.0, 310615.0, 48472.0, 48466.8, 48474.0, 48475.45, 48476.0, 48477.0, 48478.0, 48479.0, 48480.0, 48477.79, 48482.0, 48483.0, 48484.0, 48481.0, 48486.4, 48485.0, 48488.0, 48489.0, 48490.0, 48486.72, 48492.0, 48493.0, 48487.0, 48495.0, 48496.0, 48497.0, 48498.0, 48499.92, 48500.0, 48501.0, 48502.0, 48500.42, 48504.0, 48505.0, 48506.0, 48507.0, 48505.2, 48509.0, 48510.36, 48506.76, 48508.0, 48513.0, 48514.08, 48514.0, 48516.0, 48517.0, 48517.32, 48515.0, 48520.0, 48521.91, 48519.0, 48523.0, 48525.24, 48526.4, 48527.0, 48528.0, 48529.0, 48530.0, 48531.0, 48531.84, 48533.0, 48534.46, 48535.2, 48535.0, 48536.0, 48538.0, 48538.44, 48540.0, 48541.0, 48542.04, 48543.0, 48544.0, 48544.81, 48546.0, 48547.0, 48545.0, 48549.0, 48550.0, 48551.0, 48552.0, 48553.0, 48554.0, 48555.0, 48547.2, 48557.44, 48558.0, 48557.0, 48560.0, 48560.16, 48560.8, 48563.0, 48564.0, 48565.0, 48559.54, 48567.0, 48568.0, 835000.0, 48570.0, 48571.95, 48572.0, 48569.0, 48573.0, 48575.0, 48576.0, 48577.0, 48578.0, 48571.0, 48580.0, 48581.04, 48582.32, 48581.0, 48584.0, 48585.0, 48586.8, 48587.0, 48588.0, 48589.0, 48590.0, 48588.8, 48586.36, 48593.0, 48594.0, 48595.0, 48596.0, 48597.0, 48597.96, 48598.0, 48600.0, 48601.0, 48597.84, 48603.0, 48604.0, 48605.04, 48606.0, 48607.0, 48608.0, 48609.0, 48610.0, 48611.0, 48612.0, 48609.6, 48614.0, 48615.0, 48616.0, 48617.0, 48618.0, 48619.2, 48620.0, 48621.0, 48622.0, 48619.16, 48624.0, 48623.0, 48624.37, 48627.0, 48629.0, 48630.0, 48632.0, 48633.0, 48633.26, 48635.0, 48636.0, 48636.41, 48638.46, 48639.0, 48640.0, 48637.0, 48644.0, 48645.0, 48646.0, 48647.0, 48648.88, 48649.0, 48650.0, 48651.2, 48651.0, 48652.0, 48653.0, 48655.0, 48648.0, 48657.0, 48658.0, 48659.0, 48660.0, 48661.0, 48662.66, 48663.0, 48664.0, 48665.93, 48666.0, 48667.0, 48661.8, 48663.5, 48670.0, 48671.0, 48672.0, 48673.0, 48674.0, 48675.0, 48676.0, 48677.0, 48678.0, 48679.8, 48680.0, 48681.0, 48682.0, 48683.0, 48684.0, 48685.0, 48686.0, 48687.0, 48688.0, 48687.5, 48690.0, 48691.0, 48692.0, 48692.8, 48694.0, 48695.0, 48696.0, 48697.0, 48698.0, 48692.04, 48700.0, 48693.0, 48701.0, 48703.0, 48704.0, 48705.0, 48699.0, 48708.0, 48709.0, 48710.4, 48711.01, 48711.0, 48713.0, 48714.0, 48712.0, 48715.0, 48717.96, 48718.0, 48717.0, 48720.0, 48716.0, 48722.0, 48723.0, 48721.92, 48719.0, 48726.0, 48727.0, 48728.12, 48729.0, 48729.96, 48726.61, 48732.0, 48733.0, 48734.0, 48735.0, 48736.0, 48737.0, 48731.0, 48739.0, 48740.0, 48741.0, 48743.0, 48744.0, 48745.0, 48746.22, 48747.0, 48748.0, 48746.0, 48750.0, 48752.0, 48753.0, 48752.04, 48755.0, 48756.0, 48755.88, 48758.0, 48757.71, 48760.0, 48761.0, 48762.0, 48760.44, 48764.0, 48765.0, 48761.52, 48767.0, 48768.0, 48769.0, 48770.0, 835200.0, 48771.0, 48771.82, 48774.0, 48775.0, 48776.0, 48777.0, 48772.34, 48779.0, 48780.0, 48781.08, 48781.0, 48782.0, 48784.0, 48785.0, 48786.0, 48788.0, 48789.0, 48790.0, 48791.0, 48792.0, 48793.0, 48794.0, 48796.0, 48797.0, 48796.8, 48799.92, 48800.0, 48800.9, 48799.0, 48801.0, 48804.0, 48803.0, 48798.0, 310942.0, 48808.08, 48809.0, 48808.0, 48810.84, 48810.0, 48813.84, 48814.27, 48814.0, 48816.0, 48817.0, 48818.0, 48815.0, 48820.0, 48813.0, 48822.0, 48823.0, 48824.0, 48825.0, 48826.0, 48827.0, 48828.0, 48822.49, 48830.0, 48831.0, 48831.72, 48833.0, 48834.0, 48834.96, 48836.0, 48837.0, 48838.0, 48839.0, 48840.0, 48841.0, 48840.08, 48843.77, 48844.0, 48846.0, 48848.56, 48849.0, 48850.0, 48851.0, 48852.0, 48852.92, 48854.0, 311000.0, 48857.0, 48858.0, 48859.0, 48860.0, 48861.96, 48862.32, 48863.0, 48864.0, 48865.0, 48866.0, 48866.2, 48868.0, 48869.76, 48870.0, 48871.0, 48872.16, 48873.0, 48874.0, 48875.0, 48876.0, 48872.0, 48877.92, 48879.0, 48880.0, 48881.0, 48882.0, 48884.0, 48885.0, 48886.0, 48887.0, 48888.0, 48889.0, 48890.0, 48891.0, 48892.8, 48893.0, 48894.0, 48895.0, 48896.0, 48898.0, 48899.0, 48900.0, 48900.72, 48902.0, 48902.88, 48903.0, 48905.0, 48906.0, 48907.0, 48908.0, 48909.0, 48910.0, 48912.6, 48912.0, 48913.0, 48915.0, 48916.0, 48917.0, 48918.0, 48919.0, 48920.0, 48921.6, 48922.0, 48923.52, 48924.0, 48924.98, 48926.0, 48925.0, 48928.17, 48928.3, 48930.0, 48929.0, 48932.0, 48933.0, 48934.0, 48935.0, 48936.0, 48928.0, 48938.0, 48939.0, 48940.0, 48941.0, 48942.0, 48944.0, 48945.0, 48946.0, 48947.0, 48948.0, 48949.26, 48950.0, 48948.72, 48952.0, 48953.0, 48954.0, 48953.72, 48956.0, 48955.0, 48958.0, 48954.58, 48960.0, 48961.0, 48962.0, 48963.0, 48964.0, 48963.2, 48966.0, 1097541.0, 48968.0, 48969.0, 48970.0, 48971.0, 48972.0, 48973.05, 48973.0, 48975.0, 48976.0, 48974.0, 48978.0, 48980.0, 48981.0, 48983.0, 48984.0, 48985.0, 48987.0, 48987.36, 48989.0, 48990.0, 48991.0, 48992.0, 48993.0, 48994.0, 48995.0, 48996.0, 48997.0, 48998.0, 48999.0, 49000.0, 49001.0, 49002.0, 49003.0, 49004.76, 49005.0, 49004.0, 49007.0, 49008.0, 311153.0, 49010.0, 49013.0, 49014.0, 49015.0, 49016.0, 49017.6, 49018.0, 311160.0, 49020.0, 49017.0, 49022.0, 49023.0, 49024.0, 49025.0, 49026.0, 49027.0, 49028.76, 49028.0, 49030.0, 49031.0, 49032.0, 49029.0, 49033.6, 49036.0, 49038.0, 49039.0, 49040.0, 49041.0, 49042.6, 49042.0, 49043.0, 49042.5, 49046.88, 49044.0, 49046.0, 49049.0, 49050.0, 49052.0, 49053.89, 49054.0, 49055.0, 49056.0, 49053.0, 49058.0, 49059.1, 49058.52, 49061.0, 49062.0, 49062.91, 49064.0, 49060.64, 49066.0, 49067.0, 49068.0, 49067.2, 49070.0, 49070.99, 49072.0, 49073.31, 49074.9, 49078.0, 49080.0, 49080.01, 49082.0, 49083.6, 49084.0, 49085.0, 49086.31, 49087.0, 49088.0, 49089.0, 49090.0, 49091.0, 49086.0, 49092.0, 49090.91, 49096.0, 49098.0, 49099.96, 49100.0, 49099.0, 49099.34, 49101.0, 49104.0, 49105.0, 49107.0, 49108.0, 49108.49, 49110.0, 49109.0, 49110.32, 49113.0, 49114.0, 49112.0, 49116.0, 49108.8, 49118.0, 49117.0, 49120.0, 49121.0, 49122.0, 49123.0, 49124.0, 49125.0, 49126.0, 49127.0, 49128.0, 49129.0, 49130.0, 49131.0, 49132.8, 49133.0, 49132.0, 49136.0, 49137.0, 49138.0, 49139.0, 49140.0, 49142.4, 49143.0, 49144.0, 49145.0, 49146.0, 49147.0, 49150.0, 49151.0, 49152.0, 49151.41, 49154.0, 49155.0, 49156.0, 49150.8, 49152.48, 49155.08, 49160.0, 49161.0, 49162.9, 49163.52, 49164.0, 49165.0, 49166.0, 49168.0, 49170.0, 49171.0, 49172.0, 49171.12, 49174.0, 49175.0, 49176.0, 49171.51, 49179.0, 49180.0, 49182.0, 49183.0, 49185.0, 49186.0, 49187.0, 49188.0, 49189.0, 49191.0, 49192.0, 49193.0, 49194.0, 49194.65, 49196.0, 49197.0, 49198.0, 49199.0, 49200.0, 49201.0, 49202.0, 49202.14, 49205.26, 49206.0, 49206.31, 49208.0, 49205.0, 49210.0, 49211.0, 49212.0, 49214.0, 49215.0, 49216.0, 49217.0, 49218.73, 49218.0, 49220.0, 49221.61, 49222.0, 49223.0, 49217.62, 49224.0, 49225.0, 49227.36, 49228.0, 49229.7, 49227.0, 49230.0, 49232.04, 49233.0, 49234.0, 49235.0, 49236.0, 49232.0, 49238.0, 49239.0, 49240.0, 49241.0, 49242.0, 49243.0, 49244.0, 49245.78, 49245.0, 49246.0, 8700000.0, 49248.0, 49250.0, 49249.0, 49251.0, 49252.0, 49254.0, 49254.4, 49247.0, 49257.0, 49256.0, 49252.05, 49260.0, 49261.0, 49253.0, 49263.0, 49255.0, 49264.0, 49266.0, 49268.0, 49269.0, 49270.0, 49271.0, 49272.0, 49273.08, 49270.08, 49275.2, 49276.0, 49275.0, 49275.22, 49277.0, 49280.0, 49277.26, 49282.0, 49275.01, 49284.0, 49285.0, 49286.0, 49279.8, 49287.12, 49283.0, 49290.0, 49291.4, 49288.0, 49291.0, 49294.0, 49295.0, 49296.0, 49289.0, 49297.0, 49299.0, 49300.0, 49301.0, 49302.0, 49301.88, 49304.0, 49305.0, 49305.12, 49307.0, 49308.0, 49307.47, 49310.0, 311455.0, 49313.0, 49314.37, 49314.0, 49316.0, 49317.0, 49316.8, 49319.0, 49320.0, 49321.0, 49315.0, 49323.0, 49325.0, 49329.0, 49330.0, 49331.0, 49332.0, 49333.0, 49334.0, 49332.12, 49336.0, 49337.0, 49337.04, 49337.6, 49340.0, 49339.56, 49337.88, 49343.0, 2146496.0, 49344.0, 49345.0, 49345.6, 49348.0, 49349.0, 49350.0, 49351.0, 49352.0, 49353.0, 49347.0, 49355.0, 49356.0, 49356.1, 49358.0, 49359.0, 49360.0, 49361.32, 49362.0, 49362.6, 49364.26, 49365.0, 49366.0, 49367.0, 49368.0, 49363.0, 49370.0, 49371.0, 49372.0, 49373.0, 49374.0, 49375.0, 49376.0, 49377.0, 49378.0, 49379.0, 49380.0, 49378.8, 49382.0, 49383.0, 49384.0, 49378.32, 49379.2, 49385.0, 49388.0, 49389.0, 49390.0, 49386.0, 49392.0, 49393.7, 49394.0, 49391.13, 49396.0, 49391.0, 49398.0, 49399.0, 49400.0, 49401.0, 49402.0, 49403.0, 49404.0, 49405.0, 49407.0, 49409.0, 49410.0, 49411.0, 49412.0, 49413.84, 49414.56, 49415.0, 49416.36, 49417.0, 49418.0, 49419.0, 49420.0, 49421.0, 49422.0, 49417.16, 49416.0, 49425.0, 49426.0, 49428.0, 49429.68, 49430.0, 49431.0, 49432.0, 49433.0, 49434.0, 49435.0, 49436.0, 49434.34, 49434.88, 49439.0, 49440.0, 49441.0, 49441.6, 49437.0, 49444.0, 49445.0, 49445.88, 49447.0, 49450.0, 49451.0, 49452.0, 49453.0, 49451.61, 49455.0, 49454.0, 49457.0, 49458.0, 49459.0, 49460.0, 49460.82, 49462.0, 49462.4, 49464.0, 49465.0, 49462.82, 49464.2, 49466.0, 49463.0, 49470.0, 49470.84, 49472.0, 49473.0, 49474.82, 49474.0, 49476.0, 49475.0, 49478.0, 49479.0, 49480.0, 49477.63, 49482.62, 49483.0, 49485.0, 49486.0, 49487.0, 49488.0, 49489.56, 49489.0, 49491.06, 49491.98, 49492.0, 49494.6, 49495.0, 49494.0, 49493.0, 49498.0, 49499.0, 49500.0, 49501.0, 49502.0, 49497.0, 49504.0, 49505.0, 49506.76, 49504.05, 49508.0, 49509.0, 49510.0, 49511.0, 49512.0, 49506.67, 49514.0, 49515.0, 49516.0, 49517.0, 49518.0, 49519.08, 49520.0, 49521.0, 49522.0, 49522.8, 49524.0, 49525.0, 49526.0, 49527.0, 49528.0, 49527.48, 49530.0, 49529.0, 49532.0, 49533.0, 311674.0, 49535.0, 49536.0, 49537.0, 49540.96, 49541.0, 49540.0, 49543.0, 49544.0, 49545.0, 49546.0, 49542.72, 49548.0, 49542.0, 49550.0, 49551.0, 49552.0, 49551.4, 49554.0, 49555.0, 49556.0, 49553.0, 49558.0, 49559.04, 49560.0, 49561.0, 49562.0, 49563.0, 49564.0, 49565.0, 49566.0, 49566.4, 49568.0, 49565.15, 49570.0, 49571.0, 49572.0, 49573.0, 49567.0, 49575.0, 49576.0, 49577.0, 49577.6, 49578.0, 49580.88, 49580.0, 49582.0, 49583.0, 49584.0, 49580.4, 49586.0, 49587.0, 49588.0, 49589.0, 49590.0, 49591.0, 49592.0, 49593.0, 49592.4, 49595.0, 49596.0, 49598.25, 49599.0, 49600.0, 49601.0, 49602.0, 49605.0, 49606.0, 49607.0, 49608.0, 49609.0, 49606.7, 49611.0, 49612.0, 49613.0, 49614.84, 49615.0, 49614.53, 49617.88, 49617.0, 49619.0, 49620.0, 49621.0, 49616.0, 49623.0, 49624.27, 49625.0, 49626.0, 49627.0, 49628.0, 49629.0, 49630.0, 49622.0, 49632.0, 49624.0, 49634.0, 49631.0, 49631.5, 49637.0, 49638.96, 49639.0, 49640.0, 49639.62, 49642.0, 49643.0, 49644.0, 49638.0, 49646.0, 49641.0, 49648.0, 49649.0, 49650.0, 49651.0, 49648.75, 49652.0, 49654.0, 49653.0, 49656.0, 49656.48, 49658.0, 49659.45, 49660.0, 49661.0, 49662.0, 49663.0, 49664.0, 49665.0, 49666.0, 49667.0, 49668.12, 49669.56, 49670.0, 49669.0, 49672.0, 49673.0, 49674.0, 49668.0, 49676.0, 49671.0, 49678.0, 49679.0, 49680.0, 49678.74, 49681.0, 49678.4, 49684.0, 49685.0, 49686.0, 49687.0, 49683.0, 49681.68, 49690.0, 49691.0, 49692.0, 49693.16, 49694.0, 49695.0, 49695.24, 49696.0, 49698.0, 49699.0, 49700.0, 49701.68, 49701.84, 49702.0, 49704.0, 49701.0, 49697.0, 49707.0, 49708.41, 49709.4, 49710.0, 49711.0, 574000.0, 49712.0, 49712.5, 49713.0, 49715.0, 49716.96, 49714.0, 49718.0, 49716.0, 49720.0, 49721.0, 49722.0, 49723.0, 49724.0, 49725.0, 49726.0, 49727.0, 49728.0, 49730.0, 49727.31, 49732.0, 49733.0, 49731.0, 49735.44, 49734.0, 49737.0, 49736.0, 49735.0, 49740.0, 49741.0, 49742.0, 49739.0, 49744.0, 49745.74, 49746.17, 49747.0, 49748.05, 49749.0, 49750.0, 49749.96, 49752.0, 49753.0, 49753.6, 49755.0, 49756.0, 49757.0, 49754.0, 49756.32, 49760.0, 49753.92, 49762.0, 49759.0, 49764.0, 49765.0, 49758.5, 49767.0, 49769.98, 49770.0, 49771.0, 49772.0, 49773.0, 49774.69, 49774.0, 49775.0, 49776.0, 49778.0, 49779.0, 49780.0, 49780.49, 49774.4, 49783.0, 49784.0, 49785.0, 49786.0, 49787.0, 49788.0, 49789.42, 49790.0, 49789.0, 49791.0, 49795.0, 49795.2, 49797.0, 49799.0, 49800.0, 49802.0, 49806.04, 49806.0, 49808.47, 49809.96, 49810.0, 49811.0, 49812.0, 49812.82, 49814.0, 49809.0, 49816.0, 49817.0, 49818.84, 49818.0, 49815.6, 49820.0, 49821.0, 49823.0, 49824.0, 49825.0, 49826.28, 49827.0, 49822.4, 49829.0, 49830.0, 49831.86, 49831.24, 49833.0, 49834.0, 49835.48, 49836.0, 49837.0, 49836.96, 49832.0, 49840.0, 49841.0, 49842.0, 49843.0, 49844.0, 49836.8, 49846.0, 49847.0, 49848.0, 49850.0, 49852.0, 49853.0, 49854.0, 49855.0, 312000.0, 49856.0, 49857.0, 49859.0, 49860.0, 49861.0, 49862.3, 49862.4, 49864.0, 49863.0, 49859.88, 49867.0, 49867.92, 49862.0, 49870.35, 49871.0, 49872.95, 49870.0, 49872.0, 49875.0, 49875.3, 49877.0, 49878.0, 49879.14, 49880.0, 49881.0, 49882.0, 49883.0, 49884.0, 49885.0, 49886.0, 49887.07, 49888.0, 49889.0, 49890.0, 49891.56, 49887.0, 49892.67, 49894.0, 49895.0, 49896.0, 49897.0, 49897.5, 49899.0, 49900.0, 49901.2, 49902.72, 49898.0, 49904.0, 49905.0, 49901.0, 49907.0, 49908.0, 49901.52, 49910.0, 49911.0, 49912.0, 49909.14, 49906.0, 49910.04, 49916.0, 49917.0, 49918.0, 49920.0, 49921.0, 49922.0, 49923.0, 49924.0, 49924.94, 49926.0, 49923.16, 49928.0, 49929.66, 49929.0, 49929.6, 49930.0, 49933.0, 49932.0, 49938.0, 49939.0, 49940.0, 49941.29, 49943.0, 49944.0, 49945.0, 49946.0, 49947.0, 49948.0, 49949.0, 49950.0, 49951.2, 49952.0, 49951.0, 49954.0, 49955.0, 49956.0, 49950.84, 49958.0, 49959.0, 49960.0, 49961.6, 49962.0, 49961.0, 9225000.0, 49962.39, 49965.12, 49967.0, 49966.0, 49969.2, 49970.0, 49964.0, 49972.0, 49973.0, 49974.0, 49975.0, 49976.0, 49977.0, 49974.08, 49977.6, 49980.0, 49981.0, 49982.0, 49983.0, 49984.0, 49985.0, 49986.0, 49987.0, 49988.0, 49989.0, 49990.0, 49991.0, 49992.0, 49986.96, 49994.4, 49995.0, 49996.0, 49997.0, 49998.0, 49999.0, 50000.0, 50001.0, 50000.4, 50003.0, 50004.0, 50005.0, 50002.0, 50007.0, 50008.0, 50009.0, 50010.0, 50006.0, 50012.0, 50011.0, 50013.0, 50010.56, 50016.0, 50012.13, 50018.0, 50014.8, 50020.0, 50020.06, 50022.0, 50023.0, 50024.0, 50025.0, 50026.0, 50021.44, 50028.0, 50027.0, 50030.0, 50031.0, 50032.0, 50032.95, 50033.61, 50035.0, 50029.0, 50037.0, 50037.84, 50038.0, 50040.0, 50041.0, 50042.74, 50043.0, 50044.0, 50045.0, 50042.5, 50039.0, 50048.0, 50049.0, 50050.0, 50051.0, 50052.0, 50046.0, 50054.0, 50055.0, 50056.0, 50058.0, 312203.0, 50060.0, 50061.18, 50062.44, 50063.5, 50064.0, 50065.0, 50066.0, 50065.6, 50062.0, 50069.64, 50070.0, 50069.0, 50069.76, 50073.0, 50074.0, 50075.0, 50076.0, 312221.0, 50078.0, 50077.0, 50080.0, 50081.0, 50082.0, 50079.0, 50084.0, 50085.0, 50086.0, 50086.5, 50088.0, 50089.0, 50090.0, 50087.0, 50092.0, 50093.0, 50094.0, 50095.0, 50096.0, 50097.0, 50098.0, 50095.92, 50100.0, 50099.0, 50102.8, 50103.5, 50102.0, 50104.0, 50106.0, 50107.2, 50107.0, 50109.28, 50109.0, 50111.0, 50112.0, 50112.14, 50113.0, 50115.0, 50110.0, 50117.0, 50118.0, 50119.0, 50120.0, 50119.2, 50122.0, 50119.33, 50124.0, 50125.0, 50126.0, 50127.0, 50128.0, 50129.0, 50130.0, 50131.0, 50132.0, 50133.0, 50134.0, 50135.04, 50136.0, 50137.5, 50138.0, 50139.0, 50140.0, 50141.0, 50142.84, 50143.0, 50144.0, 50145.0, 50146.0, 50147.0, 50148.0, 50149.0, 50150.0, 50150.4, 50152.0, 50153.0, 50155.74, 50155.0, 50156.0, 50157.0, 50159.0, 50160.0, 50160.48, 50155.03, 50163.0, 50164.44, 50165.0, 50164.0, 50167.44, 50168.0, 50169.0, 50170.0, 50172.0, 50173.0, 50174.73, 50174.0, 50176.0, 50175.0, 50178.74, 50178.0, 50180.0, 50181.21, 50177.67, 50177.37, 50184.0, 50185.0, 50184.16, 50187.0, 312331.0, 50189.0, 50190.0, 50188.0, 50192.0, 50193.0, 50194.0, 50195.0, 50196.0, 50198.0, 50199.0, 50200.0, 50199.76, 50202.0, 50203.0, 50204.0, 50205.0, 50206.0, 50207.0, 50208.6, 62081.51, 50210.28, 50211.0, 50212.0, 50212.26, 50214.0, 50215.0, 50213.0, 50217.0, 50218.82, 50219.0, 50220.0, 50218.0, 50222.0, 50223.0, 50224.0, 50225.0, 50226.0, 50227.0, 50228.0, 50229.48, 50229.0, 50229.87, 50232.0, 50230.0, 50234.0, 50235.0, 50232.6, 50233.0, 50236.0, 50239.48, 50240.0, 50239.0, 50242.0, 50236.22, 50244.0, 50245.0, 50246.0, 50238.0, 50248.0, 50249.0, 50250.0, 50249.04, 50252.0, 50249.64, 50249.89, 50253.0, 50256.0, 50257.0, 50252.8, 50259.0, 50260.0, 50261.0, 50258.0, 50263.0, 50264.0, 50264.64, 50261.76, 50261.12, 50268.0, 50269.12, 50269.0, 50270.0, 50272.0, 50273.0, 312418.0, 50271.36, 50276.0, 50276.6, 50278.0, 50275.0, 50280.0, 50281.0, 50282.0, 50283.0, 50284.0, 50285.0, 50286.0, 50287.68, 50287.0, 50289.0, 50290.0, 50292.0, 50294.0, 50295.0, 50296.0, 50297.0, 50298.4, 50299.0, 50300.0, 50294.4, 50299.7, 50303.96, 50304.0, 50297.16, 50307.0, 50307.36, 50310.0, 50310.48, 50312.5, 50313.0, 50314.62, 50315.0, 50316.0, 50318.7, 50319.0, 50320.0, 50321.0, 50322.0, 50323.62, 50323.0, 50324.0, 50326.0, 50325.0, 50328.0, 50329.34, 50330.0, 50327.88, 50332.34, 50333.0, 50334.38, 50335.06, 50336.0, 50337.0, 50338.0, 50339.68, 50339.4, 50341.0, 50342.0, 50335.0, 50344.0, 50345.21, 50346.0, 50345.0, 50340.0, 50349.0, 50350.0, 50351.0, 50352.0, 50348.0, 50354.64, 50355.0, 50356.8, 50356.0, 50358.0, 50355.96, 50360.0, 50361.0, 50362.0, 50363.0, 50363.69, 50365.0, 50366.0, 50367.0, 50368.0, 50368.6, 50370.0, 50364.0, 50372.0, 50373.0, 50374.0, 50375.0, 50376.0, 50377.0, 50378.0, 50379.0, 50380.0, 50380.72, 50381.0, 50378.66, 50383.0, 50385.0, 50386.8, 50386.6, 50388.0, 50383.58, 50390.0, 50390.08, 50392.0, 50393.04, 50394.5, 50395.0, 50393.0, 50397.28, 50398.0, 50399.0, 50400.0, 50394.0, 50401.0, 50403.0, 50404.0, 50405.0, 50406.0, 312550.0, 50408.0, 50401.75, 50410.0, 50411.0, 50412.0, 50413.0, 50414.0, 50415.0, 50416.0, 50414.91, 50419.2, 50420.0, 50419.0, 50420.5, 50424.0, 50425.0, 50426.0, 50427.0, 50424.95, 50429.0, 50430.0, 50427.27, 50432.31, 50433.0, 50432.0, 50435.0, 50436.0, 50437.0, 50438.0, 50431.0, 50440.0, 50441.0, 50443.0, 50445.0, 50445.2, 50447.0, 50448.0, 50449.0, 50450.0, 50452.2, 50453.0, 50454.42, 50455.96, 50455.0, 50457.0, 50458.0, 50459.0, 50460.0, 50461.0, 50456.0, 50462.0, 50463.0, 50459.4, 50466.0, 50460.8, 50468.8, 50469.0, 50470.0, 50468.54, 50472.0, 50464.0, 50474.51, 50474.0, 50476.0, 50477.0, 50478.0, 50479.0, 50480.0, 50481.0, 50482.0, 50475.0, 50484.0, 50485.0, 50486.0, 50488.2, 50489.0, 50488.0, 50490.0, 50492.0, 50494.0, 50495.0, 50496.0, 50497.0, 50498.0, 50499.96, 50500.0, 312640.0, 50501.0, 50503.0, 50504.0, 50505.0, 50506.0, 50506.8, 50508.0, 50502.0, 50510.0, 62142.0, 50512.0, 50504.7, 50514.0, 50515.0, 50515.98, 50516.0, 50518.0, 50518.65, 50520.0, 50519.0, 50521.0, 50523.0, 50518.8, 50525.0, 50526.0, 50522.55, 50528.0, 50527.0, 50530.68, 50531.38, 50532.0, 50533.94, 50534.0, 50530.0, 50536.0, 50531.0, 50538.8, 50539.0, 50540.0, 50541.0, 50540.76, 50543.0, 50544.0, 50545.0, 50546.0, 50547.0, 50547.6, 50545.04, 50550.0, 50551.0, 50552.0, 50552.56, 50554.0, 50555.0, 50556.0, 50553.0, 50549.0, 50559.0, 50560.0, 50561.0, 50562.0, 50563.0, 50564.0, 50557.0, 50566.0, 50567.0, 50568.0, 50569.0, 50570.0, 50571.0, 50572.0, 50573.0, 50574.0, 50575.0, 50576.0, 50577.0, 50578.0, 50580.0, 50581.0, 50582.0, 50583.0, 50584.0, 50585.0, 50586.0, 50587.0, 50587.92, 50589.0, 50590.0, 50591.0, 50592.0, 50593.0, 50594.0, 50595.0, 50596.0, 50588.0, 50598.0, 50599.0, 50600.0, 50601.0, 50602.0, 50601.63, 50604.0, 50605.07, 50606.0, 50603.0, 50606.4, 50610.0, 50611.0, 50610.76, 50613.0, 50614.0, 50615.0, 50616.0, 50617.0, 50618.0, 50614.92, 50620.0, 50621.0, 50622.0, 50616.92, 50617.88, 50625.0, 50626.0, 50627.72, 50628.0, 50629.0, 50630.0, 50626.56, 50632.0, 50632.5, 50634.0, 50635.0, 50633.0, 50637.0, 50630.88, 50639.0, 50640.0, 50639.76, 50638.0, 50636.0, 50644.0, 50644.68, 50646.88, 50647.2, 50648.0, 50648.45, 50650.0, 50651.56, 50652.0, 50653.0, 50649.6, 50646.0, 50656.0, 50657.0, 50649.55, 50659.0, 50660.0, 50661.0, 50662.0, 50663.0, 50664.0, 50665.0, 50666.0, 50667.0, 50668.0, 50670.0, 50671.0, 50671.68, 50673.0, 50674.4, 50675.0, 50676.0, 50677.2, 50678.7, 50674.0, 50680.0, 50681.0, 50682.0, 50683.0, 50684.0, 50685.0, 50686.0, 50687.0, 50688.0, 50687.07, 50690.0, 50689.0, 50692.0, 50688.24, 50694.0, 50691.0, 50694.22, 50697.0, 50698.0, 50699.0, 50700.0, 50701.32, 50699.25, 50701.0, 50704.16, 50705.07, 50706.0, 50704.0, 50703.0, 50709.0, 50710.0, 50709.1, 575000.0, 50712.0, 50714.0, 312858.36, 50713.0, 50716.0, 50717.0, 50715.0, 50720.0, 50721.0, 50723.0, 50724.36, 50725.0, 50726.0, 50727.0, 50728.0, 50729.0, 50730.0, 50731.12, 50732.96, 50732.0, 50731.0, 50734.68, 50736.0, 50737.0, 50738.0, 50733.0, 50740.0, 50741.0, 50742.0, 50744.0, 50745.0, 50746.0, 50747.0, 50748.0, 50746.15, 50750.0, 50751.0, 50752.0, 50752.09, 50750.54, 50755.0, 50756.0, 50757.0, 50756.16, 50760.0, 50760.24, 50762.0, 50763.0, 50764.8, 50765.0, 50766.0, 50767.0, 50769.0, 50770.0, 50772.0, 50772.98, 50774.88, 50775.0, 50775.68, 50777.0, 50778.0, 50774.0, 50780.0, 50772.8, 50773.0, 50779.0, 50784.0, 50785.0, 50786.0, 50787.0, 50786.07, 50788.0, 50791.0, 50792.4, 50793.0, 50793.6, 50795.0, 50796.0, 50794.0, 50798.0, 50799.0, 50800.0, 50800.01, 50799.37, 50803.2, 50804.0, 50808.0, 50809.0, 50810.0, 50810.4, 50812.0, 50813.0, 50814.0, 50816.0, 50817.0, 50818.0, 50819.95, 50820.0, 50821.0, 50821.44, 50823.0, 50824.0, 50825.0, 50826.0, 50827.0, 50825.33, 50829.0, 50828.0, 50831.0, 50832.0, 50834.37, 50835.0, 50836.0, 50835.85, 50835.2, 50834.4, 50840.0, 50841.0, 50841.68, 50840.64, 50844.0, 50843.0, 50844.62, 50842.0, 50846.0, 50849.0, 50850.0, 50851.0, 50852.0, 50853.0, 50854.0, 50850.24, 313000.0, 50856.0, 50858.0, 50859.53, 50860.0, 50861.0, 50862.72, 50863.0, 50859.0, 50865.0, 50866.0, 50864.0, 50868.0, 50869.0, 50870.0, 50873.0, 50874.0, 50875.0, 50874.12, 50877.0, 50878.0, 50879.0, 50880.0, 50881.0, 50882.0, 50883.0, 50885.0, 50886.72, 50887.0, 50888.0, 50889.0, 50890.0, 50891.0, 50892.0, 50892.7, 50894.2, 50895.0, 62218.0, 50897.0, 50898.0, 50899.0, 50900.0, 50900.2, 50902.0, 50903.69, 50904.0, 50905.0, 50906.0, 50907.6, 50908.0, 50907.37, 50907.0, 50911.36, 50912.0, 50913.0, 62221.0, 50915.0, 50916.0, 50916.42, 50918.0, 50918.4, 50920.0, 50919.0, 50921.0, 50923.0, 50923.68, 50921.28, 50926.0, 50926.32, 50928.0, 50924.0, 50930.0, 50931.0, 50932.0, 50933.0, 50934.24, 50935.0, 62227.0, 50937.21, 50938.8, 50939.0, 50940.0, 50941.0, 50938.0, 50943.6, 50941.2, 50945.0, 50946.36, 50947.0, 50948.0, 50949.6, 50950.0, 50950.8, 50949.0, 50952.0, 50954.0, 50952.36, 50956.15, 50956.0, 50958.0, 50959.0, 50960.0, 50961.0, 50962.0, 50963.0, 50964.0, 50965.0, 50966.0, 50968.0, 50969.0, 50970.0, 50970.68, 50972.0, 50973.96, 50973.0, 50975.0, 50976.0, 50977.0, 50978.0, 50974.0, 50980.0, 50980.32, 50982.0, 50983.0, 50984.0, 50985.24, 50986.0, 50980.8, 50988.0, 50989.0, 50990.0, 50991.0, 50988.48, 50985.0, 50986.87, 50995.56, 50995.0, 50997.44, 50998.0, 50999.0, 51000.0, 51001.52, 51002.0, 51001.6, 51004.0, 51003.0, 51006.0, 51005.0, 51008.0, 51008.52, 51010.32, 51010.0, 51012.0, 51010.68, 51014.0, 51015.0, 51016.8, 51017.0, 51018.0, 51019.0, 51020.0, 51021.0, 51022.0, 51016.0, 51024.0, 51026.0, 51027.0, 51028.0, 51030.0, 51031.5, 51032.0, 51033.0, 51036.0, 51038.0, 51039.0, 51040.0, 51041.0, 51043.0, 51044.0, 51044.64, 51046.42, 51047.51, 51048.0, 51046.8, 51050.0, 51047.0, 51052.84, 51046.0, 51053.0, 51055.0, 51056.0, 51057.0, 51049.0, 51059.0, 51060.0, 51061.92, 51062.0, 51063.0, 51064.0, 51065.0, 51066.0, 51067.84, 51067.2, 51067.0, 51070.0, 51068.0, 51072.0, 51073.0, 51074.0, 51075.0, 51077.0, 51078.0, 51080.2, 51081.0, 51080.0, 51081.96, 51084.0, 51085.0, 51084.8, 51087.0, 51089.0, 51090.0, 51094.0, 51095.0, 51096.0, 51098.0, 51100.0, 51101.0, 51102.0, 51101.12, 51104.0, 51105.0, 51105.6, 51107.0, 51107.4, 51107.11, 51109.0, 51111.0, 51112.0, 51113.0, 51116.0, 51117.0, 51118.0, 51119.0, 51120.0, 51121.0, 51122.0, 51122.71, 51124.0, 51125.0, 51126.0, 51127.56, 51128.0, 51129.0, 51130.0, 51129.36, 51132.0, 51133.0, 51127.0, 51135.0, 51136.0, 313280.54, 51138.0, 51139.0, 51140.0, 51141.0, 51142.0, 51144.0, 51145.0, 51146.0, 51147.0, 51148.0, 51149.0, 51150.0, 51151.0, 51152.0, 51147.2, 51149.92, 51155.0, 51156.0, 51157.12, 51152.64, 51157.0, 51160.0, 51161.0, 51153.0, 51158.0, 51164.0, 51165.0, 51166.0, 51167.0, 51168.0, 51163.0, 51170.0, 51170.22, 51171.36, 51173.0, 51174.0, 51175.0, 51176.0, 51174.63, 51178.0, 51179.88, 51180.0, 51172.0, 51182.0, 51182.79, 51183.5, 51184.0, 51177.0, 51187.2, 51188.0, 51189.0, 51188.8, 51183.0, 51192.0, 51185.88, 51194.0, 51195.0, 51196.8, 51188.52, 51198.08, 51190.0, 51200.0, 51201.0, 51200.74, 51203.0, 51204.0, 51205.0, 51206.0, 51207.0, 51208.0, 51209.0, 51210.0, 51211.0, 51212.0, 51213.0, 51209.6, 51215.0, 51216.0, 51217.0, 51209.35, 51214.0, 51220.0, 51222.0, 51223.0, 51224.0, 51225.0, 51227.0, 51228.0, 51229.0, 51230.0, 51231.0, 51232.0, 51234.0, 51235.0, 51236.0, 51237.6, 51238.0, 51239.0, 51240.0, 51241.0, 51238.12, 51243.0, 51244.0, 51245.0, 51246.0, 51242.0, 51248.0, 51249.0, 51250.0, 51251.0, 51252.0, 51253.0, 51254.0, 51255.0, 51256.0, 51257.52, 51258.0, 51258.24, 51260.0, 51259.0, 51261.0, 51263.0, 51264.0, 51265.0, 51262.55, 51267.73, 51268.0, 51267.0, 51270.0, 51266.0, 51272.0, 51273.0, 51274.0, 51275.0, 51276.0, 51277.0, 51276.22, 51279.0, 1362000.0, 51281.64, 51282.0, 51283.0, 51281.0, 51285.0, 51286.0, 51287.0, 51288.0, 51283.38, 51290.0, 51289.0, 51292.0, 51284.0, 51294.0, 51295.0, 51298.2, 51299.75, 51300.0, 51301.38, 51302.0, 51303.48, 51302.36, 51301.0, 51305.0, 51304.0, 51308.0, 51309.0, 51310.0, 51311.0, 51312.0, 51313.0, 51314.0, 51315.0, 51313.6, 51317.0, 51317.76, 51313.66, 51320.0, 51321.0, 51322.32, 51323.0, 51324.0, 51325.0, 51326.0, 51322.0, 51328.0, 51329.0, 51330.0, 51331.68, 51327.05, 51333.0, 51334.0, 51335.7, 51335.96, 51336.0, 51336.58, 51339.0, 51340.0, 51340.8, 51341.0, 51339.48, 51344.0, 51342.0, 51345.0, 51347.0, 51348.0, 51348.6, 51350.0, 313495.0, 51352.0, 51346.0, 51354.03, 51355.2, 313500.0, 51355.0, 51357.0, 51356.0, 51355.94, 51360.0, 51360.4, 51362.4, 51363.0, 51364.0, 51365.0, 51366.0, 51362.0, 51367.0, 51369.0, 51371.0, 51372.0, 51373.0, 51374.0, 51375.0, 51376.0, 51377.0, 51378.0, 51372.77, 51380.4, 51380.0, 51382.0, 51383.0, 51384.0, 51385.0, 313526.0, 51387.0, 51388.0, 51389.0, 51390.0, 51391.0, 51392.0, 51393.72, 51393.0, 51395.0, 51396.0, 51397.0, 51400.0, 51402.0, 51403.0, 51404.0, 51405.0, 51404.28, 51407.04, 51408.0, 51407.0, 51410.0, 51411.42, 51411.0, 51411.36, 51414.46, 51415.0, 51416.0, 51417.6, 51418.0, 51417.0, 51420.0, 51421.0, 51422.0, 51420.2, 1100000.0, 51424.0, 51426.0, 51426.38, 51428.0, 51422.27, 51430.0, 51431.0, 51432.5, 51432.0, 51434.24, 51435.0, 51434.0, 51437.0, 51438.0, 51438.4, 51440.0, 51441.0, 51439.0, 51436.1, 51444.0, 51445.0, 51446.0, 51447.0, 51449.0, 51450.0, 51451.0, 51452.0, 51453.0, 51450.24, 51455.0, 51456.0, 51457.0, 51454.0, 51459.2, 51460.0, 51459.0, 51455.28, 51464.0, 51465.0, 51466.0, 51467.0, 51468.0, 51465.52, 51470.0, 51470.4, 51474.0, 51475.0, 51476.0, 51474.75, 51478.0, 51479.0, 51480.0, 51481.0, 51482.0, 51484.0, 51485.0, 51486.0, 51487.84, 51488.0, 51487.0, 51490.0, 51492.0, 51494.0, 51496.97, 51498.41, 51499.0, 51500.0, 51503.0, 51504.0, 51505.0, 51506.0, 51507.0, 51510.0, 51510.56, 51511.32, 51513.0, 51511.0, 51515.0, 51516.0, 51518.0, 51518.8, 51520.0, 51519.0, 51522.0, 51523.0, 51521.0, 51525.0, 51526.0, 51524.0, 51528.0, 51529.92, 51530.0, 51531.0, 51532.0, 51533.69, 51534.0, 51533.0, 51536.19, 51537.0, 51538.0, 51539.0, 51540.0, 62347.9, 51542.0, 51543.0, 51544.0, 51542.4, 51546.0, 51547.0, 51545.0, 51549.0, 51550.0, 51551.0, 51552.0, 51552.96, 51548.0, 51555.0, 51552.31, 51557.0, 51560.0, 51561.0, 51561.72, 51563.0, 51564.0, 51565.0, 51566.0, 51567.0, 51568.0, 51569.0, 51570.0, 51571.2, 51572.28, 51573.0, 51569.55, 51575.0, 51576.0, 51577.62, 51578.0, 51577.0, 51580.0, 51576.24, 51583.0, 51584.0, 51585.0, 51586.0, 51587.0, 51588.0, 51588.4, 51590.0, 51588.94, 51592.0, 51591.0, 51585.96, 51595.0, 51596.0, 51595.21, 51598.32, 51596.8, 51600.0, 51602.0, 51603.0, 51604.0, 51605.0, 51604.48, 313750.0, 51608.0, 51609.0, 51610.0, 62361.0, 51612.0, 51613.0, 51614.0, 51615.0, 51615.36, 51616.0, 51619.0, 51620.0, 51621.0, 51620.16, 51623.52, 51624.0, 51625.0, 51626.0, 51623.0, 51625.6, 51629.0, 51630.0, 51627.0, 51632.0, 51631.0, 51634.8, 51634.0, 51636.0, 51635.0, 51638.0, 51638.57, 51640.0, 51640.76, 51642.0, 51643.0, 51644.0, 51645.0, 51646.0, 51640.87, 51648.0, 51645.6, 51650.0, 51644.2, 51653.0, 51654.0, 51654.36, 51656.0, 51658.0, 51659.0, 51660.0, 51661.0, 51662.0, 51663.0, 51664.0, 51666.0, 51667.0, 51668.0, 51669.0, 51669.7, 51667.2, 51672.0, 51673.0, 51675.0, 51675.24, 51677.0, 51678.0, 51680.0, 51683.0, 51684.0, 51685.22, 51686.0, 51687.0, 51688.0, 51689.88, 51689.0, 51691.0, 51692.0, 51690.0, 51694.5, 51695.0, 51696.0, 51696.06, 51698.0, 51699.0, 51700.0, 51702.0, 51703.0, 51704.4, 51704.0, 51706.0, 51707.0, 51708.0, 51709.2, 51710.0, 51710.65, 576000.0, 51713.0, 51714.0, 51715.0, 51716.0, 51717.0, 51718.0, 51715.01, 51720.0, 51721.32, 51722.0, 51723.88, 51723.0, 51725.0, 51726.0, 51727.0, 51728.0, 51721.0, 51730.0, 51731.0, 51732.0, 51733.0, 51734.0, 51735.0, 51736.0, 51737.0, 51738.0, 51735.63, 51740.0, 51742.0, 51743.0, 51744.0, 51745.0, 51746.0, 51747.0, 51748.0, 51750.0, 51751.0, 51750.16, 51753.84, 51754.0, 51753.0, 51756.0, 51757.56, 51757.0, 51758.0, 51760.0, 51761.0, 51762.0, 51763.0, 51764.0, 51765.18, 51765.0, 51767.0, 51768.0, 51769.0, 51770.0, 51771.0, 51766.0, 51769.86, 51774.0, 51775.0, 51776.0, 51771.2, 51778.72, 51779.0, 51780.0, 51781.0, 51782.0, 51782.68, 51784.0, 51785.0, 51786.36, 51786.0, 51788.0, 51789.0, 51790.0, 51785.76, 51792.0, 51786.28, 51794.0, 51795.0, 51796.0, 51797.0, 51798.6, 51799.0, 51800.0, 51801.0, 51802.0, 51803.0, 51804.0, 51806.0, 313950.0, 51808.0, 51809.0, 51810.11, 51811.42, 51812.0, 51813.0, 51814.0, 51815.0, 51816.76, 51816.0, 51818.0, 51819.0, 51820.0, 51820.08, 51822.0, 51823.0, 51824.0, 51825.0, 51825.6, 51827.0, 51828.0, 51829.0, 51830.0, 51831.0, 51832.0, 51833.0, 51834.0, 51835.0, 51836.0, 51837.0, 51837.12, 51839.0, 51840.0, 51839.88, 51842.41, 51843.0, 51844.0, 51841.92, 51846.0, 51841.0, 51848.0, 51849.0, 51850.0, 51851.0, 51852.0, 51853.0, 51854.0, 51855.02, 314000.0, 51857.0, 51856.0, 51854.4, 51860.0, 51861.0, 51858.0, 51863.64, 51864.0, 51864.84, 51866.0, 51867.0, 51863.0, 51869.63, 51870.0, 51871.0, 51872.0, 51866.93, 51874.0, 51875.0, 51876.0, 51875.59, 51873.0, 51877.0, 51880.0, 51881.0, 51882.0, 51883.0, 51884.0, 51885.0, 51886.8, 51887.0, 51888.0, 51889.0, 51890.0, 51891.0, 51892.0, 51893.0, 51894.0, 51895.0, 51896.0, 51897.0, 51898.08, 51899.0, 51900.0, 51897.03, 51902.0, 51903.72, 51905.0, 51906.0, 51907.0, 51906.98, 51909.0, 51910.0, 51908.0, 51912.0, 51913.0, 51914.0, 51915.0, 51916.0, 51917.0, 51916.8, 51916.18, 51920.0, 51921.0, 51922.0, 51919.0, 51924.0, 51925.0, 51921.96, 51926.0, 51928.0, 51929.0, 51932.0, 51933.0, 51934.0, 51933.84, 51936.0, 51934.56, 51938.0, 51939.0, 51940.0, 51937.6, 51943.5, 51944.0, 51945.0, 51946.0, 51947.0, 51948.0, 51947.28, 51950.0, 51943.0, 51955.26, 51956.0, 51957.0, 51958.0, 51958.4, 51960.0, 51961.0, 51962.0, 51961.99, 51965.0, 51965.72, 51967.2, 51968.0, 51969.0, 51970.0, 51972.0, 51973.8, 51974.0, 51975.0, 51976.0, 51977.0, 51972.27, 51979.0, 51980.0, 51981.6, 51982.0, 51983.52, 51984.0, 51983.0, 51985.0, 51987.0, 51984.24, 51989.76, 51990.0, 51991.0, 51992.0, 51986.0, 51995.0, 51996.0, 51997.0, 51998.0, 51999.0, 52000.0, 52000.7, 52001.76, 52003.0, 52004.0, 52005.0, 52006.0, 52000.03, 52008.0, 52009.0, 52011.0, 52013.0, 52014.0, 52015.0, 52016.0, 52018.0, 52019.0, 52020.0, 52021.0, 52024.8, 52025.0, 52027.0, 52028.0, 52029.0, 52030.0, 52032.0, 52033.0, 52034.0, 52035.0, 52037.0, 52038.0, 52039.0, 52040.0, 52041.0, 52041.6, 52043.28, 52044.0, 52043.0, 52046.0, 52043.23, 52048.8, 52049.0, 52050.0, 52045.0, 52052.0, 52053.11, 52048.0, 52055.0, 52056.0, 52057.0, 52058.0, 52059.0, 52060.0, 52061.0, 52062.4, 52063.0, 52064.0, 52065.24, 52062.48, 52067.96, 52068.0, 52069.0, 52067.16, 52067.0, 52072.0, 52073.0, 52070.4, 52075.0, 52076.0, 52077.0, 52073.28, 62453.0, 52080.0, 52082.0, 52083.0, 52088.0, 52089.0, 52090.0, 52091.26, 52092.0, 52093.44, 52093.0, 52090.5, 52096.0, 52093.32, 52098.0, 52099.0, 52100.0, 52100.7, 52101.0, 52104.0, 52105.0, 52106.0, 52107.0, 52108.0, 52109.0, 52110.0, 52111.0, 52112.0, 52113.95, 52114.0, 52116.0, 52117.0, 52118.0, 52116.76, 52119.0, 52120.0, 52116.15, 52123.0, 52122.2, 52123.17, 52124.0, 52125.0, 52126.0, 52127.0, 52127.76, 52129.0, 52130.0, 52131.0, 52131.56, 52128.0, 52132.0, 52138.0, 52139.76, 52140.0, 52138.84, 52142.0, 52143.0, 52144.0, 52145.76, 52145.0, 52146.0, 52147.0, 52148.0, 52149.0, 52151.0, 52150.0, 52150.78, 52152.0, 52153.08, 52153.68, 52155.4, 52153.0, 52157.0, 52156.0, 52161.0, 52159.0, 52160.0, 52155.0, 52158.0, 52166.4, 52158.24, 52168.8, 52169.0, 52164.0, 52165.0, 52166.0, 52167.0, 52168.0, 52175.04, 52170.0, 52177.0, 52171.2, 52172.0, 52173.0, 52174.2, 52182.12, 52183.0, 52175.0, 52176.0, 52177.6, 52178.0, 52179.0, 52180.0, 52182.0, 52184.39, 52186.0, 52187.0, 52188.0, 52190.0, 52191.0, 52192.0, 52193.0, 52194.6, 52195.0, 52196.0, 52202.0, 52197.0, 52200.0, 52201.91, 52206.0, 52200.84, 52203.0, 52204.3, 52204.0, 52205.0, 52207.0, 52213.0, 52208.0, 52207.8, 52210.0, 52209.0, 52209.76, 52214.0, 52215.0, 52216.0, 52222.68, 52218.0, 52219.0, 52220.0, 52221.0, 52222.0, 52223.0, 52224.0, 52225.0, 52226.0, 52227.0, 52233.0, 52228.0, 52229.0, 52231.0, 52232.0, 52230.0, 52235.0, 52236.0, 52235.76, 52239.0, 52240.0, 52241.4, 52241.0, 52243.0, 52244.0, 52245.44, 52245.0, 52242.0, 52248.0, 52249.6, 52250.0, 52250.76, 52251.0, 52249.0, 52254.0, 52249.92, 52256.0, 52258.0, 52257.0, 52262.0, 52260.0, 52261.0, 52262.98, 52266.97, 52263.0, 52268.0, 52264.0, 52265.0, 52266.0, 52267.0, 52265.26, 52270.0, 52275.17, 52271.0, 52272.0, 52273.0, 52274.0, 52275.0, 52276.0, 52275.36, 52278.85, 52278.0, 52280.0, 52281.0, 52278.46, 52282.0, 52284.0, 52285.0, 52286.0, 52287.0, 52288.0, 52294.0, 52289.0, 52290.0, 52292.0, 52293.0, 52294.56, 52295.0, 52296.0, 52297.0, 52298.0, 52299.0, 52300.0, 52301.0, 52302.0, 52305.0, 52307.0, 52308.0, 52309.0, 52311.0, 52312.0, 52312.12, 52314.0, 52315.0, 52317.15, 52314.24, 52317.0, 52318.0, 52319.0, 52320.0, 52321.0, 52322.0, 52322.88, 52318.64, 52325.52, 52325.0, 52323.0, 52329.0, 52331.0, 52330.0, 52332.8, 52334.04, 52332.0, 52333.0, 52337.0, 52335.0, 52339.2, 52336.0, 52337.52, 52338.0, 52343.29, 52339.0, 52340.0, 52341.0, 52347.0, 52342.0, 52343.0, 52344.0, 52345.0, 52344.95, 52347.9, 52346.0, 52349.65, 52350.0, 52351.0, 52353.6, 52354.0, 52355.0, 52356.0, 52357.0, 52359.0, 52360.0, 52358.0, 52362.0, 52363.0, 52364.0, 52365.0, 52368.0, 52369.0, 52371.0, 52372.0, 52374.69, 52375.0, 52376.0, 52377.0, 52378.58, 52379.0, 52374.0, 52377.56, 52380.0, 52381.0, 52378.0, 52383.0, 52384.0, 52383.18, 52387.0, 52389.0, 52388.0, 52385.0, 52390.0, 52392.0, 52394.0, 52395.0, 52387.37, 52396.0, 52395.2, 52399.0, 52397.0, 52399.48, 52400.0, 52403.0, 52400.23, 52401.77, 52401.0, 52404.0, 52405.0, 52406.0, 52407.0, 52408.0, 52406.8, 52411.68, 52412.21, 52412.0, 52413.0, 52414.0, 52418.0, 52416.0, 52417.0, 52418.08, 52419.0, 52423.0, 52420.0, 52421.52, 52421.76, 52427.05, 52427.0, 1101000.0, 52424.59, 52431.6, 52432.0, 52433.93, 52426.0, 52435.2, 52428.0, 52430.0, 52431.0, 52439.8, 52433.0, 52434.0, 52435.0, 52436.0, 52437.0, 52438.0, 52439.0, 52440.0, 52441.0, 52443.0, 52444.0, 52445.0, 52446.0, 52444.44, 52445.9, 52449.72, 52450.0, 52452.0, 52453.0, 52454.0, 52455.84, 52456.0, 52457.0, 52463.76, 52463.21, 52456.2, 52460.0, 52461.0, 52461.84, 52463.25, 52464.0, 52471.0, 52465.0, 52466.0, 52467.78, 52468.0, 52469.76, 52470.0, 52471.56, 52472.04, 52473.6, 52474.0, 52473.0, 52476.0, 52475.0, 52479.33, 52480.0, 52487.0, 52481.62, 52482.21, 52484.64, 52485.0, 52486.0, 52484.0, 52488.0, 52489.0, 52490.0, 52490.14, 52498.0, 52492.8, 52494.0, 52496.0, 52502.0, 52503.0, 52497.0, 52499.0, 52500.0, 52501.8, 52499.2, 52504.0, 52505.58, 52506.0, 52505.0, 52505.4, 52508.0, 52507.0, 52511.0, 52512.0, 52513.0, 52517.0, 52518.0, 52519.0, 52522.0, 52520.0, 52521.0, 52518.72, 52523.0, 52524.0, 52525.0, 52526.0, 52527.0, 52528.0, 52529.0, 52530.0, 52529.4, 52532.0, 52534.0, 52535.0, 52536.0, 52537.0, 52540.08, 52541.0, 52538.0, 52539.0, 52540.4, 52540.0, 52542.0, 52542.76, 52544.0, 52545.0, 52547.0, 52548.0, 52543.0, 52550.0, 52550.04, 52546.0, 52552.0, 52554.16, 52555.0, 52556.0, 52556.46, 52554.0, 52559.0, 52560.0, 52561.0, 52562.0, 52564.0, 52565.0, 52566.0, 52567.0, 52568.0, 52568.18, 52569.0, 52571.0, 52572.0, 52573.0, 52576.0, 52577.0, 52578.12, 52575.0, 52575.84, 52578.0, 52579.0, 52580.0, 52580.73, 52582.0, 52584.0, 52585.0, 52585.64, 52582.56, 52588.0, 52583.0, 52590.0, 52586.0, 52592.0, 52593.0, 52595.0, 52596.0, 52597.0, 52599.84, 52594.0, 52599.0, 52600.0, 52599.96, 52602.0, 52603.0, 52604.78, 52605.0, 52604.0, 52607.52, 52608.0, 52610.0, 52611.0, 52613.0, 52612.0, 52614.0, 52615.0, 52610.88, 52617.24, 52618.0, 52619.0, 52620.0, 52621.0, 52622.0, 52623.0, 52624.0, 52625.0, 52626.0, 52627.8, 52629.0, 52630.0, 52631.0, 52632.0, 52633.0, 52634.0, 52635.0, 52636.0, 52637.23, 52637.0, 52639.0, 52640.0, 52641.79, 52641.96, 52642.0, 52641.0, 52644.0, 52645.0, 52643.0, 52648.0, 52646.0, 52647.09, 52649.0, 52650.0, 52653.97, 52652.0, 52655.0, 52655.28, 52653.84, 52658.0, 52659.6, 52654.0, 52653.0, 52656.0, 52663.2, 52657.0, 52665.35, 52657.64, 52660.0, 52662.0, 52669.63, 52663.0, 52664.0, 52665.0, 52666.0, 52668.0, 52669.0, 52670.0, 52671.0, 52672.0, 52671.9, 52674.0, 52681.0, 52675.0, 52676.0, 52677.0, 52679.0, 52680.0, 52681.61, 52683.0, 52684.0, 314828.8, 52686.0, 52685.0, 52688.0, 52694.0, 52689.96, 52690.0, 52689.0, 52692.0, 52693.0, 52695.0, 52696.0, 52696.54, 52698.0, 52699.0, 52700.0, 52701.0, 52701.22, 52703.0, 52709.0, 52704.0, 52702.0, 52706.0, 52707.0, 52710.0, 52711.0, 52712.0, 52713.0, 52714.8, 52715.0, 52714.0, 52716.0, 52718.0, 52719.0, 52720.0, 52721.0, 52722.0, 52725.0, 52726.0, 52728.0, 52730.0, 52732.0, 52733.0, 52734.0, 52736.0, 52737.0, 52736.68, 52739.0, 52740.0, 52742.0, 52744.0, 52745.0, 52746.0, 52747.0, 52748.0, 52749.0, 52750.0, 52751.0, 52752.0, 62588.0, 52754.0, 52755.0, 52756.0, 52757.4, 52758.0, 52757.0, 52760.0, 52761.0, 52762.0, 52754.88, 52764.0, 52765.44, 52766.0, 52766.48, 52768.0, 52769.6, 52769.0, 52770.0, 52772.64, 52771.0, 52774.0, 52775.0, 52775.2, 52768.42, 52776.0, 52779.0, 52777.0, 52781.71, 52776.1, 52780.0, 52784.0, 52781.0, 52786.0, 52787.54, 52782.64, 52781.52, 52782.0, 52785.0, 52787.0, 52788.0, 52788.36, 52790.0, 52790.4, 52789.0, 52792.0, 52793.0, 52796.0, 52799.0, 52800.0, 52795.0, 52804.0, 52804.5, 52806.32, 52805.0, 52806.0, 52809.12, 52808.0, 52811.0, 52812.0, 52814.76, 52814.0, 52816.0, 52817.0, 52818.0, 52819.0, 52818.16, 52821.49, 52820.0, 52823.0, 52824.6, 52821.43, 52822.0, 52821.6, 52824.96, 52825.0, 52826.4, 52824.0, 52829.0, 52830.0, 52832.0, 52834.0, 52828.0, 52836.0, 52833.0, 52838.72, 52838.0, 52840.0, 52841.0, 52843.0, 52839.0, 52845.0, 52846.0, 52847.0, 2150000.0, 52848.0, 52850.0, 52851.0, 52852.0, 52853.0, 52854.0, 52851.4, 315000.0, 52856.0, 52852.57, 52859.0, 52860.0, 52858.0, 52862.0, 52855.0, 52864.0, 52865.0, 52866.0, 52867.0, 52868.0, 52869.0, 52870.0, 52871.0, 52872.44, 52872.0, 52874.0, 52875.0, 52876.0, 52877.0, 52878.0, 52879.0, 52880.0, 52878.8, 52883.0, 52884.36, 52884.0, 52886.0, 52885.0, 52888.0, 52885.2, 52887.0, 52889.0, 52892.0, 52893.0, 52889.2, 52890.0, 52893.47, 52894.0, 52896.0, 315040.0, 52897.0, 52896.76, 52900.0, 52901.04, 52898.0, 52899.0, 52902.0, 52905.0, 52906.0, 52908.0, 52910.0, 52911.0, 52912.0, 52913.0, 52915.0, 52916.0, 52920.0, 52920.32, 52922.0, 52923.0, 52924.0, 52920.12, 52923.96, 52927.0, 52928.0, 52925.0, 52930.0, 52926.06, 52932.0, 52933.0, 52931.0, 52935.0, 52936.0, 52934.0, 52938.0, 52940.0, 52941.0, 52942.0, 52942.14, 52944.0, 52943.0, 52946.0, 52948.0, 52949.05, 52950.0, 52951.07, 52950.24, 52952.0, 52954.0, 52953.0, 52955.0, 52955.76, 52956.0, 52959.0, 52960.0, 52961.0, 52962.0, 315104.0, 315108.0, 52963.04, 52964.0, 52965.0, 52966.92, 52967.0, 52968.0, 52967.88, 52970.0, 52971.84, 52972.0, 52974.83, 52975.0, 52969.0, 52977.0, 52977.6, 52979.94, 52980.0, 52981.0, 52982.0, 52978.0, 52984.0, 52985.0, 52987.0, 52988.0, 52989.0, 52990.0, 52990.71, 52986.0, 52993.0, 52994.86, 52992.0, 52991.0, 52997.6, 52994.0, 52999.96, 52995.65, 52995.0, 52997.0, 52998.4, 52998.0, 53000.0, 53001.0, 52999.0, 52999.92, 53004.0, 53005.0, 53006.0, 53003.0, 53010.0, 53011.0, 53012.34, 53014.0, 53012.0, 53016.0, 53018.0, 53019.0, 53020.0, 53022.4, 53023.0, 53024.0, 53025.0, 53027.0, 53028.0, 53029.0, 53030.0, 53032.2, 53032.0, 53035.0, 53037.24, 53038.06, 53038.0, 53038.52, 53040.0, 53037.83, 53042.0, 53044.9, 53043.68, 53044.96, 53045.28, 53045.0, 53048.0, 53050.38, 53049.0, 53050.0, 53050.22, 53051.0, 53053.0, 53052.0, 53055.0, 53056.0, 53057.0, 53054.0, 53059.0, 53060.0, 53060.8, 53063.31, 53064.0, 53065.0, 53068.08, 53068.0, 53068.6, 53070.0, 53072.0, 53073.0, 53075.0, 53076.0, 53077.0, 53080.0, 53081.0, 53082.0, 53083.0, 53085.0, 53085.8, 53086.92, 53088.0, 53087.0, 53090.0, 53086.0, 53092.0, 53093.0, 53094.0, 53095.0, 53096.0, 53097.88, 53089.52, 53099.28, 53098.0, 53099.0, 53100.0, 53103.0, 53104.0, 53101.0, 53102.0, 53102.4, 53105.0, 53106.0, 53107.06, 53108.0, 53109.0, 53110.0, 53111.0, 53112.0, 53113.0, 53114.0, 53113.51, 53116.0, 53116.8, 53121.0, 53117.0, 53119.25, 53120.0, 53121.94, 53122.0, 53118.0, 53124.0, 53125.0, 53130.0, 53126.0, 53127.66, 53128.0, 53129.0, 53129.4, 53132.0, 53133.0, 53134.0, 53135.4, 53136.0, 53137.0, 53135.0, 53139.8, 53140.0, 53139.0, 53142.0, 53144.0, 53146.0, 53145.6, 53148.0, 53151.0, 53147.75, 53150.0, 53151.6, 53152.0, 53154.0, 53155.0, 53156.0, 53157.0, 53158.0, 53159.0, 53160.0, 53162.0, 53164.0, 53163.0, 53165.0, 53167.0, 53169.0, 53170.0, 53171.0, 53172.0, 53173.2, 53173.0, 53175.0, 53174.0, 53176.0, 53177.0, 53178.0, 53177.09, 53180.0, 53181.0, 53182.0, 53181.12, 53185.0, 53184.0, 53187.84, 53188.0, 53187.0, 53190.0, 53183.0, 53190.78, 53191.0, 53192.0, 53193.0, 53194.0, 53195.0, 53198.0, 53196.0, 53193.96, 53199.0, 53200.0, 53203.2, 53200.08, 53202.0, 53203.0, 53204.0, 53208.0, 53205.0, 53206.0, 53206.4, 53209.0, 53210.0, 53211.6, 53212.0, 53214.0, 53217.0, 53217.2, 53215.0, 53216.0, 53213.0, 53218.0, 53220.0, 53221.0, 53222.0, 53223.0, 53224.0, 53225.0, 53226.32, 53227.2, 53231.0, 53228.0, 53227.0, 53230.0, 53227.38, 53232.0, 53229.0, 53235.0, 53237.0, 53238.86, 53238.0, 53240.0, 53242.0, 53241.0, 53244.0, 53245.32, 53246.0, 53245.0, 53248.0, 53249.0, 53250.0, 53251.0, 53252.68, 53253.0, 53255.0, 53256.84, 53256.0, 53258.0, 53259.8, 53260.0, 53260.8, 53262.0, 53263.0, 53264.0, 53265.0, 53266.0, 53261.52, 53268.8, 53268.0, 53270.0, 53269.0, 53272.0, 53273.0, 53274.0, 53275.0, 53276.0, 53271.02, 53278.0, 53280.0, 53282.0, 53284.92, 53284.0, 53286.0, 53285.0, 53287.0, 53288.64, 53289.0, 53290.0, 53287.32, 53293.0, 53294.0, 53292.0, 53288.0, 53291.0, 53295.0, 53299.68, 53296.0, 53298.0, 53299.0, 53300.0, 53301.0, 53302.95, 53301.84, 53304.0, 53305.0, 53309.19, 53306.0, 53307.0, 53312.62, 53308.0, 53305.1, 53310.0, 53311.0, 53312.0, 53318.4, 53313.0, 53314.0, 53316.0, 53317.0, 53319.0, 53320.0, 53321.0, 53322.0, 53323.0, 53328.0, 53324.0, 53330.64, 53331.0, 53325.0, 53333.91, 53326.0, 53327.0, 53329.0, 53330.0, 53331.2, 53332.0, 53333.0, 53335.0, 53336.0, 53337.0, 53338.0, 53339.0, 53340.0, 53343.0, 53345.0, 53346.0, 53345.43, 53351.0, 53348.0, 53349.0, 53354.64, 53350.0, 53351.32, 53352.0, 53353.0, 53359.2, 53360.32, 53354.08, 53362.2, 53355.0, 53356.0, 53365.88, 53366.0, 53358.0, 53368.0, 53360.0, 53361.0, 53362.0, 53363.0, 53373.0, 53365.76, 53369.0, 53370.0, 53371.0, 53372.0, 53374.0, 53373.28, 53376.0, 53378.0, 53379.2, 53377.0, 53380.0, 53381.0, 53387.62, 53387.0, 53383.0, 53384.0, 53384.76, 53386.0, 53388.0, 53389.0, 53390.0, 53391.0, 53392.15, 53393.0, 53394.0, 53395.0, 53396.0, 53395.76, 53398.0, 53399.0, 53400.0, 53402.0, 53404.0, 53405.6, 53404.2, 53407.0, 53408.0, 53409.0, 53410.0, 53412.0, 53406.98, 53414.0, 53417.0, 53420.0, 53421.0, 53422.0, 53423.0, 53424.0, 53425.0, 53421.68, 53427.34, 53427.0, 53429.0, 53430.0, 53431.0, 53432.0, 53434.0, 53435.0, 53436.0, 53435.2, 53437.0, 53436.68, 53440.68, 53440.0, 53441.0, 53442.0, 53443.0, 577733.0, 53444.0, 53445.0, 53446.0, 53449.0, 53448.0, 53451.0, 53449.52, 53453.4, 53450.0, 53452.0, 315600.0, 53454.0, 53455.0, 53456.0, 53453.0, 53458.0, 53454.52, 53460.0, 53459.0, 53462.4, 53457.0, 53464.0, 53465.0, 53466.0, 53467.0, 53468.0, 53470.0, 53471.0, 53472.0, 53473.83, 53473.0, 53475.0, 53476.0, 53471.08, 53478.0, 53479.74, 53480.0, 53483.0, 53481.84, 53482.0, 53486.88, 53481.36, 53484.0, 53485.0, 53490.16, 53491.0, 53486.09, 53486.0, 53488.0, 53489.0, 53490.0, 53491.59, 53492.0, 53494.0, 53493.0, 53496.0, 53497.6, 53498.0, 53499.0, 53500.0, 53501.0, 53504.35, 53505.0, 53502.9, 53508.0, 53511.57, 53511.05, 53512.44, 53509.0, 53510.0, 53511.0, 53517.0, 53512.25, 53519.4, 53512.8, 53521.0, 53513.0, 53515.0, 53516.24, 53518.0, 53520.0, 53520.72, 53528.0, 53522.0, 53523.0, 53524.0, 53526.0, 53527.0, 53529.0, 53527.6, 53528.96, 53532.0, 53538.72, 53530.0, 53535.0, 53536.0, 53542.0, 53537.0, 53538.12, 53539.0, 53540.0, 53541.0, 53543.0, 53544.0, 53545.0, 53546.0, 53548.0, 53549.0, 53550.0, 53551.0, 53552.0, 53548.8, 53554.0, 53555.0, 53556.0, 53558.0, 53560.0, 53561.0, 53564.0, 53565.0, 53566.0, 840000.0, 53568.0, 53570.0, 53571.0, 53569.0, 53572.0, 53574.6, 53573.93, 53576.0, 53577.0, 53574.76, 53575.0, 53580.8, 53576.04, 53574.0, 53578.0, 53584.0, 53578.03, 53580.0, 53587.0, 53581.0, 53582.0, 53583.84, 53585.0, 53592.56, 53586.0, 53588.0, 53595.2, 53591.0, 53592.0, 53593.0, 53590.0, 53595.0, 53596.0, 53597.17, 53598.0, 53600.0, 53601.0, 53602.0, 53600.32, 53604.0, 53605.0, 53606.0, 53607.0, 53608.0, 53609.0, 53614.8, 53610.0, 53611.0, 53612.76, 53618.4, 53612.47, 53615.54, 53616.72, 53616.0, 53615.0, 53619.6, 53620.0, 53621.0, 53622.0, 53623.0, 53621.57, 53625.0, 53624.0, 53627.0, 53628.0, 53634.0, 53635.0, 53629.0, 53631.0, 53632.0, 53632.24, 53633.0, 53636.0, 53636.16, 53638.0, 53644.0, 53644.92, 53639.0, 53640.0, 53641.0, 53642.0, 53643.0, 53643.2, 53645.0, 53646.0, 53648.0, 53649.0, 53650.0, 53652.55, 53652.0, 53654.88, 53655.0, 53654.0, 53653.0, 53658.0, 53657.0, 53660.0, 53661.0, 53667.68, 53663.0, 53664.0, 53665.0, 53666.0, 53667.0, 53668.0, 53674.45, 53669.0, 53667.28, 53671.0, 53670.0, 53674.0, 53675.0, 53681.25, 53676.0, 53683.44, 53679.0, 53685.84, 53680.0, 53681.0, 53680.2, 53683.0, 53690.76, 53684.0, 53685.0, 53693.0, 53686.0, 53686.16, 53688.0, 53689.0, 53690.0, 53691.0, 53692.0, 53692.2, 53702.0, 53694.0, 53695.0, 53696.0, 53697.84, 53700.0, 53701.12, 53709.84, 53701.0, 53703.0, 53704.0, 53705.0, 53706.0, 53707.0, 53709.08, 53710.42, 53711.53, 53712.0, 53711.0, 578000.0, 53716.0, 53714.88, 53719.0, 53720.0, 53722.0, 53722.5, 53724.0, 53726.0, 53721.0, 53729.28, 53732.04, 53729.0, 53734.08, 53730.0, 53731.0, 53733.0, 53734.0, 315879.0, 53736.0, 53737.0, 53740.0, 53741.0, 53738.0, 53742.0, 53744.0, 53745.0, 53746.0, 53747.2, 53748.0, 53751.0, 53752.0, 53749.0, 53750.0, 53748.66, 53752.03, 53754.0, 53755.0, 53756.0, 53753.0, 53761.11, 53758.8, 53758.0, 53760.0, 53761.0, 53762.0, 53763.84, 53763.0, 53765.0, 53767.0, 53768.0, 53769.78, 53770.0, 53772.0, 53775.0, 53773.0, 53777.64, 53774.0, 53774.76, 53771.0, 53776.0, 53778.0, 53779.0, 53780.0, 53781.0, 53782.0, 53783.0, 53784.95, 53786.61, 53787.0, 53788.0, 53790.0, 53786.0, 53791.0, 53793.0, 53789.0, 53796.0, 53798.0, 53799.0, 53800.0, 53803.0, 53806.0, 53807.0, 53808.0, 53809.0, 53810.0, 53809.75, 53812.0, 53811.0, 53813.0, 53815.0, 53816.0, 53819.0, 53820.0, 53821.0, 53822.0, 53823.0, 53823.49, 53825.0, 53826.0, 53828.28, 53829.0, 53830.0, 53831.0, 53832.0, 53833.0, 53834.0, 53835.0, 53836.0, 53837.0, 53838.0, 53839.0, 53840.0, 53842.0, 53842.18, 53844.0, 53843.0, 53846.0, 53846.52, 53845.0, 53847.0, 53849.0, 53850.0, 53851.0, 53853.0, 53852.0, 53851.2, 53854.0, 53855.0, 53856.0, 53859.0, 316000.0, 53858.88, 53860.0, 53861.0, 53862.0, 53863.0, 53864.0, 53865.0, 53866.0, 53867.0, 53868.0, 53870.0, 53871.0, 53872.0, 53873.0, 53875.96, 53874.0, 53875.0, 53878.0, 53876.0, 53876.64, 53880.0, 53881.16, 53882.0, 53884.0, 53885.44, 53885.0, 53887.08, 53886.0, 53888.0, 53890.0, 53891.0, 53892.96, 53892.5, 53892.8, 53892.0, 53893.0, 53895.0, 53896.0, 53897.89, 53893.87, 53899.0, 53900.0, 53897.0, 53903.0, 53904.0, 53905.0, 53907.6, 53906.88, 53907.0, 53908.0, 53909.0, 53910.0, 53912.0, 53914.0, 53913.8, 53914.12, 53916.0, 53913.0, 53919.0, 53920.0, 53921.0, 53919.12, 53922.0, 53924.0, 53925.0, 53926.0, 53927.0, 53928.0, 53929.0, 53930.0, 53929.44, 53932.0, 53933.0, 53934.0, 53931.0, 53936.0, 53937.0, 53938.28, 53939.0, 53940.0, 53941.44, 53939.88, 53945.0, 53946.0, 53947.0, 53948.0, 53949.0, 53950.0, 53951.48, 53952.0, 53953.0, 53954.0, 53955.0, 53955.2, 53954.68, 53958.0, 53956.0, 53956.82, 53957.0, 53962.0, 53959.2, 53960.0, 53964.0, 53961.0, 53966.0, 53959.0, 53968.0, 53969.0, 53970.0, 53971.0, 53972.0, 53973.0, 53974.0, 53975.0, 53976.0, 53976.8, 53978.0, 53979.0, 53981.0, 53980.0, 53983.0, 53984.0, 53985.92, 53986.32, 53986.0, 53982.0, 53989.0, 53990.92, 53990.0, 53992.0, 53991.0, 53994.0, 53993.0, 53996.28, 53997.54, 53994.2, 53995.0, 53996.8, 53997.0, 53998.0, 53997.96, 54000.0, 53996.0, 54002.0, 54003.0, 54004.0, 54005.0, 54006.0, 54008.0, 54010.0, 54011.55, 54012.0, 54013.44, 54011.0, 54015.5, 54016.0, 54017.0, 54020.0, 54018.0, 54019.0, 316160.0, 54022.0, 54023.91, 54024.0, 54027.0, 54023.86, 54026.0, 54025.0, 54028.0, 54029.56, 54029.0, 54032.0, 54033.0, 54036.12, 54035.68, 54036.0, 54035.0, 54038.0, 54040.0, 54041.0, 54042.0, 54038.4, 54044.0, 54046.58, 54046.0, 54045.0, 54046.26, 54047.0, 54048.0, 54048.6, 54050.0, 54051.0, 54052.0, 54051.65, 54057.0, 54054.0, 54059.2, 54051.63, 54056.42, 54056.0, 54058.0, 54059.0, 54060.0, 54061.0, 54059.5, 54062.0, 54063.0, 54065.0, 54066.0, 54064.0, 54070.0, 54074.0, 54071.2, 54072.0, 54071.0, 54073.0, 54075.06, 54075.0, 54077.0, 54078.56, 54078.0, 54080.0, 54081.0, 54081.84, 54079.0, 54084.0, 54081.3, 54088.0, 54089.0, 54090.0, 54085.0, 54093.0, 54095.0, 54096.0, 54097.0, 54096.52, 54099.0, 54096.76, 54100.0, 54101.0, 54100.8, 54104.0, 54102.0, 54100.56, 54105.0, 316250.0, 54107.0, 54108.0, 54103.88, 54105.39, 54111.0, 54111.2, 54113.0, 54114.0, 54115.0, 54116.0, 54116.28, 54119.0, 54120.0, 54121.0, 54122.0, 54122.02, 54124.8, 54124.0, 54125.0, 54127.0, 54127.1, 54126.0, 54123.0, 54131.48, 54132.0, 54132.48, 54133.0, 54134.64, 54137.0, 54136.0, 54139.0, 54140.0, 54142.0, 54143.0, 54144.0, 54146.0, 54147.0, 54148.5, 54148.0, 54150.0, 54152.0, 54154.0, 54155.0, 54156.84, 54156.0, 54156.6, 54157.0, 54158.0, 54155.46, 54160.0, 54161.0, 54162.0, 54163.0, 54164.0, 54162.12, 54166.0, 54168.0, 54169.0, 54170.0, 54171.0, 54168.14, 316317.0, 54174.0, 54175.0, 54173.0, 54177.96, 54178.0, 54180.0, 54181.0, 54176.0, 54177.19, 54184.0, 54185.0, 54186.6, 54187.0, 54188.0, 54186.0, 54183.0, 54191.0, 54192.0, 54193.0, 54194.0, 54195.0, 54196.0, 54194.36, 54198.0, 54199.0, 54200.0, 54201.0, 54203.0, 54204.0, 54205.0, 316350.67, 54207.0, 54208.0, 54209.0, 54210.0, 54211.0, 54206.0, 54208.85, 54215.0, 54216.0, 54217.0, 54218.0, 54216.24, 54220.0, 54221.0, 54222.0, 54220.05, 54224.0, 54225.6, 54225.0, 54226.0, 54223.0, 54229.0, 54230.0, 54228.0, 54232.0, 54233.0, 54234.08, 54235.26, 54235.0, 54236.0, 54237.0, 54238.59, 54239.0, 54240.0, 54241.0, 54234.0, 54243.0, 54245.64, 54246.0, 54241.44, 54244.88, 54246.4, 54247.56, 54248.0, 54249.0, 54250.0, 54252.0, 54255.0, 54256.0, 54252.32, 54253.12, 54254.0, 54257.0, 54258.0, 54258.28, 54260.0, 54262.0, 54263.0, 54264.0, 54265.61, 54268.0, 54268.68, 54267.0, 54267.2, 54269.0, 54270.0, 54267.92, 54272.04, 3200000.0, 54274.08, 54274.0, 54275.0, 54277.0, 54276.0, 54279.0, 54283.0, 54280.0, 54281.0, 54282.6, 54282.0, 54284.97, 54286.0, 54287.0, 54288.0, 54289.0, 54293.4, 54290.0, 54291.5, 54291.0, 54293.0, 54295.0, 54296.0, 54298.0, 54299.0, 54300.0, 54301.0, 54302.0, 54297.0, 54304.0, 54304.56, 54308.8, 54305.88, 54307.0, 54308.0, 54310.0, 54311.0, 54312.0, 54313.0, 54314.0, 54312.6, 54316.0, 54315.0, 54318.0, 54319.0, 54320.0, 54317.0, 54323.29, 54324.0, 54325.0, 54327.0, 54324.28, 54326.0, 54328.0, 54329.0, 54331.0, 54333.0, 54334.0, 54332.0, 54330.0, 54334.5, 316482.0, 54335.0, 54336.0, 54337.0, 54338.0, 54343.8, 54339.84, 54340.0, 54346.92, 54341.73, 54339.0, 54343.0, 54350.4, 54351.36, 54345.0, 54346.0, 54347.0, 54349.0, 316500.0, 54350.0, 54352.0, 54353.0, 54355.0, 54356.0, 54357.0, 54358.0, 54359.0, 54365.0, 54360.0, 54361.0, 54362.0, 54363.0, 54364.0, 54364.8, 54366.0, 54367.2, 54367.0, 54369.0, 54370.0, 54371.0, 54372.04, 54372.0, 54374.0, 54375.0, 54376.0, 54377.0, 54378.0, 54379.0, 54381.0, 54382.0, 54388.0, 54384.0, 54385.0, 54383.42, 54392.92, 54386.28, 54389.0, 54386.0, 54391.0, 54397.51, 54398.6, 54392.0, 54392.22, 54394.0, 54393.0, 54396.0, 54395.0, 54399.0, 54400.0, 54401.0, 54402.0, 54403.84, 54410.0, 54404.0, 54405.0, 54406.96, 54407.0, 54408.0, 54411.0, 54412.0, 54412.8, 54413.0, 54415.0, 54414.0, 54416.0, 54423.0, 54418.0, 54419.04, 54420.0, 54421.0, 54428.36, 54428.0, 54430.0, 54422.0, 54424.0, 54433.6, 54425.0, 54426.97, 54427.0, 54428.99, 54429.0, 54432.0, 54433.0, 54434.0, 54442.88, 54435.0, 54436.0, 54437.64, 54438.03, 54447.0, 54439.0, 54440.0, 54441.0, 54442.0, 54443.0, 54444.0, 54445.0, 54446.0, 54448.0, 54449.0, 54450.0, 54451.0, 54460.25, 54452.0, 54462.36, 54454.0, 54455.0, 54456.0, 54466.88, 54458.0, 54459.96, 54460.0, 54462.0, 54464.0, 54465.62, 54466.0, 54467.0, 54468.0, 316611.0, 54470.0, 54471.0, 54473.88, 54474.0, 54475.0, 54476.0, 54475.2, 54479.0, 54480.0, 54481.0, 54483.0, 54485.0, 54486.0, 54484.68, 54488.0, 54489.0, 54487.0, 54494.0, 54491.0, 54492.0, 54493.0, 54491.85, 54495.0, 54496.0, 54498.0, 54499.0, 54500.0, 54500.83, 54502.47, 54503.0, 54504.0, 54505.0, 54502.19, 54508.0, 54509.0, 54512.0, 54513.0, 54514.92, 54514.0, 54516.0, 54517.0, 54518.0, 54519.36, 54520.0, 54519.24, 54522.0, 54515.0, 54524.0, 54525.0, 54527.42, 54528.0, 54527.88, 54530.0, 54528.93, 54532.0, 54533.0, 54535.0, 54536.0, 54537.0, 54538.56, 54538.0, 54540.48, 54540.41, 54540.0, 54541.95, 54542.0, 54543.0, 54545.0, 54546.0, 54544.0, 54548.0, 54549.0, 54550.0, 54551.0, 54552.0, 54554.0, 54555.0, 54556.0, 54557.0, 54558.0, 54559.0, 54560.0, 54561.0, 54562.0, 54563.0, 54564.0, 54565.0, 54566.04, 54566.0, 54562.64, 54568.0, 54570.0, 54567.0, 54572.0, 54573.0, 54574.0, 54575.0, 54576.0, 54577.44, 54578.0, 54577.0, 54580.0, 54579.2, 54579.0, 54581.6, 54582.0, 54581.0, 54584.0, 54585.0, 54583.0, 54588.0, 54589.0, 54590.0, 54591.0, 54587.0, 54586.0, 54594.0, 54596.0, 54595.0, 54598.0, 54595.8, 54597.23, 54597.0, 54599.0, 54600.0, 54602.61, 54603.0, 54604.0, 54605.0, 54606.0, 54607.0, 54601.0, 54609.0, 54610.0, 54612.0, 54614.0, 54614.04, 54616.0, 54618.96, 54618.72, 54618.0, 54619.08, 54620.0, 54620.8, 54621.0, 54619.0, 54624.23, 54625.0, 54623.0, 54627.0, 54628.0, 54630.0, 54632.0, 54631.0, 54634.8, 54631.75, 54633.0, 54634.16, 54635.0, 54636.0, 54637.0, 54636.35, 54640.0, 54641.0, 54642.0, 54643.0, 54645.0, 54638.0, 54648.0, 54649.0, 54650.0, 54651.0, 54652.0, 54653.0, 54655.0, 54656.0, 54657.0, 316800.0, 54659.0, 54660.0, 54662.0, 54667.0, 54668.0, 54670.0, 54671.48, 54672.92, 54672.0, 54673.94, 54675.0, 54676.0, 54677.0, 54678.0, 54679.0, 54680.0, 54681.6, 54682.0, 54683.0, 54684.0, 54685.0, 54684.69, 54687.0, 54688.0, 54690.0, 54691.0, 54694.0, 54695.37, 54695.0, 54696.38, 54697.0, 54696.0, 316843.0, 54701.0, 54700.0, 54700.8, 54702.0, 54703.0, 54704.0, 54705.0, 54703.8, 54708.0, 54709.0, 54710.0, 54712.0, 54713.0, 579000.0, 54715.56, 54716.0, 54714.0, 54715.0, 54718.0, 54718.04, 54720.0, 54721.0, 54717.0, 54723.12, 54725.0, 316870.0, 54726.63, 54728.0, 54730.0, 54731.0, 54732.0, 54733.0, 54732.54, 54736.0, 54737.0, 54740.73, 54740.0, 54741.0, 54743.0, 54744.0, 54745.0, 54745.6, 54747.0, 54748.0, 54749.0, 54742.0, 54750.0, 54752.0, 54753.6, 54754.0, 54754.18, 54756.0, 54757.0, 54753.0, 54758.0, 54755.0, 54760.0, 54761.0, 54761.88, 54761.16, 54764.0, 54762.8, 54766.0, 54765.0, 54768.0, 54768.29, 54772.0, 54773.0, 54776.0, 54777.0, 54778.0, 54779.0, 54778.8, 54780.0, 54781.0, 54782.0, 54783.0, 54785.0, 54786.36, 54784.0, 54787.0, 54789.24, 54788.0, 54790.0, 54792.0, 54791.0, 54789.0, 54793.0, 54794.0, 54795.0, 54796.86, 54796.0, 54798.0, 54799.0, 54800.0, 54801.0, 54797.0, 54797.6, 54804.0, 54804.12, 54806.0, 54806.28, 54808.0, 54809.24, 54810.0, 54811.0, 54812.0, 54812.9, 54809.0, 54814.0, 54816.0, 54820.18, 54820.0, 54823.0, 54826.0, 54827.73, 54828.0, 54827.0, 54830.0, 54829.32, 54829.0, 54833.52, 54833.0, 54831.0, 54834.0, 54835.0, 54836.0, 54837.0, 54838.0, 54841.0, 54840.0, 54841.85, 54842.0, 54843.0, 54844.8, 54844.0, 54839.0, 54849.6, 54847.0, 54848.0, 54849.0, 54850.6, 54850.0, 54852.0, 54852.96, 54855.0, 317000.0, 54857.4, 54858.0, 54861.0, 54862.0, 54859.0, 54860.0, 54861.56, 54863.0, 54864.16, 54865.72, 54864.0, 54866.0, 54869.0, 54870.0, 54867.0, 54872.0, 54875.04, 54873.0, 54870.4, 54878.0, 54875.0, 54876.0, 54877.0, 317018.0, 54879.84, 54880.0, 54879.0, 54882.0, 54887.28, 54883.0, 54884.0, 54890.28, 54885.0, 54886.48, 54893.85, 54894.0, 54895.68, 54888.0, 54889.0, 54890.0, 54891.2, 54892.8, 54901.0, 54902.52, 54895.0, 54896.0, 54905.64, 54897.0, 54898.41, 54899.0, 54900.0, 54902.0, 54903.0, 54904.0, 54905.0, 54908.0, 54909.0, 54911.44, 54912.0, 54913.15, 54913.0, 54914.56, 54921.43, 54914.0, 54920.0, 54921.0, 54925.0, 54922.0, 54927.0, 54923.0, 54924.0, 54926.0, 54929.0, 54930.0, 54931.0, 54932.31, 54932.52, 54933.0, 54932.0, 54936.0, 54934.0, 54938.0, 54939.0, 54940.0, 54940.56, 54943.0, 54945.0, 54946.0, 54947.28, 54948.0, 54949.0, 54950.0, 54952.0, 54953.0, 54954.0, 54953.6, 54956.0, 54958.4, 54960.0, 54961.0, 54962.0, 54964.0, 54966.96, 54967.0, 54968.0, 54966.0, 54970.0, 54971.0, 54972.0, 54972.6, 54974.0, 54975.0, 54974.4, 54976.0, 54978.0, 54973.0, 54980.0, 54981.12, 54981.0, 54983.0, 54984.0, 54985.0, 54987.0, 54988.0, 54988.84, 54990.0, 54989.0, 54992.0, 54995.0, 54996.0, 54997.0, 54995.2, 54998.0, 55000.99, 55000.08, 55001.0, 54999.88, 55004.0, 55000.0, 55006.44, 55007.0, 54999.54, 55000.04, 55005.0, 55006.0, 55008.0, 55013.0, 55009.56, 55009.0, 55011.0, 55010.0, 55018.0, 55010.6, 55015.0, 55016.0, 55014.0, 55017.0, 55020.0, 55025.0, 7657196.0, 55022.0, 55023.0, 55024.0, 55026.0, 55027.2, 55027.0, 55029.0, 55030.0, 55032.0, 55033.31, 55034.0, 55035.0, 55036.0, 55033.94, 55033.0, 55039.0, 55040.0, 55041.0, 55042.0, 55044.0, 55047.0, 55045.0, 55046.0, 55048.0, 55049.0, 55050.0, 55047.48, 55052.0, 55054.0, 55055.0, 55056.0, 55057.0, 55058.0, 55059.0, 55060.8, 55060.0, 55062.0, 55059.49, 55064.0, 55065.0, 55061.0, 55067.0, 55068.0, 55069.8, 55070.0, 55069.0, 55071.15, 55074.0, 55075.0, 55076.0, 55078.4, 55078.0, 55080.0, 55081.0, 55082.0, 55084.0, 55088.0, 55089.0, 55090.0, 55091.76, 55092.0, 55093.0, 55094.0, 55095.0, 55096.0, 55096.43, 55093.44, 55098.0, 55099.0, 55100.0, 55097.04, 55102.0, 55102.32, 55104.0, 55105.0, 55106.0, 55107.0, 55108.0, 55109.0, 55110.0, 55111.0, 55115.0, 55116.0, 55117.0, 55118.72, 55119.34, 55118.0, 55121.0, 55122.83, 55120.0, 55124.94, 55119.0, 55123.0, 55122.0, 55128.0, 55125.0, 55130.0, 55126.0, 55127.0, 55129.0, 55134.5, 55131.02, 55133.52, 55135.0, 55136.0, 55132.0, 55140.0, 55133.0, 55142.74, 55143.0, 55139.88, 55138.0, 55141.01, 55142.0, 55148.0, 55149.24, 55144.0, 55145.0, 55146.0, 55147.0, 55150.0, 55150.4, 55152.0, 55151.46, 55154.0, 55155.0, 55156.0, 55157.0, 55158.0, 55159.0, 55160.04, 55161.0, 55160.0, 55162.0, 55164.0, 55165.0, 55166.0, 55167.0, 55169.0, 55170.0, 55172.0, 55173.0, 55175.0, 55176.0, 55177.0, 55171.0, 55180.0, 55181.0, 55182.0, 55182.4, 55184.0, 55185.0, 55188.0, 55190.0, 55192.0, 55193.0, 55194.0, 55194.12, 55196.0, 55197.0, 55198.0, 55199.0, 55200.0, 55201.0, 55201.8, 55203.0, 55204.0, 55205.0, 55206.0, 55207.88, 55208.0, 55209.0, 55210.0, 55211.0, 55212.0, 55213.0, 55214.0, 55215.0, 55216.0, 55217.0, 55218.0, 55219.0, 55220.0, 55221.0, 55220.65, 55223.0, 55223.4, 55225.0, 55224.0, 55224.55, 55225.51, 55229.0, 55227.0, 55231.41, 55228.0, 55230.0, 55231.8, 55232.0, 55231.55, 55233.0, 55235.0, 55236.0, 55238.0, 55241.0, 55242.0, 55239.0, 55240.0, 55238.4, 55244.0, 55245.0, 55246.0, 55244.8, 55248.0, 55249.0, 55250.0, 55253.7, 55251.6, 55251.0, 55256.04, 55256.0, 55254.0, 55255.0, 55253.0, 55261.09, 55257.0, 55258.78, 55259.0, 55260.0, 55261.0, 55267.56, 55267.2, 55269.04, 55262.4, 55264.0, 55265.0, 55273.08, 55274.08, 55267.0, 55269.12, 55269.0, 55270.0, 55272.0, 55273.0, 55274.7, 55275.0, 55276.8, 55276.0, 55280.0, 55286.82, 55287.0, 55285.0, 55284.0, 55285.44, 55288.0, 55289.0, 55290.0, 55286.0, 55289.88, 55294.0, 55295.0, 55298.0, 55296.0, 55297.0, 55298.4, 55296.28, 55300.0, 55301.0, 55302.0, 55303.0, 55304.0, 55305.0, 55307.0, 55308.0, 55310.0, 55309.0, 55312.56, 55314.0, 55315.0, 55316.04, 55309.13, 55314.85, 55315.2, 55315.36, 55317.0, 55318.0, 55323.79, 55324.0, 55319.0, 55320.0, 55321.0, 55328.52, 55323.0, 55325.0, 55322.0, 55327.0, 55333.29, 55328.0, 55330.0, 55331.0, 55332.0, 55333.0, 55334.0, 55335.0, 55336.0, 55337.0, 317480.0, 55338.0, 55340.0, 55341.0, 55342.0, 55344.0, 55345.0, 55345.42, 55348.0, 55350.0, 55350.54, 55352.0, 55353.0, 55351.0, 55355.0, 55356.0, 55357.0, 55358.0, 55360.0, 55362.6, 55361.0, 55362.0, 55364.0, 55365.0, 55367.0, 55368.0, 55369.6, 55369.22, 55370.04, 55370.0, 55372.8, 55373.0, 55375.0, 55369.0, 55371.0, 55378.0, 55374.89, 55379.0, 55380.0, 55376.0, 55383.45, 55382.0, 55383.0, 55384.68, 55385.0, 55386.4, 55389.0, 55386.0, 55391.0, 55387.0, 55388.0, 55394.13, 55395.26, 55390.4, 55390.0, 55392.0, 55393.0, 55396.32, 55397.0, 55398.0, 55394.0, 55400.0, 55401.0, 55402.0, 55404.0, 55403.0, 55405.0, 55407.0, 55408.0, 55410.0, 55410.96, 55412.0, 55411.0, 55414.0, 55415.0, 55416.0, 55411.56, 55418.0, 55420.0, 55421.0, 55422.0, 55423.0, 55425.0, 55424.0, 55427.09, 1104000.0, 55426.8, 55427.0, 55428.0, 55430.0, 55431.19, 55432.0, 55431.41, 55434.0, 55435.0, 55436.0, 55432.32, 55438.0, 55440.0, 55442.28, 55434.6, 55444.0, 55445.0, 55446.83, 55446.0, 55448.0, 55449.0, 55450.0, 55450.8, 55452.8, 55452.0, 55454.0, 55455.0, 55456.0, 55449.73, 55458.0, 55459.0, 55460.0, 55461.0, 55463.0, 55464.0, 55465.0, 55466.0, 55467.0, 55468.8, 55469.0, 55470.0, 55468.0, 55472.0, 55473.0, 55474.0, 55475.0, 55476.0, 55478.89, 55479.0, 55480.0, 55478.24, 55482.0, 317625.0, 55484.0, 55485.0, 55478.0, 55486.0, 55488.0, 55489.0, 55490.0, 55491.0, 55488.24, 55493.0, 55494.0, 55495.0, 55496.0, 55497.78, 55497.0, 55499.0, 55498.0, 55499.16, 55500.0, 55494.4, 55501.43, 55501.0, 55504.0, 55507.0, 55505.0, 55506.24, 55506.82, 55508.0, 55503.0, 55510.0, 55511.0, 55512.0, 55513.0, 55514.0, 55515.0, 55516.0, 55517.0, 55518.0, 55518.81, 55519.0, 55520.0, 55522.68, 55523.13, 55524.0, 55523.64, 55525.0, 55523.24, 55530.0, 55529.0, 55526.0, 55525.6, 55535.78, 55535.0, 55530.72, 55536.0, 55537.0, 55538.0, 55539.0, 55542.34, 55542.0, 55540.0, 55541.8, 55541.23, 55543.0, 55544.0, 55545.0, 55546.0, 55547.0, 55548.0, 55548.63, 55550.0, 55549.0, 55552.0, 55554.0, 55555.0, 55556.8, 55560.7, 55557.0, 55558.1, 55558.0, 55560.0, 55561.0, 55557.69, 55564.0, 55566.0, 55567.0, 55568.0, 842000.0, 55565.0, 55571.0, 55572.0, 55572.56, 55572.08, 55575.0, 55576.0, 55577.0, 55574.0, 55578.0, 55580.0, 55577.16, 55584.0, 55585.0, 55586.45, 55586.0, 55588.44, 55589.0, 55590.0, 55588.0, 55592.0, 55593.0, 55591.0, 55595.0, 55596.0, 55598.0, 55598.4, 55600.0, 55602.0, 55604.0, 55605.0, 55606.8, 55606.0, 55608.0, 55609.0, 55610.0, 55604.94, 55612.0, 55611.0, 55614.0, 55615.0, 55616.0, 55617.0, 55619.0, 55620.0, 55621.0, 55622.0, 55623.0, 55622.24, 55625.0, 55626.92, 55627.0, 55624.0, 55629.0, 55630.0, 55631.0, 55632.0, 55633.0, 55634.0, 55635.12, 55635.0, 55637.0, 55638.0, 55632.68, 55640.62, 55640.0, 55642.08, 55643.44, 55641.0, 55643.0, 55646.94, 55644.0, 55645.0, 55644.16, 55650.66, 55646.0, 55648.1, 55647.36, 55650.0, 55655.0, 55650.12, 55652.0, 55658.0, 55659.12, 55653.0, 55654.0, 55656.0, 55657.0, 55656.9, 55659.92, 55660.0, 55661.0, 55662.0, 55663.2, 55670.15, 55664.0, 55665.0, 55666.0, 55667.0, 55675.0, 55676.8, 55668.0, 55669.0, 55670.0, 55672.0, 55673.0, 55674.0, 55676.0, 55678.0, 55680.0, 55681.0, 55683.0, 55682.0, 55685.48, 55686.0, 55687.0, 55688.0, 55684.0, 55690.56, 55691.4, 55692.0, 55697.2, 55693.0, 55699.8, 55694.0, 55697.0, 55698.0, 55699.0, 55700.0, 55699.76, 55702.0, 55701.0, 55704.71, 55705.0, 55706.0, 55702.4, 55704.0, 55713.0, 55710.0, 55711.0, 580000.0, 55712.0, 55714.0, 55715.0, 55716.0, 55717.22, 55717.0, 55718.0, 55720.0, 55721.0, 55722.0, 55723.0, 55724.0, 55721.64, 55728.0, 55730.0, 55729.0, 55729.18, 55730.78, 55734.0, 55733.0, 55736.0, 55737.0, 55738.0, 55739.0, 55740.0, 55742.0, 55743.96, 55744.0, 55743.0, 55746.0, 55741.0, 55748.0, 55750.0, 55752.0, 55753.0, 55754.0, 55755.0, 317900.0, 55758.0, 55759.0, 55760.0, 55761.0, 55761.45, 55763.0, 55762.0, 55764.0, 55766.95, 55764.6, 55768.08, 55763.86, 55766.15, 55768.0, 55768.21, 55770.0, 55771.0, 55774.0, 55775.0, 55776.0, 55778.64, 55777.0, 55774.32, 55779.0, 55782.96, 55780.0, 55781.76, 55778.28, 55783.0, 55784.0, 55785.0, 55785.88, 55782.0, 55789.0, 55790.0, 55791.0, 55792.0, 55794.0, 55788.0, 55796.0, 55797.0, 55798.0, 55799.04, 55800.0, 55801.0, 55802.0, 55804.9, 55802.16, 55806.0, 55803.0, 55804.0, 55806.4, 55807.0, 55808.22, 55809.0, 55810.0, 55808.0, 55812.0, 55814.0, 55815.0, 55816.0, 55817.64, 55818.0, 55817.0, 55820.0, 55822.0, 55823.16, 55824.0, 55825.0, 55826.0, 55827.0, 55827.2, 55830.0, 8706582.0, 55831.0, 55832.0, 55834.0, 55834.32, 55833.0, 55833.6, 55832.11, 55836.0, 55838.0, 55839.0, 55840.0, 55842.0, 55843.0, 55844.0, 55845.0, 55844.96, 55846.0, 55848.0, 55848.52, 55850.0, 55851.0, 55852.0, 55854.0, 55854.24, 55855.0, 55857.0, 318000.0, 55856.0, 55858.0, 55859.0, 55860.0, 55861.0, 55862.0, 55863.0, 55864.0, 55867.0, 55865.0, 55866.0, 55862.4, 55868.0, 55868.95, 55870.0, 55871.0, 55872.0, 55873.16, 55873.0, 55874.0, 55876.0, 55877.0, 55878.0, 55879.25, 55875.0, 55880.0, 55882.0, 55883.0, 55884.0, 55884.09, 55886.0, 55890.0, 55885.0, 55887.0, 55889.0, 55889.08, 55890.33, 55891.18, 55893.36, 55894.0, 55892.0, 55896.0, 55897.92, 55897.0, 55897.49, 55900.0, 55901.0, 55903.0, 55902.0, 55907.0, 55909.0, 55910.0, 55911.0, 55912.0, 55913.0, 55908.0, 55915.0, 55916.0, 55917.0, 55920.0, 55921.0, 55922.0, 55925.9, 55925.0, 55926.0, 55927.0, 55928.0, 55930.0, 55931.0, 55929.0, 55933.0, 55932.0, 55934.0, 55935.0, 55936.0, 55937.0, 55938.0, 55939.0, 55940.0, 55941.71, 55942.0, 55941.0, 55944.0, 55945.0, 55946.0, 55948.0, 55949.0, 55950.0, 55951.65, 55952.0, 55951.0, 55954.0, 55956.0, 55956.84, 55958.0, 55958.21, 55960.0, 55962.0, 55963.0, 55962.62, 55964.0, 55965.0, 55966.0, 55968.59, 55967.0, 55968.0, 55969.0, 55970.0, 55971.0, 55972.0, 55973.0, 55972.8, 55974.0, 55975.0, 55976.0, 55978.0, 55979.0, 55980.0, 55983.0, 55981.0, 55982.76, 55982.0, 55984.0, 55983.36, 55986.0, 55985.0, 55988.0, 55989.0, 55993.6, 55994.0, 55990.0, 55991.0, 55992.0, 55992.58, 55994.85, 55993.0, 55996.0, 55995.0, 56003.0, 55998.0, 55999.0, 56000.0, 56007.0, 56001.0, 56004.0, 56005.0, 56006.0, 56008.0, 56010.24, 56011.0, 56012.0, 56011.09, 56014.0, 56015.73, 56013.0, 56014.32, 56015.0, 56019.0, 56016.0, 56022.0, 56025.84, 56023.0, 56024.0, 56025.0, 56026.0, 56027.0, 56028.0, 56029.0, 56033.0, 56030.0, 56031.82, 56032.0, 56037.12, 56031.0, 56039.18, 56034.0, 56035.0, 56042.76, 56036.0, 56035.2, 56045.0, 56038.0, 56040.0, 56039.0, 56042.0, 56050.8, 56044.0, 56046.78, 56046.0, 56048.0, 56050.0, 56047.0, 56052.0, 56053.0, 56051.0, 318204.0, 56055.32, 56056.0, 56055.0, 56059.0, 56060.0, 56061.0, 56062.0, 56063.0, 56064.0, 56065.0, 56066.0, 56063.04, 56068.73, 56068.74, 56069.0, 56068.0, 56074.0, 56076.0, 56079.0, 56075.0, 56076.8, 56078.0, 56080.0, 56076.13, 56082.0, 56083.0, 56084.0, 56085.36, 56086.0, 56087.0, 56088.0, 56090.0, 56091.0, 56092.0, 56093.0, 56093.76, 318239.0, 56096.0, 56099.0, 56097.0, 56101.8, 56098.0, 56100.0, 56101.0, 56102.0, 56103.0, 56105.0, 56106.0, 56107.0, 56108.0, 56104.0, 56110.18, 56110.0, 56112.0, 56113.27, 56116.0, 56116.8, 56118.0, 56116.48, 56120.0, 56118.4, 56121.24, 56117.0, 56124.0, 56125.0, 56126.0, 56127.0, 56128.0, 56129.0, 56130.0, 56132.0, 56133.0, 56133.24, 56135.0, 56136.0, 56137.0, 63261.6, 56139.0, 56140.0, 56139.28, 56142.0, 56139.2, 56145.0, 56146.0, 56148.0, 56150.0, 56151.0, 56154.0, 56155.0, 56155.56, 56157.0, 56158.66, 56156.0, 56160.0, 56161.0, 56161.52, 56162.0, 56164.0, 56165.0, 56166.0, 56158.0, 56168.0, 56162.16, 56170.0, 56163.0, 56172.0, 56169.0, 56174.76, 56175.96, 56175.0, 56177.0, 56178.0, 56176.0, 56180.0, 56181.0, 56182.0, 56183.88, 56184.0, 56185.0, 56186.4, 56187.0, 56186.0, 56187.05, 56183.0, 56190.0, 56187.61, 56192.0, 56193.0, 56187.6, 56191.0, 56196.0, 56189.0, 56198.0, 56199.0, 56200.0, 56201.0, 56199.6, 56204.0, 56203.0, 56202.0, 56205.0, 56207.19, 56208.0, 56207.0, 56210.0, 56211.0, 56212.0, 56214.0, 56215.0, 56217.0, 56218.0, 56220.0, 56221.0, 56222.0, 56223.2, 56224.0, 56225.0, 56226.21, 56227.0, 56228.0, 56229.0, 56230.0, 56223.0, 56232.88, 56232.0, 56233.0, 56234.0, 56235.0, 56237.74, 56236.09, 56236.0, 56238.0, 56241.19, 318383.0, 56240.0, 56241.0, 56245.0, 56246.22, 318391.0, 56242.0, 56243.0, 56244.0, 56243.89, 56246.0, 56253.97, 56253.0, 56247.0, 56250.0, 56257.0, 56249.0, 56250.94, 56254.8, 56261.92, 56255.0, 56256.0, 56258.0, 56259.0, 56260.0, 56261.0, 56259.84, 56263.0, 56264.0, 56265.0, 56266.0, 56267.0, 56268.0, 56269.0, 56270.0, 56272.0, 56273.0, 56275.0, 56276.0, 56280.0, 56281.0, 56282.0, 56283.0, 56284.0, 56285.0, 56286.0, 318427.9, 56289.0, 56290.0, 56291.0, 56292.0, 56293.0, 56294.0, 56291.4, 56290.14, 56297.0, 56299.0, 56300.0, 56301.0, 56302.0, 56304.0, 56305.6, 56306.0, 56307.0, 56305.0, 56305.56, 56308.0, 56309.0, 56310.0, 56309.29, 56312.0, 56311.0, 56313.0, 56315.0, 56316.0, 56319.0, 56320.44, 56320.0, 56321.0, 56323.0, 56323.67, 56324.0, 56325.0, 56326.0, 56326.8, 56328.0, 56329.0, 56331.24, 56331.0, 56330.0, 56332.0, 56333.0, 56334.0, 56335.0, 56333.36, 56337.0, 56336.0, 56339.0, 56340.0, 56341.92, 56341.0, 56336.88, 56344.0, 56345.0, 56346.0, 56347.0, 56347.2, 56343.71, 56350.0, 56352.0, 56353.0, 56354.0, 56356.0, 56357.0, 56360.0, 56362.0, 56364.0, 56365.0, 56366.0, 56368.0, 56369.82, 56370.0, 56372.16, 56372.0, 56374.8, 56375.0, 56376.0, 56375.8, 56377.0, 56373.0, 56380.0, 56383.0, 56385.0, 56387.0, 56388.0, 56389.35, 56388.8, 56390.0, 56392.33, 56391.0, 56392.0, 56391.12, 56390.64, 56395.0, 56396.0, 56399.53, 56400.24, 56397.0, 56399.64, 56400.0, 56401.0, 56402.0, 56403.0, 56407.0, 56404.32, 56409.0, 56405.0, 56406.0, 56408.0, 56413.0, 56408.15, 56410.0, 56411.0, 56417.4, 56412.0, 318558.0, 56415.0, 56417.0, 56418.0, 56419.68, 56420.0, 56421.0, 56419.0, 56423.0, 56424.0, 56424.24, 56427.0, 56429.0, 56430.0, 56431.0, 56434.56, 56432.62, 56433.0, 56434.0, 56435.29, 56439.0, 56435.0, 56437.0, 56436.0, 56440.0, 56441.15, 56441.0, 56443.2, 56447.0, 56442.0, 56446.0, 56444.0, 56448.0, 56449.0, 56450.0, 56447.86, 56452.0, 56453.0, 56451.84, 56455.0, 56456.0, 56457.0, 56458.0, 56460.0, 56463.0, 56461.0, 56462.0, 56463.6, 56465.16, 56466.0, 56469.0, 56465.0, 56462.08, 56470.0, 56471.0, 56472.0, 56469.37, 56473.0, 56475.0, 56477.0, 56479.0, 56480.0, 56480.87, 56481.0, 56483.16, 56484.0, 56485.0, 56479.92, 56478.48, 56487.0, 56488.0, 56489.86, 56490.0, 56492.8, 56493.0, 56491.0, 56492.4, 56492.0, 56494.0, 56495.48, 56495.0, 56498.0, 56492.3, 56500.0, 56496.0, 56502.0, 56504.0, 56505.96, 56508.12, 56508.0, 56510.52, 56510.0, 56512.0, 56513.0, 56514.36, 56514.0, 56516.0, 56509.0, 56517.0, 56511.0, 56519.0, 56520.0, 56522.58, 56515.0, 56524.0, 56525.9, 56525.0, 56526.6, 56527.0, 56528.0, 56529.0, 56531.0, 56532.0, 56533.0, 56526.75, 56535.03, 56534.4, 56535.0, 56538.0, 56534.0, 56537.0, 56540.0, 56541.18, 56541.52, 56543.0, 56544.0, 56545.0, 56547.4, 56548.0, 56540.64, 56550.0, 56551.68, 56552.0, 56551.0, 56554.0, 318699.0, 56555.0, 56557.0, 56556.0, 56559.55, 56555.28, 56555.2, 56559.0, 56560.0, 56561.0, 56562.0, 56563.0, 56562.96, 56565.0, 56566.0, 56567.0, 56568.0, 56564.0, 56570.0, 56571.0, 56569.0, 56574.0, 56575.92, 56576.0, 318720.0, 56578.0, 56579.0, 56580.0, 56581.0, 56584.0, 56582.0, 56583.0, 56583.34, 56585.28, 56586.0, 56587.68, 56588.0, 56589.0, 56585.0, 56592.0, 56590.0, 56595.0, 56597.0, 56598.0, 56599.0, 56600.0, 56603.0, 56604.0, 56605.0, 56606.52, 56607.2, 56605.28, 56607.0, 56608.0, 56610.0, 56611.0, 56611.19, 56613.0, 56614.0, 56613.46, 56615.0, 56617.0, 56618.0, 56616.0, 56612.0, 56619.0, 56622.96, 56624.0, 56625.0, 56626.0, 56623.0, 56627.28, 56628.0, 56629.0, 56630.0, 56631.0, 56633.0, 56632.0, 56633.88, 56634.0, 56635.0, 56636.0, 56639.4, 56638.0, 56639.0, 56640.0, 56643.36, 56642.0, 56643.12, 56637.0, 56645.0, 56646.0, 56646.8, 56650.0, 56651.0, 56652.0, 56650.04, 56654.0, 56655.0, 56656.6, 56656.0, 56657.0, 56658.0, 56659.0, 56658.66, 56661.48, 56662.0, 56660.0, 56664.0, 56665.0, 56666.0, 56667.0, 56668.0, 56667.03, 56670.0, 56669.0, 56673.0, 56674.0, 56675.0, 56676.0, 56677.2, 56677.0, 56678.4, 56680.0, 56682.0, 56683.0, 56684.0, 56685.0, 56686.0, 56688.0, 56689.0, 56690.0, 56693.4, 56693.0, 56693.19, 56696.0, 56697.24, 56698.0, 56697.0, 56700.0, 56701.0, 56701.8, 56703.0, 56704.8, 56704.0, 56705.0, 56707.0, 56706.0, 56708.0, 56710.0, 56711.0, 581000.0, 56713.0, 56713.54, 56714.0, 56712.0, 56715.0, 56718.0, 56718.89, 56720.67, 56720.0, 56722.0, 56721.6, 56721.0, 56723.0, 56724.0, 56719.0, 56725.0, 56726.0, 56728.0, 56730.0, 56731.0, 56732.0, 56732.71, 56730.6, 56735.0, 56736.0, 56733.48, 56738.0, 56739.0, 56740.0, 56741.0, 56743.2, 56742.0, 56743.0, 56744.64, 56744.0, 56745.0, 56749.0, 56746.0, 56748.0, 56748.64, 56750.0, 56751.0, 56752.0, 56753.0, 56754.0, 56755.0, 56756.0, 56757.0, 56758.0, 56762.0, 56759.0, 56760.0, 56761.0, 56763.0, 56765.04, 56766.0, 56767.0, 56768.0, 56764.0, 318909.0, 56768.63, 56772.0, 56773.6, 56774.0, 56775.0, 56771.0, 56779.0, 56780.0, 56781.0, 56776.0, 56783.0, 56783.9, 56785.6, 56785.0, 56784.0, 56787.0, 56788.0, 56784.24, 56791.0, 56787.31, 56793.0, 56790.0, 56795.0, 56792.0, 56789.0, 56798.0, 56799.0, 56800.0, 56801.0, 56796.0, 56803.0, 56804.0, 56805.0, 56806.0, 56807.0, 56808.0, 56804.8, 56810.0, 56811.0, 56812.0, 56813.0, 56813.28, 56816.0, 56817.0, 56818.0, 56819.0, 56820.92, 56820.0, 56821.0, 56821.76, 56822.0, 56824.0, 56825.0, 56824.95, 56827.0, 56829.0, 56830.0, 56826.0, 56832.0, 56833.0, 56834.8, 56835.6, 56835.0, 56836.0, 56837.95, 56838.0, 56839.0, 56840.0, 56840.4, 56841.0, 56844.0, 56844.32, 56846.0, 56843.0, 56848.0, 56846.4, 56850.0, 56851.0, 56852.0, 56847.0, 56854.0, 56855.0, 319000.0, 56856.0, 56858.0, 56859.0, 56857.0, 56860.0, 56860.74, 56862.0, 56863.0, 56864.0, 56866.0, 56866.74, 56868.0, 56867.0, 56870.0, 56871.0, 56865.0, 56873.28, 56874.0, 56875.0, 581159.0, 56877.0, 56876.0, 56879.0, 56880.0, 56881.0, 56882.0, 56883.0, 56881.38, 56886.0, 56887.0, 56888.0, 56888.53, 56890.0, 56891.0, 56892.0, 56893.53, 56893.94, 56893.0, 56895.0, 56896.0, 56896.82, 56898.62, 56898.0, 56900.0, 56899.0, 56901.52, 56902.0, 56904.0, 56903.0, 56901.0, 56907.0, 56908.0, 56910.0, 56912.0, 56913.0, 56914.8, 56914.08, 56916.0, 56915.0, 56918.0, 56914.0, 56920.0, 56923.0, 56924.22, 56924.0, 56925.0, 56926.0, 56923.2, 56929.0, 56930.0, 56928.0, 56930.53, 56925.96, 56933.0, 56935.0, 56936.0, 56937.45, 56940.0, 56942.0, 56943.0, 56944.0, 56946.0, 56948.0, 56950.0, 56952.0, 56955.0, 56956.32, 56957.0, 56958.0, 56958.42, 56960.0, 56961.0, 56962.0, 56964.0, 56965.0, 56967.0, 56968.0, 56969.0, 56970.0, 56971.0, 56972.0, 56971.98, 56974.0, 56976.0, 56977.0, 56979.0, 56980.0, 56982.0, 56983.0, 56984.0, 56985.0, 56986.0, 56987.0, 56988.8, 56989.44, 56989.0, 56992.0, 56993.0, 56995.0, 319140.0, 56997.36, 56998.0, 56997.0, 56999.91, 57000.0, 57001.0, 56999.0, 57003.0, 56996.0, 57005.03, 57006.0, 57006.5, 57008.0, 57007.0, 57010.0, 57011.0, 57012.0, 57013.0, 57014.0, 57012.8, 57016.18, 57016.44, 57018.0, 57019.1, 57020.0, 57015.0, 57016.0, 57023.0, 57024.0, 57026.0, 57028.0, 57028.23, 57030.0, 57031.0, 57032.0, 57033.0, 57034.0, 57035.32, 57036.0, 57037.0, 57038.0, 57039.0, 57040.5, 57040.0, 57042.36, 57043.14, 57044.0, 57045.0, 57046.0, 57042.0, 57048.0, 57047.0, 57050.0, 57043.0, 57049.0, 57054.0, 57055.0, 57056.62, 57057.0, 57060.0, 57061.0, 57065.0, 57067.0, 57068.0, 63446.4, 57070.0, 57071.0, 57072.0, 57073.0, 57072.06, 57074.0, 57075.5, 57075.0, 57077.0, 57076.0, 57078.0, 57081.0, 57079.0, 57083.0, 57080.0, 57082.0, 57080.2, 57084.16, 57085.0, 57086.0, 57084.0, 57089.0, 57090.0, 57088.0, 57092.0, 57094.0, 57095.0, 57096.0, 57099.0, 57100.0, 57101.0, 57100.86, 57103.0, 57104.0, 57105.0, 57106.0, 57107.0, 57108.0, 57109.0, 57110.0, 57108.4, 57112.0, 57113.47, 57106.94, 319259.0, 57115.0, 57116.0, 57117.0, 57118.0, 57120.0, 5300000.0, 57120.95, 57116.8, 57124.0, 57125.0, 57126.0, 57127.0, 57128.52, 57129.0, 57128.0, 57130.44, 57130.0, 57132.0, 57134.52, 57131.0, 57136.0, 57134.0, 57131.04, 57135.0, 57137.0, 57137.6, 57139.94, 57140.0, 57141.0, 57140.04, 57143.0, 57147.0, 57144.0, 57145.0, 57146.23, 57151.0, 57148.0, 57149.0, 57150.0, 57151.98, 57151.08, 57157.0, 57153.0, 57159.0, 57154.0, 57155.0, 57156.0, 57157.5, 57158.0, 57158.4, 57160.0, 57161.44, 57162.0, 57163.08, 57164.0, 57163.0, 57164.12, 57167.0, 57168.0, 57169.0, 57170.0, 57172.0, 57176.26, 57177.0, 57176.0, 57179.0, 57180.0, 57178.0, 57183.32, 57184.0, 57183.0, 57183.14, 57182.0, 57188.0, 57189.0, 57191.0, 57190.8, 57190.0, 57192.0, 57193.0, 57195.0, 57196.0, 57194.32, 57198.0, 57199.76, 57200.0, 57200.16, 57200.21, 57204.0, 57205.5, 57205.0, 57206.0, 57207.0, 57208.0, 57209.0, 57209.04, 57211.0, 57212.0, 57213.0, 57215.0, 57214.0, 57216.0, 57217.0, 57219.0, 57218.0, 57219.63, 57220.0, 57221.0, 57222.0, 57223.0, 57223.71, 57225.0, 57226.0, 57220.8, 57228.0, 57224.0, 319374.0, 57233.0, 57230.0, 57229.0, 57231.0, 57236.0, 57234.0, 57238.0, 57232.0, 57240.0, 57241.0, 57242.0, 57240.6, 57244.59, 57246.0, 57245.0, 57247.0, 57248.0, 57249.0, 57251.57, 57250.0, 57251.0, 57252.0, 57254.0, 57255.0, 57256.0, 57258.0, 57259.0, 57257.29, 57257.96, 57253.0, 57260.0, 57262.0, 57263.0, 57264.0, 57265.0, 57266.0, 57264.84, 57270.0, 57271.0, 57272.0, 57270.45, 57276.0, 57280.0, 1368000.0, 57283.0, 57284.0, 57283.2, 57286.8, 57287.88, 57288.0, 57286.0, 57290.0, 57291.0, 57285.0, 57293.0, 57294.0, 57292.0, 57296.4, 57297.0, 57298.0, 57300.0, 57302.0, 57303.0, 57304.0, 57306.0, 57307.0, 57308.48, 57308.0, 57309.0, 57310.0, 57307.98, 57312.0, 57313.0, 57315.0, 57316.65, 57317.0, 57319.0, 57320.0, 57322.0, 57323.0, 57324.0, 57325.0, 57324.8, 57326.0, 57328.0, 57330.0, 57331.0, 57332.88, 57333.0, 57334.0, 57335.0, 57336.0, 57337.28, 57339.0, 57340.0, 57342.71, 57343.0, 57344.0, 57345.0, 57346.0, 57347.0, 57348.0, 57348.76, 57349.0, 57350.0, 57351.0, 57352.0, 57353.0, 57354.0, 57354.7, 57356.0, 57358.0, 57359.0, 57355.0, 57357.48, 57357.0, 57360.0, 57361.0, 57362.0, 57363.0, 57364.0, 57365.4, 57366.0, 57365.0, 57368.0, 57372.0, 57373.0, 57374.0, 57375.0, 57376.02, 57377.0, 57378.0, 57380.0, 57382.0, 57383.0, 57384.0, 57385.0, 57386.0, 57387.2, 57387.0, 57388.0, 57384.08, 57390.0, 57391.0, 57392.4, 57392.0, 57394.0, 57395.0, 57396.0, 57397.0, 57398.0, 57399.0, 57400.0, 57401.59, 57402.0, 57403.0, 57404.0, 57405.0, 57406.0, 57406.91, 57408.0, 57408.79, 57410.0, 57405.84, 57412.0, 57415.0, 57416.0, 57418.0, 57420.0, 57421.0, 57422.0, 57423.0, 57425.0, 57427.0, 57428.8, 57428.0, 57430.0, 57431.0, 57429.0, 57433.0, 57434.0, 57432.0, 57436.0, 57437.0, 57438.0, 57435.0, 57440.0, 57444.0, 57445.0, 57448.0, 57450.0, 57451.0, 57452.0, 57455.0, 57456.0, 57460.0, 57462.96, 57463.0, 57464.0, 57465.0, 57468.0, 57469.92, 57470.4, 57470.0, 57469.0, 57472.0, 57474.0, 57475.0, 57476.0, 57473.0, 57478.0, 57470.32, 57480.0, 57481.0, 57482.0, 57484.0, 57485.0, 57486.0, 57488.24, 57488.0, 57490.0, 57491.0, 57492.0, 57493.0, 57494.0, 57489.0, 57496.0, 57497.0, 57498.0, 57498.73, 57499.0, 57500.0, 57501.0, 57501.89, 57503.0, 57504.0, 57502.0, 57506.0, 57508.0, 57505.08, 57510.0, 57511.91, 57512.0, 57507.0, 57505.34, 57515.14, 57515.12, 57516.0, 57517.0, 57519.0, 57518.0, 57521.0, 57522.51, 57515.0, 319668.0, 57520.0, 57522.0, 57522.63, 57524.0, 57522.72, 57523.0, 57525.0, 57528.0, 57529.0, 57530.0, 57531.0, 57532.0, 57533.0, 57534.0, 57536.76, 57533.04, 57541.8, 57535.0, 57539.0, 57540.0, 57541.0, 57542.0, 57543.84, 57544.0, 57545.0, 57546.0, 57543.0, 57548.0, 57549.0, 57550.0, 57550.1, 57552.0, 57553.0, 57555.0, 57556.0, 57557.0, 57559.0, 57560.0, 57562.82, 57563.0, 57564.0, 57566.0, 57567.0, 57565.0, 57565.25, 57570.12, 57571.0, 57566.02, 57570.0, 57571.56, 57572.0, 57569.0, 57574.0, 57578.0, 57575.0, 57576.0, 57577.0, 57576.72, 57579.0, 57580.0, 57581.0, 57583.0, 57584.0, 57588.0, 57585.43, 57586.0, 57587.0, 57588.76, 57589.0, 57590.0, 57595.92, 57591.0, 57592.88, 57593.0, 57594.0, 57595.0, 57592.0, 57598.38, 57600.0, 57601.0, 57602.0, 57603.23, 57604.0, 57608.0, 57610.0, 57612.0, 57612.19, 57614.0, 63554.14, 57616.0, 57617.0, 57618.0, 57616.52, 57620.0, 57621.0, 57622.0, 57623.9, 57624.0, 57623.54, 57626.0, 57625.0, 57628.0, 57630.0, 57631.0, 57632.0, 57633.0, 57634.0, 57635.0, 57636.0, 57637.0, 57638.0, 57635.76, 57640.0, 57639.0, 57642.0, 57643.0, 57644.0, 57646.0, 57647.13, 57648.0, 57648.24, 57650.0, 57651.0, 57652.0, 57653.0, 57654.0, 57655.0, 57656.0, 57656.04, 57657.0, 57659.0, 57660.0, 57661.0, 57662.0, 57663.0, 57664.0, 57665.0, 57666.0, 57667.0, 57668.0, 57669.0, 2416960.0, 57671.0, 57672.7, 57672.0, 57673.0, 57674.61, 57676.0, 57675.0, 57678.36, 57679.0, 57677.0, 57681.0, 57678.0, 57680.0, 57681.84, 57682.0, 57683.0, 57684.0, 57685.0, 57686.0, 57682.04, 57688.0, 57689.0, 57690.0, 57686.97, 57692.0, 57693.0, 57692.16, 57695.0, 57696.0, 57696.86, 57697.0, 57699.0, 57700.0, 57702.0, 57698.0, 57706.0, 57704.0, 57705.0, 57707.0, 57708.0, 57709.0, 57710.0, 57713.74, 57711.0, 57712.0, 57714.0, 57715.0, 582000.0, 57717.0, 57718.0, 57719.28, 57720.0, 57719.0, 57721.0, 57724.0, 57726.0, 57723.0, 57728.0, 57729.0, 57732.58, 57733.34, 57732.0, 57733.0, 57735.0, 57736.0, 57738.0, 57737.0, 57739.0, 57741.0, 57740.0, 57740.8, 57742.0, 57743.0, 57744.0, 57745.0, 57746.0, 57747.0, 57748.0, 57749.0, 57750.0, 57751.0, 57750.58, 57753.0, 57754.0, 57755.0, 57756.0, 57758.88, 57760.0, 57761.0, 57762.0, 57763.0, 57764.0, 57760.53, 57766.0, 57767.0, 57768.0, 57770.0, 57772.0, 57774.0, 57775.0, 57776.0, 57777.72, 57777.0, 57778.8, 57778.0, 57780.0, 57781.0, 57782.0, 57783.0, 57784.0, 57779.0, 57786.0, 57787.0, 57789.0, 57788.0, 57789.96, 57790.0, 57793.0, 57791.0, 57792.0, 57794.0, 57795.0, 57794.8, 57797.0, 57798.0, 57799.2, 57800.0, 57801.0, 57802.0, 57803.0, 57804.0, 57805.0, 57806.0, 57807.0, 57808.23, 57809.0, 57811.0, 57812.0, 57815.0, 57816.0, 57817.0, 57818.0, 57819.0, 57820.0, 57821.0, 57823.0, 57824.0, 57825.0, 57827.0, 57828.84, 57828.0, 57830.0, 57831.0, 57832.0, 57833.0, 57834.0, 57829.0, 57836.08, 57833.04, 57835.0, 57831.84, 57838.0, 57839.0, 57840.0, 57841.0, 57843.0, 57844.0, 57845.0, 57846.0, 319992.0, 57849.0, 57850.0, 57851.0, 57852.0, 57853.0, 319999.0, 57856.0, 57856.07, 57858.0, 320000.0, 57860.03, 320004.0, 320006.0, 57857.0, 57855.0, 57865.6, 57859.0, 57867.0, 57868.0, 57860.0, 57861.0, 57862.0, 57864.0, 57873.0, 57865.0, 57867.53, 57870.16, 57870.12, 57870.0, 9757200.0, 57875.0, 57876.0, 57878.0, 57880.0, 57881.76, 57882.0, 57883.0, 57885.0, 57886.0, 57887.0, 57888.0, 57889.0, 57890.0, 57892.0, 57891.0, 57894.0, 57895.0, 57896.0, 57897.0, 57898.0, 57899.0, 57900.0, 57901.2, 57903.0, 57905.0, 57906.0, 57907.0, 57908.0, 320049.0, 57910.0, 57912.42, 57912.0, 57914.0, 57915.0, 57916.0, 57913.0, 57917.0, 57918.0, 57920.0, 57921.81, 57920.64, 57924.59, 57924.0, 57926.0, 57925.0, 57928.0, 57930.0, 57931.2, 57932.0, 57933.0, 57934.5, 57931.0, 57934.0, 57935.0, 57936.0, 57937.32, 57938.4, 57939.0, 57940.0, 57941.0, 57942.0, 57943.0, 57944.0, 57948.8, 57948.0, 57949.0, 57949.77, 57950.0, 57953.0, 57955.49, 57955.0, 57957.0, 57956.0, 57960.0, 57961.8, 57963.0, 57964.17, 57966.0, 57968.64, 57969.0, 57968.0, 57971.0, 57972.0, 57973.0, 57970.0, 57970.2, 57976.0, 57977.0, 57978.0, 57979.0, 57980.0, 57981.0, 57982.0, 57975.65, 57984.0, 57985.0, 57987.0, 57988.0, 57988.2, 57990.0, 57991.0, 57990.4, 57992.0, 57989.0, 57995.0, 57996.0, 57997.0, 57998.0, 57999.0, 58000.0, 58000.08, 58003.0, 58004.0, 58006.0, 58008.0, 58009.0, 58010.0, 58011.0, 58012.0, 58011.2, 58014.0, 58015.0, 58017.0, 58018.0, 58019.0, 58019.26, 58020.0, 58021.0, 58022.0, 58024.0, 58025.0, 58026.0, 58025.05, 58028.0, 58029.72, 58030.0, 58031.0, 58032.0, 58033.0, 58037.0, 58038.0, 58039.0, 58040.0, 58041.55, 58042.0, 58041.0, 58043.16, 58044.0, 58045.0, 58046.0, 58047.0, 58048.0, 58049.0, 58050.5, 58050.0, 58052.95, 58047.72, 58054.0, 58052.0, 58056.0, 58057.0, 58058.0, 58060.0, 58061.0, 58062.0, 58060.44, 58064.0, 58065.0, 58064.18, 58066.0, 58068.0, 58066.25, 58070.0, 58072.0, 58073.0, 58074.0, 58075.0, 58073.66, 58077.0, 58076.0, 58079.0, 58080.0, 58081.0, 58083.0, 58084.0, 58085.0, 58084.8, 58087.0, 58088.0, 58084.08, 58084.38, 58090.0, 58091.0, 58092.0, 58094.4, 58095.0, 58095.14, 58097.0, 58098.0, 58099.0, 58100.0, 58101.0, 320245.38, 58103.0, 58104.0, 58102.85, 58102.0, 58106.0, 58107.0, 58108.0, 58109.0, 58110.0, 58111.13, 58112.0, 58112.12, 58111.0, 58115.0, 58116.0, 58116.8, 58118.0, 58119.0, 58120.0, 58121.0, 58122.0, 58124.0, 58125.0, 58125.48, 58128.0, 58129.32, 58130.0, 58129.0, 58132.0, 58133.0, 58131.0, 58135.0, 58136.0, 58135.69, 58135.44, 58139.6, 58140.0, 58141.0, 58142.0, 58137.84, 58144.0, 58137.0, 58146.0, 58147.0, 58148.6, 58149.0, 58150.0, 58149.65, 58152.0, 58153.26, 58154.0, 58155.0, 58151.0, 58156.0, 58157.0, 58158.48, 58158.22, 58160.0, 58158.0, 58162.0, 58163.0, 58164.0, 58166.0, 58167.0, 58168.0, 58170.0, 58171.0, 58172.0, 58173.0, 58174.0, 58176.0, 58177.0, 58178.0, 58179.0, 58180.0, 58181.0, 58177.6, 58183.0, 58184.0, 58178.64, 58186.0, 58186.68, 58188.0, 58189.0, 58190.0, 58192.74, 58193.0, 58195.0, 58196.4, 58197.06, 58198.0, 58199.0, 58200.0, 58201.0, 58202.0, 58196.0, 58204.0, 58197.0, 58206.0, 58208.0, 58210.0, 58212.0, 58213.0, 58214.0, 320357.0, 58216.0, 58217.0, 58218.0, 58219.0, 58220.0, 58219.45, 58222.0, 58223.0, 58224.0, 58225.0, 58226.67, 58227.0, 58228.68, 58228.0, 58230.0, 58231.0, 58232.0, 58233.0, 58235.0, 58236.0, 58237.0, 58239.0, 58240.0, 58239.96, 58242.24, 58243.0, 58242.0, 58243.64, 58245.0, 58247.0, 58248.0, 58244.0, 58250.0, 58246.0, 58252.0, 58253.0, 58254.0, 58255.0, 58256.0, 320400.0, 58258.0, 58260.0, 58261.0, 58262.0, 58263.0, 58264.0, 58265.0, 58266.0, 58267.0, 58270.0, 58271.22, 58272.0, 58271.0, 58274.0, 58275.0, 58276.0, 58278.96, 58279.0, 58280.0, 58281.0, 58282.0, 58283.0, 58284.0, 58285.0, 58286.0, 58287.0, 58288.0, 58289.49, 58292.0, 58293.0, 58294.0, 58295.0, 58296.0, 58297.0, 58298.0, 58299.96, 58300.0, 58301.0, 58302.0, 58303.0, 58300.32, 58305.0, 58299.72, 58308.0, 58309.8, 58310.0, 58311.0, 58312.0, 58313.0, 58314.0, 58315.0, 58316.04, 58317.0, 58318.0, 58318.44, 58320.0, 58321.0, 58322.0, 58323.0, 58323.2, 58325.0, 58326.68, 58327.04, 58326.0, 58329.93, 58324.0, 58328.0, 58330.0, 58331.0, 58334.0, 58332.0, 58327.0, 58329.0, 58336.89, 58336.0, 58338.0, 58338.82, 58340.0, 58342.18, 58343.0, 58344.0, 58345.0, 58347.0, 58348.13, 58345.23, 58343.22, 58342.0, 58349.5, 58350.0, 58349.0, 58355.0, 58349.2, 58352.0, 58358.0, 58354.08, 58356.0, 58354.0, 58353.0, 58363.0, 58360.0, 58361.0, 58362.12, 58362.0, 58364.8, 58365.77, 58364.0, 58365.0, 58366.0, 58369.64, 58370.0, 58375.64, 58368.0, 58372.0, 58373.0, 58379.19, 58374.0, 58375.2, 58382.88, 58376.0, 58375.0, 58380.0, 58381.48, 58378.0, 58383.0, 58384.0, 58385.0, 58386.0, 58387.0, 58388.0, 58390.0, 58388.04, 58392.0, 58393.0, 58394.17, 58395.0, 58396.0, 58397.0, 58402.0, 58399.0, 58400.45, 58400.0, 58401.0, 58404.0, 58406.0, 58407.0, 58408.0, 58406.4, 58410.0, 58412.0, 58414.0, 58415.0, 58416.0, 58418.0, 58419.71, 58420.0, 58420.77, 58422.24, 58423.0, 58422.0, 58421.0, 58425.0, 58426.2, 58428.0, 58427.0, 58430.4, 58427.2, 58427.28, 58431.0, 58426.0, 58434.0, 58435.0, 58435.32, 58437.0, 58438.0, 58433.0, 58440.0, 58442.0, 58443.0, 58444.0, 58445.0, 58446.0, 58447.67, 58448.0, 58449.0, 58450.9, 58451.0, 58451.07, 58450.0, 58452.0, 58451.73, 58454.48, 58455.0, 58456.0, 58457.0, 58458.0, 58459.44, 58462.82, 58460.0, 58459.0, 58462.0, 58463.0, 58464.0, 58465.0, 58466.0, 58468.0, 58469.25, 58470.0, 58467.24, 58469.0, 58473.0, 58474.0, 58475.0, 58476.0, 58478.32, 58479.0, 58480.92, 58480.0, 58481.0, 58482.0, 58483.0, 58485.0, 58486.0, 58487.0, 58489.8, 58488.0, 58489.6, 58492.0, 58490.0, 58494.0, 58491.12, 58492.44, 58493.0, 58489.0, 58495.0, 58496.0, 58495.32, 58502.0, 58500.0, 58501.0, 58503.0, 58504.0, 58505.0, 58506.0, 58507.0, 58508.0, 58509.0, 58512.32, 58510.4, 58514.73, 58510.0, 58511.0, 58512.0, 58515.0, 58516.0, 58517.98, 58518.0, 58518.72, 58520.0, 58521.0, 58517.0, 58524.0, 58523.0, 58526.0, 58527.0, 58528.0, 58530.0, 58531.4, 58533.0, 58532.0, 58531.0, 58533.46, 58531.2, 58536.0, 58537.0, 58540.41, 58541.0, 58542.0, 58540.0, 58544.0, 58545.0, 58546.0, 58547.0, 58548.0, 58549.0, 58549.01, 58548.57, 58550.0, 58552.0, 58553.4, 58554.0, 58556.0, 58557.0, 58555.0, 58553.0, 58554.12, 58561.0, 58559.0, 58560.0, 58562.0, 58564.95, 58565.0, 58564.0, 845000.0, 58567.08, 58570.26, 58570.0, 58563.0, 58567.0, 58570.72, 58572.0, 58572.8, 58574.0, 58574.07, 58576.0, 58578.0, 58579.0, 58580.0, 58575.0, 58582.0, 58584.0, 58585.0, 58588.0, 58590.0, 58590.12, 58592.0, 58593.0, 58593.6, 58595.0, 58596.0, 58597.0, 58598.0, 58599.84, 58594.0, 58600.0, 58598.49, 58603.29, 58604.0, 58601.0, 58605.0, 58599.24, 58608.43, 58608.16, 58610.0, 58608.0, 58608.94, 58613.0, 58614.0, 58615.0, 58612.0, 58617.0, 58618.0, 58619.0, 58620.0, 58621.0, 58622.4, 58623.39, 58624.0, 58623.0, 58622.88, 58625.0, 58626.0, 58627.0, 58628.0, 58630.0, 58632.96, 58629.0, 58632.84, 58635.72, 58635.2, 58634.0, 58635.0, 58639.0, 58633.0, 58641.0, 58638.0, 58640.0, 58644.0, 58636.0, 58642.0, 58647.0, 58645.0, 58646.0, 58647.12, 58645.08, 58649.0, 58650.0, 58651.2, 58652.0, 58653.0, 58648.0, 58654.0, 58656.0, 58655.0, 58658.0, 58660.0, 58661.0, 58663.0, 58664.0, 58665.0, 58666.0, 58667.0, 58668.0, 58668.12, 58670.0, 58671.0, 58672.0, 58673.16, 58674.0, 58673.0, 58676.0, 58677.0, 58679.04, 58678.0, 58679.0, 58680.0, 58681.0, 58681.08, 58684.0, 58684.86, 58686.0, 58687.0, 58688.0, 58685.0, 58690.0, 58691.0, 58692.0, 58693.0, 58695.0, 58696.0, 58697.0, 58698.0, 58699.54, 58700.0, 58699.0, 58702.0, 58695.52, 58704.0, 58705.0, 58703.0, 58707.0, 58708.0, 58709.0, 58710.0, 58711.0, 58712.56, 58713.0, 58712.0, 58715.0, 58716.0, 58717.0, 58718.0, 58719.0, 58720.0, 58720.8, 58719.96, 58724.0, 58725.0, 58726.0, 58727.0, 58728.0, 58729.0, 58730.0, 58732.0, 58733.0, 58734.0, 58735.0, 58737.45, 58738.0, 58739.0, 58740.0, 58741.0, 58742.0, 58744.0, 58745.0, 58746.0, 58747.0, 58748.0, 58749.0, 58750.0, 58752.0, 58754.0, 58755.82, 58755.0, 58756.0, 58755.6, 58758.0, 58759.0, 58760.0, 58761.0, 58762.0, 58758.26, 58764.0, 58765.0, 58766.0, 58767.0, 58768.0, 58769.51, 58770.0, 58771.0, 58772.0, 58769.0, 58774.0, 58775.0, 58776.0, 58777.0, 58779.0, 58780.0, 58779.84, 58782.0, 58783.6, 58783.78, 58781.0, 58780.8, 58784.0, 58786.0, 58787.0, 58788.0, 58783.0, 58790.0, 58785.0, 58792.0, 58793.54, 58791.0, 58795.0, 58796.0, 58797.0, 58799.0, 58800.0, 58802.0, 58801.0, 58798.78, 58804.0, 58803.0, 58807.0, 58808.0, 58809.0, 58810.0, 58805.0, 58812.0, 58813.0, 58808.84, 58815.0, 58811.48, 58818.12, 58819.0, 58820.0, 58819.2, 58822.0, 58823.0, 58824.0, 58825.8, 58826.0, 58827.0, 58827.22, 58829.0, 58830.0, 58828.0, 58831.9, 58833.0, 58834.0, 58835.0, 58836.0, 58837.0, 58838.0, 58840.0, 58842.0, 58843.0, 58844.0, 58843.8, 58845.0, 58846.8, 58847.0, 58848.0, 58849.0, 58850.0, 58852.56, 58851.0, 58852.0, 58853.0, 58854.0, 58856.0, 321000.0, 58857.0, 58858.0, 58860.0, 58861.0, 58857.24, 58864.0, 58865.0, 58866.0, 58864.26, 58868.0, 58867.0, 58869.0, 58870.0, 58872.0, 58873.0, 58869.24, 58875.0, 58874.0, 58876.0, 58877.0, 58871.0, 58880.0, 58881.0, 58882.0, 58882.92, 58883.0, 58884.8, 58885.0, 58884.0, 58888.0, 58889.0, 58890.0, 58886.08, 58892.0, 58893.0, 58890.63, 58894.0, 58895.0, 58897.0, 58896.0, 58899.92, 58895.8, 58898.0, 58899.0, 58900.0, 58900.67, 58902.0, 58903.0, 58904.0, 58905.0, 58905.72, 58906.0, 58908.0, 58909.16, 58909.0, 58914.0, 58911.0, 58912.0, 58917.0, 58918.44, 58910.0, 58915.0, 58916.0, 58913.9, 58923.12, 58918.0, 58919.0, 58926.0, 58920.0, 58921.0, 58922.0, 58925.0, 58924.0, 58929.0, 58928.0, 58930.0, 58932.0, 58933.0, 58934.0, 58937.0, 58938.0, 58938.75, 58940.0, 58941.0, 58943.0, 58936.0, 58947.0, 58948.76, 58948.0, 58950.0, 58951.0, 58952.0, 58953.0, 58954.0, 58955.0, 58956.0, 321100.0, 58949.0, 58960.0, 58960.72, 845394.0, 58963.0, 58962.02, 58965.0, 58962.0, 58967.0, 58968.0, 58969.0, 58970.0, 58969.32, 58968.4, 58974.0, 58975.0, 58975.08, 58977.0, 58976.0, 58979.0, 58980.0, 58981.0, 58982.0, 58979.28, 58981.52, 58985.6, 58986.0, 58987.0, 58988.0, 58988.8, 58989.0, 58990.0, 58991.0, 58992.0, 58994.0, 58995.0, 58996.0, 58997.49, 58998.24, 58999.0, 59000.0, 59001.0, 58998.0, 59003.0, 59004.0, 59005.0, 59004.92, 59006.0, 59007.0, 59008.0, 59010.0, 59011.0, 59012.0, 59013.0, 59014.0, 59015.82, 59016.0, 59017.0, 59009.6, 59019.0, 59020.0, 59019.84, 59021.0, 59023.0, 59024.0, 59025.0, 59026.0, 59020.51, 59028.0, 59029.7, 59030.0, 59030.4, 59031.0, 59032.0, 59033.0, 59035.0, 59036.0, 59031.04, 59038.68, 59039.0, 59040.0, 59042.0, 59043.0, 59044.0, 59046.0, 59048.0, 59049.67, 59050.0, 59051.2, 59051.0, 59049.48, 59054.0, 59055.0, 59052.0, 59053.16, 59051.04, 59058.24, 59060.0, 59061.0, 59062.0, 59056.0, 59064.0, 59064.35, 59066.0, 59067.0, 59067.2, 59069.3, 59069.0, 59070.0, 59072.0, 59065.0, 59071.18, 59075.0, 59068.0, 59076.0, 59077.0, 59078.0, 59080.55, 59074.0, 59080.0, 59081.0, 59082.0, 59083.0, 59084.0, 59085.0, 59086.0, 59087.0, 59090.0, 59088.32, 59090.98, 59088.0, 59093.0, 59094.0, 59095.44, 59097.0, 59098.0, 59099.04, 59100.08, 59100.0, 59101.0, 59099.0, 59103.0, 59104.0, 59106.0, 59103.91, 59108.0, 59109.0, 59110.0, 59112.0, 59113.0, 59114.0, 59115.42, 59118.0, 59119.2, 59120.0, 59122.0, 59123.0, 59124.0, 59125.0, 59126.0, 59128.0, 59129.0, 59130.0, 59132.0, 59133.76, 59134.0, 63856.46, 59136.0, 59137.52, 59138.0, 59139.0, 59140.0, 59137.0, 59142.0, 59141.0, 59145.93, 59146.0, 59147.0, 59148.0, 59149.0, 59150.0, 59151.0, 59154.0, 59155.0, 59156.0, 59155.2, 59158.0, 59159.0, 59160.0, 59163.0, 59164.0, 59165.0, 59167.0, 59168.0, 59169.0, 59171.0, 59172.0, 59173.0, 59174.0, 59172.8, 321316.0, 59177.7, 59176.0, 59179.0, 59180.0, 59181.13, 59181.0, 59182.0, 59180.16, 59184.0, 59186.0, 59183.0, 59188.0, 59189.0, 59190.0, 59187.0, 59190.37, 59193.0, 59194.0, 59195.0, 59196.0, 59196.8, 59198.0, 59199.0, 59200.0, 59200.8, 59202.0, 59205.0, 59207.0, 59208.0, 59209.0, 59210.0, 59210.12, 59212.0, 59213.0, 59214.0, 59214.92, 59216.0, 59217.86, 59218.0, 59219.0, 59220.0, 59217.0, 59221.0, 59223.0, 59225.0, 59227.0, 59228.0, 59229.0, 59230.0, 59231.0, 59232.0, 59233.0, 59234.0, 59235.0, 59237.0, 59238.0, 59239.0, 59240.0, 59241.0, 59238.4, 59243.0, 59244.0, 59245.0, 59246.0, 59242.32, 59248.0, 63879.0, 59250.0, 59251.0, 59254.0, 59255.0, 59256.0, 59257.0, 59258.0, 59259.0, 59260.0, 59261.52, 59262.12, 59262.0, 59264.0, 59265.0, 59268.0, 59269.0, 59270.16, 59270.5, 59270.0, 59273.0, 59273.56, 59275.0, 59276.0, 59271.0, 59279.0, 59280.0, 59281.0, 1370000.0, 59282.0, 59286.24, 59288.0, 59292.0, 59294.0, 59295.16, 59295.96, 59295.0, 59299.76, 59300.0, 59301.0, 59302.0, 59300.8, 59304.0, 59299.0, 59306.0, 59307.0, 59303.0, 59309.0, 59310.0, 59311.0, 59312.0, 59313.0, 59312.16, 59316.0, 59317.0, 59318.0, 59320.0, 59321.0, 59322.0, 59324.0, 59325.0, 59326.8, 59327.0, 59328.0, 59330.0, 59331.0, 59332.0, 59333.0, 59334.0, 59336.0, 59337.0, 59338.0, 59339.0, 59340.0, 59340.8, 59342.0, 59337.75, 59345.0, 59346.0, 59346.24, 59348.0, 59350.0, 59351.0, 59352.0, 59353.0, 59354.0, 59355.0, 59355.76, 59356.0, 59355.96, 59360.0, 59362.0, 59363.2, 59363.0, 59364.0, 59366.0, 59367.48, 59369.84, 59370.0, 59369.0, 59372.0, 59373.0, 59375.28, 59376.0, 59377.93, 59378.0, 59380.7, 59380.0, 321526.0, 59383.0, 59384.0, 59385.6, 59386.0, 59382.0, 59384.52, 59387.0, 59388.0, 59389.0, 59390.0, 59391.56, 59394.0, 59393.0, 59396.0, 59397.0, 59398.81, 59399.0, 59400.0, 59402.0, 59404.0, 59404.8, 59405.0, 59410.0, 59411.0, 59412.0, 59414.0, 59415.0, 59416.0, 59417.0, 59418.0, 59419.0, 59420.0, 59421.0, 59422.16, 59423.0, 59424.0, 59425.0, 59426.0, 59429.98, 59430.0, 59431.0, 59432.0, 59433.0, 59434.0, 59435.0, 59436.0, 59437.68, 59429.01, 59439.0, 59440.0, 59441.0, 59442.0, 59445.0, 59446.0, 59448.0, 59448.48, 59450.0, 59449.0, 59452.0, 59449.97, 59453.0, 59454.2, 59455.0, 59456.0, 59451.0, 59458.0, 59459.0, 59460.0, 59458.81, 59462.0, 59461.0, 59465.09, 59466.16, 59467.0, 59468.0, 59469.0, 59470.0, 59472.0, 59473.0, 59474.0, 59477.0, 59478.0, 59479.0, 59480.0, 59478.72, 59482.0, 59483.0, 59484.0, 59485.0, 59486.62, 59481.0, 59488.0, 59480.42, 59490.0, 59491.0, 59492.0, 59493.0, 59494.0, 59495.0, 59496.0, 59498.0, 59498.88, 59500.0, 59501.0, 59502.0, 59506.0, 59508.0, 59508.8, 59510.0, 59511.25, 59512.0, 59509.0, 59514.0, 59511.0, 59516.0, 59517.0, 59518.0, 59519.0, 59520.0, 59521.0, 59519.72, 59523.0, 59524.0, 59525.52, 59525.0, 59527.71, 59526.0, 59529.0, 59530.0, 59531.0, 59532.0, 59533.5, 59534.88, 59535.12, 59534.0, 59537.0, 59535.0, 59536.0, 59540.0, 321685.0, 59542.0, 59543.0, 59539.23, 59541.0, 59546.0, 59547.0, 59547.8, 59549.64, 59550.0, 59550.4, 59552.22, 59554.0, 59555.0, 59556.0, 59557.0, 59558.0, 59558.25, 59560.0, 59562.0, 59562.63, 59563.56, 59564.82, 59565.0, 59566.0, 59567.0, 59568.0, 59563.0, 59571.0, 59572.0, 59569.0, 59574.0, 59575.0, 59571.2, 59577.0, 59578.12, 59578.44, 59580.0, 59581.0, 59582.25, 59582.0, 59575.98, 59585.0, 59586.0, 59585.26, 59588.0, 59589.0, 59587.0, 59591.0, 59592.0, 59593.0, 59595.0, 59596.8, 59596.0, 59598.0, 59599.0, 59600.0, 59601.0, 59602.0, 59604.0, 59608.0, 59609.0, 59610.0, 59612.0, 59613.0, 59614.0, 59613.24, 59616.0, 59617.48, 59618.0, 59613.03, 59620.0, 59612.76, 59619.0, 59623.0, 59624.0, 59625.0, 59628.0, 59630.0, 59631.0, 59633.0, 59634.0, 59635.0, 59636.0, 59637.0, 59638.0, 59640.0, 59642.0, 59643.0, 59644.0, 59645.0, 59646.0, 59647.2, 59648.0, 59650.0, 59651.0, 59652.0, 59653.88, 59654.0, 59655.0, 59653.0, 59657.0, 59658.0, 59656.0, 59660.0, 59661.0, 59664.0, 59665.0, 59664.2, 59667.5, 59668.0, 59669.0, 59670.0, 59672.0, 59673.0, 59674.68, 59675.0, 59676.0, 59676.96, 59678.0, 59677.28, 59680.0, 59681.0, 59683.0, 59684.0, 59685.0, 59686.0, 59687.0, 59688.0, 59689.0, 59690.0, 59691.0, 59693.0, 59694.0, 59695.0, 59696.0, 59697.0, 59698.0, 59699.0, 59700.0, 59698.24, 59701.0, 59704.0, 59706.0, 59707.0, 59710.0, 59711.0, 59712.0, 59712.6, 584000.0, 59715.0, 59716.0, 59717.0, 59718.0, 59712.94, 59720.0, 59713.0, 59722.0, 59723.0, 59724.0, 59725.0, 59726.0, 59727.0, 59724.9, 59729.0, 59730.84, 59728.0, 59732.0, 59733.0, 59735.0, 59736.72, 59736.0, 59737.0, 59737.6, 59740.0, 59740.2, 59743.84, 59743.0, 59745.09, 59746.0, 59747.0, 59748.0, 59744.0, 59750.0, 59750.6, 59752.0, 59745.0, 59754.0, 59758.0, 59759.0, 59760.0, 59761.0, 59762.0, 59763.39, 59764.0, 59765.0, 59766.0, 59767.0, 59768.0, 59769.0, 59768.02, 59772.0, 59772.6, 59774.0, 59775.0, 59776.0, 59777.0, 59778.0, 59780.0, 59782.0, 59783.0, 59784.61, 59785.0, 59784.0, 59785.01, 59788.0, 59788.32, 59789.0, 59790.0, 59791.0, 59792.0, 59793.17, 59793.06, 59795.0, 59796.0, 59797.0, 59798.0, 59798.14, 59800.0, 59799.0, 59800.55, 59803.68, 59804.0, 59805.0, 59806.0, 59808.0, 59801.0, 59811.0, 59812.0, 59813.0, 59813.4, 59815.0, 59816.0, 59817.0, 59814.8, 59819.0, 59819.97, 59820.24, 59822.0, 59821.0, 59820.8, 59820.0, 59825.0, 59823.0, 59826.0, 59829.0, 59830.0, 59832.0, 59834.0, 59835.0, 59836.0, 59837.0, 59838.0, 59839.0, 59840.0, 59841.0, 59842.0, 59843.16, 59844.0, 59845.0, 59846.0, 59843.0, 59848.0, 59847.0, 59850.0, 59852.0, 59853.0, 59855.36, 59855.0, 59857.0, 322000.0, 59856.0, 59858.0, 59859.0, 59860.0, 59861.0, 59864.0, 322008.0, 59862.26, 59862.0, 59863.0, 59862.4, 59866.0, 59865.0, 59868.0, 59869.0, 59870.0, 59871.0, 59872.0, 59874.0, 59875.0, 59876.0, 59877.0, 59878.0, 59875.2, 59880.0, 59880.68, 59882.0, 59883.0, 59884.0, 59885.0, 59886.0, 59887.0, 59888.0, 59890.0, 59889.0, 59892.0, 59895.6, 59891.0, 59895.0, 59896.0, 59899.0, 59897.56, 59898.0, 322040.0, 59900.0, 59897.0, 59903.0, 59904.0, 59902.0, 59901.0, 59905.0, 59908.0, 59909.0, 59906.0, 59911.0, 59907.0, 59913.0, 59914.0, 59915.0, 59916.0, 59917.56, 59917.0, 59919.0, 59920.0, 59918.0, 59923.0, 59924.8, 59925.0, 59926.0, 59924.0, 59928.0, 59929.0, 59931.0, 59932.0, 59933.0, 59934.0, 59935.0, 59938.0, 59940.0, 59941.0, 59942.0, 59943.54, 59944.0, 59945.6, 59946.0, 59947.0, 59948.0, 59950.0, 59952.0, 59953.0, 59952.76, 59954.16, 59956.0, 59958.0, 59959.0, 59960.57, 59960.0, 59963.0, 59964.0, 59966.0, 59967.0, 59968.0, 59969.0, 59970.0, 59971.0, 59972.0, 59974.8, 59975.0, 59976.0, 59977.0, 59979.0, 59980.0, 59982.96, 59983.0, 59982.0, 59984.0, 59984.32, 59986.0, 59987.28, 59989.0, 59985.0, 59987.64, 59992.0, 59993.4, 59990.0, 59991.0, 59987.0, 59993.44, 59995.0, 59999.62, 59996.0, 59997.0, 59998.0, 60000.0, 59999.0, 59999.48, 60002.0, 60001.0, 60005.0, 60004.0, 60007.0, 60008.0, 60009.0, 60006.0, 60011.0, 60012.0, 60013.0, 60010.0, 60014.24, 60017.0, 60018.0, 60019.0, 60020.0, 60021.0, 60016.0, 60023.0, 60024.0, 60025.0, 60023.4, 60027.0, 60026.64, 60029.0, 60030.0, 60028.0, 60032.0, 60033.0, 60034.0, 60035.0, 60036.0, 60037.0, 60039.0, 60040.0, 60042.8, 60042.0, 60041.76, 60042.55, 60046.8, 60043.0, 322192.0, 60044.0, 60045.0, 60047.0, 60048.0, 60049.0, 60050.0, 60051.0, 60052.0, 60050.56, 60054.0, 60055.46, 60055.0, 60056.0, 60058.0, 60060.0, 60060.79, 60062.14, 60061.0, 60064.0, 60066.6, 60067.0, 60068.0, 60069.0, 60070.0, 60070.8, 60072.0, 60071.0, 60075.0, 60070.4, 60077.0, 60078.58, 60079.2, 60080.0, 60081.0, 60078.0, 60083.0, 60084.0, 60085.0, 60087.0, 60088.0, 60089.0, 60090.0, 60091.0, 60092.0, 60093.0, 60094.0, 60094.06, 60096.0, 60097.0, 60098.0, 60099.0, 60100.0, 60100.08, 60101.0, 60103.0, 60104.0, 60105.0, 60106.0, 60107.92, 60108.0, 60107.16, 60102.64, 60111.0, 60112.0, 60113.74, 60107.0, 60114.0, 60115.0, 60116.0, 60117.0, 60113.0, 60120.0, 60122.0, 60123.0, 60124.32, 60125.0, 60126.0, 60127.0, 60128.0, 60129.0, 60130.0, 60131.18, 60131.0, 60132.0, 60134.0, 60133.0, 60131.09, 60135.0, 60136.0, 60137.0, 60138.0, 60132.82, 60140.0, 60141.0, 60142.0, 60143.0, 60144.5, 60144.0, 60145.6, 60147.0, 60141.53, 60146.0, 60150.0, 60148.44, 60152.0, 60155.0, 60153.6, 60154.0, 60153.0, 60159.0, 60156.0, 60157.0, 60158.0, 60158.88, 60160.0, 60161.75, 322305.0, 60163.0, 60162.18, 60165.0, 60166.89, 60168.0, 60169.2, 60171.0, 60172.0, 60175.96, 60176.0, 60174.0, 60175.0, 60174.4, 60177.0, 60178.0, 60174.78, 60180.0, 60181.0, 60182.0, 60186.62, 60183.0, 60184.0, 60185.0, 60186.0, 60187.0, 60185.57, 60189.12, 60190.0, 60195.45, 60195.2, 60197.53, 60191.0, 60192.0, 60193.04, 60193.0, 60195.0, 60195.47, 60196.44, 60198.0, 60200.0, 60202.0, 60203.0, 60204.0, 60203.52, 60206.0, 60208.0, 60209.0, 60210.0, 60211.0, 60212.0, 60213.12, 60214.0, 60213.0, 60216.0, 60218.0, 60218.6, 60219.0, 60220.0, 60222.0, 60223.0, 60224.0, 60225.0, 60221.0, 60229.0, 60230.0, 60231.0, 60228.0, 60232.0, 60234.0, 60235.0, 60236.8, 60237.0, 60238.8, 60238.15, 60240.0, 60241.0, 60242.0, 60244.0, 60245.0, 60247.0, 60248.0, 60249.37, 60250.0, 60249.0, 60252.0, 60253.0, 60254.0, 60255.0, 322400.0, 60256.0, 60257.73, 60258.0, 60259.0, 60257.0, 60261.0, 60260.0, 60263.0, 60264.34, 60265.0, 60266.0, 60264.0, 60268.0, 60269.0, 60270.0, 60272.0, 60267.0, 60271.0, 60273.0, 60274.0, 60275.0, 60276.0, 60279.0, 60276.17, 60278.0, 60279.48, 60280.0, 60281.0, 60282.0, 60282.96, 60285.0, 60286.0, 60287.0, 60288.0, 60289.18, 60291.0, 60292.3, 60292.0, 60295.19, 60296.8, 60296.0, 60297.6, 60299.0, 60300.0, 60301.0, 60302.52, 60303.0, 60304.0, 60305.0, 60306.0, 60307.0, 60308.0, 60309.0, 60310.0, 60312.0, 60313.0, 60314.0, 60315.0, 60314.57, 60317.04, 60318.37, 60320.0, 60321.0, 60322.0, 60323.0, 60324.0, 60325.0, 60326.0, 60327.0, 60330.0, 60331.0, 60334.0, 60336.0, 60336.64, 60337.0, 60340.0, 60341.0, 60341.55, 60343.44, 60343.0, 60343.2, 60346.0, 60347.28, 60347.0, 60348.0, 60349.31, 60350.0, 60351.0, 60352.0, 60352.8, 60353.0, 60355.0, 60357.7, 60356.4, 60357.0, 60358.0, 60360.0, 60362.0, 60363.36, 60364.0, 60365.0, 60366.0, 60367.08, 60368.0, 60369.0, 60370.0, 60370.08, 60372.0, 60373.58, 60374.0, 60375.0, 60373.1, 60373.0, 60378.0, 60376.0, 60380.0, 60380.91, 60382.0, 60383.73, 60384.0, 60385.0, 322530.0, 60386.0, 60387.84, 60389.0, 60390.0, 60391.5, 60392.53, 60393.0, 60394.0, 60391.0, 60396.0, 60398.64, 60399.0, 60400.0, 60402.0, 60403.0, 60404.0, 60405.0, 60403.2, 60408.0, 60410.29, 60411.0, 60412.0, 60414.0, 60415.0, 60416.08, 60416.68, 60418.0, 60417.0, 60420.0, 60422.0, 60422.04, 60424.0, 60425.04, 60425.0, 60428.0, 60429.0, 60430.0, 60432.0, 60433.0, 60434.0, 60435.0, 60436.0, 60437.0, 60438.0, 60440.0, 60440.3, 60442.0, 60441.84, 60444.0, 60445.0, 60450.0, 60451.45, 60451.0, 60453.48, 60452.0, 60453.0, 60453.12, 60455.29, 60456.0, 60457.0, 60455.9, 60459.0, 60460.0, 60461.0, 60462.8, 60463.08, 60460.56, 60465.0, 60466.0, 60462.0, 60468.0, 60469.0, 60470.0, 60471.0, 60472.0, 60473.0, 60474.82, 60475.0, 60476.0, 60478.0, 60479.0, 60480.0, 60475.39, 60482.0, 60483.0, 60484.0, 60485.0, 60486.0, 60487.0, 60486.4, 60489.0, 60492.9, 60493.52, 60494.0, 60495.2, 60496.0, 60495.0, 60495.4, 60498.0, 60500.0, 60500.85, 60500.64, 60503.78, 60503.0, 60499.0, 60504.0, 60507.0, 60509.73, 60512.0, 60513.96, 60514.0, 60515.0, 60516.0, 60517.0, 60514.8, 60514.68, 60520.0, 60518.9, 60522.0, 60523.0, 60524.0, 60525.0, 60526.0, 60518.0, 60528.0, 60529.0, 60530.0, 60531.0, 60532.0, 60533.0, 60534.0, 60536.0, 60537.0, 60538.0, 60539.0, 60540.0, 60541.0, 60542.0, 60543.0, 60544.0, 60545.0, 60546.0, 60548.0, 60549.0, 60550.0, 60552.0, 60553.0, 60555.0, 60556.0, 60557.0, 60560.0, 60561.0, 60562.0, 60563.0, 60564.0, 60565.0, 60566.0, 60567.0, 60568.0, 60569.0, 60570.0, 60571.0, 60572.0, 60573.22, 60573.0, 60574.0, 60576.0, 60578.16, 60578.0, 60580.0, 60581.0, 60582.0, 60585.0, 60586.0, 60585.83, 60588.48, 60588.0, 60589.0, 60590.0, 60590.4, 60592.0, 60587.0, 60595.32, 60589.6, 60594.0, 60596.0, 60596.11, 60593.24, 60600.0, 60601.0, 60595.0, 60603.0, 60604.7, 60604.0, 60608.0, 60610.0, 60611.0, 60612.0, 60613.0, 60614.0, 60615.0, 60616.0, 60616.07, 60620.0, 60621.0, 60623.0, 584912.0, 60625.8, 60625.0, 60624.0, 60626.0, 60629.92, 60627.0, 60628.0, 60629.0, 60630.0, 60631.0, 60632.0, 60633.0, 60632.4, 60636.0, 60637.0, 60633.82, 60634.2, 60640.0, 60641.0, 60635.0, 60643.0, 60639.48, 60645.0, 60647.0, 60648.0, 60650.0, 60651.0, 60651.4, 60652.0, 60653.0, 60655.0, 60656.0, 60657.0, 60650.24, 60659.0, 60660.0, 60660.6, 60662.0, 60663.0, 60664.0, 60665.0, 60666.0, 60667.0, 60668.0, 60666.41, 60671.0, 60672.0, 60673.0, 60674.0, 60675.0, 60676.8, 60675.83, 60678.0, 60675.84, 60680.0, 60681.0, 60682.0, 60679.0, 60684.0, 60685.0, 60686.0, 60687.0, 60688.0, 60689.0, 60690.0, 60691.0, 60692.0, 60694.4, 60694.0, 60696.0, 60697.0, 60695.0, 60699.0, 60700.0, 60701.0, 60702.0, 60705.0, 60707.0, 60708.0, 60709.0, 60710.4, 60711.0, 585000.0, 60713.0, 60710.0, 60715.0, 60716.0, 60716.82, 60716.91, 60718.0, 60720.0, 60721.0, 60722.0, 60723.0, 60724.0, 60725.0, 60726.0, 60719.0, 60729.94, 60730.0, 60732.0, 60733.0, 60735.0, 60736.0, 60737.0, 60738.0, 60739.0, 60740.0, 60741.0, 60742.0, 60744.0, 60745.0, 60746.0, 60747.84, 60747.0, 60750.0, 60751.0, 60752.0, 60750.34, 60754.0, 60755.0, 60756.96, 60757.7, 60758.0, 60756.0, 60757.0, 60760.0, 60761.0, 60760.3, 60764.28, 60765.0, 60766.0, 60767.61, 60768.0, 60767.0, 60770.0, 60772.0, 60773.96, 60774.0, 60775.0, 60776.0, 60777.0, 60778.0, 60779.0, 60780.0, 60781.0, 60782.0, 60776.04, 60784.0, 60785.0, 60786.84, 60786.0, 60787.0, 60789.88, 60789.0, 60791.0, 60792.0, 60790.0, 60794.0, 60795.56, 60795.0, 60788.0, 60798.0, 60798.4, 60800.0, 60799.99, 60802.0, 60804.0, 60805.0, 60806.0, 60807.0, 60809.0, 60810.0, 60812.0, 60813.0, 60814.08, 60815.0, 60816.0, 60815.82, 60814.0, 60819.0, 60820.0, 60821.0, 60822.0, 60818.0, 60824.0, 60825.0, 60826.0, 60828.0, 60829.0, 60830.0, 60828.92, 60829.5, 60833.0, 60834.0, 60836.0, 60839.0, 60840.0, 60841.0, 60842.78, 60843.0, 60844.0, 60845.0, 60846.0, 60843.05, 60849.39, 60850.0, 60849.0, 60852.0, 60854.0, 60855.0, 323000.0, 60857.0, 60855.56, 60859.0, 60860.0, 60861.0, 60856.68, 60863.0, 60864.0, 60865.0, 60866.0, 60867.0, 60868.0, 60869.0, 60870.0, 60871.0, 60872.0, 60872.11, 60873.76, 60875.0, 60871.92, 60876.0, 60869.25, 60879.0, 60880.0, 60880.32, 60881.0, 60883.0, 60885.0, 60886.0, 60887.0, 60888.0, 60889.0, 60890.0, 60891.0, 60891.93, 60892.0, 60894.0, 60895.0, 60894.08, 60897.0, 60898.0, 60899.0, 60900.0, 60892.5, 60902.0, 60902.55, 60903.0, 60905.6, 60904.0, 60904.06, 60908.0, 60910.0, 60911.0, 60912.0, 60916.0, 60917.61, 60918.0, 60920.0, 60921.0, 60923.0, 60924.0, 60923.41, 60926.0, 60927.16, 60928.0, 60931.0, 60932.0, 60934.0, 60934.8, 60936.0, 60937.0, 60938.0, 60935.0, 60940.0, 60941.0, 60940.8, 60939.0, 60944.88, 60944.0, 60946.0, 60947.0, 60948.0, 60949.0, 60950.0, 60945.35, 60943.0, 60952.0, 60954.0, 60955.0, 60953.0, 60957.0, 60958.0, 60959.0, 60960.0, 60960.48, 60962.0, 60963.0, 60964.49, 60965.0, 60966.0, 60964.8, 60963.6, 60969.0, 60970.0, 60971.0, 60972.0, 60973.0, 60974.0, 60975.0, 60978.0, 323123.0, 60980.0, 60979.0, 60982.0, 60983.0, 60984.0, 60983.41, 60985.0, 60981.0, 60988.0, 60989.0, 60990.0, 60991.0, 60993.0, 60995.0, 60996.0, 60997.0, 60998.0, 60999.0, 61000.0, 61001.0, 61003.35, 61004.0, 61005.34, 61006.0, 61005.0, 61008.0, 61009.0, 61010.0, 61011.0, 61006.4, 61013.0, 61014.0, 61015.0, 61017.24, 61019.0, 61020.0, 61022.0, 61023.0, 61024.0, 61025.0, 323169.0, 61027.0, 61028.0, 61027.2, 61031.0, 61032.0, 61033.0, 61034.0, 61035.0, 324750.0, 61037.0, 61038.0, 61038.84, 61040.0, 61041.0, 61043.45, 61044.0, 61044.36, 61045.0, 61047.0, 61048.0, 61049.0, 61050.0, 61051.0, 61052.0, 61053.72, 61054.0, 61046.0, 61056.0, 61058.0, 61059.0, 61060.0, 61058.4, 61063.72, 61063.0, 61064.0, 61065.0, 61066.0, 61066.09, 61068.8, 61067.34, 61071.0, 61072.0, 61073.09, 61074.0, 61072.44, 61069.0, 61077.0, 61078.0, 61079.0, 61080.0, 61079.03, 61081.0, 61083.0, 61084.0, 61085.0, 61086.0, 61082.0, 61088.28, 61089.0, 61090.0, 61091.0, 61092.0, 61090.74, 61096.3, 61097.0, 61096.0, 61098.0, 61100.0, 61102.8, 61104.0, 61105.0, 61105.16, 61108.0, 61109.4, 61110.4, 61111.0, 61112.0, 61110.0, 61110.96, 61115.0, 61116.0, 61117.28, 61113.93, 61118.0, 61119.5, 61120.0, 61120.72, 61123.0, 61124.0, 61125.0, 61126.0, 61127.0, 61128.0, 61122.72, 61130.0, 61131.0, 61135.0, 61136.0, 61138.0, 61139.0, 61140.0, 61141.6, 61141.0, 61142.0, 61143.0, 61144.0, 61145.0, 61145.76, 61146.0, 61148.0, 61149.0, 61150.0, 61147.0, 61152.0, 61153.0, 61155.0, 61156.0, 61151.61, 61158.36, 61159.0, 61160.0, 61157.0, 61162.0, 61163.0, 61164.0, 61165.0, 61166.0, 61165.52, 61166.4, 61170.0, 61171.0, 61171.2, 61171.47, 61172.0, 61173.0, 61175.0, 61176.0, 61177.0, 61175.88, 61180.0, 61174.0, 61178.54, 61183.84, 61182.0, 61178.0, 61183.0, 61187.0, 61184.0, 61188.0, 61189.56, 61190.0, 61191.0, 61189.92, 61193.0, 61189.0, 61195.0, 61193.6, 61196.76, 61198.0, 61200.1, 61199.94, 61200.0, 61201.0, 61202.0, 61203.0, 61204.0, 61199.0, 61206.0, 61207.04, 61208.0, 61209.0, 61210.0, 61211.04, 61214.4, 61212.84, 61213.0, 61214.0, 61212.0, 61215.0, 61218.56, 61218.0, 61220.0, 61221.0, 61222.0, 61223.0, 61224.0, 61225.0, 61228.0, 61230.0, 61232.0, 61233.0, 61234.0, 61235.0, 61236.0, 61235.2, 61237.0, 61239.0, 61240.0, 61240.32, 61243.0, 61244.0, 61245.0, 61246.0, 61247.0, 61248.0, 61249.0, 61250.0, 61251.0, 61252.0, 61254.0, 61256.0, 61257.0, 61258.0, 61259.0, 61260.0, 61261.0, 61260.96, 61260.08, 61264.0, 61264.96, 61265.0, 61267.0, 61267.44, 61269.12, 61269.0, 61270.68, 61270.0, 61272.0, 61273.0, 61274.0, 61275.0, 61276.0, 61277.0, 61278.0, 61279.0, 61280.0, 61278.6, 61279.92, 61284.0, 61276.8, 61286.0, 61287.0, 61288.0, 61289.0, 61290.0, 61291.0, 61292.0, 61293.0, 61287.6, 61287.36, 61296.0, 61297.6, 61298.0, 61299.0, 61300.0, 61301.0, 61295.0, 61303.0, 61305.92, 61305.0, 61307.0, 61309.0, 61310.0, 61312.0, 61313.92, 61314.0, 61314.22, 61316.0, 61317.51, 61318.0, 61318.4, 61320.0, 61321.0, 61322.0, 61323.0, 61324.0, 61325.0, 61326.0, 61327.0, 61328.0, 61329.0, 61330.0, 61331.0, 61332.0, 64292.0, 61334.0, 61335.0, 61334.49, 61336.65, 61338.0, 61339.0, 61340.0, 61339.2, 61341.0, 61343.0, 61344.0, 61345.0, 61346.0, 61347.0, 61348.0, 61349.0, 61350.0, 61352.0, 61354.44, 61354.0, 61356.0, 61357.56, 323500.0, 61359.0, 61360.0, 61361.0, 61360.8, 61358.0, 61357.0, 61364.0, 61365.0, 61366.0, 61367.0, 61368.0, 61370.0, 61368.15, 61372.0, 61373.0, 61374.36, 61375.0, 61369.0, 61377.0, 61369.65, 61379.81, 61380.0, 61381.0, 61379.0, 61383.0, 61384.0, 61385.74, 61386.48, 61382.4, 61388.0, 61389.0, 61383.88, 61386.0, 61392.0, 61387.0, 61394.0, 61395.0, 61396.08, 61397.0, 61398.0, 61396.0, 61400.0, 61400.91, 61402.0, 61403.0, 61404.0, 61403.4, 61401.0, 61407.0, 61400.76, 61409.0, 61410.0, 61414.0, 61416.0, 61417.9, 61417.0, 61418.03, 61420.0, 61421.36, 61421.0, 61423.0, 61422.0, 61422.4, 1110000.0, 61424.0, 61425.0, 61426.0, 61427.0, 61430.0, 61431.0, 61433.0, 61428.0, 61435.0, 61435.31, 61437.0, 61438.0, 61439.0, 61440.0, 61436.0, 61442.0, 61443.2, 61444.0, 61445.0, 61443.0, 61446.0, 61448.0, 61450.0, 61451.0, 61452.0, 61453.0, 61454.0, 61455.6, 61455.0, 61458.0, 61459.0, 61459.08, 61460.0, 61462.0, 61463.0, 61464.0, 61465.0, 61466.16, 61466.0, 61470.0, 61471.0, 61472.0, 61474.0, 61476.0, 61477.0, 61478.0, 61480.0, 61481.25, 61480.04, 61483.0, 61484.0, 61485.36, 61485.0, 61486.0, 61488.0, 61481.0, 61490.0, 61487.0, 61492.0, 61494.0, 61495.0, 61496.0, 61498.0, 61499.0, 61500.0, 61503.52, 61505.0, 61506.0, 61507.0, 61508.2, 61509.0, 61510.0, 61511.47, 61512.0, 61513.0, 61516.0, 61517.0, 61520.0, 61522.0, 323666.0, 61524.04, 61524.0, 61525.0, 61526.0, 61526.4, 61527.57, 61530.0, 61528.0, 61532.0, 61532.2, 61533.0, 61534.0, 61536.0, 61537.84, 61538.0, 61539.0, 61540.0, 61541.0, 61542.95, 61543.29, 61544.0, 61543.0, 61543.2, 61547.0, 61548.0, 61546.0, 61550.0, 61551.0, 61552.0, 61553.0, 61554.0, 61557.0, 61558.0, 61559.0, 61560.0, 61560.06, 61564.0, 61565.0, 61566.6, 61568.0, 61569.16, 61570.0, 61572.0, 61574.0, 61575.49, 61576.0, 61575.0, 61578.0, 61579.0, 61580.0, 61581.0, 61582.0, 61584.0, 61585.0, 61586.0, 61587.0, 61588.0, 61588.6, 61589.0, 61590.0, 61592.0, 61593.0, 61594.0, 61595.0, 61596.0, 61597.0, 61598.0, 61600.0, 61602.32, 61602.0, 61604.0, 61605.96, 61606.0, 61604.98, 61605.0, 61608.0, 61609.0, 61610.0, 61611.0, 61612.0, 61609.6, 61615.0, 61616.0, 61617.0, 58458.8, 61619.0, 61620.0, 61622.0, 61620.37, 61624.0, 61625.0, 61626.0, 61627.0, 61630.0, 61631.11, 61632.0, 61635.0, 61636.0, 61637.0, 61638.0, 61639.0, 61640.0, 61642.1, 61643.0, 61644.8, 61644.0, 61645.0, 61647.0, 61648.0, 61650.0, 61651.0, 61651.2, 61652.0, 61654.0, 61655.0, 61656.0, 61657.0, 61659.41, 61660.0, 61660.13, 61662.0, 61659.0, 61664.0, 61665.0, 61667.0, 61668.0, 61669.0, 61670.0, 61671.0, 61672.0, 61674.0, 61675.0, 61676.0, 61677.0, 61678.0, 61674.82, 61680.0, 61681.66, 61681.0, 61683.0, 61684.0, 61682.0, 61686.0, 61687.0, 61688.0, 61690.0, 61691.0, 61692.0, 61693.0, 61694.0, 61695.0, 61692.8, 61691.64, 61700.0, 61702.0, 61704.0, 61706.98, 61707.36, 61708.0, 61709.0, 61706.0, 61711.0, 61712.0, 61713.0, 61712.97, 61707.0, 586000.0, 61716.0, 61717.0, 61718.0, 61719.0, 61721.02, 61720.0, 61721.0, 61722.0, 61723.0, 61724.0, 61725.0, 61720.6, 61727.21, 61729.0, 61730.0, 61731.26, 61732.0, 61733.0, 61734.0, 61735.0, 61737.81, 61737.94, 61739.0, 61740.0, 61734.4, 61742.0, 61744.0, 61744.43, 61747.2, 61748.0, 61749.0, 61750.0, 61748.8, 61752.0, 61753.0, 61755.0, 61756.0, 61755.2, 61758.0, 61759.0, 61760.0, 61763.0, 61764.0, 61765.0, 61766.04, 61766.0, 61767.0, 61769.0, 61770.0, 61771.0, 61772.0, 61773.0, 61775.0, 61776.0, 61777.0, 61778.0, 61780.0, 61781.0, 61781.68, 61782.0, 61784.88, 61785.0, 61786.0, 61787.0, 61788.96, 61788.0, 61789.0, 61791.0, 61790.0, 61788.06, 61795.0, 61796.0, 61797.0, 61796.8, 61800.0, 61802.0, 61803.0, 61805.0, 61808.0, 61809.0, 61810.0, 61811.0, 61812.0, 61810.8, 61814.0, 61815.0, 61817.0, 61818.0, 61818.2, 61819.0, 61820.0, 61822.0, 61823.0, 61824.0, 61825.0, 61817.6, 61827.0, 61828.0, 61829.0, 61830.0, 61831.0, 61828.76, 61833.0, 61834.0, 61835.0, 61828.08, 61831.86, 61838.0, 61839.0, 61838.4, 61840.0, 61836.0, 61842.0, 61843.0, 61844.0, 61845.0, 61846.0, 61847.0, 61848.0, 61845.68, 61850.0, 61851.0, 61852.0, 61849.0, 324000.0, 61856.0, 61857.0, 61859.0, 61860.0, 61861.0, 61864.0, 61865.0, 61867.0, 61868.0, 61869.0, 61870.0, 61872.0, 61874.0, 61875.0, 61877.58, 61878.0, 61879.0, 61880.0, 61879.79, 61882.0, 61883.0, 61884.0, 61883.16, 61887.65, 61888.0, 61889.0, 61890.0, 61887.0, 324037.0, 61894.0, 61896.0, 61897.0, 61899.0, 61900.0, 61902.0, 61903.0, 61902.96, 61905.0, 61905.17, 61907.0, 61908.0, 61909.63, 61910.0, 61912.0, 61913.76, 61914.03, 61915.0, 61916.28, 61918.0, 61919.25, 61920.0, 61920.24, 61921.0, 61919.0, 61918.8, 61925.0, 61926.0, 61927.0, 61929.92, 61930.0, 61931.0, 61932.0, 61934.0, 61935.0, 61936.02, 61936.0, 61938.0, 61939.0, 61940.0, 61942.0, 61943.0, 61944.0, 61945.0, 61946.0, 61948.0, 61949.0, 61950.0, 61951.0, 61952.8, 586240.0, 61954.0, 61952.0, 61955.0, 61956.0, 61957.0, 61958.0, 61953.96, 61960.0, 61961.0, 61959.0, 61963.0, 61965.0, 61964.0, 61962.0, 61968.0, 61969.0, 61970.0, 61971.0, 61972.0, 61973.0, 61974.0, 61977.0, 61978.0, 61979.0, 61980.0, 61981.0, 61983.0, 61984.0, 61985.0, 61988.0, 61990.0, 61992.0, 61993.0, 61994.0, 61995.0, 61996.0, 61997.25, 61997.0, 61999.0, 62000.0, 62001.0, 62002.0, 61998.0, 62000.72, 61997.52, 62004.0, 62005.0, 62005.68, 62008.0, 62006.0, 62010.0, 62011.0, 62010.24, 62013.6, 62015.0, 62016.0, 62017.0, 62018.0, 62019.0, 62020.0, 62022.0, 62024.0, 62025.0, 62026.0, 62025.6, 62028.0, 62030.0, 62032.0, 62033.0, 62035.0, 62036.0, 62037.0, 62040.0, 62042.0, 62043.0, 62044.0, 62043.26, 62046.4, 62046.0, 62047.08, 62045.5, 62049.79, 62050.0, 62052.48, 62053.0, 62054.0, 62052.0, 62056.0, 62057.0, 62051.0, 62059.0, 62060.0, 62061.0, 62062.0, 62063.0, 62064.0, 62065.0, 62066.0, 62067.0, 62068.0, 62068.8, 62070.0, 62070.72, 62072.0, 62073.0, 62072.64, 62075.0, 62075.84, 62075.26, 62076.0, 62079.0, 62080.0, 62081.0, 62082.0, 62083.07, 62084.0, 62085.0, 62086.0, 62087.0, 62088.0, 62085.57, 62087.43, 62093.0, 62095.0, 62096.0, 62097.0, 62098.28, 62099.0, 62100.0, 62101.0, 62103.0, 62104.0, 62105.0, 62106.0, 62107.0, 62108.8, 62108.28, 62108.0, 62107.5, 62111.0, 62112.36, 62112.0, 62114.0, 62116.52, 62116.0, 62118.0, 62119.0, 62120.0, 62122.0, 62123.0, 62124.0, 62125.0, 62126.0, 62125.92, 62128.0, 62129.0, 62130.0, 62131.92, 62127.0, 62134.0, 62137.84, 62137.0, 62139.0, 62140.0, 62138.0, 62138.88, 62143.0, 62144.0, 62145.0, 62146.0, 62147.0, 62148.0, 62141.0, 62150.0, 62150.4, 62151.0, 62152.0, 62154.2, 62155.0, 62156.0, 62157.0, 62158.0, 324300.0, 62160.0, 62159.0, 62162.0, 62162.64, 62164.0, 62158.53, 62165.0, 62167.0, 62168.75, 62169.0, 62170.92, 62171.0, 62172.0, 62173.0, 62169.6, 62175.0, 62168.0, 62176.0, 62179.0, 62180.0, 62182.0, 62183.0, 62184.0, 62185.0, 62186.0, 62188.0, 62189.0, 62190.0, 62191.0, 62192.0, 62193.28, 62194.0, 62195.0, 62192.4, 62197.0, 62198.0, 62199.0, 62200.0, 62201.0, 62202.0, 62203.44, 62204.0, 62199.72, 62203.0, 62203.18, 62205.0, 62206.0, 62207.0, 62208.0, 62209.4, 62210.0, 62211.0, 62212.8, 62213.0, 62214.0, 62212.0, 62216.0, 62217.0, 62218.44, 62219.0, 62220.0, 62219.82, 62222.0, 62223.0, 62224.0, 62225.0, 62226.0, 62228.0, 62230.0, 62229.21, 62232.0, 62233.0, 62234.0, 62235.0, 62236.0, 62231.0, 62238.0, 62239.0, 62240.0, 62241.0, 62243.16, 62244.0, 62245.0, 62246.0, 324388.0, 62248.0, 62250.0, 62253.82, 62254.0, 62255.0, 324400.0, 62256.0, 62258.0, 62259.0, 62260.0, 62261.0, 62262.0, 62263.0, 62264.0, 62266.12, 62268.0, 62270.0, 62271.0, 62272.0, 586560.0, 62274.0, 62275.0, 62276.0, 62270.82, 62278.0, 62273.0, 62280.0, 62281.0, 62282.0, 62283.0, 62282.36, 62286.0, 62289.0, 62290.0, 62291.0, 62292.0, 62294.0, 62295.0, 62296.0, 62297.0, 62298.0, 62296.04, 62300.0, 62300.68, 62301.0, 62303.0, 62304.0, 62302.32, 62306.0, 62307.0, 62308.0, 62310.0, 62311.0, 62311.8, 62311.32, 62311.56, 62315.0, 62316.0, 62315.28, 62316.6, 62320.0, 62322.0, 62323.0, 62324.0, 62325.42, 62326.0, 62327.0, 62328.0, 62325.0, 62330.0, 62331.0, 62331.87, 62333.0, 62334.0, 62335.0, 62336.0, 62337.0, 62338.0, 62339.0, 62340.0, 62341.0, 62342.41, 62342.0, 62342.4, 324480.0, 62345.0, 62344.0, 62347.2, 62349.92, 62347.0, 62351.59, 62349.0, 62350.0, 62351.0, 62352.0, 62353.0, 62354.0, 62355.0, 62359.12, 62356.0, 62357.0, 62358.4, 62358.41, 62360.0, 62358.0, 62362.0, 62360.28, 62364.0, 62365.9, 62370.0, 62366.0, 62365.0, 62369.96, 62371.0, 62366.4, 62373.24, 62374.8, 62373.0, 62376.0, 62377.0, 62379.0, 62380.0, 62375.0, 62383.0, 62384.0, 62385.0, 62385.24, 62387.0, 62388.0, 62389.0, 62387.97, 62385.89, 62390.0, 62393.0, 62392.0, 62395.0, 62395.2, 62397.83, 62396.4, 62399.0, 62400.0, 62402.63, 62403.0, 62402.4, 62401.0, 324550.0, 62407.0, 62408.0, 62409.0, 62410.0, 62411.0, 62412.0, 62413.0, 62415.0, 62417.0, 62418.0, 62420.0, 62421.0, 62420.8, 62423.93, 62424.0, 62425.18, 62425.0, 62427.0, 62427.41, 62430.0, 62431.2, 62431.0, 62433.0, 62434.0, 62435.0, 62436.0, 1111011.0, 62438.0, 62440.74, 62441.0, 62442.0, 62440.8, 62444.76, 62445.0, 62446.0, 62440.0, 62448.0, 62449.0, 62450.0, 62451.0, 62452.0, 62444.0, 62454.0, 62455.0, 62456.34, 62457.0, 62458.0, 62459.0, 62460.0, 62456.0, 62462.0, 62459.04, 62465.0, 62466.0, 62467.0, 62468.0, 62469.0, 62469.11, 62471.04, 62472.0, 62473.0, 62474.0, 62475.0, 62476.0, 62477.0, 62472.45, 62478.0, 62480.0, 62478.48, 62482.61, 62483.04, 62484.0, 62483.0, 62486.84, 62487.0, 62482.0, 62489.0, 62490.0, 62491.0, 62492.0, 62494.0, 62495.0, 62496.0, 62497.0, 62498.0, 62498.21, 62499.0, 62500.0, 62501.0, 62497.09, 62504.94, 62502.0, 62504.0, 62503.0, 62506.0, 62505.0, 62508.0, 62511.8, 62508.15, 62510.0, 62511.0, 62512.21, 62512.0, 62514.0, 62515.0, 62516.0, 62517.0, 62518.0, 62519.0, 62520.0, 62521.0, 62522.0, 62518.21, 62524.0, 62525.0, 62529.35, 62530.0, 62531.0, 62532.0, 62533.0, 62534.0, 62535.0, 62536.0, 62537.0, 62535.1, 62539.0, 62540.0, 62540.76, 62542.41, 62543.0, 62544.0, 62542.0, 62546.0, 62547.0, 62548.0, 62545.0, 62550.0, 62551.0, 62552.0, 62553.0, 62549.0, 62554.0, 62555.0, 62556.0, 62557.0, 62558.0, 62559.0, 62560.0, 62561.0, 62561.42, 62563.0, 62563.08, 62566.0, 62567.0, 62568.0, 62569.0, 62570.0, 62565.0, 62573.0, 62575.0, 62576.0, 62577.0, 62578.0, 62579.89, 62579.0, 62580.0, 62581.0, 62582.0, 62583.0, 62584.0, 62585.0, 62586.9, 62586.36, 62587.0, 62589.0, 62590.0, 62587.2, 62592.0, 62594.0, 62594.64, 62596.0, 62597.08, 62594.04, 62595.0, 62599.0, 62600.0, 62602.44, 62601.38, 62603.0, 62605.87, 62598.0, 62604.0, 62606.0, 62607.0, 62608.0, 62611.6, 62609.0, 62610.0, 324755.84, 62612.0, 62613.0, 62614.0, 62616.0, 62615.0, 62618.8, 62619.96, 62620.0, 62621.0, 62622.0, 62623.0, 62624.0, 62621.4, 62625.0, 62627.0, 62628.0, 62628.28, 62628.8, 62629.0, 62626.0, 62635.32, 62636.0, 62637.0, 62640.0, 62641.0, 62642.0, 62643.0, 62644.0, 62645.0, 62646.0, 62647.0, 62648.64, 62649.0, 62650.0, 62652.0, 62654.0, 62656.0, 62658.0, 62659.0, 62660.0, 62662.92, 62663.0, 62664.71, 62665.0, 62664.0, 62662.0, 62668.0, 62669.0, 62670.8, 62670.0, 62672.0, 62673.0, 62671.0, 62675.0, 62676.0, 849109.0, 62677.0, 62679.0, 62680.0, 62680.92, 62682.0, 62682.81, 62684.0, 62684.57, 62686.0, 62687.0, 62681.0, 62688.0, 62689.0, 62691.0, 62692.0, 62693.0, 62694.0, 62695.0, 62694.62, 62691.2, 62698.08, 62690.88, 62700.0, 62699.0, 62696.0, 62704.0, 62707.2, 62708.0, 62710.0, 62711.0, 62712.0, 587000.0, 62714.0, 62715.0, 62716.68, 62718.0, 62720.0, 62721.0, 62722.4, 62723.0, 62722.0, 62725.0, 62724.0, 62727.0, 62726.4, 62728.0, 62730.0, 62731.0, 62731.5, 62732.0, 62733.0, 62735.0, 62736.0, 62736.6, 62738.0, 62739.0, 62740.0, 62741.0, 62742.0, 62741.85, 62744.0, 62745.0, 62748.0, 62749.0, 62750.0, 62752.0, 62753.0, 62754.0, 62758.11, 62759.0, 62760.0, 62758.0, 62762.0, 62763.0, 62764.0, 62767.0, 62768.0, 62767.93, 62770.0, 62769.73, 62772.0, 62773.32, 62774.0, 62775.0, 62776.0, 62776.76, 62774.4, 62779.0, 62780.0, 62780.64, 62778.0, 62783.0, 62784.0, 62785.0, 62783.84, 62786.0, 62788.0, 62789.63, 62790.0, 62789.0, 62791.0, 62794.0, 62796.0, 62799.0, 62800.0, 62801.0, 62803.3, 62804.0, 62805.0, 62806.0, 62807.64, 62808.0, 62809.0, 62810.0, 62804.76, 62812.0, 62813.0, 62814.24, 62815.0, 9500000.0, 62816.0, 62818.0, 62818.92, 62820.0, 62821.0, 62822.0, 62823.0, 62826.0, 62828.0, 62830.0, 62832.0, 62833.78, 62834.0, 62835.4, 62836.0, 62836.8, 62837.23, 62839.0, 62840.0, 62833.47, 62833.0, 62843.0, 62844.38, 62844.0, 62845.0, 62847.0, 62848.0, 62848.92, 62850.0, 62851.0, 324996.0, 62853.0, 325111.0, 62855.0, 325000.0, 62856.0, 62856.04, 62855.44, 62857.0, 62861.73, 62860.0, 62863.92, 62863.0, 62864.0, 62866.0, 62863.83, 62863.07, 62867.0, 62868.0, 62869.94, 62870.0, 62871.0, 62872.0, 62873.0, 62875.0, 62876.0, 62877.0, 62878.4, 62879.0, 62880.0, 62881.92, 62878.0, 62881.0, 62884.0, 62885.0, 62886.0, 62886.82, 62889.24, 62888.0, 62889.0, 62890.0, 62891.0, 62892.0, 62895.38, 62893.0, 62897.0, 62894.0, 62893.2, 62895.0, 62897.12, 62902.0, 62898.0, 62899.0, 62900.0, 62901.0, 62901.5, 62903.0, 62904.0, 62905.0, 62907.0, 62907.36, 62908.0, 62910.72, 62910.0, 62912.0, 62912.38, 62914.0, 62915.0, 62916.0, 62916.6, 62918.0, 62920.0, 62922.0, 62925.0, 62925.96, 62927.54, 62928.0, 62929.0, 62930.0, 62931.0, 62932.0, 62933.64, 62933.0, 62935.0, 62934.0, 62933.82, 62939.0, 62940.0, 62941.0, 62943.0, 62943.54, 62945.0, 62946.0, 62947.0, 62948.0, 62944.0, 62950.0, 62952.0, 62953.56, 62954.0, 62955.0, 325100.0, 62956.8, 62958.0, 62956.0, 62960.0, 62961.0, 62962.0, 62956.29, 62964.0, 62959.0, 62966.0, 62967.13, 62961.6, 62968.0, 62969.0, 62970.0, 62971.68, 62972.0, 62973.0, 62975.0, 62976.0, 62978.0, 62979.0, 62981.0, 325126.0, 62983.0, 62984.0, 62985.0, 62986.0, 62982.0, 62988.0, 62982.96, 62990.0, 62991.0, 62992.0, 62993.0, 62994.0, 62995.64, 62996.0, 62995.0, 62998.0, 62999.0, 63000.0, 325144.0, 62997.0, 63003.0, 63004.0, 63003.2, 63006.0, 325152.0, 63009.0, 63010.0, 63011.0, 63012.0, 63013.0, 63015.0, 63016.0, 63018.0, 325162.08, 63020.0, 63020.92, 63023.0, 63024.0, 63025.0, 63026.0, 63027.0, 63028.14, 63029.0, 58514.8, 63031.0, 63034.0, 63035.0, 63036.0, 63037.0, 63038.0, 63035.65, 63040.0, 63034.12, 63041.0, 63043.0, 63044.8, 63045.0, 63047.0, 63048.0, 63049.0, 63050.0, 63052.61, 63052.0, 63054.0, 63055.0, 63056.0, 63057.0, 63058.0, 63059.0, 63059.4, 63062.0, 63063.0, 63064.0, 63065.0, 63066.18, 63067.0, 63068.0, 63071.0, 63072.0, 63073.0, 63074.0, 63075.0, 63076.0, 63078.0, 63079.0, 63080.0, 63080.64, 63078.48, 63083.0, 63084.0, 63085.0, 63086.4, 63086.0, 63079.55, 63089.0, 63090.0, 63091.0, 63093.0, 63094.0, 63095.0, 63096.0, 63096.96, 63098.0, 63099.0, 63100.0, 63098.24, 63102.0, 63103.0, 63104.04, 63105.0, 63106.0, 63104.0, 63107.0, 63109.0, 63110.0, 63111.0, 63109.44, 63113.0, 63112.48, 63114.0, 63115.0, 63116.0, 63117.24, 63118.95, 63119.0, 63120.0, 63117.0, 63121.0, 63123.0, 63119.04, 63126.0, 63125.0, 63124.0, 63129.36, 63127.0, 63128.0, 63129.0, 63130.0, 63132.0, 63133.0, 63129.6, 63137.0, 63135.0, 325280.0, 63139.0, 63140.0, 63141.0, 63142.0, 63143.0, 63144.0, 63146.0, 63138.0, 63148.0, 63149.0, 63150.0, 63149.68, 63153.0, 63154.0, 63155.0, 63156.0, 63157.0, 63158.0, 63159.0, 63160.0, 63161.0, 63157.44, 63161.28, 63164.48, 63163.0, 63166.0, 63164.88, 63167.19, 63168.0, 63170.0, 63169.6, 63169.0, 63171.13, 63172.0, 63173.0, 63175.0, 63176.0, 63177.0, 63175.25, 63174.0, 63180.0, 63181.0, 63184.0, 63185.0, 63185.43, 63187.2, 63187.0, 63188.0, 63185.13, 63190.8, 63192.6, 63191.0, 63192.0, 63190.0, 63193.0, 63197.42, 63195.0, 63196.0, 63200.74, 63197.0, 63199.0, 63200.0, 63201.0, 63202.0, 63203.0, 63204.0, 63205.0, 63206.0, 63209.0, 63210.0, 63211.0, 63212.0, 63210.39, 63214.0, 63215.0, 63216.0, 63211.32, 63219.0, 63220.0, 63218.0, 63222.0, 63223.0, 63223.32, 63224.0, 63225.6, 63225.0, 63227.0, 63228.0, 63226.63, 63230.0, 63232.0, 63233.0, 63234.48, 63235.0, 63233.64, 63234.0, 63238.0, 63235.92, 63240.0, 63238.5, 63242.0, 63239.8, 63237.0, 63245.0, 63245.52, 63247.0, 63248.0, 63249.0, 63250.0, 63251.0, 63252.0, 63252.96, 63254.0, 63253.0, 63256.0, 63257.0, 63257.26, 63259.0, 63260.0, 63261.0, 63261.74, 63263.0, 63264.0, 63265.0, 63266.0, 63267.0, 63261.12, 63268.9, 63270.0, 63271.0, 63272.0, 63273.0, 63266.58, 63275.0, 63276.0, 63277.22, 63273.6, 63279.0, 63280.0, 63281.0, 63283.8, 63283.0, 63285.0, 63284.0, 63287.0, 63288.0, 63289.0, 63290.0, 63286.0, 63293.0, 63294.0, 63295.0, 63296.0, 63297.76, 63297.0, 63299.3, 63300.0, 63301.0, 63302.0, 63304.0, 63305.0, 63307.0, 63308.0, 63309.0, 63310.0, 63312.0, 63313.0, 63315.0, 63316.0, 63319.0, 63320.0, 63319.48, 63322.0, 63323.0, 63324.0, 63325.0, 63326.4, 63327.0, 63327.01, 63328.0, 63330.0, 63331.0, 63332.0, 63333.0, 63334.0, 63331.03, 63336.0, 63337.0, 63335.0, 63340.0, 63341.0, 63344.0, 63345.0, 63347.0, 63348.0, 63350.0, 63353.0, 63354.0, 63353.42, 325500.0, 63356.0, 63357.0, 63359.0, 63356.88, 63360.0, 63362.91, 63363.0, 63361.0, 63362.0, 63358.0, 63364.32, 63365.0, 63364.0, 63366.0, 63368.88, 63369.0, 63370.0, 63368.0, 63372.0, 63373.0, 63374.0, 63375.0, 63376.0, 63377.0, 63378.0, 63382.0, 63379.01, 63380.0, 63382.8, 63383.0, 63384.0, 63386.46, 63386.0, 63388.0, 63389.88, 63390.96, 63390.6, 63391.0, 63386.04, 63395.0, 63396.0, 63398.0, 63399.0, 63400.0, 63398.4, 63401.0, 63402.0, 63404.0, 63405.0, 63405.76, 63407.0, 63408.0, 63409.0, 63406.2, 63410.0, 63411.0, 63412.2, 63413.0, 63414.0, 63415.0, 63416.0, 63412.0, 63419.0, 63420.0, 63421.0, 63422.0, 63423.0, 63424.0, 63425.0, 63426.0, 63427.0, 63428.0, 63425.64, 63431.0, 325575.0, 63432.0, 63433.0, 63435.0, 63436.0, 63437.0, 63438.0, 63440.0, 63441.21, 63443.0, 63444.0, 63445.0, 63446.53, 63447.0, 63448.0, 63449.96, 63450.0, 63451.0, 63452.0, 63453.0, 63446.0, 63454.0, 63455.0, 63456.0, 63457.0, 63458.0, 63459.0, 63460.0, 63461.0, 325604.0, 63464.8, 63464.0, 63466.0, 63467.0, 63465.0, 63468.0, 63470.0, 63471.0, 63472.0, 63473.0, 63474.0, 63475.0, 63476.0, 63477.84, 63477.19, 63479.0, 63480.0, 63481.6, 63482.0, 63478.0, 63483.0, 63485.0, 63487.0, 63488.0, 63489.0, 63491.0, 63492.0, 63495.0, 63496.0, 63498.0, 63499.5, 63500.0, 63499.0, 63502.0, 63504.0, 63506.02, 63507.0, 63508.0, 63508.8, 63510.0, 63506.0, 63515.0, 63516.0, 63517.0, 63516.28, 63519.0, 63520.0, 63521.0, 63518.46, 63523.0, 63524.0, 63525.0, 63525.6, 63523.08, 63528.0, 63529.0, 63530.0, 63531.0, 63531.04, 63533.0, 63534.0, 63536.0, 63537.0, 63539.0, 63540.0, 63539.05, 63542.0, 63543.0, 63542.33, 63544.0, 63541.0, 63546.08, 63548.0, 63549.0, 63550.69, 63546.0, 63548.16, 63547.0, 63550.0, 63555.0, 63550.8, 63551.88, 63552.0, 63553.0, 63551.0, 63556.0, 63554.0, 63554.4, 63559.0, 63560.0, 63563.0, 63564.8, 63565.0, 63564.0, 63562.12, 850000.0, 63568.0, 63570.0, 63573.0, 63574.0, 63576.0, 63576.32, 63578.0, 63577.41, 63578.16, 63579.0, 63582.88, 63580.0, 63581.0, 63577.0, 63583.0, 63584.0, 63585.0, 63587.0, 63588.0, 63587.64, 63590.0, 63591.0, 63592.0, 63593.0, 63591.06, 63595.0, 63596.0, 63597.0, 63598.0, 63599.0, 63600.0, 63594.0, 63601.0, 63602.0, 63605.0, 63606.0, 63607.0, 63608.0, 63609.72, 63610.0, 63612.0, 63620.0, 63621.0, 63622.0, 63623.0, 63624.0, 63625.44, 63625.0, 63626.0, 63627.0, 63627.49, 63625.28, 63630.0, 63629.8, 63633.0, 63634.0, 63636.0, 63636.56, 63640.0, 63642.0, 63643.0, 63642.88, 63645.0, 63646.0, 63647.0, 63648.0, 63649.0, 63649.5, 63650.0, 63649.04, 63652.0, 63653.75, 63654.0, 63655.0, 63656.0, 63653.99, 63658.0, 63659.88, 63660.0, 63659.0, 63663.0, 63664.0, 63665.0, 63666.0, 63667.0, 63668.0, 63669.0, 63670.0, 63672.0, 63673.0, 63674.0, 63675.0, 63676.0, 63678.0, 63679.0, 63680.0, 63681.35, 63682.0, 63683.0, 63684.0, 63685.0, 63686.0, 63681.0, 63688.0, 63689.0, 63690.0, 63691.0, 63692.0, 63693.0, 63694.15, 63695.0, 63696.0, 63694.0, 63697.2, 63699.83, 63700.0, 63697.0, 63704.38, 63705.0, 63706.0, 63704.0, 63708.0, 63710.0, 63710.4, 588000.0, 63712.0, 63714.0, 63715.0, 63711.0, 63717.0, 63719.0, 63720.0, 63722.0, 63723.0, 63724.0, 63725.0, 63728.0, 63729.0, 63730.0, 63731.2, 63732.0, 63731.0, 63733.0, 63735.0, 325879.0, 325872.8, 63738.0, 63739.75, 63740.0, 63741.0, 63742.0, 63734.0, 63744.0, 63745.0, 63743.0, 63748.02, 63749.0, 63750.0, 63751.0, 63748.0, 63753.0, 63752.0, 63754.08, 63749.28, 63756.0, 63757.0, 63755.0, 63760.0, 63761.0, 63762.0, 63762.81, 63756.94, 63765.0, 63767.0, 63768.0, 63767.5, 63770.0, 63771.0, 63772.0, 63768.12, 63767.06, 63775.0, 63776.0, 63777.0, 63779.0, 63780.0, 63782.0, 63783.0, 63786.0, 63787.0, 63790.0, 63790.33, 63792.0, 63793.0, 63793.6, 63797.0, 63798.0, 63800.0, 63801.0, 63800.76, 63803.0, 63804.0, 63805.0, 63806.0, 63808.0, 63809.0, 63810.0, 63811.0, 63814.0, 63815.0, 63816.0, 63814.4, 63819.0, 63820.0, 63821.0, 63823.0, 63827.0, 63828.0, 63829.0, 63830.0, 63832.0, 63834.0, 63835.0, 63834.24, 63837.0, 63839.0, 63840.0, 63843.0, 63844.0, 63847.52, 63848.0, 63847.0, 63850.0, 63849.62, 63852.0, 63854.0, 63855.84, 326000.0, 63856.0, 63858.0, 63859.0, 63860.0, 63859.95, 63860.02, 63863.0, 63864.0, 63865.0, 63861.0, 63862.0, 63870.0, 63872.0, 63874.0, 63875.0, 63876.0, 63877.0, 63877.8, 63878.0, 63880.02, 63880.0, 63881.0, 63883.2, 63884.0, 63882.0, 63885.12, 63885.0, 63887.0, 63888.0, 63886.0, 63890.0, 7141778.0, 63892.0, 63893.0, 63894.0, 63896.0, 63897.0, 63898.0, 63899.0, 63900.0, 63901.0, 63904.0, 63904.41, 63906.0, 63906.34, 63908.0, 63909.0, 63910.08, 63911.12, 63910.0, 63912.0, 63913.44, 63915.0, 63913.0, 63917.0, 63919.2, 63920.0, 63919.89, 63923.0, 63923.04, 63924.0, 63925.0, 63926.0, 63927.0, 63929.92, 63929.0, 63930.0, 63928.8, 63932.0, 63931.0, 63928.0, 63935.0, 63936.0, 63937.0, 63938.0, 63939.0, 63944.0, 63945.0, 63946.0, 63944.16, 63948.0, 63949.0, 63950.0, 63949.04, 63952.52, 63952.0, 63955.0, 63958.0, 63959.0, 63960.0, 63961.45, 63962.24, 63963.75, 63964.0, 63963.0, 63962.0, 63965.0, 63966.0, 63967.33, 63968.0, 63969.0, 63970.0, 63971.0, 63972.0, 63973.0, 63967.0, 63975.0, 63975.84, 63977.0, 63979.0, 63980.28, 63980.0, 63983.92, 63984.0, 63980.8, 63985.0, 63988.0, 63990.0, 63991.0, 63992.52, 63994.88, 63995.0, 63996.0, 63999.0, 64000.0, 64001.0, 63999.6, 64003.0, 64004.0, 64004.36, 64006.68, 64006.0, 64008.0, 64009.0, 64010.0, 64011.0, 64013.94, 64014.0, 64015.0, 64016.0, 64017.0, 64020.0, 64021.22, 64022.0, 64023.6, 64023.0, 64025.0, 64022.33, 64027.0, 64028.0, 64029.0, 64030.0, 64031.0, 64032.84, 64032.6, 64033.0, 64032.0, 64036.0, 64039.0, 64040.0, 64041.0, 64042.0, 64043.32, 64044.0, 64043.28, 64046.0, 64045.2, 64049.0, 64050.0, 64051.0, 64052.0, 64053.0, 64054.0, 64055.0, 64056.0, 64050.48, 64059.0, 64060.0, 64061.0, 64062.0, 64062.84, 64064.0, 64064.88, 64066.4, 64069.0, 64070.0, 64070.14, 64072.0, 64070.28, 64071.0, 64076.0, 64077.0, 64078.0, 64080.0, 64082.0, 64083.0, 64084.8, 64085.0, 64086.0, 64087.0, 64084.0, 64089.0, 64091.0, 64092.0, 64093.0, 64095.0, 64097.0, 64099.56, 64099.0, 64100.0, 64101.0, 64102.0, 64103.0, 64104.0, 64105.0, 64106.0, 64107.0, 64108.0, 64110.0, 64111.0, 64112.0, 64113.48, 64114.0, 64115.0, 64116.0, 64117.0, 64118.0, 64119.45, 64119.0, 64120.0, 64115.52, 64122.0, 64116.52, 64124.0, 64125.0, 64126.0, 588415.0, 64128.0, 64130.0, 64127.0, 64132.0, 64136.0, 64137.27, 64138.0, 64137.1, 64140.0, 64141.0, 64136.79, 64139.0, 64144.0, 64145.0, 64146.14, 64147.0, 64148.0, 64144.08, 64150.0, 64151.0, 64152.0, 64146.0, 64154.0, 64155.0, 64156.0, 64157.0, 64158.0, 64159.0, 64160.0, 64162.0, 64164.0, 64166.0, 64168.0, 64170.0, 64172.0, 64172.72, 64174.0, 64175.0, 64176.0, 64178.0, 64179.0, 64180.0, 64184.0, 64185.0, 64186.78, 64187.0, 64188.8, 64186.61, 64188.0, 64191.0, 64192.0, 64194.0, 64195.0, 64196.23, 64197.0, 64199.0, 64200.0, 64202.0, 64204.0, 64204.44, 64206.0, 64208.0, 64209.0, 64210.0, 64211.0, 64212.0, 64213.0, 64214.0, 64215.0, 64216.0, 64219.0, 64220.0, 64221.5, 64222.0, 64223.0, 64224.0, 64225.92, 64225.0, 64227.0, 64228.32, 64229.0, 64230.0, 64230.4, 64232.0, 64235.0, 64236.0, 64238.0, 64240.0, 64241.0, 64242.0, 64243.68, 64244.0, 64242.36, 64246.0, 64247.0, 64245.0, 64248.0, 64249.0, 64251.2, 64252.0, 64250.0, 64251.0, 64251.28, 64254.0, 64255.0, 326400.0, 64258.0, 64259.0, 64260.0, 64256.0, 64262.4, 64263.0, 64262.0, 64261.0, 64266.42, 64267.28, 64267.0, 64270.0, 64271.0, 64272.0, 64273.0, 64274.0, 64275.0, 64277.88, 64278.24, 64280.0, 64282.0, 64284.0, 64285.0, 64286.0, 64287.72, 64287.0, 64289.16, 64288.0, 64290.48, 64290.0, 64290.72, 64293.0, 64293.52, 64296.0, 64291.61, 64298.0, 64299.0, 64300.0, 64298.28, 64302.0, 64303.0, 64296.72, 64306.0, 64307.2, 64308.9, 64308.0, 64310.0, 64310.19, 64312.0, 64313.0, 64311.0, 64313.6, 64314.0, 64315.0, 64316.0, 64317.0, 64317.57, 64319.0, 64320.0, 64318.0, 64322.0, 64323.0, 64324.0, 64325.0, 64326.0, 64327.0, 64328.0, 64330.0, 64329.0, 64332.0, 64331.0, 64334.0, 850766.0, 64336.0, 64338.0, 64339.0, 64340.0, 64343.0, 64344.0, 64346.0, 64347.0, 64348.0, 64349.0, 64350.28, 64350.0, 64351.0, 64353.0, 64354.0, 64356.0, 64357.0, 64358.0, 64359.0, 326500.0, 64361.0, 64360.0, 64363.21, 64364.0, 64365.0, 64366.0, 64367.0, 64368.0, 64370.0, 64371.0, 64374.0, 64375.0, 64375.92, 64377.0, 64376.0, 64379.9, 64380.0, 64379.13, 64382.0, 64381.0, 64384.0, 64384.76, 64387.0, 64388.0, 64389.0, 64390.0, 64392.0, 64395.0, 64396.0, 64397.0, 64398.0, 64399.0, 64400.0, 64400.4, 64402.0, 64403.0, 64404.0, 64402.7, 64410.0, 64411.0, 64412.0, 588700.0, 64413.0, 64414.0, 64415.0, 64416.0, 64417.0, 64418.0, 64417.6, 64419.0, 64421.0, 64420.0, 64424.0, 64425.0, 64422.09, 64422.0, 64428.0, 64432.0, 64434.0, 64435.0, 64436.0, 64438.0, 64438.4, 64440.0, 64442.0, 64444.0, 64445.0, 64447.0, 64449.0, 64450.0, 64449.68, 64452.0, 64452.5, 64454.0, 64453.0, 64456.0, 64457.0, 64458.0, 64459.0, 64460.0, 64459.2, 64464.0, 64465.44, 64465.0, 64468.0, 64470.0, 64471.0, 64472.0, 64474.0, 64475.0, 64476.0, 64476.64, 64477.0, 64477.2, 64480.0, 64483.68, 64484.0, 64485.0, 64486.0, 64483.0, 64488.0, 64487.0, 64490.92, 64490.0, 64486.2, 64491.96, 64489.0, 64495.0, 64494.0, 64497.0, 64498.0, 64500.0, 64501.0, 64502.0, 64504.0, 64505.0, 64506.0, 64508.0, 64509.0, 64510.0, 64511.0, 64512.0, 64513.0, 64514.0, 64515.0, 64518.0, 64519.0, 64520.0, 64521.0, 326667.0, 64523.0, 64525.0, 64524.0, 64525.04, 64528.0, 64529.0, 64528.92, 64532.0, 64534.0, 64536.0, 64538.0, 64539.49, 64540.0, 64539.0, 64542.0, 64543.0, 64541.0, 64545.0, 64540.32, 64542.4, 64548.0, 64545.48, 64550.0, 64552.0, 64553.83, 64553.0, 64555.0, 326700.0, 64557.0, 64559.0, 64560.0, 64562.0, 64563.0, 64563.2, 64564.0, 64566.17, 64565.0, 64566.0, 64567.05, 64568.0, 64567.0, 64570.0, 64568.28, 64572.0, 64573.0, 64574.0, 64575.0, 64576.0, 64577.0, 64578.0, 64580.0, 64580.98, 64582.0, 64583.0, 64584.0, 64585.0, 64587.0, 64589.32, 64590.0, 64589.0, 64592.0, 64593.0, 64594.0, 64595.0, 64596.0, 64598.0, 64599.0, 64600.0, 64601.0, 64598.36, 64604.0, 64605.0, 64604.51, 64604.76, 64608.0, 64609.0, 64608.18, 64611.96, 64612.0, 64611.0, 64614.0, 64615.44, 64615.0, 64615.71, 64617.0, 64619.23, 64620.0, 64620.37, 64621.0, 64622.0, 64624.0, 64625.0, 64619.0, 64627.0, 64628.0, 64629.0, 64631.0, 64632.0, 64633.4, 64634.0, 64635.0, 64636.0, 64633.0, 64638.0, 64639.0, 64640.0, 64641.0, 64642.0, 64643.0, 64639.42, 64644.0, 64646.0, 64647.0, 64649.0, 64650.0, 64651.0, 64652.0, 64653.0, 64654.0, 64655.0, 64656.0, 64653.72, 64658.0, 64651.92, 64660.0, 64955.26, 64664.0, 64666.0, 64667.0, 64668.0, 64669.0, 64674.06, 64675.0, 64676.0, 64677.0, 64677.86, 64679.0, 64680.0, 64681.0, 64682.0, 64683.0, 64686.0, 64687.0, 64688.0, 64690.0, 64691.0, 64692.0, 64693.0, 64694.0, 64695.0, 64696.0, 64697.0, 64698.0, 64694.5, 64700.0, 64701.0, 64699.0, 64703.0, 64704.99, 64704.0, 64706.0, 64707.0, 64708.0, 64709.0, 64710.0, 64711.0, 64712.0, 64713.0, 64714.0, 64714.88, 64716.0, 64712.74, 64717.0, 64719.0, 64720.0, 64721.0, 64717.56, 64723.0, 64725.0, 64726.94, 64727.0, 64725.12, 64726.0, 64728.0, 64729.18, 64730.0, 64731.0, 64733.0, 64728.48, 64735.0, 64738.0, 64739.0, 64740.0, 64741.0, 64738.2, 64743.0, 64744.0, 64746.0, 64747.0, 64749.0, 64750.0, 64751.0, 64752.0, 64753.0, 64752.68, 64750.4, 64756.0, 64756.62, 64755.0, 64759.0, 64760.0, 64761.0, 64762.0, 64764.0, 64765.0, 64766.0, 64767.78, 64770.0, 64771.8, 64772.16, 64771.0, 64774.0, 64774.68, 64772.0, 64777.55, 64773.78, 64771.2, 64775.0, 64776.0, 64777.0, 64778.0, 64779.0, 64780.0, 64781.0, 64782.0, 64783.0, 64784.0, 64784.72, 64785.0, 64788.0, 64789.0, 64790.0, 64792.0, 64794.0, 64795.0, 64793.0, 64797.0, 64798.0, 64799.0, 64800.0, 64801.0, 64802.0, 64803.0, 64804.0, 64805.0, 64807.0, 64808.0, 64809.0, 64810.0, 64811.0, 64812.8, 64812.0, 64817.0, 64818.0, 64819.2, 64819.0, 64820.0, 64821.0, 64818.24, 64824.0, 64825.0, 64826.0, 64822.0, 64828.0, 64829.0, 64830.0, 64830.48, 64832.0, 64833.95, 64834.0, 64835.0, 64836.0, 64833.0, 64838.0, 64837.0, 64840.0, 64841.0, 64842.0, 64844.0, 64845.0, 64844.48, 64848.0, 64849.0, 64850.0, 64852.0, 64854.0, 327000.0, 64857.0, 64858.0, 64856.0, 64860.0, 64863.0, 64864.0, 64865.0, 64865.22, 64867.0, 64869.0, 64870.0, 64872.0, 64873.0, 64874.0, 64875.0, 64876.0, 64880.0, 64881.0, 64882.0, 64883.0, 64884.0, 64885.0, 64886.0, 64888.0, 64890.0, 64890.24, 64892.0, 64893.0, 64895.0, 64896.0, 64899.0, 64900.0, 64901.04, 64902.0, 64904.0, 64906.0, 64908.0, 64910.0, 64912.0, 64913.0, 64916.88, 64916.0, 64917.0, 64918.0, 64919.05, 64920.0, 64921.0, 64922.0, 64924.0, 64925.0, 64928.0, 64928.46, 64930.0, 64931.0, 64932.0, 64930.21, 64934.0, 65009.88, 64936.0, 64937.0, 64938.0, 64939.0, 64940.0, 64942.0, 64944.0, 64945.0, 64947.0, 64947.81, 64949.0, 64950.0, 64951.0, 64952.0, 64953.0, 64954.2, 64955.0, 64956.0, 64956.4, 64958.0, 64955.28, 64960.0, 64959.0, 64962.0, 64962.97, 64963.0, 64964.0, 64966.0, 64962.12, 64968.0, 64970.88, 64971.0, 64970.0, 64972.2, 64972.0, 64973.0, 64975.0, 64976.0, 64974.0, 64978.0, 64979.0, 64980.0, 64983.0, 64985.0, 64986.0, 64987.0, 64988.0, 64989.0, 64990.0, 64992.92, 1900000.0, 64992.0, 64993.0, 64995.0, 64997.0, 64998.0, 64999.92, 64999.64, 64999.98, 65000.0, 65003.0, 65001.0, 65005.0, 65000.32, 64999.0, 65004.0, 65002.0, 65006.0, 65007.0, 65008.0, 65009.0, 65010.0, 65011.0, 65012.0, 1900021.0, 65015.0, 65016.0, 65019.0, 65020.0, 65022.42, 65024.0, 65025.0, 65026.0, 65027.0, 65028.0, 65025.72, 65030.0, 65031.0, 65032.0, 65033.0, 65035.0, 65037.0, 65039.0, 65040.0, 65041.0, 65042.0, 65039.64, 65045.0, 65046.0, 65050.0, 65052.0, 65055.0, 65056.0, 65057.0, 65059.0, 65061.0, 65062.4, 65062.0, 65064.0, 65065.0, 65068.0, 65069.0, 65071.0, 65072.0, 65074.8, 65075.0, 65076.0, 65077.0, 65078.0, 65079.0, 65080.0, 65081.0, 65082.0, 65083.0, 65084.0, 65083.2, 65085.0, 65086.8, 65087.0, 65088.0, 65089.0, 65090.0, 65087.36, 65092.0, 65093.0, 65095.0, 65096.0, 65097.0, 65098.0, 65099.0, 65100.0, 65101.0, 65102.0, 65101.71, 65104.0, 65105.0, 65108.0, 65109.0, 65110.0, 65111.0, 65112.6, 65112.0, 65111.29, 65115.0, 65117.0, 65118.0, 65119.0, 65120.0, 65121.0, 65124.0, 65125.0, 65126.0, 65124.8, 65128.0, 65129.0, 65130.0, 65131.0, 65132.0, 65133.0, 65129.07, 65135.0, 65136.0, 65137.68, 65137.0, 65136.6, 65140.0, 65144.75, 65146.0, 65147.34, 65147.0, 65150.0, 65151.0, 65152.0, 65154.0, 65156.76, 65156.0, 65158.92, 65159.0, 65160.0, 65161.0, 65162.0, 65164.0, 65164.92, 65166.4, 65167.0, 65166.0, 65169.0, 65170.0, 65171.52, 65172.0, 65173.0, 65168.54, 65175.6, 65175.0, 65177.0, 65178.0, 65179.0, 65180.0, 65176.0, 65184.0, 65186.0, 65187.0, 65188.5, 65187.37, 65190.0, 65188.0, 65192.0, 65193.0, 65194.0, 65189.0, 65196.0, 65195.0, 65198.0, 65199.0, 65200.0, 65201.0, 65203.0, 65204.0, 65205.0, 65205.36, 65207.0, 65208.0, 65205.48, 65210.0, 65211.0, 65211.24, 65214.0, 65215.0, 65214.5, 65217.0, 65218.14, 65218.0, 65220.0, 65221.0, 65222.0, 65223.0, 65219.0, 65225.0, 65226.0, 65227.0, 65228.0, 65228.8, 65230.0, 65231.0, 65232.0, 65229.0, 65234.0, 65235.0, 65236.05, 65237.0, 65238.0, 65239.0, 65240.0, 65241.0, 65242.1, 65236.0, 65244.0, 65245.0, 65239.18, 65247.0, 65248.0, 65249.0, 65250.0, 65249.6, 65243.0, 65253.0, 65254.0, 65255.0, 65255.32, 65256.0, 65258.0, 65259.0, 65260.0, 65261.0, 65262.0, 65257.0, 65264.0, 65266.0, 65267.0, 65268.0, 65269.6, 65269.0, 65270.0, 65272.0, 65273.0, 65274.0, 65275.0, 65276.0, 65275.17, 65279.58, 65280.0, 65280.02, 1376000.0, 65281.0, 65286.0, 65287.0, 65288.0, 65289.0, 65290.0, 65291.0, 65292.0, 65293.0, 65294.0, 65295.0, 65294.26, 65298.0, 65299.0, 65300.0, 65303.0, 65303.99, 65305.0, 65304.0, 65307.0, 65307.36, 65309.0, 65310.0, 65311.0, 65312.0, 65307.28, 65314.0, 65308.0, 65316.0, 65315.0, 65318.0, 65319.09, 65318.52, 65320.39, 65321.0, 65322.0, 65321.43, 65324.0, 65325.0, 65323.0, 65327.0, 65328.0, 65330.0, 65331.42, 65332.0, 65331.0, 65333.0, 65332.75, 65332.8, 65338.0, 65339.0, 65340.0, 65341.0, 65343.0, 65344.0, 65345.64, 65345.28, 65345.0, 65347.0, 65349.0, 65350.0, 65351.0, 65352.0, 65353.0, 65354.0, 65355.0, 65353.8, 65357.0, 65358.0, 327495.0, 65356.0, 65363.0, 65364.0, 65365.0, 65366.0, 65368.0, 65369.0, 65370.0, 65372.0, 65373.0, 65374.0, 65373.42, 65375.0, 65376.0, 65378.0, 65379.0, 65380.0, 327525.0, 65382.6, 65383.0, 65382.0, 65385.0, 65386.0, 65388.0, 65390.0, 65391.0, 65392.0, 65393.0, 65394.96, 65395.0, 65395.2, 65397.0, 327542.0, 65393.04, 65400.0, 65394.0, 65402.0, 65403.0, 65405.0, 65407.0, 65408.0, 65408.54, 65410.0, 65411.0, 65412.0, 4784000.0, 65414.0, 65415.0, 65416.0, 65417.79, 65418.0, 65417.28, 65420.0, 65421.0, 65417.0, 65423.0, 65424.0, 65425.85, 65425.0, 65427.0, 65428.0, 65429.16, 65431.0, 65433.0, 65434.0, 65436.0, 65436.8, 65438.0, 65439.0, 65440.0, 65437.0, 65442.0, 65443.0, 65444.0, 65445.0, 65446.0, 65447.0, 65448.0, 65449.0, 65450.0, 65451.42, 65452.0, 65453.0, 65451.0, 65455.0, 65456.23, 65457.0, 65458.0, 65456.0, 65459.0, 327600.0, 65460.0, 65460.96, 65461.0, 65457.6, 65466.0, 65467.0, 65465.4, 65469.0, 65470.0, 65471.0, 65472.0, 65464.0, 65474.78, 65474.21, 65475.0, 65476.0, 65478.0, 65479.0, 65480.59, 65480.0, 65478.4, 65484.0, 65485.0, 65487.0, 65488.0, 65489.0, 65490.0, 65491.0, 65492.0, 65493.0, 65494.0, 65495.0, 65495.6, 65496.0, 65497.0, 65499.0, 65500.0, 65501.0, 65502.0, 65503.06, 65504.0, 65501.28, 65504.89, 65505.0, 65508.0, 65509.0, 65510.0, 65508.04, 65507.0, 65515.0, 65515.37, 65515.44, 65520.0, 65523.0, 65524.0, 65525.0, 65527.0, 65528.0, 65530.0, 65531.0, 65532.0, 65534.0, 65535.0, 65536.0, 65537.0, 65538.2, 65539.0, 65540.0, 65542.0, 65544.0, 65545.0, 65546.0, 65547.0, 65548.0, 65549.0, 65550.0, 65551.0, 65553.0, 65554.0, 65555.0, 65554.45, 65557.0, 65556.0, 65558.0, 65560.0, 65561.0, 65562.0, 65563.0, 65561.46, 65564.0, 65565.0, 65559.0, 65568.0, 852000.0, 65570.0, 65568.48, 65566.16, 65571.0, 65574.0, 65575.0, 65576.0, 65569.0, 65578.0, 65579.0, 65580.0, 65582.0, 65583.0, 65584.0, 65585.0, 65586.0, 65587.0, 65588.0, 65589.0, 65590.0, 65591.0, 65592.0, 65593.0, 65595.0, 65596.0, 65596.88, 65599.0, 65600.0, 65601.94, 65603.0, 65604.0, 65605.0, 65606.0, 65609.0, 65610.0, 65612.0, 65616.0, 65618.0, 65619.84, 65619.0, 65621.8, 65621.0, 65622.0, 2162772.0, 65625.0, 65626.0, 65626.47, 65628.0, 65628.48, 65630.0, 65631.9, 65631.0, 65633.88, 65634.53, 65632.06, 65636.0, 65633.0, 65638.0, 65632.0, 65640.0, 65641.0, 65642.0, 65643.0, 65644.0, 65645.0, 65646.0, 65643.24, 65649.0, 65650.0, 65651.0, 65653.0, 65654.0, 65656.0, 65660.0, 65661.0, 65662.0, 65664.0, 65665.0, 65666.0, 65667.0, 65668.0, 65670.0, 65671.0, 65672.64, 65673.0, 65670.25, 65674.0, 65676.0, 65677.0, 65678.0, 65679.2, 65680.0, 65681.0, 65679.0, 65682.35, 65684.0, 65685.0, 65686.4, 65686.0, 65685.28, 65688.0, 65690.0, 65689.0, 65692.0, 65690.4, 65685.46, 65695.0, 65696.25, 65697.0, 65696.0, 65699.0, 65700.0, 327843.0, 65702.0, 65705.0, 65706.0, 65707.0, 65708.0, 65709.0, 65710.0, 65711.0, 65712.0, 590000.0, 65713.0, 65714.0, 65716.0, 65717.0, 65718.0, 65719.0, 65720.0, 65165.04, 65722.0, 65723.0, 65165.84, 65725.0, 65726.43, 65726.0, 65728.0, 65729.0, 65730.0, 65731.0, 65729.82, 65733.0, 65734.0, 65735.0, 65736.0, 65729.04, 65738.0, 65740.0, 65742.0, 65743.0, 65744.0, 65745.0, 65746.2, 65747.0, 65748.0, 65749.0, 65750.0, 65751.0, 65753.0, 65753.6, 65755.0, 65756.0, 65757.0, 65759.0, 65760.0, 65764.0, 65765.0, 65766.0, 65767.0, 65768.0, 65769.0, 65770.0, 65771.55, 65771.0, 65773.0, 65774.0, 65775.0, 65776.0, 65777.0, 65778.11, 65778.0, 65780.0, 65781.0, 65782.0, 65783.52, 65784.0, 65783.0, 65786.0, 65788.0, 65789.0, 65790.0, 65791.0, 65792.0, 65793.0, 65794.0, 65795.0, 65796.0, 65797.0, 65798.0, 65799.0, 65800.0, 65801.0, 65802.0, 65803.0, 65804.0, 65805.0, 65806.25, 65807.42, 65808.0, 327950.0, 65810.0, 65808.47, 65812.0, 65813.0, 65814.0, 65815.0, 65811.2, 65818.0, 65820.0, 65821.0, 65823.72, 65824.0, 65827.0, 65828.0, 65827.74, 65830.0, 65832.0, 65833.0, 65834.0, 65835.0, 65834.32, 65837.0, 65836.8, 65839.0, 65840.0, 65841.0, 65842.0, 65843.0, 65844.0, 65845.0, 65846.0, 65847.0, 65848.2, 65849.8, 65850.0, 65851.4, 65852.0, 65853.0, 65854.56, 65856.0, 328000.0, 65860.0, 65861.0, 65863.2, 65864.0, 65866.36, 65867.94, 65868.0, 65867.0, 65867.36, 65871.0, 65872.0, 65873.0, 65873.8, 65875.0, 65873.56, 65873.6, 65878.0, 65879.0, 65880.0, 65876.0, 65882.52, 65877.9, 65875.3, 65884.0, 65885.0, 65886.0, 65888.0, 65883.0, 65890.0, 65889.0, 65886.96, 65893.0, 65894.0, 65895.0, 65896.0, 65897.0, 65892.0, 65899.0, 65900.0, 65904.0, 65909.0, 65910.0, 65911.0, 65912.0, 65913.0, 65915.0, 65916.42, 65917.0, 65918.0, 65916.0, 65920.0, 65922.44, 65923.0, 65922.0, 65925.0, 65926.0, 65928.0, 65929.0, 65930.0, 65932.0, 65935.0, 65936.0, 65937.0, 65939.0, 65940.0, 65940.86, 65942.0, 65947.0, 65949.0, 65950.0, 65952.0, 65954.72, 65955.0, 65956.0, 65958.0, 65960.0, 65960.14, 65964.0, 65967.0, 65968.0, 65969.0, 65970.0, 65973.0, 65974.0, 65975.0, 65976.0, 65977.0, 65975.01, 65979.0, 65980.0, 65982.0, 65984.0, 65985.0, 65986.0, 328128.0, 65988.0, 65989.0, 65990.0, 65991.0, 65992.0, 65993.0, 65994.0, 65995.0, 65996.0, 65997.0, 65998.0, 65999.0, 66000.0, 66002.0, 66003.0, 66004.0, 66005.0, 66006.68, 66007.72, 66008.13, 66008.0, 66007.0, 66010.0, 66012.0, 66014.0, 66016.0, 66017.0, 66018.0, 66019.0, 66020.0, 66022.23, 66022.68, 66024.0, 66026.0, 66030.0, 66031.0, 66031.32, 66033.0, 66034.0, 66036.0, 66036.86, 66039.0, 66040.0, 66042.0, 66043.0, 66045.0, 66047.0, 66048.0, 66049.0, 66050.0, 66052.0, 66055.0, 66056.0, 66058.56, 66060.0, 66060.8, 66065.0, 66066.0, 66067.0, 66068.0, 66070.0, 66072.0, 66073.0, 66075.0, 66080.0, 66081.0, 66081.96, 66083.0, 66084.0, 66085.92, 66086.0, 66087.6, 66088.0, 66089.0, 66090.0, 66083.42, 66092.0, 66093.0, 66085.0, 66095.0, 66096.0, 66099.0, 66100.0, 66101.0, 66102.0, 66103.0, 66105.0, 66106.68, 65242.0, 66108.0, 66109.0, 66106.0, 66111.0, 66112.0, 66110.0, 66114.0, 66115.0, 66113.6, 66117.0, 66118.0, 66119.0, 66120.0, 66121.0, 66122.64, 66123.0, 66124.0, 66125.0, 66126.0, 66127.0, 66123.84, 66129.6, 66129.0, 66131.04, 66132.0, 66133.0, 66131.0, 66130.0, 66136.0, 66137.0, 66140.0, 66141.0, 66142.0, 66143.0, 66144.0, 66143.38, 66146.0, 66148.0, 66149.0, 66150.0, 66148.97, 66152.0, 66153.0, 66154.0, 66156.0, 66157.0, 66158.0, 328300.0, 66161.0, 66161.78, 66163.0, 66164.0, 66165.0, 66166.0, 66167.0, 66168.0, 66170.0, 66171.0, 66172.0, 66172.84, 66173.0, 65255.62, 66176.0, 66175.0, 66178.0, 66179.0, 66180.0, 66181.0, 66182.0, 66183.0, 66184.0, 66185.0, 66185.08, 66187.0, 66186.0, 66189.0, 66190.0, 66191.0, 66192.0, 66190.32, 66189.91, 66195.0, 66196.0, 66195.28, 66196.22, 66199.22, 66200.0, 66197.0, 66202.0, 66203.0, 66204.0, 66200.2, 66207.0, 66208.0, 66215.0, 66216.0, 66217.0, 66218.0, 66216.96, 66220.0, 66222.0, 66223.53, 66224.0, 66225.0, 66226.0, 66227.0, 66228.0, 66229.0, 66230.0, 66231.0, 66231.33, 66233.0, 66234.0, 66232.0, 66236.0, 66238.0, 66240.0, 66241.96, 66243.0, 66244.0, 66245.0, 66247.0, 66248.0, 66249.0, 66250.0, 66251.0, 66252.0, 66253.0, 66254.0, 66255.0, 66257.0, 66258.0, 66259.2, 66260.0, 66261.0, 66259.0, 66263.0, 66264.0, 66262.0, 66268.68, 66268.0, 66270.0, 66270.6, 66272.0, 66273.63, 66271.0, 66275.0, 66277.0, 66278.0, 66280.0, 66280.4, 66282.0, 66283.0, 66287.0, 66288.0, 66289.0, 66291.12, 66292.0, 66294.0, 66295.0, 66296.0, 66298.0, 66300.0, 66301.0, 66304.0, 66305.0, 66306.0, 66307.03, 66306.25, 66309.0, 66310.0, 66310.39, 66312.0, 66313.0, 66308.52, 66315.0, 66317.0, 66318.0, 66320.0, 66321.0, 66322.0, 66323.0, 66324.2, 66324.58, 66326.0, 66324.0, 66328.0, 66329.0, 66330.0, 66331.42, 66331.0, 66333.0, 66334.0, 66335.0, 66336.0, 66338.0, 66339.0, 66340.0, 66341.0, 66342.0, 66343.0, 66346.0, 66347.0, 66348.0, 66350.0, 66351.0, 66352.0, 66353.0, 66355.0, 66356.0, 66357.0, 66355.2, 66355.9, 66360.0, 66361.0, 66362.04, 66363.0, 66366.0, 66367.35, 66368.0, 66368.89, 66370.0, 66367.0, 66372.0, 66373.0, 66372.8, 66375.48, 66371.0, 66375.0, 66374.0, 66379.0, 66380.0, 66376.0, 66382.0, 66383.0, 66384.0, 66384.56, 66386.0, 66387.0, 66384.95, 66389.76, 66390.0, 66389.44, 66392.93, 66392.0, 66394.0, 66393.0, 66396.0, 66395.0, 66398.0, 66399.0, 66400.0, 66400.44, 66399.85, 66397.0, 66404.0, 66405.0, 66406.67, 66406.0, 66408.0, 66409.0, 66410.0, 66411.0, 66412.0, 66406.08, 66414.0, 66417.75, 66418.0, 66420.0, 66420.11, 66420.25, 66423.0, 66424.0, 66425.0, 1115000.0, 66428.0, 66429.0, 66430.8, 66430.0, 66432.0, 66433.91, 66431.0, 66433.0, 66436.0, 66437.0, 66438.0, 66439.0, 66435.0, 66440.0, 66442.0, 66443.0, 66444.0, 66445.0, 66447.0, 66447.1, 66449.0, 66450.0, 66453.0, 66454.0, 66455.0, 66456.0, 66457.0, 328600.0, 66459.0, 66458.0, 66461.0, 66462.0, 66462.24, 66464.0, 66465.0, 66466.0, 66467.02, 66467.0, 66468.48, 66470.0, 66469.97, 66472.0, 66473.0, 66468.0, 66475.0, 66476.0, 66472.2, 66479.67, 66480.0, 66481.0, 66482.0, 66483.0, 66479.0, 66485.0, 66486.0, 66487.76, 66488.0, 66484.0, 66490.0, 66491.0, 66492.0, 65319.0, 66494.0, 66495.0, 66496.0, 66497.0, 65320.0, 66493.0, 66500.0, 66492.6, 66502.0, 66503.75, 66504.0, 66505.0, 66506.88, 66502.2, 66508.66, 66509.0, 66510.0, 66511.0, 66508.0, 66513.0, 66512.0, 66515.0, 66516.0, 66516.7, 66518.0, 66519.0, 66520.0, 65323.2, 66523.0, 66524.0, 66525.0, 66526.0, 66528.0, 66531.0, 66534.0, 66535.0, 66536.0, 66539.0, 66540.0, 66541.0, 66540.8, 66543.0, 66544.0, 66540.72, 66546.0, 66547.0, 66548.0, 66542.0, 66550.0, 66551.34, 66552.0, 66550.2, 66553.0, 66555.0, 66556.0, 66557.0, 66558.0, 66551.0, 66560.0, 66561.0, 66562.0, 66563.0, 66564.0, 66565.06, 66566.0, 66567.0, 66565.0, 66569.0, 66570.0, 66571.0, 66572.0, 66573.0, 66574.0, 66566.4, 66576.0, 66577.0, 66580.0, 66582.0, 66583.86, 66583.0, 66583.49, 66584.0, 66588.0, 66589.0, 66590.0, 66593.0, 66594.0, 66596.0, 66599.0, 66600.0, 66601.0, 66601.6, 66603.0, 66604.0, 66605.0, 66606.0, 66607.0, 66608.95, 66611.0, 66612.0, 66613.0, 66614.06, 66613.5, 66616.0, 66617.0, 66618.0, 66618.09, 66620.0, 66621.0, 66622.08, 66622.0, 66624.0, 66625.0, 66623.0, 66628.0, 66630.0, 66631.0, 66633.0, 66634.0, 66635.4, 66636.0, 66637.0, 66640.0, 66641.0, 66642.0, 66643.0, 66644.0, 66645.0, 66646.0, 66644.48, 66648.0, 66649.0, 66650.0, 66652.0, 66653.94, 66654.0, 66653.0, 66656.0, 66657.89, 66658.0, 66658.03, 66660.0, 66659.0, 66655.67, 66663.0, 66664.0, 66665.0, 66666.0, 66667.0, 66668.42, 66669.0, 66670.36, 66670.0, 66672.0, 66673.0, 66670.24, 66675.0, 66674.0, 66678.0, 66680.0, 66681.0, 66682.0, 66684.0, 66685.0, 66689.0, 66690.0, 66693.0, 66695.12, 66696.0, 66697.0, 66695.88, 66699.84, 66700.0, 66701.0, 66702.0, 66703.0, 66696.08, 66705.0, 66709.0, 66710.0, 66711.0, 591000.0, 66713.0, 66714.84, 66714.0, 66716.0, 66712.0, 66718.0, 66713.29, 66720.0, 66717.0, 66723.0, 66724.0, 66726.0, 66726.4, 66730.35, 66731.0, 66732.0, 66733.0, 66730.0, 66735.0, 66736.57, 66737.0, 66738.0, 66736.0, 66742.0, 66743.0, 66744.0, 66744.72, 66746.3, 66747.0, 66749.0, 66750.0, 66752.0, 66754.0, 66755.0, 66756.0, 66755.42, 66758.84, 66759.0, 66758.0, 66760.0, 66762.0, 66763.0, 66761.0, 66765.0, 66766.0, 66767.0, 66768.0, 66767.1, 66768.48, 66770.0, 66775.0, 66775.17, 66777.0, 66779.0, 66780.0, 66781.0, 66784.0, 66785.0, 66786.0, 66788.94, 66788.0, 66790.0, 66791.0, 66791.76, 66796.0, 66797.0, 66798.74, 66798.0, 66800.0, 66801.0, 66799.0, 66796.88, 66804.6, 66805.0, 1115380.0, 66804.0, 66809.0, 66810.9, 66812.0, 66813.0, 66814.0, 66816.0, 66817.0, 66818.0, 66819.84, 66820.0, 66819.0, 66822.0, 66824.0, 66825.0, 66827.0, 66828.0, 328973.0, 66830.0, 66832.0, 66833.4, 66833.0, 66836.0, 66837.0, 66838.0, 66840.0, 66841.0, 66842.0, 66843.0, 66844.0, 66846.0, 66847.0, 66848.0, 66849.0, 66850.0, 66851.2, 66851.0, 66852.0, 66853.0, 66855.0, 329000.0, 66855.84, 66858.0, 66857.0, 66860.0, 66861.0, 66862.9, 66863.0, 66862.0, 66863.64, 66866.0, 66867.0, 66868.0, 66864.0, 66870.0, 66871.0, 66872.0, 66873.0, 66865.0, 66875.0, 66876.0, 66875.22, 66878.4, 66880.0, 66882.0, 66883.2, 66885.0, 66886.8, 66887.0, 66888.0, 66886.0, 66890.0, 66891.72, 66890.32, 66893.0, 66894.0, 66895.0, 66897.0, 66898.0, 66899.0, 66900.0, 66901.0, 66905.0, 66906.0, 66907.0, 66908.28, 66908.0, 66911.0, 66912.0, 66913.6, 66914.0, 66915.84, 66916.0, 66917.0, 66914.8, 66919.0, 66920.0, 66920.16, 66922.0, 66923.0, 66924.0, 66925.0, 66926.0, 66927.57, 66928.0, 66930.73, 66931.0, 66932.0, 66930.0, 66934.0, 66935.0, 66936.0, 66933.0, 66940.0, 66941.0, 66943.92, 66944.0, 66946.56, 66947.0, 66948.52, 66948.0, 66950.0, 66951.0, 66952.0, 66955.0, 66957.0, 66958.0, 66959.98, 66960.0, 66959.19, 66962.0, 66963.0, 66964.0, 66965.0, 66966.0, 66971.0, 66972.0, 66973.0, 66974.0, 66975.0, 66976.0, 66977.0, 66978.0, 66980.0, 66981.0, 66982.0, 66983.0, 66984.44, 66984.0, 66980.82, 66988.0, 66989.0, 66990.0, 66991.63, 66990.79, 66994.0, 66995.0, 66996.0, 66997.0, 66999.99, 67000.0, 66999.0, 66999.92, 67002.0, 67004.95, 67005.0, 67006.86, 67006.0, 67008.0, 67009.0, 67012.0, 67014.48, 67017.6, 67018.0, 67019.0, 67020.0, 67021.0, 67022.88, 67023.0, 67024.0, 67025.11, 67026.0, 67025.0, 67028.01, 67030.32, 67032.0, 67034.0, 67035.44, 67036.08, 67037.0, 67038.96, 67040.0, 67041.84, 67040.04, 67043.6, 67044.0, 67045.0, 67046.0, 67042.0, 67048.0, 67049.0, 67050.0, 67051.0, 67053.0, 67055.0, 67056.0, 67057.0, 67058.0, 67059.0, 67060.0, 67062.0, 67063.0, 67064.0, 67065.0, 67068.0, 67069.0, 67068.42, 67071.98, 67072.0, 67073.55, 67073.0, 67075.0, 67070.0, 67077.0, 67078.0, 67079.0, 67080.0, 67081.2, 67081.0, 67082.0, 67084.0, 67085.28, 67085.0, 67087.0, 67088.0, 67089.0, 67090.0, 67091.0, 67092.0, 67093.0, 67094.0, 67095.6, 67096.0, 67095.0, 67098.0, 67100.0, 67101.0, 67104.0, 67105.0, 67106.0, 67107.0, 67108.0, 67104.96, 67110.0, 67111.0, 67113.48, 67114.0, 67115.0, 67116.0, 67117.42, 67118.0, 67119.0, 67120.0, 67121.0, 67122.0, 67117.0, 67124.0, 67125.0, 67126.0, 67127.0, 67128.0, 67123.0, 67130.44, 67131.0, 67132.0, 67132.8, 67134.0, 67135.0, 67136.0, 67137.0, 67138.0, 67130.0, 67140.0, 67141.0, 67142.0, 67140.8, 67144.0, 67145.0, 67146.0, 67147.0, 67148.0, 67150.0, 67151.0, 67152.0, 67153.0, 67151.75, 67155.0, 67160.0, 67160.01, 67161.0, 67163.07, 67164.0, 67165.0, 67166.0, 67163.0, 67168.0, 67170.0, 67171.0, 67172.0, 67173.51, 65454.0, 67175.0, 67177.59, 67177.0, 65455.07, 67180.58, 67180.0, 65456.5, 67183.0, 67184.0, 67183.52, 67186.0, 65457.66, 67188.0, 67189.0, 67190.0, 65457.84, 67187.0, 67185.0, 67197.0, 67198.0, 67197.12, 67200.0, 67202.0, 67204.0, 67205.0, 67207.0, 67208.0, 67208.39, 67210.0, 67211.0, 67212.0, 67213.0, 67215.0, 67216.0, 67217.0, 65463.52, 67219.0, 67220.0, 67221.0, 67222.0, 67224.0, 67225.0, 67225.77, 67226.0, 67227.0, 65465.0, 67230.0, 67231.0, 67232.0, 67228.0, 67234.0, 67235.04, 67236.0, 67237.92, 67238.0, 65467.89, 67240.0, 67242.0, 67245.0, 67247.92, 67248.0, 67249.0, 67250.0, 67253.0, 67254.0, 67255.0, 67256.85, 67257.0, 67258.0, 329400.0, 67260.0, 67256.0, 65471.25, 67263.0, 67263.44, 67264.0, 67266.0, 67267.0, 67267.2, 65473.0, 67268.0, 67271.0, 67272.0, 67275.0, 67276.0, 67277.0, 67279.0, 67280.0, 67282.0, 67283.0, 67284.0, 67285.0, 67286.0, 67286.96, 67288.0, 67289.0, 67283.49, 67291.0, 67292.0, 67293.0, 67294.0, 67295.0, 67296.0, 67297.0, 67298.0, 67296.24, 67300.0, 67294.2, 67302.0, 67303.0, 67304.0, 67305.0, 67306.0, 67307.0, 67308.0, 67310.0, 67312.0, 67314.0, 67315.0, 67318.0, 67319.0, 67320.0, 67321.0, 67322.0, 67323.48, 67324.0, 67325.0, 67326.0, 67327.0, 67328.76, 67329.0, 67330.0, 67331.0, 67332.0, 67332.05, 67334.0, 67335.0, 67336.0, 67337.0, 67339.2, 67340.0, 67341.0, 67342.0, 67343.75, 67344.74, 67345.0, 67343.0, 67347.0, 67348.0, 67349.0, 67350.0, 67352.0, 67353.0, 67354.0, 67355.0, 67356.0, 67357.92, 67358.0, 329497.0, 67360.0, 67361.0, 67362.0, 67361.68, 67363.0, 67364.0, 67367.0, 67368.0, 67369.0, 67370.0, 67371.2, 67371.0, 67374.0, 67375.0, 67376.0, 67377.0, 67377.82, 67379.0, 67380.0, 67381.0, 67374.4, 67384.0, 67385.0, 67386.0, 67387.0, 67388.0, 67385.04, 67390.0, 67392.0, 67394.0, 67395.0, 67397.0, 67398.0, 65499.96, 67400.0, 67400.8, 67402.0, 67403.0, 67404.0, 67405.0, 67406.56, 67399.0, 67408.0, 67409.0, 67408.8, 67410.0, 67412.8, 67412.28, 67412.0, 67415.63, 67416.0, 67415.0, 67417.0, 67413.0, 67420.0, 67418.0, 67422.0, 67423.0, 67424.0, 67425.0, 1116000.0, 67427.0, 67428.0, 67429.33, 67430.0, 67432.0, 67433.0, 67435.0, 67436.0, 67437.0, 67438.0, 67439.0, 67440.0, 67441.0, 67442.0, 67445.0, 67446.0, 67448.0, 67449.0, 67450.0, 67450.8, 67452.0, 67452.05, 329597.0, 67453.0, 67456.0, 67457.0, 67458.0, 67459.0, 67460.0, 67458.56, 67463.2, 67464.0, 67463.0, 67463.88, 67467.0, 67467.2, 67464.8, 67470.0, 67471.0, 67465.0, 67473.0, 67474.0, 67475.24, 67476.0, 67477.0, 67475.2, 67479.0, 67480.0, 67477.08, 67482.0, 67483.0, 67484.0, 67481.0, 67486.64, 67485.0, 67488.84, 67488.0, 67490.0, 67489.0, 67492.0, 67494.0, 67495.0, 67496.0, 67495.37, 67499.0, 67500.0, 67501.99, 67502.0, 67503.41, 67499.9, 67504.0, 67499.99, 67505.0, 67509.0, 67510.0, 67511.0, 67512.0, 67513.0, 67514.0, 67515.0, 67516.0, 67517.0, 67518.0, 67519.0, 67520.0, 67517.64, 67522.0, 67523.0, 67524.76, 67524.0, 67526.4, 67525.0, 67528.0, 67527.36, 67530.0, 67531.0, 67532.0, 67527.0, 67534.0, 67533.0, 67536.0, 67537.0, 67537.6, 67539.0, 67540.0, 67541.0, 67542.0, 67543.29, 67543.0, 67545.0, 67544.0, 67548.0, 67549.0, 67550.0, 67551.0, 67549.7, 67554.0, 67555.0, 67555.41, 67557.0, 67558.0, 67559.0, 67560.0, 67556.0, 67562.23, 67562.0, 67564.0, 67565.0, 67566.79, 67567.0, 67567.44, 67569.0, 67570.0, 67571.0, 67572.0, 67565.88, 67566.0, 67571.14, 67575.0, 67568.0, 67578.0, 67579.0, 67580.0, 67579.2, 67581.0, 67583.0, 67584.0, 67584.17, 67580.43, 67585.0, 67588.0, 67589.0, 67590.0, 67591.0, 67587.0, 67593.0, 67594.0, 67595.0, 67596.0, 329740.0, 67598.0, 67600.0, 67601.94, 67602.0, 67603.0, 67608.0, 67609.0, 67610.0, 67611.0, 67612.0, 67612.74, 67613.0, 67615.0, 67616.0, 67617.0, 67618.0, 67619.0, 67620.0, 67621.84, 67622.0, 67619.72, 67626.0, 67627.08, 67627.04, 67630.0, 67631.0, 67632.0, 67632.62, 67637.0, 67638.38, 67638.0, 67640.0, 67641.6, 67642.0, 67643.0, 67644.0, 67645.0, 67646.0, 67646.3, 67639.0, 67649.0, 67650.0, 67651.0, 67652.0, 67653.0, 67654.0, 67650.19, 67656.0, 67656.99, 67660.0, 67661.0, 67664.0, 329809.0, 67666.0, 67667.0, 67668.0, 67669.0, 67670.0, 67671.0, 67675.0, 67676.96, 67677.0, 67678.0, 67675.08, 67680.0, 67681.0, 67682.0, 67683.0, 67684.0, 67685.0, 67686.0, 67687.0, 67688.0, 67689.0, 65557.44, 67691.0, 67692.35, 67693.0, 67694.4, 67692.0, 67695.36, 67690.0, 67689.6, 67700.0, 67701.0, 67702.4, 67703.0, 67704.0, 329844.0, 67706.0, 67707.0, 67708.42, 67709.0, 67710.0, 67711.0, 592000.0, 67712.0, 67713.0, 67715.64, 67716.0, 67717.0, 67717.5, 67719.42, 67720.0, 67721.0, 67718.0, 67723.0, 67724.0, 67724.8, 67725.0, 67726.0, 67728.0, 67729.0, 67730.0, 67734.0, 67735.0, 67736.0, 67737.0, 67738.0, 67738.12, 67740.0, 67740.8, 67743.0, 67744.0, 67745.0, 67746.0, 67749.0, 67750.0, 67752.44, 67752.0, 67752.66, 67757.36, 67758.0, 67759.0, 67760.0, 67761.0, 67762.24, 67762.0, 67764.0, 67760.4, 67758.82, 67767.0, 67767.72, 67766.0, 67770.0, 67769.0, 67765.0, 67773.0, 67774.0, 67775.0, 67776.0, 67777.0, 67778.0, 67779.0, 67780.0, 67773.88, 67782.0, 67783.0, 67784.0, 67785.0, 67786.0, 67787.2, 67788.0, 67789.91, 67790.0, 67787.0, 67792.5, 67789.0, 67791.0, 67795.0, 67789.84, 67797.0, 67792.0, 67799.0, 67800.0, 67801.0, 67803.48, 67804.0, 67805.0, 67807.0, 67808.0, 67810.0, 67812.0, 67813.0, 67815.0, 67816.0, 67815.33, 67818.0, 65582.92, 67820.0, 67821.0, 67822.0, 67819.0, 67824.64, 67825.0, 67826.0, 67827.0, 67828.0, 67829.0, 67830.0, 67832.0, 67835.0, 67836.0, 67837.87, 67835.64, 67839.84, 4000000.0, 67840.0, 67839.0, 67841.0, 67844.0, 67845.0, 67846.0, 67842.0, 67848.0, 67849.0, 67850.0, 67847.0, 67846.08, 67852.0, 67846.2, 67855.0, 330000.0, 67857.0, 67858.0, 67859.61, 67860.0, 67861.0, 67859.0, 67862.0, 330008.0, 67865.0, 67864.0, 67868.0, 67869.0, 67870.0, 67871.0, 67872.0, 67874.0, 67875.0, 67879.0, 67880.0, 4000040.0, 67882.0, 67880.88, 67884.97, 67885.33, 67885.0, 67890.0, 67891.0, 67892.0, 67893.0, 67894.0, 67895.0, 67896.0, 67891.22, 67898.68, 67898.0, 67900.0, 67900.28, 67901.0, 67902.0, 67899.0, 67906.0, 67907.55, 67908.0, 67909.04, 67910.0, 67909.68, 67912.0, 67913.0, 67907.0, 67915.0, 67911.0, 67920.0, 67921.0, 67922.0, 67923.0, 67925.0, 67928.0, 67929.0, 67931.0, 67932.0, 67933.0, 67932.8, 4000095.0, 67935.0, 67937.0, 67938.48, 67936.0, 67940.0, 67940.88, 67942.0, 67937.11, 67944.0, 67945.0, 67942.13, 67947.0, 67948.92, 67949.0, 67950.0, 67951.0, 67950.84, 67953.0, 67954.25, 67954.0, 67956.0, 67956.41, 67958.0, 67959.32, 67960.0, 67961.0, 67962.0, 67954.9, 67964.0, 67965.0, 67966.0, 67967.0, 67968.0, 67969.98, 67970.0, 67971.0, 67971.81, 67972.0, 67973.1, 67974.0, 67968.68, 67977.6, 67978.0, 67979.0, 67980.0, 67975.0, 67982.0, 67984.0, 67985.0, 67986.0, 67988.0, 67990.0, 67991.88, 67992.0, 67993.0, 67994.19, 67995.0, 67996.74, 67997.0, 67996.0, 67999.0, 68000.0, 68001.0, 68001.58, 68004.0, 68004.44, 68006.41, 68007.0, 68008.0, 68009.0, 68010.0, 68011.0, 68012.0, 68015.0, 68016.0, 68017.0, 68016.92, 68020.0, 68021.0, 68023.0, 68024.0, 68025.0, 68027.0, 68028.0, 68029.0, 68031.0, 68032.0, 68032.33, 68034.0, 68035.0, 68036.0, 68037.0, 68036.71, 68039.4, 68040.0, 68041.0, 68042.0, 68043.0, 65627.0, 68046.0, 68048.0, 68050.0, 68052.0, 68053.0, 65629.8, 68054.0, 68057.0, 68058.0, 68060.0, 68061.24, 68062.0, 68060.98, 68064.0, 68065.44, 68066.0, 68067.0, 68068.0, 68069.0, 68065.0, 330216.0, 68073.0, 68075.0, 68076.0, 68078.0, 68080.0, 68082.0, 68083.0, 68084.0, 68085.0, 68086.0, 68087.0, 68088.0, 68090.0, 68092.24, 68092.0, 65637.0, 68095.0, 68096.0, 68093.0, 68099.0, 68100.0, 68100.72, 68101.0, 68103.0, 68104.0, 68105.52, 68106.0, 68107.0, 68108.0, 68109.86, 68110.0, 68111.0, 68112.0, 68109.0, 68114.0, 68119.0, 68120.0, 68122.8, 68124.0, 68125.0, 68126.0, 68127.0, 68128.0, 68126.1, 68130.0, 68131.0, 68132.0, 68133.0, 68134.0, 68135.0, 68136.0, 68137.7, 68138.0, 68137.0, 68140.0, 68140.8, 68141.4, 68142.8, 68139.0, 68144.0, 68143.0, 68147.11, 68148.0, 68147.0, 68150.0, 68151.0, 68145.0, 68153.0, 68154.0, 68155.0, 68156.76, 68156.0, 68157.0, 68159.0, 68160.0, 68157.36, 68158.0, 68166.0, 68168.0, 68169.0, 68170.44, 68172.0, 68173.0, 68174.02, 68175.39, 68176.0, 68175.0, 68178.0, 68179.92, 68174.0, 68181.0, 68182.4, 68181.44, 68184.0, 68177.0, 68186.0, 68185.0, 68179.0, 68180.0, 68190.0, 68191.0, 68189.11, 68193.36, 68196.0, 68196.15, 68199.88, 68200.0, 68201.0, 68199.0, 68203.0, 68204.0, 68205.0, 68202.0, 68208.0, 68209.0, 68210.0, 68211.0, 68212.98, 68212.0, 68214.0, 68215.0, 68216.0, 68215.53, 68213.0, 68220.0, 68221.44, 68222.0, 68224.0, 68225.0, 68226.36, 68227.0, 68226.0, 68230.0, 68231.0, 68232.0, 68233.0, 68234.0, 68235.0, 68236.0, 68237.0, 68233.1, 68238.0, 68240.0, 68241.0, 68242.0, 68239.0, 68244.8, 68245.0, 68246.64, 68244.0, 68247.0, 68249.5, 68250.0, 68251.0, 68252.0, 68249.0, 68254.0, 68255.0, 68256.0, 68255.86, 68258.0, 68259.0, 68260.0, 68261.0, 68262.0, 68263.0, 68265.0, 68266.0, 68265.6, 68268.0, 68269.0, 68267.0, 68271.0, 68270.0, 68266.6, 68274.0, 68275.0, 68276.0, 68277.0, 68276.57, 68278.0, 68280.0, 68279.88, 68281.0, 68283.0, 68284.0, 68285.0, 68286.4, 68286.22, 68288.0, 68289.0, 68290.0, 68291.0, 68292.0, 68292.28, 68294.0, 68295.0, 68293.78, 68295.94, 68298.0, 68299.92, 68300.0, 68296.8, 68302.6, 68303.0, 68304.0, 68305.38, 68305.0, 68306.0, 68307.0, 68302.0, 68310.0, 68309.0, 68312.0, 68308.8, 68314.0, 68315.0, 68316.0, 68311.0, 65682.0, 68320.0, 68322.0, 68323.0, 68324.0, 68325.0, 65683.0, 68327.52, 68328.0, 68327.0, 68330.0, 68332.0, 68333.0, 68334.0, 68336.0, 65686.39, 68340.0, 68341.0, 68342.0, 68343.0, 68340.84, 68345.0, 65687.0, 68347.97, 68348.0, 68349.0, 68350.0, 68347.76, 68352.0, 68353.0, 68354.0, 68355.0, 68356.0, 68357.0, 68358.0, 68357.14, 68360.0, 68361.78, 68362.0, 68361.0, 68364.0, 68365.44, 65691.0, 68367.0, 68367.46, 68365.0, 68369.6, 68371.0, 68372.28, 68373.0, 68374.0, 68372.0, 68376.0, 68377.0, 68378.0, 68375.0, 68380.0, 68381.0, 68382.0, 68383.0, 68383.32, 68385.0, 68386.0, 68387.0, 68388.0, 68389.0, 68390.0, 68392.0, 68394.39, 68395.0, 68396.0, 68397.06, 68397.0, 68399.0, 68400.0, 68401.0, 68402.16, 68404.0, 68406.0, 68407.0, 68408.0, 68409.0, 68410.0, 68411.0, 68412.0, 68413.0, 68414.0, 68415.0, 68417.0, 68418.22, 68418.0, 68420.0, 68421.0, 68420.28, 68423.52, 68424.0, 68423.0, 68426.09, 68427.0, 68425.0, 68428.0, 68430.38, 68431.0, 68432.0, 68433.0, 68430.0, 68436.0, 68438.98, 68439.12, 68440.0, 68441.87, 68442.0, 68439.0, 68444.05, 68445.0, 68444.0, 68447.0, 68448.0, 68446.0, 68450.0, 68451.0, 68452.0, 68454.0, 68455.0, 68456.0, 68455.14, 68460.0, 68463.0, 68464.0, 68465.0, 68466.0, 68467.0, 68468.0, 68465.5, 68470.0, 68471.0, 68472.0, 68473.0, 68474.0, 68475.0, 68476.0, 68471.28, 68478.0, 68479.88, 68480.0, 68479.0, 68481.0, 68482.0, 68484.0, 65714.19, 68488.0, 68490.0, 68491.0, 68492.0, 68493.04, 68494.0, 68495.0, 68496.0, 68497.0, 68498.0, 68499.0, 68500.0, 68496.2, 68504.0, 68505.0, 68504.4, 68505.6, 68508.0, 68507.76, 68510.27, 68510.0, 68512.0, 68511.0, 68514.0, 68515.0, 68513.76, 68517.0, 68518.0, 68517.64, 68520.0, 68521.0, 68517.07, 68523.0, 68524.0, 68525.0, 68522.0, 68527.0, 68528.32, 68528.0, 68530.0, 68531.0, 68532.0, 68533.0, 68534.0, 68535.0, 68536.0, 68537.0, 68532.88, 68539.0, 68540.0, 68541.0, 68542.0, 68543.0, 68544.0, 68545.0, 68546.0, 68547.0, 68548.0, 68549.0, 68550.0, 65727.0, 68552.0, 68553.0, 68555.0, 68556.0, 68557.0, 68558.0, 68559.0, 68560.0, 68561.52, 68559.96, 68563.2, 68565.0, 68567.0, 68568.0, 855000.0, 68570.0, 68572.0, 68573.0, 68574.94, 68575.0, 68576.0, 68577.0, 68578.0, 68579.0, 68580.0, 68581.0, 68582.0, 68577.36, 68584.62, 68585.0, 68586.0, 68587.0, 68588.0, 68588.64, 68590.0, 68591.0, 68592.0, 68593.0, 68589.0, 68595.72, 68596.0, 68593.67, 68598.0, 68598.56, 68600.0, 68601.96, 68601.0, 68597.0, 68604.0, 68605.0, 68604.81, 68607.0, 68608.0, 68609.0, 68610.48, 68604.8, 68612.0, 68613.75, 68614.0, 68616.36, 68617.56, 68619.2, 68619.0, 68620.0, 68619.74, 68623.0, 68624.0, 68625.0, 68626.0, 68628.96, 68628.0, 68630.0, 68631.0, 68632.0, 68633.0, 68635.18, 68635.0, 68637.0, 68638.44, 68640.0, 68641.0, 68642.0, 68643.0, 68642.64, 68645.0, 68646.84, 68646.0, 68648.26, 68650.0, 68652.0, 68652.48, 68654.0, 68655.0, 68656.0, 68657.0, 68653.0, 68659.0, 68660.0, 68661.0, 68663.0, 68664.0, 68664.72, 68666.0, 68666.96, 68668.0, 68669.0, 68670.0, 68672.0, 68674.0, 68675.0, 68676.0, 68674.32, 68678.0, 68679.0, 68680.0, 68681.0, 68680.3, 68675.95, 68684.0, 68685.0, 68686.0, 68687.0, 68688.0, 68689.0, 68690.0, 68691.0, 68692.0, 68693.0, 68694.0, 68689.8, 68695.0, 68698.0, 68699.0, 68700.0, 68701.0, 68702.0, 68703.0, 68700.54, 68705.0, 68705.28, 68706.0, 68708.0, 68709.0, 68707.0, 68712.0, 68713.0, 68714.88, 593000.0, 68716.0, 68717.0, 68718.0, 68719.0, 68720.0, 68718.61, 68722.0, 68723.2, 68724.0, 68725.0, 68723.0, 68727.0, 68728.0, 68729.0, 68730.0, 68726.76, 68732.0, 68733.0, 68734.0, 68735.0, 68736.0, 68738.0, 68740.0, 68741.0, 68742.0, 68743.0, 68744.0, 68745.0, 68746.0, 68746.71, 68748.0, 68749.0, 68750.0, 68747.6, 68746.33, 68753.0, 68753.52, 68755.0, 68757.0, 65769.6, 68759.0, 68760.0, 68763.0, 68764.0, 68765.0, 68766.0, 68767.0, 68768.0, 68769.0, 68770.0, 68771.0, 68769.22, 68773.0, 65772.0, 68775.0, 68776.0, 68777.0, 68778.0, 68779.92, 68780.0, 68781.0, 68782.0, 68783.0, 68784.0, 68785.0, 68786.0, 68787.0, 68788.0, 68789.76, 68790.0, 68791.0, 68792.0, 68793.0, 68794.0, 68795.0, 68796.0, 68797.0, 68798.0, 68799.0, 68800.0, 68802.0, 68803.0, 68804.88, 68804.0, 68806.0, 68806.48, 68808.0, 68809.0, 68810.0, 68806.4, 68809.6, 68814.27, 68815.0, 68816.0, 68818.03, 68819.0, 68820.0, 68821.0, 68818.0, 68824.75, 68825.88, 68825.12, 68827.0, 68824.0, 68825.0, 68830.0, 68829.74, 68832.0, 68833.0, 68835.0, 68836.0, 68838.0, 68839.0, 68840.0, 68841.0, 68842.0, 68844.0, 68847.23, 68848.0, 68849.0, 68850.0, 68852.0, 68853.0, 68854.0, 68855.0, 331000.0, 68857.0, 68858.0, 68856.0, 68860.0, 68864.0, 68865.0, 68866.0, 68868.0, 68868.8, 68870.0, 68871.0, 68869.18, 68874.42, 68875.0, 68876.93, 68877.0, 68876.0, 68879.0, 68880.0, 68878.29, 68882.0, 68883.0, 68881.28, 68885.0, 65793.82, 68882.04, 68888.0, 68889.0, 68890.0, 68891.0, 68888.3, 68892.0, 68894.0, 68895.0, 68896.0, 68900.0, 68901.0, 68901.36, 68902.67, 68904.0, 68903.0, 68906.0, 68907.0, 68909.0, 68910.0, 68911.0, 68912.0, 68913.84, 68916.0, 68917.0, 68918.0, 68920.0, 68921.6, 68921.0, 68925.0, 68926.08, 68928.0, 68929.0, 68928.34, 68931.0, 68932.0, 68933.0, 68930.0, 68935.0, 68936.0, 68937.0, 68939.0, 68940.0, 68941.0, 68942.62, 68943.0, 68945.0, 68946.27, 68945.24, 65807.4, 65807.0, 68950.0, 68951.28, 68952.0, 68953.29, 68953.0, 68951.76, 68956.0, 68957.0, 68958.0, 68959.0, 68960.0, 68961.0, 68962.0, 68963.0, 68958.5, 68965.0, 68964.0, 68967.0, 68959.2, 68970.0, 68972.0, 68973.0, 68974.0, 68973.36, 68975.0, 68975.78, 68978.0, 68977.0, 68980.0, 68981.41, 68982.0, 68983.0, 68984.0, 68985.0, 68986.0, 68988.0, 68989.0, 68990.0, 68991.0, 68992.0, 68993.0, 68994.48, 68993.6, 68996.0, 68998.0, 68999.52, 69000.0, 69000.01, 68999.0, 69004.0, 69006.0, 69008.0, 69009.0, 69010.0, 69011.0, 69010.32, 69013.0, 69014.0, 69015.0, 69016.0, 69017.0, 69020.0, 69022.0, 69023.0, 69024.0, 69025.0, 69026.5, 69027.0, 69029.0, 69030.0, 69030.67, 69032.0, 1379753.0, 69031.0, 69035.0, 69032.8, 69036.0, 69038.0, 69039.0, 69038.46, 69033.0, 69040.0, 69044.0, 69045.0, 69046.0, 69047.0, 69048.0, 69050.0, 69051.0, 69052.5, 69054.0, 69055.0, 69056.0, 69057.0, 331200.0, 69056.93, 69058.8, 69062.0, 69063.72, 69069.0, 69070.0, 69071.0, 69072.0, 69073.0, 69074.0, 69076.0, 69077.0, 69078.0, 69080.0, 69081.0, 69084.53, 69084.0, 69086.0, 69088.0, 69089.0, 69088.17, 69091.0, 69092.0, 69093.0, 69094.0, 69095.0, 69096.0, 69090.0, 69098.39, 69099.21, 69100.0, 69101.21, 69101.0, 69102.0, 65838.0, 69105.0, 69097.0, 69108.0, 69109.36, 69109.54, 69111.0, 69113.0, 69114.0, 69115.0, 69116.0, 69117.0, 69118.0, 69120.0, 69122.0, 69123.0, 69125.0, 69126.0, 69127.0, 69128.0, 69129.0, 69130.0, 69131.0, 69132.0, 69132.24, 69134.0, 69135.0, 69136.0, 69138.0, 69139.0, 69140.0, 69141.0, 69143.0, 69144.0, 69145.0, 69146.0, 69147.0, 69148.0, 69148.8, 69150.0, 69152.0, 69154.0, 69155.52, 69156.0, 69156.48, 69158.0, 69160.0, 69162.0, 69163.68, 69166.0, 69168.0, 69168.22, 69172.0, 69174.0, 69175.0, 69176.0, 69177.0, 69178.0, 69179.0, 69180.14, 69180.0, 69182.0, 69183.0, 69183.92, 69184.0, 69186.0, 69186.84, 69188.0, 69189.0, 69190.0, 69191.0, 69192.0, 69187.0, 69193.0, 69195.07, 69196.0, 69197.24, 69198.0, 69199.0, 69200.0, 69201.0, 69202.0, 69203.76, 69204.0, 69203.0, 69205.0, 69207.0, 69201.6, 69210.0, 69211.0, 69211.44, 69214.5, 69215.0, 69216.0, 69217.0, 69218.0, 69217.13, 69220.0, 69214.0, 69222.0, 69222.4, 69224.39, 69225.0, 69226.0, 69223.96, 69228.0, 69229.21, 69230.0, 69231.0, 69232.0, 69232.44, 69234.9, 69233.0, 69236.0, 69238.0, 69239.0, 69240.0, 69239.03, 69242.0, 69242.6, 69244.0, 69245.0, 65866.5, 69243.0, 69250.0, 69251.0, 69252.0, 69254.0, 69255.84, 69255.0, 69257.0, 69260.0, 69261.0, 69262.0, 69264.0, 65870.0, 69266.0, 69267.5, 69268.0, 69267.96, 69269.0, 69270.0, 69272.0, 69273.0, 69274.76, 69275.0, 69276.0, 69277.0, 69278.0, 69277.8, 69280.0, 1380000.0, 69281.0, 69282.0, 69284.8, 69285.0, 69286.0, 69279.0, 69288.0, 69283.0, 69290.0, 65874.0, 69292.17, 69293.0, 69292.0, 69295.0, 69296.0, 69292.77, 69299.28, 69300.0, 69299.0, 69301.0, 69305.0, 69306.53, 69307.0, 69308.0, 69309.0, 69307.19, 69312.0, 69313.0, 69314.0, 69315.0, 69316.0, 69317.0, 69318.0, 69319.0, 69320.0, 69321.0, 65881.0, 69324.0, 69325.0, 69326.0, 69327.0, 69328.0, 69326.4, 69330.0, 69331.11, 69329.0, 69334.0, 69336.0, 69337.73, 69338.0, 69339.34, 69340.0, 69339.8, 69342.0, 69343.0, 69344.0, 69341.0, 69346.0, 69347.0, 69348.0, 69345.0, 69350.0, 69351.0, 69352.0, 69352.46, 69354.0, 69355.0, 69353.12, 69357.0, 69358.0, 69356.0, 69360.0, 69357.6, 69363.0, 69364.0, 69366.0, 69368.0, 69370.0, 69372.0, 593660.0, 69373.0, 69375.0, 69376.0, 69377.0, 69379.87, 69380.16, 69381.0, 69380.0, 69383.0, 69384.0, 69385.0, 69382.8, 69386.0, 69389.0, 69390.0, 69391.0, 69392.02, 69393.0, 69394.8, 69394.0, 69396.0, 69397.0, 69398.0, 69399.22, 69400.0, 69395.0, 69398.68, 69404.0, 69405.0, 69407.0, 69408.0, 69409.0, 69410.0, 69407.06, 69412.0, 69413.0, 69415.0, 69416.0, 69417.84, 69418.0, 69419.0, 69420.0, 69418.17, 69422.0, 69423.0, 69424.0, 1118000.0, 69426.0, 69427.0, 69428.0, 69425.0, 69430.0, 69430.4, 69432.0, 69431.0, 69435.0, 69436.0, 69437.0, 69439.0, 69440.0, 69440.64, 69442.0, 69443.0, 69444.0, 69445.0, 69446.0, 69446.4, 69449.0, 69450.0, 69451.0, 69451.2, 69453.0, 69451.46, 69450.03, 69456.71, 69456.0, 69458.0, 69455.0, 69460.0, 69461.0, 69454.2, 69463.0, 69464.0, 69462.0, 69466.0, 69467.0, 69468.0, 69471.0, 69472.0, 69473.0, 69474.0, 69475.0, 69476.0, 69478.0, 69480.0, 69482.0, 69484.58, 69485.0, 69487.0, 69489.0, 69490.08, 69490.0, 69492.0, 69493.0, 69494.0, 69493.35, 69496.0, 69497.0, 69498.0, 69499.0, 69500.0, 69497.34, 69502.0, 69503.0, 69504.0, 69505.0, 69506.0, 69507.0, 69508.0, 69510.0, 69511.0, 69512.28, 69513.0, 69514.0, 69515.0, 69516.0, 69517.0, 69518.0, 69519.0, 69520.0, 69522.0, 69523.0, 69524.0, 69525.0, 69525.6, 69526.0, 69528.0, 69530.0, 69533.0, 69534.0, 69534.39, 69536.0, 69537.93, 69538.0, 69539.0, 69540.0, 69541.0, 69538.1, 69543.0, 69544.0, 69545.0, 69546.0, 69547.0, 69548.0, 69548.52, 69550.0, 69551.0, 69550.37, 69552.0, 69551.52, 69555.2, 69556.0, 69557.0, 69558.0, 69559.14, 69560.0, 69561.0, 69562.0, 69563.0, 69564.0, 69565.0, 69566.5, 69567.0, 69564.71, 69569.0, 69570.0, 69572.0, 69575.0, 69576.0, 69577.0, 69578.0, 69579.0, 69580.0, 69581.95, 69581.0, 69582.0, 69584.0, 69585.0, 69586.0, 69587.0, 69583.0, 69589.0, 69591.34, 69592.0, 69593.0, 69591.96, 69596.0, 69597.64, 69600.0, 69602.0, 69603.0, 69604.0, 69605.0, 69606.0, 69609.62, 69610.0, 69611.0, 69612.0, 69613.0, 69612.84, 69615.0, 69616.0, 69618.0, 69620.0, 69621.0, 69624.0, 69625.4, 69628.0, 69630.0, 69631.0, 69633.0, 69634.0, 69635.0, 69636.0, 69634.2, 69638.4, 69640.0, 69640.8, 69642.0, 69643.66, 69644.0, 69645.0, 69643.0, 69641.25, 69648.0, 69649.0, 69650.0, 69651.0, 69652.0, 69654.0, 69655.0, 69656.9, 69657.0, 69658.0, 69659.0, 69660.0, 69654.78, 69662.0, 69663.45, 69664.0, 331800.0, 69666.0, 69667.0, 69668.0, 69670.0, 69671.0, 69672.0, 69674.0, 69675.0, 69676.0, 69677.0, 69678.0, 69679.0, 69680.0, 69681.0, 69680.16, 69683.0, 69684.0, 69690.0, 69691.0, 69692.0, 69693.0, 69694.0, 69695.03, 69696.0, 69695.0, 69695.45, 69698.0, 69700.0, 69701.0, 69702.0, 69703.0, 69705.0, 69707.0, 69708.0, 69709.68, 69710.0, 69709.0, 594000.0, 69713.0, 69712.0, 69715.0, 69708.08, 69718.66, 69720.0, 69722.0, 69724.0, 69725.0, 69726.0, 69727.0, 69728.0, 69729.0, 69730.0, 69732.0, 69733.0, 69734.0, 69735.0, 69736.0, 69736.16, 69739.0, 69740.0, 69741.0, 69742.4, 69742.0, 69744.0, 69745.8, 69745.0, 69747.0, 69748.0, 69749.0, 69750.0, 69743.0, 69752.0, 69753.0, 69754.0, 69755.0, 69756.0, 69757.0, 69758.8, 69759.0, 69760.0, 69762.0, 69763.0, 69764.0, 69762.62, 69766.0, 69767.0, 69768.0, 69770.0, 69771.0, 69774.0, 69775.0, 69776.0, 69777.0, 69779.0, 69780.0, 69782.0, 69782.51, 69784.0, 69785.0, 69786.0, 69787.54, 69788.0, 69782.8, 69790.0, 69789.0, 69792.0, 69794.0, 69795.0, 69796.0, 69798.56, 69799.0, 69800.0, 69801.48, 69802.0, 69803.0, 69804.0, 69805.0, 69798.0, 69808.0, 69810.0, 69811.0, 69810.91, 69813.0, 69814.0, 69815.0, 69816.0, 69818.0, 69820.0, 69821.0, 69822.0, 69824.0, 69825.0, 69826.0, 69827.68, 69828.0, 69829.0, 69830.0, 69831.0, 69827.0, 69834.0, 69835.21, 69836.0, 69837.0, 69837.99, 69839.02, 69840.0, 69841.0, 69842.0, 69845.0, 69846.0, 69847.0, 69848.0, 69849.0, 69850.0, 69852.0, 69853.0, 69854.0, 69855.0, 332000.0, 69856.0, 69857.0, 69859.0, 69860.0, 69861.6, 69862.7, 69862.0, 69864.0, 69865.0, 69866.0, 69867.0, 69868.0, 69869.0, 69870.0, 69871.0, 69872.0, 69873.0, 69870.14, 69875.0, 69876.0, 69877.0, 69878.0, 69879.0, 69880.0, 69881.0, 69882.0, 69883.0, 69885.0, 69886.0, 69887.0, 69888.0, 69890.0, 69891.99, 69896.0, 69898.0, 69900.0, 69901.0, 69904.0, 69905.0, 69906.0, 69906.72, 69908.76, 69908.8, 69909.0, 69910.0, 69908.0, 69912.0, 69913.0, 69915.17, 69915.0, 69917.0, 69919.0, 69920.0, 69921.0, 69922.0, 69923.0, 69924.0, 69921.24, 69926.0, 69927.0, 69929.0, 69930.0, 69931.0, 69933.0, 69935.52, 69936.0, 69937.0, 69938.58, 69935.2, 69940.0, 69941.32, 69938.0, 69935.0, 69948.0, 69950.0, 69952.0, 69953.0, 69955.0, 69957.0, 69959.0, 69960.0, 69961.0, 69962.0, 69963.0, 69964.8, 69965.4, 69966.0, 69964.0, 69968.0, 69969.0, 69970.0, 69971.0, 69972.37, 69972.0, 69973.28, 69975.0, 69976.0, 69977.64, 69978.0, 69979.86, 69980.0, 69974.0, 69981.91, 69983.0, 69984.0, 69986.0, 69987.0, 69988.0, 69990.0, 69991.0, 69992.0, 69993.0, 69993.88, 69995.0, 69996.0, 69997.6, 69998.0, 69999.0, 70000.0, 70001.0, 70002.0, 69999.96, 70004.0, 70005.0, 70006.0, 70007.0, 70008.0, 70000.08, 70012.0, 70013.0, 70012.8, 70015.0, 70016.75, 70016.0, 70018.0, 70019.4, 70020.0, 70021.0, 70022.0, 70018.82, 70024.0, 58792.05, 70027.0, 70028.0, 70029.0, 70030.0, 70032.0, 70033.0, 70033.6, 70034.96, 70034.0, 70033.33, 70038.0, 70039.71, 70040.0, 70035.0, 70042.0, 70043.0, 70044.0, 70047.0, 70048.0, 70050.0, 70051.0, 70052.0, 70053.0, 70054.0, 70054.4, 70056.0, 70057.0, 70058.0, 70059.7, 70060.0, 70061.0, 70062.0, 70059.0, 70065.0, 70066.0, 70068.0, 70070.0, 70071.0, 70072.0, 70073.0, 70075.0, 70077.0, 70078.0, 70079.2, 70080.0, 70081.0, 70082.64, 70077.67, 70084.0, 70084.8, 70086.0, 70088.0, 70090.0, 70092.0, 70093.0, 70094.0, 70095.0, 70096.0, 70098.0, 70099.0, 70100.0, 70102.0, 70104.0, 70105.0, 70106.0, 70107.0, 70108.0, 70109.03, 70110.0, 70111.0, 70113.0, 70115.0, 70116.0, 70115.5, 70118.0, 70119.0, 70119.14, 70120.0, 70118.13, 70123.0, 70116.8, 70121.0, 70126.0, 70125.0, 70128.0, 70129.0, 70130.0, 70131.0, 70132.0, 70133.0, 70134.0, 70136.0, 70137.0, 70138.0, 70139.0, 70140.0, 70141.0, 70143.0, 70145.0, 70149.0, 70150.0, 70151.0, 70152.0, 70154.0, 70155.0, 70156.0, 70158.0, 70159.0, 70160.0, 70161.75, 70163.6, 70164.0, 70167.24, 70167.0, 70168.68, 70170.0, 70170.72, 70172.0, 70175.71, 70176.0, 70177.0, 70178.0, 70179.2, 70180.0, 70175.0, 70182.0, 70183.0, 70183.29, 70185.0, 70178.28, 70187.0, 70188.0, 70189.0, 70192.0, 70193.0, 70194.53, 70195.0, 70196.0, 70192.51, 70194.0, 70199.0, 70200.0, 70197.0, 70202.0, 70203.0, 70204.0, 70198.56, 70206.0, 70209.0, 70210.0, 70211.52, 70212.0, 70211.0, 70214.0, 70216.0, 70219.0, 70220.0, 70221.32, 70222.56, 70223.0, 70224.0, 70221.0, 70226.0, 70222.0, 70228.0, 70221.94, 70230.0, 70234.0, 70235.0, 70236.0, 70237.0, 70238.0, 70239.0, 70240.0, 70240.32, 70240.71, 70243.0, 70244.0, 70241.0, 70242.0, 70247.0, 70248.0, 70241.6, 70250.0, 70251.5, 70252.0, 70251.31, 70254.48, 70255.0, 70254.02, 70254.0, 70256.0, 70259.0, 70260.0, 70258.0, 70262.0, 70261.0, 70261.87, 70265.0, 70266.0, 70268.0, 70269.84, 70270.0, 70271.0, 70272.0, 70273.44, 70273.0, 70275.0, 70274.0, 332416.0, 70278.0, 70279.49, 70280.0, 70281.0, 70282.0, 70283.2, 70284.0, 70279.0, 70286.0, 70287.0, 70288.0, 70289.0, 70290.0, 332435.0, 70293.0, 70295.57, 70296.0, 70297.0, 70298.0, 70299.0, 70300.0, 70302.0, 70303.0, 70304.0, 70306.91, 70306.0, 70308.0, 70310.0, 70312.0, 70312.5, 70315.0, 70316.0, 70316.69, 70318.0, 70319.0, 70320.0, 70321.92, 70322.0, 70323.0, 70324.0, 70325.0, 70326.0, 66081.6, 70328.0, 70329.0, 70330.0, 70331.0, 70332.0, 70333.0, 70334.46, 70335.0, 70334.0, 70338.0, 70339.0, 70340.0, 70341.0, 332482.0, 70343.56, 70344.0, 70345.0, 70346.16, 70346.0, 70348.0, 70349.0, 70350.0, 70352.0, 70353.0, 70355.0, 70356.0, 332500.0, 70358.0, 70359.0, 70360.0, 70361.0, 70363.0, 70364.0, 70365.0, 70366.4, 70367.04, 70368.0, 70367.0, 70370.0, 70366.0, 70372.0, 70373.0, 70374.0, 70373.88, 70370.66, 70377.0, 70378.0, 70379.2, 70380.0, 70380.24, 70382.0, 70383.0, 70384.0, 70385.0, 70386.0, 70387.0, 70388.0, 70388.5, 70390.0, 70389.0, 70392.0, 70396.0, 70396.98, 70397.0, 70399.0, 70400.0, 70401.0, 70402.0, 70404.0, 70405.0, 70407.0, 70408.0, 70409.03, 70412.0, 70416.0, 70417.0, 70418.0, 70419.0, 70420.0, 70421.0, 70422.0, 70425.0, 70426.0, 70427.5, 70428.8, 70427.0, 70428.0, 70431.0, 70432.0, 70429.0, 70430.0, 70435.08, 332580.0, 70439.0, 70440.0, 70441.0, 70443.0, 70443.48, 70444.0, 70446.0, 70447.0, 70448.52, 70449.0, 70450.0, 70450.81, 70452.0, 70453.0, 70451.56, 70455.0, 70456.0, 70457.0, 70460.0, 70462.0, 70464.0, 70465.0, 70466.0, 70467.0, 70468.0, 70469.64, 70470.24, 70470.0, 70471.0, 70473.0, 70471.08, 70475.0, 70476.0, 70474.56, 70478.0, 70480.0, 70483.44, 70483.0, 70486.0, 70488.0, 70490.0, 70491.52, 70492.0, 70492.76, 70494.0, 70495.0, 70496.0, 70491.0, 70498.0, 70499.0, 70500.0, 70499.5, 70499.75, 70504.76, 70505.0, 70504.55, 70508.0, 70509.0, 70510.0, 70512.0, 70514.0, 70515.0, 70516.0, 70514.11, 70518.0, 70517.0, 70520.4, 70520.0, 70522.0, 70523.0, 70524.0, 70521.36, 70526.0, 70527.0, 70522.26, 70521.48, 70530.0, 70531.0, 70532.0, 70534.0, 70535.0, 70536.0, 70539.39, 70540.0, 70539.0, 70543.0, 70544.07, 70545.0, 70546.8, 70547.0, 70546.0, 70549.0, 70550.0, 70551.0, 70552.0, 70555.0, 70556.0, 70557.0, 70558.0, 70560.0, 70561.0, 70563.0, 70564.0, 70565.0, 70566.0, 66128.0, 70568.0, 70569.0, 70570.0, 70567.0, 70572.0, 70573.0, 70575.0, 70576.0, 70577.0, 70578.0, 70580.0, 70582.0, 70583.0, 70584.0, 70586.0, 70588.0, 70588.92, 70592.0, 70593.0, 70594.8, 70595.0, 70594.1, 70597.0, 70596.96, 70596.0, 70600.0, 70600.92, 70602.0, 70594.4, 70606.0, 70607.0, 70608.0, 70609.0, 70610.0, 70611.0, 70613.0, 70614.0, 70615.0, 70616.0, 70617.0, 70618.0, 70619.23, 70620.0, 70620.43, 70615.61, 70624.69, 70624.0, 70626.0, 70627.0, 70628.0, 70629.0, 70630.0, 70632.0, 70633.0, 70635.0, 70637.0, 70638.0, 70640.0, 70641.0, 70643.0, 70644.0, 70643.28, 70643.81, 70647.08, 70648.24, 70649.0, 70650.0, 70649.76, 3740668.0, 70647.0, 70645.0, 70655.0, 70656.0, 332800.0, 70657.53, 70660.0, 70661.0, 70662.0, 70663.0, 70665.66, 70666.0, 70667.0, 70668.0, 70669.0, 70666.08, 70668.12, 70672.0, 70674.0, 70675.0, 70676.0, 70677.0, 70678.0, 70680.0, 70681.0, 70685.69, 70686.0, 70688.0, 6100000.0, 70690.0, 70691.0, 70692.0, 70688.4, 70694.0, 70695.0, 70697.0, 70698.0, 70699.2, 70700.0, 70699.0, 70702.0, 70701.0, 70704.0, 70705.0, 70706.0, 70707.0, 70708.0, 70704.39, 70708.52, 70702.8, 595000.0, 70712.0, 70714.0, 70715.0, 70718.0, 70719.0, 70720.0, 70719.11, 70720.08, 70724.0, 70725.0, 70726.5, 70727.88, 70728.0, 70726.92, 70726.0, 70731.0, 70732.0, 70729.0, 70727.0, 70735.0, 70736.0, 70737.0, 70738.0, 70730.0, 70740.0, 70741.0, 70742.0, 70740.72, 70744.0, 70745.0, 70746.0, 70747.0, 70739.0, 70749.0, 70750.0, 70751.0, 70752.0, 70753.0, 70754.0, 70755.0, 70756.0, 70758.9, 70760.0, 70761.0, 70762.0, 70760.37, 70764.0, 70765.87, 70766.0, 70765.0, 70768.0, 70769.0, 70770.0, 70771.0, 70772.0, 70773.0, 70774.0, 70775.4, 70776.0, 70775.0, 70777.0, 70775.28, 70781.0, 70782.0, 70782.4, 70784.0, 70785.0, 70786.0, 70787.0, 70788.0, 70786.71, 70790.0, 70791.76, 70789.0, 70795.0, 70796.0, 70797.0, 70800.0, 70800.99, 70802.0, 70801.49, 70803.0, 70801.0, 70803.24, 70807.0, 70808.0, 70807.02, 70810.0, 70811.0, 70812.0, 70814.0, 70815.73, 70816.0, 70818.0, 70819.72, 70820.0, 70821.0, 70824.0, 70825.0, 70826.0, 70824.2, 70825.8, 70829.0, 70830.0, 70831.0, 70832.0, 70828.0, 70834.0, 70833.0, 70836.0, 70837.0, 70838.0, 70838.52, 70840.0, 70841.0, 70835.0, 70843.0, 70844.0, 70845.0, 70842.0, 70848.0, 70849.0, 70850.0, 70850.4, 70852.0, 70853.0, 70854.0, 70855.0, 333000.0, 70856.0, 70858.0, 70858.45, 70860.0, 70857.0, 70862.0, 70863.0, 70862.98, 70865.0, 70866.0, 70867.0, 70868.0, 70869.0, 70870.0, 70865.34, 70865.6, 70872.0, 70874.0, 70875.0, 70876.0, 70871.0, 70877.0, 70879.0, 70880.0, 66190.2, 70882.0, 70878.0, 70884.0, 70885.0, 70886.0, 70888.0, 70889.78, 70890.0, 70888.08, 70889.0, 70893.52, 70893.0, 70895.0, 70896.0, 70897.06, 70894.0, 70899.0, 70900.0, 70898.0, 70902.0, 70903.0, 70904.0, 70897.0, 70901.0, 70905.0, 70908.0, 70909.17, 70910.0, 70912.0, 70916.0, 70917.0, 70918.0, 2954500.0, 70920.0, 70923.0, 70924.0, 70924.8, 70926.0, 70925.0, 70928.0, 70929.0, 70930.0, 70934.0, 70938.0, 70939.0, 70940.0, 70941.0, 70942.8, 70943.0, 70944.0, 70945.0, 70944.42, 70948.8, 70949.0, 70950.0, 70951.0, 70953.0, 70956.0, 70958.0, 70960.0, 70961.0, 70964.0, 70965.0, 70967.0, 70968.0, 70975.0, 70976.0, 70978.37, 70980.0, 70981.0, 70980.6, 70983.0, 70985.0, 70986.85, 70987.0, 70988.0, 70989.96, 70990.0, 70992.0, 70994.4, 70995.0, 70994.0, 70996.0, 70998.0, 70999.8, 71000.0, 70999.0, 71001.0, 71002.0, 71004.0, 70997.0, 71007.0, 71010.0, 71011.0, 71012.0, 71015.04, 71016.0, 71017.2, 71018.4, 71017.69, 71020.0, 71019.0, 71022.0, 71018.0, 71024.0, 71023.08, 71027.26, 71028.0, 71030.0, 71031.0, 71032.0, 71033.87, 71034.0, 71039.0, 71040.0, 71041.0, 71042.0, 71043.0, 71044.0, 71045.0, 71046.0, 71046.94, 71048.0, 66224.48, 71050.0, 71051.0, 71052.8, 71052.0, 71054.5, 71055.0, 71056.0, 71057.0, 71058.0, 71053.0, 71060.0, 71062.0, 71064.0, 71065.0, 71066.0, 71068.0, 71070.0, 71073.0, 71074.8, 71074.0, 71076.0, 71075.0, 71080.0, 71081.0, 71084.0, 71085.0, 71086.53, 71087.0, 71088.0, 71089.0, 71092.0, 71093.0, 71094.0, 71095.0, 71097.0, 71098.0, 71099.0, 71100.0, 71101.0, 71102.0, 71103.0, 71104.0, 71106.0, 71110.0, 71112.0, 71113.67, 71114.0, 71115.0, 71115.2, 71117.67, 71117.48, 71119.26, 71120.0, 71119.0, 71121.0, 71123.0, 71124.0, 71125.0, 71126.16, 71127.0, 71126.0, 71129.0, 71129.39, 71127.6, 71130.0, 71131.12, 71128.0, 71135.0, 71136.0, 1644000.0, 71138.63, 71138.0, 71140.0, 71141.54, 71141.0, 71137.0, 71144.0, 71143.0, 71146.0, 71147.0, 71149.0, 71150.0, 71151.0, 71154.0, 71156.0, 71157.0, 71158.0, 71159.0, 71160.0, 71162.0, 71164.0, 71165.0, 71164.28, 71167.0, 71168.0, 71169.0, 71170.0, 71172.0, 71173.0, 71173.98, 71175.0, 71176.85, 71177.6, 71177.0, 71174.0, 71180.0, 71181.0, 71182.0, 71176.0, 71184.0, 71185.0, 71184.8, 71188.0, 71190.0, 71191.0, 71194.0, 71195.0, 71196.0, 71197.44, 71198.0, 71199.0, 71200.0, 595489.1, 71202.0, 71203.75, 71201.0, 71203.0, 71206.0, 71204.0, 71208.0, 71209.0, 71210.0, 71209.88, 71212.0, 71216.64, 71216.0, 71218.0, 71219.2, 71220.0, 71222.0, 71223.0, 71224.0, 71225.0, 71227.27, 71230.0, 71231.0, 71232.0, 71233.0, 71234.0, 71234.52, 71236.0, 71238.0, 71239.0, 71240.0, 71241.0, 71243.0, 71244.0, 71245.44, 71245.0, 71247.0, 71248.0, 71249.0, 71250.0, 71250.14, 71251.0, 71250.24, 71253.0, 71255.0, 71256.0, 71258.0, 71259.0, 71260.0, 71261.0, 333406.0, 71263.0, 71265.0, 71266.0, 71267.88, 71267.0, 71269.0, 71270.0, 71272.0, 71274.36, 71277.0, 71278.0, 71280.0, 71281.0, 71282.0, 71283.0, 71285.0, 71286.0, 71288.0, 71290.0, 71292.0, 71294.0, 71295.0, 71296.0, 71297.0, 71298.0, 71299.0, 71300.0, 71299.8, 71302.0, 71301.0, 71304.0, 71302.15, 71308.0, 71309.0, 71310.0, 71311.0, 71312.25, 71313.0, 71308.08, 71315.0, 71316.0, 71317.0, 71319.0, 71320.0, 71321.0, 71322.0, 71324.0, 71325.0, 71326.08, 71326.0, 71328.0, 71329.0, 71330.0, 71331.0, 71332.0, 71334.0, 71335.0, 71336.0, 71338.0, 71339.0, 71340.0, 71341.0, 71342.0, 71343.86, 71344.0, 71343.0, 71346.0, 71347.0, 71349.0, 71350.0, 71351.0, 71352.0, 71353.0, 71352.96, 71355.0, 71356.0, 71358.0, 71359.0, 71360.0, 71361.0, 71362.64, 71364.0, 71367.0, 71368.0, 71369.0, 71370.0, 71372.04, 71373.0, 71374.0, 71375.0, 71376.0, 71373.45, 71377.16, 71379.0, 71380.0, 71381.0, 71385.0, 71386.5, 71388.0, 71390.0, 71391.0, 71392.0, 71392.45, 71394.0, 71395.0, 71396.0, 71397.0, 71398.0, 71399.0, 71400.0, 71398.59, 71402.0, 71395.38, 71404.0, 71400.16, 71406.4, 71407.0, 71408.0, 71409.0, 71410.0, 71406.0, 71412.0, 71411.36, 71409.48, 71411.0, 71416.0, 71417.0, 71409.34, 71419.33, 71420.0, 71421.0, 71422.0, 71415.0, 71424.0, 71424.48, 71425.0, 71427.0, 71427.2, 1120000.0, 71430.04, 71431.0, 71432.0, 71426.0, 71434.0, 71435.0, 71436.0, 71436.34, 71438.0, 71439.0, 71440.0, 71440.08, 71438.4, 71444.0, 71445.0, 71447.0, 71448.0, 71449.0, 71450.0, 71452.0, 71453.0, 71454.0, 71454.25, 333600.0, 71455.0, 71458.0, 71459.56, 71460.0, 71462.0, 71464.0, 71466.0, 71467.0, 71468.0, 71469.0, 71466.72, 71470.14, 71472.96, 71472.0, 71473.0, 71475.0, 71478.0, 71479.72, 71480.0, 71481.0, 71481.73, 71484.0, 71486.0, 71487.0, 71488.0, 71489.0, 71490.0, 71491.0, 71492.0, 71493.0, 71495.0, 71496.0, 71498.0, 71499.0, 71500.0, 71498.4, 71502.0, 71502.36, 71504.0, 71506.0, 71508.52, 71508.0, 71510.0, 71511.0, 71512.0, 71513.04, 71514.0, 71516.0, 71517.44, 71520.0, 71522.0, 71523.0, 71524.4, 71525.0, 71526.0, 71524.0, 71528.0, 71527.62, 71524.44, 71531.2, 71532.0, 71530.44, 71534.0, 71535.0, 71536.0, 71537.0, 71540.0, 71542.0, 71544.0, 71545.0, 71548.0, 71550.0, 71552.0, 71553.69, 71554.0, 66325.0, 71556.0, 71556.87, 71557.0, 71560.0, 71564.0, 71565.0, 858000.0, 71569.0, 71568.0, 71570.0, 71572.0, 71573.32, 71574.6, 71575.0, 71574.72, 71577.0, 71578.0, 71579.0, 71580.0, 71572.8, 71581.0, 71580.65, 71584.0, 71585.0, 71586.0, 71588.0, 71589.0, 71590.0, 71592.0, 71593.68, 71592.04, 71595.0, 71596.32, 71596.0, 71598.0, 71599.0, 71600.0, 71601.0, 71597.0, 71603.0, 71604.72, 71604.0, 71605.0, 71607.0, 71610.0, 71611.0, 71613.0, 71614.1, 71615.0, 71616.0, 71617.0, 71618.0, 71619.0, 71620.66, 71620.0, 71621.0, 71623.0, 71624.07, 71625.0, 71624.0, 71627.08, 71628.6, 71628.0, 71630.0, 71631.0, 71629.0, 71632.0, 71627.0, 71635.0, 71629.2, 71638.64, 71639.62, 71640.0, 71639.0, 71641.0, 71643.14, 71642.0, 71645.0, 71649.5, 71650.0, 71651.0, 71652.0, 71653.02, 71654.0, 71649.0, 71656.0, 71657.52, 71653.0, 71659.0, 71660.0, 71661.0, 71662.76, 71663.0, 71664.0, 71665.0, 71666.0, 71666.22, 71668.81, 71668.0, 71670.0, 71662.0, 71672.0, 71673.0, 71674.0, 71675.0, 71676.0, 71677.0, 71678.0, 71676.8, 71680.0, 71682.0, 71683.0, 71684.0, 71685.0, 71686.0, 71688.0, 71689.8, 71690.0, 71689.0, 71692.0, 71694.0, 71695.0, 71697.0, 71698.0, 71699.0, 71700.0, 71703.0, 71704.0, 71703.63, 71705.0, 71707.0, 71708.0, 71709.0, 71710.0, 71711.0, 71712.0, 71709.57, 71714.0, 71706.0, 71716.0, 71717.0, 71718.4, 71719.21, 71720.0, 71718.0, 71722.0, 71724.0, 71725.0, 71724.48, 71726.0, 71728.0, 71729.0, 71730.0, 71727.5, 71729.04, 71733.0, 71734.26, 71732.5, 71736.32, 71736.0, 71738.0, 71739.62, 71740.0, 71739.0, 71741.0, 71744.0, 71745.0, 71747.0, 71749.42, 71750.0, 71751.0, 71752.0, 71753.0, 71754.0, 71755.0, 71750.12, 71757.0, 71758.0, 71758.2, 71760.0, 71762.0, 71763.6, 71763.0, 71764.0, 71766.0, 71767.0, 71766.66, 71765.0, 71770.0, 71771.0, 71773.0, 71774.0, 71775.0, 71776.0, 4266080.0, 71778.0, 71779.0, 71780.0, 71780.31, 71782.0, 71783.0, 71784.7, 71785.0, 71781.0, 71787.0, 71784.0, 71791.0, 71791.2, 71793.8, 71793.0, 333938.0, 71796.0, 71797.0, 71798.0, 71799.0, 71800.0, 71801.0, 71802.0, 71803.2, 71805.0, 71806.0, 71807.0, 71808.0, 71812.0, 71813.0, 71814.0, 71815.0, 66377.32, 71817.24, 71818.0, 66377.0, 71820.0, 66378.0, 71822.4, 71823.0, 71825.0, 71825.92, 71828.0, 71829.0, 71830.0, 71831.0, 71832.0, 71833.0, 71836.0, 71837.0, 71838.0, 71839.0, 71840.0, 71841.0, 71842.0, 71843.0, 71844.0, 71845.0, 4004000.0, 71847.0, 71848.0, 71849.0, 71850.0, 71852.0, 71854.0, 71855.0, 334000.0, 71856.0, 66385.56, 71859.0, 71859.92, 71860.0, 71862.0, 71864.0, 71865.0, 71864.98, 71867.0, 71868.0, 71873.0, 71874.0, 71876.0, 66389.0, 71879.0, 71880.0, 71880.38, 71882.0, 71883.0, 71884.0, 71885.0, 71886.0, 71887.0, 71888.0, 71889.0, 71890.43, 71890.0, 71892.0, 71889.94, 66392.19, 71895.2, 71891.0, 71897.0, 71898.0, 71899.0, 71900.0, 71901.0, 71902.0, 66393.6, 71904.0, 71906.0, 71907.0, 66395.6, 71909.0, 71910.0, 71911.0, 71912.34, 71906.56, 71914.0, 71916.0, 66397.44, 71918.0, 71919.2, 71920.0, 71921.0, 71922.0, 71925.0, 71926.4, 71927.0, 71928.0, 596215.0, 71930.0, 71928.24, 71926.0, 71935.0, 71936.0, 71937.0, 71938.0, 71939.0, 71936.41, 71941.0, 71942.0, 71940.0, 71944.0, 66402.0, 71946.0, 71947.0, 71948.0, 71950.0, 71952.0, 71953.0, 71955.0, 71959.0, 71960.0, 71964.0, 71967.0, 71968.0, 71972.17, 71974.0, 71975.0, 71976.0, 71975.03, 71978.0, 71980.0, 71981.0, 71982.98, 71982.0, 71983.0, 71984.0, 71983.89, 71986.0, 71987.0, 71988.0, 71985.0, 71990.0, 71991.0, 71992.0, 71989.69, 71994.0, 71995.0, 71988.8, 71998.68, 71999.0, 72000.0, 71998.0, 72002.58, 72002.0, 72003.0, 72005.0, 72006.0, 72007.0, 72008.0, 72009.0, 72010.0, 72011.0, 72012.0, 72015.0, 72016.0, 72019.0, 72020.0, 72022.0, 72024.0, 72025.0, 72026.0, 72029.0, 72030.0, 72030.5, 72032.0, 72035.29, 72036.0, 72037.0, 72035.0, 72038.0, 72040.0, 72041.0, 72042.0, 72044.0, 72046.0, 72049.0, 72050.0, 72051.0, 72054.0, 72055.0, 72057.0, 72060.0, 72061.0, 72065.0, 72071.0, 72072.0, 72072.97, 72074.0, 72076.0, 72077.0, 72078.0, 72079.0, 72080.0, 72081.0, 72077.87, 72084.0, 72085.0, 72087.0, 66431.54, 72090.39, 72090.0, 72092.0, 72095.0, 72096.0, 72095.88, 72097.0, 72099.0, 72100.0, 72102.42, 72102.0, 72103.0, 72106.12, 72108.0, 72109.0, 72110.0, 72108.4, 72112.0, 72113.6, 72114.0, 72115.0, 72110.04, 72116.0, 72118.0, 72119.0, 72120.0, 72120.29, 72122.0, 72124.0, 72125.0, 72126.0, 72127.0, 72130.0, 72131.0, 72132.0, 72134.0, 72135.0, 72136.0, 72137.0, 72139.0, 72140.0, 72141.0, 72141.2, 72142.0, 72144.0, 72145.0, 72146.88, 72147.24, 72147.0, 72148.0, 72150.0, 72151.68, 72152.0, 72152.04, 72153.0, 72155.0, 72156.0, 72158.0, 72158.84, 72160.8, 72160.0, 72162.11, 72161.0, 72164.0, 72167.0, 72168.0, 72168.6, 72171.0, 72172.0, 72173.0, 72174.0, 72175.0, 72176.0, 72178.08, 72179.0, 72180.0, 72182.0, 72183.0, 72185.0, 72186.0, 72188.0, 72189.0, 72190.0, 72191.0, 72192.0, 72193.0, 72194.0, 72195.0, 72196.0, 72197.95, 72198.0, 72200.0, 72201.0, 72201.94, 72203.0, 72204.0, 72204.22, 72205.0, 72208.8, 72209.9, 72209.0, 72210.0, 72212.0, 72212.28, 72214.0, 72215.76, 72216.0, 72217.6, 72218.0, 72219.0, 72220.0, 72217.0, 72222.0, 72219.32, 72225.28, 72225.0, 72227.0, 72228.0, 72226.0, 72230.0, 72231.0, 72233.0, 72234.0, 72235.0, 66460.0, 72238.0, 72240.0, 72241.0, 72244.0, 72244.32, 72246.96, 72247.0, 72248.0, 72249.0, 72250.0, 72245.0, 72252.0, 72253.11, 72254.0, 72252.89, 72256.0, 72258.0, 72259.0, 72260.0, 72263.0, 72264.0, 72265.68, 72266.0, 72267.0, 72269.0, 72270.0, 72271.0, 72272.0, 72276.0, 72277.0, 72280.0, 72281.0, 72283.0, 72284.0, 72285.0, 72286.0, 72287.0, 72287.42, 72289.0, 72288.0, 72291.0, 72296.0, 72300.0, 72301.0, 72303.0, 72304.0, 72305.0, 72306.0, 72307.8, 72306.65, 72309.0, 72308.6, 72311.0, 72312.79, 72313.0, 72314.28, 72315.0, 72312.0, 72317.0, 72318.0, 72319.0, 72320.0, 72321.0, 72322.0, 72324.0, 72325.09, 72326.0, 72329.0, 72330.0, 72331.0, 72332.0, 72331.77, 72334.0, 72335.0, 72336.0, 72337.0, 72340.0, 72341.12, 72342.0, 72343.0, 72345.0, 72346.0, 72347.0, 72348.0, 72349.0, 72350.0, 72351.0, 72352.0, 72353.28, 72354.0, 72353.0, 72356.0, 72357.0, 72358.0, 72359.0, 72360.0, 72363.0, 72365.0, 72366.0, 72368.0, 72369.0, 72370.0, 72371.0, 72372.0, 72373.0, 72374.46, 72375.0, 72374.0, 66489.0, 72380.0, 72381.0, 72382.0, 72383.0, 72384.0, 72385.51, 72386.0, 72385.0, 72388.0, 72392.0, 72395.0, 72396.0, 72398.0, 72399.0, 72400.0, 72402.0, 72403.0, 72408.0, 72409.0, 72410.0, 72409.1, 72412.0, 72415.0, 72416.0, 72417.0, 72420.0, 72421.0, 72423.0, 72424.0, 72425.0, 72425.5, 72427.0, 72428.0, 72429.0, 72430.0, 72431.0, 72432.0, 72433.0, 72434.0, 72436.0, 72438.0, 72440.0, 72441.0, 72442.0, 72444.0, 72445.0, 72446.0, 72447.0, 72450.0, 66503.0, 72452.0, 72453.0, 72454.0, 72455.0, 72456.0, 72453.6, 72457.56, 72460.0, 72462.71, 72463.0, 72464.0, 72465.0, 72466.0, 72467.0, 72468.0, 72469.07, 72470.0, 72471.0, 72469.0, 72473.0, 72474.0, 72475.0, 72476.53, 72477.0, 72478.0, 72476.0, 72479.0, 72481.0, 72482.06, 72483.0, 72482.0, 72485.98, 72486.0, 72480.0, 72488.0, 72490.0, 72492.0, 72493.0, 72495.0, 72496.98, 72497.0, 72498.0, 72499.0, 72500.0, 72501.56, 72501.0, 72503.0, 72504.0, 72503.84, 72499.96, 72500.04, 72508.0, 72509.0, 72510.0, 72508.8, 72512.0, 72511.1, 72515.0, 72516.0, 72517.0, 72520.0, 72521.0, 72522.0, 72520.76, 72524.0, 72528.0, 72529.0, 596818.0, 72529.6, 72530.0, 72533.0, 72534.0, 72535.0, 334680.0, 72537.0, 72538.0, 72539.0, 72540.0, 72541.0, 72542.0, 72543.0, 72544.0, 72545.0, 72546.56, 72547.0, 72548.16, 72548.0, 72550.0, 72551.0, 72552.0, 72547.62, 72555.0, 72556.0, 72557.0, 72558.0, 72559.0, 72560.84, 72561.0, 72560.0, 72562.0, 72564.0, 72565.0, 72566.0, 72567.0, 72568.0, 72569.0, 72570.0, 72563.0, 72572.0, 72573.0, 72571.0, 72575.0, 72576.0, 72577.0, 72579.0, 72580.0, 72581.0, 72582.0, 72583.0, 72583.24, 72585.0, 72586.0, 72587.0, 72588.0, 72589.0, 72586.56, 72591.0, 72592.0, 72594.0, 72595.0, 72598.0, 72599.0, 72600.0, 72602.0, 72604.0, 72605.0, 72606.0, 72608.0, 72610.0, 72612.0, 72613.32, 72613.0, 72615.68, 72615.0, 72615.84, 72616.0, 72619.0, 72620.0, 72621.0, 72618.0, 72623.0, 72624.0, 72625.0, 72626.99, 72627.0, 72629.0, 72630.0, 72631.0, 72632.0, 72633.6, 72634.0, 72633.0, 72631.35, 72637.0, 72638.0, 72635.0, 72640.0, 72636.0, 72642.0, 72643.0, 72644.0, 72645.0, 72645.36, 72643.37, 72649.0, 72650.0, 72652.0, 72654.0, 72655.0, 334800.0, 72657.0, 72658.0, 72659.0, 72660.0, 72658.78, 72662.0, 72656.0, 72664.0, 66545.0, 72666.0, 72669.0, 72670.0, 72671.0, 72672.0, 72674.0, 72675.0, 72676.8, 72677.0, 72679.0, 72680.0, 72681.0, 72683.0, 72684.0, 72685.0, 72686.0, 72687.0, 72689.0, 72690.0, 72692.0, 72693.0, 72694.0, 72696.0, 72697.0, 72698.0, 72699.0, 72700.0, 72701.0, 72702.24, 72703.0, 72704.0, 72706.0, 72707.88, 72708.0, 72710.0, 72711.0, 72712.0, 597000.0, 72710.88, 72715.0, 72716.0, 72714.0, 72713.0, 72719.14, 72720.0, 72719.0, 72721.19, 72723.0, 72722.0, 72726.0, 72729.0, 72730.0, 72731.0, 72731.59, 72733.0, 72734.0, 72735.0, 72732.0, 72738.0, 334884.88, 72740.0, 72742.0, 72742.15, 72744.0, 72745.0, 72746.64, 72747.0, 72744.48, 72749.0, 72750.0, 72751.0, 72752.0, 72748.0, 72746.0, 72755.0, 72756.0, 72757.16, 72758.0, 72753.0, 72760.0, 72761.0, 72759.0, 72764.0, 72765.0, 72768.0, 72772.0, 72773.0, 72774.0, 72775.0, 72776.91, 72776.0, 72777.0, 72779.0, 72780.0, 72778.0, 72782.76, 72783.0, 72784.0, 72786.0, 72787.0, 72788.0, 72790.0, 72791.0, 72792.0, 72793.0, 72794.0, 72796.0, 72799.0, 72800.0, 72802.0, 72803.81, 72804.0, 72803.0, 72806.0, 72808.0, 72809.5, 72810.0, 72812.0, 72813.0, 72814.0, 72815.0, 72816.0, 72817.16, 72817.0, 72819.0, 72820.0, 72821.0, 72822.0, 72818.0, 72824.0, 72825.0, 72826.0, 72827.0, 72828.6, 72828.0, 72830.24, 72829.0, 72830.0, 72833.0, 72830.6, 72831.0, 72836.0, 72835.0, 72838.0, 72839.0, 72840.0, 72841.0, 72836.35, 72843.0, 72844.0, 72844.8, 72846.24, 72847.0, 72848.0, 72850.0, 72852.0, 72853.0, 72855.0, 335000.0, 72856.0, 72858.0, 72859.0, 72860.0, 72861.0, 72862.0, 72858.96, 72864.0, 72865.0, 72866.0, 72867.0, 72868.0, 72870.0, 72871.0, 72872.0, 72874.0, 72875.0, 72876.0, 72877.0, 72878.0, 72880.0, 72881.0, 72883.0, 72885.0, 72886.0, 72886.39, 72890.0, 72891.0, 72892.0, 72893.0, 72895.02, 72895.0, 72895.68, 72898.57, 72899.0, 72900.0, 72899.74, 72896.0, 72898.0, 72904.0, 72905.0, 72906.0, 72908.0, 72909.0, 72911.0, 72912.0, 72913.0, 72916.0, 72917.0, 72919.0, 72920.0, 72922.98, 72923.0, 72924.0, 72925.0, 72926.0, 72922.0, 72927.0, 72930.0, 72934.0, 72936.0, 72937.0, 72937.59, 72938.0, 72940.0, 72941.0, 72943.0, 72945.0, 72946.0, 72947.0, 72946.56, 72949.0, 72950.0, 72952.0, 72957.0, 72958.0, 72960.0, 72961.0, 72965.0, 72966.0, 72967.0, 72970.0, 72971.0, 72972.0, 72972.35, 72974.0, 72975.0, 72976.0, 72977.0, 72980.0, 72984.0, 72985.0, 72987.0, 72988.0, 72989.0, 72989.76, 72990.0, 72987.2, 72993.0, 72994.0, 72995.0, 72996.0, 72998.0, 72999.0, 73000.0, 72999.99, 73001.0, 72999.42, 73006.0, 73007.0, 73008.0, 73010.0, 73011.0, 73012.0, 66615.68, 73015.0, 73015.8, 73018.56, 73020.0, 73021.0, 73023.0, 73025.0, 73026.72, 73027.0, 73028.0, 73030.0, 73032.0, 73034.0, 73034.25, 73037.0, 73040.0, 73041.0, 73041.21, 73044.0, 73045.0, 73046.0, 73049.0, 73052.38, 73052.0, 73054.0, 73055.0, 73056.0, 73057.0, 73058.0, 73059.0, 73060.0, 73061.0, 73053.13, 73063.56, 73064.0, 73065.0, 73066.0, 73061.08, 73068.0, 73069.0, 73070.4, 73070.0, 73073.26, 73075.96, 73075.0, 73077.0, 73076.94, 73079.48, 73080.0, 73079.0, 73081.0, 73083.0, 73078.0, 73080.16, 73084.0, 73087.0, 73088.0, 73091.0, 73092.0, 73093.0, 73094.0, 73095.32, 73096.0, 73095.0, 73098.0, 73099.0, 73100.0, 73101.0, 335246.0, 73102.0, 73104.0, 73105.0, 73106.0, 73102.8, 73109.3, 73110.0, 73111.0, 73112.0, 73113.0, 73114.84, 73115.0, 73114.8, 73117.78, 73118.0, 73116.0, 73120.0, 73121.0, 73122.0, 73117.0, 73124.0, 73125.24, 73125.0, 73127.0, 73128.0, 73129.0, 73130.0, 73131.0, 73132.0, 73130.04, 73134.84, 73135.0, 73136.0, 73138.0, 73139.0, 73140.0, 73142.0, 73144.0, 73145.92, 73146.0, 73147.0, 73147.2, 73149.0, 73150.0, 73152.0, 73153.0, 73154.0, 73156.0, 73157.0, 73158.0, 73159.0, 73160.0, 73161.0, 73162.0, 73163.2, 73164.0, 73165.0, 73166.0, 73162.14, 73163.44, 73163.68, 73170.0, 73171.0, 73172.0, 73173.0, 73174.0, 73174.4, 73176.0, 73177.0, 73175.5, 73180.0, 73184.0, 73185.0, 73184.06, 73187.0, 73188.0, 73188.36, 73190.0, 73191.8, 73192.0, 73194.0, 73195.0, 73196.0, 73195.2, 73197.0, 73199.0, 73200.0, 73200.48, 73204.08, 73205.43, 73205.0, 73207.0, 73208.0, 73209.0, 73210.0, 73209.03, 73212.0, 73211.0, 73215.0, 73215.48, 73217.0, 335360.0, 73216.0, 73220.0, 73221.2, 73222.59, 73223.0, 73224.0, 73222.08, 73226.34, 73227.0, 73222.0, 73229.0, 66657.0, 73230.0, 73232.0, 73233.0, 73235.0, 73236.0, 73237.0, 73238.0, 73239.0, 73240.0, 73241.0, 73242.0, 73237.81, 66661.0, 73246.8, 73247.0, 73248.0, 73249.0, 73250.0, 73248.14, 73252.0, 73245.0, 73253.0, 73255.0, 73256.0, 73257.0, 73258.0, 73257.6, 73260.0, 73261.0, 73262.0, 335406.0, 73265.0, 73266.0, 66665.64, 73268.0, 73269.0, 73270.0, 73271.0, 73272.0, 73275.0, 73278.0, 73280.0, 73281.09, 73282.0, 73281.0, 73284.0, 73288.56, 73288.0, 73291.0, 1121869.0, 73296.0, 73297.0, 73298.0, 73299.0, 73300.0, 73301.0, 73298.16, 73303.0, 73298.08, 73305.0, 73306.0, 73300.2, 73308.0, 73309.8, 73308.72, 73311.0, 73312.0, 73313.0, 73314.0, 73310.0, 73316.0, 73315.0, 73319.0, 73320.0, 73324.8, 73325.0, 73326.0, 73325.36, 73328.0, 73327.0, 73330.67, 73330.0, 73332.0, 73327.8, 73334.0, 73335.0, 73336.0, 73337.0, 73338.0, 73335.92, 73340.0, 73334.2, 73339.81, 73343.0, 73344.0, 73340.8, 73346.0, 73347.0, 73339.0, 73350.0, 73350.12, 73353.0, 73356.36, 73356.0, 73358.22, 73359.0, 73360.0, 73358.66, 73362.84, 73363.0, 73364.0, 73365.54, 73366.0, 73367.0, 73368.0, 73363.33, 73370.0, 73371.0, 73372.0, 73374.0, 73375.0, 73376.0, 73378.0, 73380.0, 73381.2, 73382.0, 73383.32, 73384.0, 73385.0, 73386.0, 73381.0, 73388.0, 73390.0, 73392.0, 73395.0, 73396.0, 73396.68, 73397.0, 73398.0, 73400.0, 73401.0, 73402.0, 73403.0, 73403.16, 73404.0, 73406.0, 73403.2, 73408.0, 73410.0, 73411.0, 73412.0, 73413.6, 73416.0, 66695.0, 73417.0, 73419.0, 73420.0, 73421.0, 73424.0, 73425.64, 73426.0, 73427.0, 73428.0, 73425.0, 73430.48, 73430.0, 73432.0, 73433.99, 73434.0, 73433.0, 73436.07, 73436.0, 73440.0, 73441.0, 73444.0, 73445.0, 73445.4, 73448.0, 73450.0, 73452.0, 73453.0, 73454.0, 73455.96, 73456.0, 73457.0, 73458.0, 73459.0, 73460.0, 73461.0, 73462.0, 73463.0, 73464.0, 73465.6, 73465.0, 73467.12, 73464.2, 73466.0, 73470.0, 73471.0, 73472.0, 73473.0, 73476.0, 73477.0, 73478.0, 73479.0, 73480.0, 73478.08, 73483.0, 73484.0, 73485.0, 73485.1, 73486.0, 73488.0, 73489.0, 73490.0, 73492.0, 73494.0, 73495.0, 73496.0, 73494.94, 73497.0, 73499.0, 73500.0, 73501.0, 73502.0, 73503.88, 73505.0, 73506.0, 73507.0, 73509.0, 73510.0, 73511.0, 73512.0, 73513.0, 73514.0, 73515.0, 73516.0, 73517.0, 73518.0, 73519.0, 73520.0, 73521.5, 73522.0, 73517.09, 73524.0, 73525.0, 73526.0, 73527.0, 73528.0, 73521.0, 73530.0, 73531.0, 73532.0, 73527.96, 73534.0, 73536.0, 73537.0, 73538.0, 73539.6, 73540.0, 73541.11, 73542.0, 73544.0, 73545.0, 73546.0, 73548.0, 73549.0, 73550.0, 73551.0, 73553.0, 73554.0, 73556.0, 73557.0, 73558.4, 73558.0, 73560.66, 73560.0, 73562.0, 73563.0, 73568.0, 860000.0, 73570.0, 73569.0, 73572.0, 73574.0, 73575.0, 73578.0, 73579.44, 73580.0, 73581.0, 73582.0, 73579.0, 73584.0, 73585.0, 73586.0, 73587.0, 73588.0, 73589.0, 73590.0, 73590.4, 73594.0, 73595.0, 73596.0, 73597.0, 73599.0, 73600.0, 73602.0, 73603.0, 73604.0, 73605.0, 73608.0, 73609.0, 73611.0, 73618.0, 73619.76, 73620.0, 73622.0, 73623.0, 73626.0, 73628.0, 73632.0, 73633.0, 73634.0, 73635.0, 73636.0, 73638.0, 73639.77, 73640.0, 73641.0, 73642.95, 73643.0, 73644.0, 73645.0, 73646.0, 73644.96, 73648.0, 73649.0, 73650.0, 73652.0, 73653.0, 73654.0, 73655.0, 73656.0, 73658.0, 73659.0, 73660.0, 73661.0, 73662.0, 73664.0, 73665.0, 73666.0, 73665.22, 73668.0, 73669.0, 73670.0, 73671.0, 73668.96, 73673.6, 73674.0, 73675.0, 73672.0, 73677.0, 73678.61, 73678.0, 73680.0, 73673.0, 73682.0, 73683.0, 73685.0, 73686.0, 73687.0, 73688.0, 73690.0, 73691.2, 73691.0, 73693.0, 73694.0, 73695.0, 73696.0, 73697.0, 73698.0, 73692.0, 73700.0, 73703.0, 73704.0, 73706.0, 73707.0, 73709.0, 73710.0, 73711.0, 73712.0, 598000.0, 73714.0, 73715.2, 73715.0, 73716.0, 73717.0, 73715.88, 73720.0, 73721.0, 73723.0, 73724.0, 73724.51, 73728.0, 73730.0, 73731.09, 73731.0, 73732.0, 73734.0, 73735.0, 73736.0, 73737.0, 73738.0, 73740.0, 73741.0, 73743.67, 73744.0, 73745.0, 73746.0, 73747.0, 73744.56, 73749.0, 73750.0, 73751.0, 73752.0, 73756.0, 73757.0, 73759.0, 73760.0, 73761.0, 73760.88, 73763.0, 73764.0, 73762.0, 73766.4, 73767.0, 73768.5, 73769.0, 73770.16, 73771.0, 73772.0, 73765.0, 73775.0, 73776.0, 73777.0, 73779.0, 73780.0, 73783.0, 73784.0, 73788.0, 73790.0, 73792.0, 73793.0, 73796.83, 73798.0, 73799.0, 73800.0, 73799.75, 73802.96, 73802.26, 73804.8, 73805.61, 73806.0, 73804.5, 73808.0, 73801.0, 73803.0, 73811.0, 73812.0, 335950.0, 73809.0, 73815.0, 73819.0, 73820.0, 73822.4, 73822.0, 73824.0, 73825.0, 73822.44, 598116.0, 73829.0, 73832.0, 73833.0, 73834.54, 73839.0, 73840.0, 73842.0, 73844.0, 73845.0, 73846.0, 73847.0, 73848.0, 73850.0, 73851.82, 73853.0, 73854.0, 73855.0, 336000.0, 73856.0, 73858.0, 73860.0, 73861.8, 73860.55, 73863.0, 73863.3, 73865.0, 73861.0, 73867.0, 73862.0, 73869.0, 73868.0, 73871.0, 73869.36, 73865.42, 73866.92, 73876.47, 73876.0, 73878.0, 73879.0, 73880.0, 73880.04, 73882.0, 73884.24, 73884.25, 73887.0, 73890.0, 73891.0, 73893.0, 73894.0, 73895.0, 73896.0, 73897.0, 73899.0, 73900.0, 73899.13, 73902.0, 73901.0, 73904.0, 73905.0, 73903.0, 73907.0, 73908.0, 73909.0, 73911.0, 73912.0, 73913.0, 73917.0, 73918.0, 73920.0, 73922.0, 73923.0, 73928.0, 73929.92, 73932.0, 73937.0, 73939.0, 73940.0, 73941.0, 73942.0, 73942.76, 73944.0, 73942.47, 73948.52, 73949.72, 73950.0, 73951.0, 73952.0, 73955.0, 73956.0, 73957.0, 73957.2, 73959.12, 73960.0, 73961.0, 73962.0, 73959.57, 73966.0, 73968.0, 73969.0, 73972.0, 73974.0, 73975.0, 73978.0, 73979.0, 73980.0, 73982.0, 73985.6, 73986.0, 73985.0, 73988.0, 73990.0, 73992.0, 73993.0, 73996.0, 73997.0, 73999.0, 74000.0, 74001.0, 73999.92, 74003.0, 74004.0, 74005.0, 74006.0, 73999.74, 74008.08, 74009.0, 74008.0, 74011.0, 74012.0, 74013.0, 74015.0, 74016.0, 74020.0, 74022.0, 74022.47, 74024.0, 74025.0, 74022.63, 74027.0, 74028.0, 74029.56, 74030.0, 74027.2, 74032.0, 74029.0, 74033.0, 74036.24, 74036.0, 74038.0, 74040.0, 74042.0, 74045.0, 74049.0, 74050.0, 74052.0, 74054.0, 74055.84, 74056.0, 74057.0, 74058.0, 74059.0, 74060.0, 74061.0, 74060.6, 74064.0, 74067.0, 74068.0, 74069.0, 74070.0, 74072.42, 74073.0, 74074.0, 74075.0, 74076.0, 74072.0, 74080.0, 74081.0, 74082.0, 74082.58, 74086.0, 74087.0, 74088.0, 74089.0, 74090.0, 74091.0, 74094.0, 74095.0, 74096.0, 74097.0, 74098.2, 74098.0, 74100.0, 74101.0, 74103.0, 74104.0, 74105.39, 74106.0, 74107.0, 74105.75, 74110.0, 74110.39, 74112.0, 74113.0, 74114.0, 74115.0, 74116.0, 74116.44, 74117.0, 74110.4, 74120.0, 74118.0, 74122.0, 74123.0, 74124.0, 74125.0, 74125.96, 74126.28, 74128.0, 74129.0, 74130.0, 74131.0, 74130.76, 74133.0, 74134.08, 74134.0, 74136.0, 74138.0, 74139.0, 74140.0, 74141.0, 74142.0, 74143.68, 74144.0, 74145.06, 74147.0, 74148.0, 74149.4, 74149.0, 74151.22, 74151.0, 74152.0, 74150.0, 74155.8, 74155.0, 74157.46, 74154.0, 598447.0, 74160.0, 74158.0, 74162.0, 74166.53, 74167.44, 74168.0, 74170.0, 74171.0, 74172.8, 74172.0, 74174.36, 74173.0, 74176.0, 74178.0, 74179.0, 66847.3, 74180.0, 74182.0, 74183.0, 74184.0, 74185.0, 74187.0, 74188.0, 74189.0, 74190.0, 74191.2, 74192.0, 74193.6, 74193.0, 74195.0, 74196.0, 74194.0, 74198.0, 74200.0, 74202.0, 74206.0, 74208.0, 74209.56, 74210.0, 74211.0, 74210.84, 74214.0, 74215.0, 74216.0, 74214.4, 74218.0, 74219.0, 74220.04, 74220.0, 74221.0, 74223.11, 74221.87, 74225.0, 74226.0, 74222.0, 74226.36, 74229.0, 74230.0, 74231.0, 74232.0, 74233.0, 74234.0, 74235.0, 74236.0, 74237.28, 74238.62, 74239.0, 74240.78, 74240.0, 74233.73, 74243.0, 74244.0, 74245.0, 74245.44, 74246.0, 74248.0, 74247.6, 74250.0, 74250.05, 74252.0, 74253.0, 74249.76, 74256.0, 74260.0, 74261.0, 74262.0, 74263.0, 74264.0, 74265.0, 74266.0, 74267.0, 74268.0, 74270.0, 74272.0, 74274.0, 74275.0, 74276.8, 74277.0, 74277.36, 74279.0, 74280.0, 74281.0, 74275.38, 74283.0, 74277.48, 74276.0, 74289.0, 74290.0, 74291.0, 74292.0, 74292.48, 74295.0, 74297.0, 74298.0, 74299.0, 74300.0, 74301.0, 74305.0, 74306.0, 74308.0, 74309.0, 74310.0, 74311.0, 74312.0, 74314.0, 74315.0, 74316.0, 74315.75, 74318.0, 74318.39, 74320.0, 74321.0, 74322.83, 74323.0, 74324.0, 74325.0, 74326.0, 74322.0, 74328.0, 74329.32, 74330.0, 74331.0, 74326.08, 74333.0, 74327.52, 74335.0, 74336.0, 74337.0, 74338.0, 74339.0, 74340.0, 74337.72, 74342.0, 74343.0, 74344.0, 74345.0, 74346.0, 74349.0, 74350.0, 74351.0, 74352.0, 74353.0, 74356.0, 74357.19, 74358.0, 74358.92, 74360.0, 74361.96, 74363.64, 74364.0, 74366.0, 74368.0, 74371.6, 74373.47, 74374.0, 74375.0, 74376.0, 74376.43, 74378.0, 74379.0, 74380.0, 74381.76, 74382.0, 74383.0, 74380.8, 74385.0, 74386.0, 74384.86, 74390.0, 74392.1, 74393.0, 74392.24, 74395.43, 74397.0, 74400.0, 74401.0, 74401.51, 74404.0, 74412.0, 74413.0, 74414.0, 74415.0, 336558.0, 74418.0, 74420.0, 74423.0, 74424.0, 1123000.0, 74425.0, 74426.48, 74426.0, 74428.0, 74430.0, 74432.0, 74433.6, 74434.0, 74435.0, 74436.0, 74434.96, 74439.0, 74440.0, 74441.16, 74443.77, 74443.2, 74445.0, 74443.0, 74447.0, 74448.0, 74449.0, 74450.0, 74451.0, 74450.54, 74450.5, 74454.0, 336595.0, 74456.0, 74457.0, 74458.08, 74457.6, 74460.0, 74461.0, 74462.0, 74463.0, 74464.0, 74465.0, 74466.0, 74458.93, 74468.0, 74469.46, 74470.78, 74470.0, 74472.0, 74471.0, 74467.0, 74475.0, 5579500.0, 74477.0, 74473.0, 74479.0, 74480.0, 74481.0, 74476.0, 74484.0, 74485.0, 74487.0, 74488.0, 74490.0, 74491.0, 74496.0, 336640.0, 74498.0, 74499.0, 74500.0, 74502.75, 74502.0, 74504.0, 74505.0, 74506.0, 74507.0, 74505.08, 74509.0, 74512.0, 74513.0, 66913.0, 74515.0, 74516.0, 74517.0, 74519.0, 74520.0, 74521.0, 74523.77, 74524.0, 74525.0, 74524.29, 74527.0, 74528.0, 74529.0, 1123100.0, 74531.0, 74532.0, 74533.0, 74535.0, 74536.0, 74538.0, 66918.0, 74540.0, 74541.32, 74542.0, 74544.0, 74546.0, 74547.0, 74548.0, 74547.2, 74550.0, 74551.0, 74552.0, 74546.83, 74554.0, 74555.0, 74556.0, 74557.0, 74558.08, 74558.07, 74560.0, 74553.0, 74558.0, 74563.0, 74562.0, 74565.0, 74566.0, 74567.0, 74568.0, 74569.0, 74572.0, 74574.0, 74575.0, 74579.0, 74580.0, 74584.0, 74586.0, 74587.0, 74588.0, 74590.0, 74591.36, 74592.0, 74593.0, 74591.0, 74595.0, 74597.0, 74598.0, 74599.0, 74600.0, 74601.0, 74602.0, 74603.0, 74604.0, 74605.08, 336746.0, 74608.0, 74609.0, 74610.0, 74611.0, 74612.0, 74614.0, 74615.0, 74616.0, 74617.0, 74618.0, 74619.0, 74620.0, 74616.96, 74622.0, 74623.5, 74624.0, 74625.0, 74626.0, 74618.45, 74628.0, 74628.52, 74630.0, 74638.0, 74640.0, 74641.0, 74644.0, 74646.0, 74647.62, 74649.0, 74650.0, 74651.0, 74652.0, 74654.0, 74655.0, 74657.0, 74658.0, 74660.0, 74662.0, 74664.0, 74666.0, 74669.0, 74670.0, 74671.0, 74672.0, 74673.0, 74675.0, 74676.0, 74676.24, 74678.0, 74680.0, 74682.0, 74683.0, 74684.0, 74685.0, 74686.0, 74687.0, 74688.0, 74689.0, 74690.0, 74691.0, 74692.8, 74692.0, 74694.0, 74695.0, 74696.0, 74697.0, 74698.0, 74699.0, 74700.0, 74701.0, 74702.0, 74704.96, 74706.84, 74710.0, 74712.86, 74713.0, 74714.0, 74715.4, 74715.0, 74717.0, 74718.0, 74716.0, 74720.0, 74722.0, 74724.0, 74725.0, 74726.0, 74727.0, 74731.0, 74732.0, 74733.22, 74734.0, 74736.0, 74737.0, 74739.0, 74740.0, 74741.0, 74742.0, 74743.0, 74744.0, 74745.0, 74747.0, 74748.0, 74750.0, 74751.0, 74752.48, 74751.6, 74754.25, 74754.0, 74755.0, 74757.0, 74758.0, 74759.0, 74760.0, 74761.2, 74762.0, 74765.0, 74766.0, 74767.0, 74768.56, 74768.0, 74769.0, 74770.0, 74772.0, 74774.0, 74775.0, 74776.0, 74777.92, 74778.0, 74779.0, 74780.0, 74781.83, 74784.0, 74788.0, 74789.0, 74790.0, 74788.26, 74791.0, 74793.0, 74794.0, 74795.0, 74796.8, 74796.0, 74797.0, 74792.0, 74800.0, 74804.08, 74805.12, 74808.0, 74810.0, 74811.0, 74812.0, 74812.16, 74815.0, 74816.0, 74818.0, 74819.0, 74820.0, 74821.0, 74824.0, 336970.0, 74827.0, 74829.0, 74832.0, 74833.0, 74835.0, 74837.0, 74838.4, 74838.0, 74840.0, 74841.0, 74842.0, 74839.91, 74844.0, 74846.0, 74847.0, 74848.0, 74850.0, 74851.0, 74852.88, 74854.0, 74855.0, 337000.0, 74857.0, 74854.83, 74856.0, 74860.0, 74861.0, 74859.0, 74863.8, 74866.98, 74867.0, 74868.0, 74869.0, 74870.0, 74871.0, 74872.0, 74873.0, 74874.0, 74876.0, 74876.76, 74878.0, 74880.0, 74881.0, 74882.0, 74883.0, 74884.0, 74884.2, 74885.0, 74888.0, 74889.0, 74890.0, 74891.0, 74892.0, 74893.0, 74894.0, 74895.0, 74898.0, 74899.0, 74900.0, 74899.81, 74903.0, 74904.64, 74905.0, 74904.0, 74907.0, 74908.0, 74909.0, 74907.6, 74911.0, 74912.0, 74910.0, 74909.88, 74915.0, 74916.0, 74918.0, 74918.59, 74920.0, 74921.49, 74922.0, 74924.0, 74925.5, 74926.0, 74928.0, 74930.0, 74932.0, 74934.0, 74935.0, 74936.0, 74940.0, 74942.4, 74944.0, 74945.0, 74949.0, 74950.0, 67000.08, 74952.0, 67000.68, 74954.0, 74955.0, 74958.0, 74960.0, 74961.0, 74962.0, 74964.0, 74965.0, 74967.0, 74967.72, 74969.0, 74970.0, 74971.0, 74972.0, 74973.96, 74968.0, 74976.0, 74977.0, 74978.0, 74979.0, 74980.0, 74979.79, 74984.0, 74985.0, 74986.0, 74987.0, 74989.0, 74990.0, 74992.0, 74994.67, 74994.55, 74994.0, 74998.0, 74999.0, 75000.0, 75001.0, 75002.0, 75002.27, 75004.8, 75004.0, 75006.0, 75005.0, 75000.64, 75009.23, 75009.0, 75010.0, 75012.0, 75011.64, 75011.0, 75015.0, 75016.0, 75018.0, 75021.0, 75023.0, 75024.0, 75025.6, 75025.0, 75028.0, 75031.0, 75032.0, 75032.62, 75034.0, 75035.0, 75036.0, 75037.0, 75038.0, 75039.48, 75040.0, 75041.0, 75043.0, 75044.0, 75046.0, 75047.0, 75048.0, 75049.0, 75050.0, 75052.0, 75054.0, 75054.98, 75055.0, 75057.0, 75058.0, 75059.0, 75060.0, 75061.0, 75062.0, 75063.0, 75066.0, 75067.0, 75072.0, 75072.6, 75074.0, 75075.0, 75076.0, 75072.4, 75080.0, 75084.0, 75085.0, 75086.39, 75087.0, 75088.0, 75090.0, 75091.0, 75093.0, 75094.0, 75095.28, 75096.0, 75100.0, 75102.8, 75103.0, 75104.44, 75105.0, 75106.0, 75106.8, 75108.0, 75109.0, 75110.0, 75111.4, 75112.14, 75113.64, 75114.84, 75115.0, 75111.0, 75117.6, 75118.0, 75120.0, 75122.0, 75125.0, 75127.0, 75128.0, 75129.0, 75130.0, 75132.0, 75136.0, 75138.48, 75140.0, 75142.0, 75143.0, 75144.0, 75147.24, 75150.0, 75150.4, 75152.0, 75153.0, 75154.0, 75155.0, 75156.0, 75157.0, 75159.0, 75160.0, 75161.0, 75160.44, 75164.0, 75166.0, 75167.0, 75168.5, 75169.0, 75168.0, 75171.0, 75172.0, 75170.0, 75173.0, 75176.0, 75177.0, 75178.0, 75179.0, 75180.0, 75181.21, 75180.6, 75183.0, 75181.0, 75185.0, 75186.0, 75187.0, 75187.6, 75189.0, 75190.0, 75191.0, 75192.0, 75188.0, 75195.0, 75196.0, 75198.0, 75199.0, 75200.0, 75201.0, 75200.16, 75202.0, 75204.0, 75206.0, 75209.94, 75210.0, 75211.0, 75209.0, 75212.0, 75214.0, 75215.0, 75216.0, 75218.0, 75219.0, 75220.0, 75222.0, 75224.0, 75225.0, 75226.0, 75227.0, 75228.0, 75229.0, 75230.0, 75232.0, 75233.6, 75234.0, 75235.0, 75233.0, 75237.0, 75238.0, 75239.0, 75240.0, 75241.0, 75240.51, 75247.0, 75249.36, 75250.0, 75250.08, 75252.0, 75253.0, 75254.0, 337400.0, 75258.0, 75259.0, 75260.0, 75261.0, 75264.0, 75266.0, 75267.0, 75268.0, 75271.0, 75272.0, 75274.0, 75275.0, 75276.0, 75277.0, 75274.94, 75279.0, 75280.0, 75281.03, 75285.0, 75286.0, 75288.0, 75289.92, 75290.88, 75291.0, 75290.0, 75294.44, 75296.0, 75298.08, 75299.0, 75300.0, 75298.0, 75302.0, 75303.0, 75302.71, 75306.39, 75307.52, 75308.22, 75309.0, 75310.0, 75311.0, 75312.0, 75313.6, 75314.0, 75313.0, 75308.31, 75317.0, 75315.0, 75319.0, 75320.0, 75321.0, 75323.0, 75324.0, 75325.0, 75326.0, 75327.0, 75328.92, 75329.0, 75332.0, 75333.0, 75335.0, 75336.0, 75337.6, 75338.0, 75338.12, 75338.36, 75341.0, 75340.0, 75342.0, 75344.0, 75345.0, 75343.0, 75343.21, 75348.0, 75349.0, 75350.96, 75350.0, 75352.0, 75353.74, 75350.75, 75355.56, 337500.0, 75357.0, 75356.0, 75359.0, 75360.0, 75362.0, 75363.45, 75364.0, 75365.0, 75366.0, 75367.26, 75368.0, 75369.0, 75370.0, 75367.0, 75372.0, 75373.0, 75374.0, 75375.0, 75376.0, 75377.0, 75376.32, 75379.0, 75380.0, 75381.15, 75382.0, 75384.0, 75386.0, 75386.04, 75388.0, 75390.0, 75391.06, 75395.0, 75396.0, 75398.0, 75400.0, 75402.0, 75404.04, 75405.0, 75406.0, 75407.0, 75408.0, 75409.0, 75411.0, 75412.0, 75415.0, 75416.0, 75419.0, 75420.0, 75421.0, 75422.0, 75423.0, 75425.0, 75428.88, 75429.0, 75430.0, 75433.0, 75434.0, 75433.8, 75435.0, 75437.0, 75438.0, 75439.0, 75440.0, 75441.0, 75442.0, 75443.0, 75444.0, 75445.0, 75437.52, 75447.0, 75450.0, 75452.0, 75454.0, 75456.0, 75457.0, 75458.13, 75459.0, 75460.0, 75461.0, 75462.4, 337600.0, 75465.0, 75466.0, 75467.0, 75468.0, 75469.0, 75470.0, 75472.0, 75473.0, 75474.0, 75475.0, 75478.0, 75480.0, 75483.2, 75483.0, 75485.0, 75486.0, 75484.0, 75488.0, 75489.0, 75490.0, 75491.0, 75492.0, 75495.0, 75496.0, 75496.61, 75498.0, 75500.0, 75501.0, 75504.0, 75505.0, 75506.0, 75509.0, 75510.0, 75513.84, 75514.0, 75515.0, 75519.0, 75520.0, 67113.0, 75524.0, 75525.0, 75524.8, 75528.0, 75530.0, 75531.0, 75532.86, 75534.0, 75536.0, 75537.0, 75538.0, 75539.0, 75540.0, 75541.0, 75543.0, 75544.0, 75547.0, 75548.0, 75549.91, 75550.0, 75551.0, 75552.0, 75553.0, 75554.0, 75555.0, 75556.0, 75557.0, 75558.0, 337703.0, 75560.0, 75561.0, 75562.0, 75566.4, 75566.0, 75568.0, 75569.0, 75570.0, 75571.0, 75572.0, 75573.0, 75574.0, 75575.0, 75576.0, 75580.0, 75582.0, 75584.13, 75585.0, 75586.0, 75587.0, 75588.0, 75589.0, 75590.0, 75591.0, 75586.5, 75593.0, 75595.3, 75596.0, 75596.51, 75597.48, 67129.0, 75600.0, 75601.0, 75602.0, 75597.0, 75595.0, 75599.0, 75607.0, 75608.0, 75608.09, 75610.0, 75612.0, 75613.68, 75614.0, 75619.0, 75620.0, 75621.0, 75622.0, 75623.0, 75624.0, 75628.0, 75629.0, 75631.0, 75633.0, 75634.0, 75638.0, 75639.0, 75639.72, 75640.39, 75640.0, 75643.0, 75644.0, 75643.2, 75646.0, 75641.0, 75648.0, 75649.0, 75650.0, 75649.6, 75652.0, 75654.0, 75655.0, 75654.72, 75659.0, 75660.0, 75661.0, 75662.0, 75663.0, 75664.19, 75667.0, 75668.0, 75669.0, 75670.0, 75672.0, 75673.0, 75674.0, 75675.0, 75677.0, 75678.0, 75680.0, 75681.0, 75682.0, 75683.0, 75685.0, 75686.0, 75688.0, 75689.4, 75689.0, 75691.0, 75690.0, 75693.81, 75692.0, 75695.0, 75696.0, 75697.0, 75698.0, 75695.93, 75700.0, 75702.0, 75703.0, 75704.0, 75705.0, 75705.55, 75704.16, 75708.0, 75706.0, 75710.0, 75709.0, 600000.0, 75712.0, 600003.0, 75716.0, 75718.0, 75718.87, 75720.0, 75721.0, 75722.0, 75723.0, 75724.0, 75719.74, 75727.44, 337871.0, 75730.0, 75732.0, 75734.0, 75735.12, 75736.0, 75737.0, 75739.0, 75740.0, 75741.0, 75742.0, 75744.0, 75745.0, 75746.16, 75747.0, 75748.0, 75746.0, 75750.0, 75751.0, 75752.0, 75753.0, 75754.0, 75755.09, 75756.0, 75755.0, 75749.0, 75759.0, 75760.0, 75761.0, 75763.0, 75764.0, 75765.0, 75766.0, 75768.0, 75769.0, 75773.0, 75774.0, 75775.0, 75776.4, 75776.0, 75778.0, 75780.0, 75782.0, 75784.0, 75785.0, 75787.0, 75788.0, 75790.0, 75791.0, 75790.52, 75793.0, 75792.0, 75796.0, 75797.0, 75796.56, 75799.0, 75800.0, 75798.0, 75802.0, 75804.0, 75805.0, 75806.0, 75807.0, 75808.0, 75809.0, 75810.0, 75811.0, 75812.0, 75813.0, 75814.0, 75815.0, 75816.0, 75818.88, 75820.0, 75821.0, 75822.72, 75822.0, 75824.0, 75825.44, 75826.0, 337966.0, 75828.0, 75830.47, 75830.0, 75832.0, 75833.0, 75831.0, 75835.0, 75836.14, 75836.0, 75837.0, 75839.27, 75840.0, 75839.0, 75842.0, 75842.2, 75844.0, 75845.0, 75846.0, 75846.16, 75841.0, 75849.0, 75850.0, 75851.0, 75852.0, 75852.79, 67179.0, 75856.58, 338000.0, 75857.0, 75857.69, 75858.0, 75860.0, 75861.42, 75863.0, 75864.0, 75865.03, 75866.0, 75867.0, 75868.0, 75869.0, 75870.0, 75872.0, 75873.0, 75872.6, 75875.0, 75876.13, 75877.0, 75876.0, 75875.4, 75877.37, 75878.0, 75882.0, 75880.0, 75884.0, 75885.96, 75885.0, 75886.0, 75888.0, 75889.0, 75890.0, 75891.0, 75892.0, 75894.0, 75896.28, 75896.0, 75897.78, 75899.0, 75900.0, 75901.75, 75902.0, 75905.0, 75907.0, 75911.0, 75912.0, 75911.52, 75915.0, 75916.0, 75916.39, 75918.0, 75919.0, 75920.0, 75918.84, 75922.0, 75918.36, 75926.0, 75928.0, 75935.0, 75936.96, 75937.0, 75938.0, 75939.0, 75940.0, 75942.84, 75943.0, 75942.0, 75945.87, 75945.0, 75944.0, 75948.0, 75949.0, 75950.0, 75951.0, 75952.0, 75952.24, 75955.0, 75956.0, 75960.0, 75964.0, 75965.0, 75966.0, 75967.0, 75968.0, 75971.0, 75971.76, 75973.0, 75974.0, 75972.0, 75976.56, 75977.0, 75976.0, 75975.0, 75980.0, 75981.0, 75981.49, 75982.0, 75984.0, 75985.0, 75986.0, 75988.0, 75990.0, 75991.0, 75996.0, 75998.0, 75999.0, 76000.0, 75999.96, 76003.2, 76004.0, 76004.9, 76006.0, 76007.87, 76008.0, 76007.0, 76010.0, 76012.0, 76013.0, 76015.0, 76018.8, 76020.0, 76023.0, 76024.0, 76025.07, 76028.67, 76028.0, 76031.0, 76032.0, 76034.0, 76035.0, 76038.0, 76040.0, 76041.18, 76041.0, 76042.0, 76044.0, 338188.0, 76048.0, 76049.98, 76050.0, 338196.0, 76052.0, 76054.0, 76056.0, 76057.0, 76058.0, 76060.0, 76064.0, 76069.0, 76070.0, 76073.76, 76074.0, 76075.0, 76076.0, 76077.93, 76078.0, 76079.0, 76080.0, 76081.19, 76082.0, 76081.0, 76085.0, 76086.0, 76087.0, 76085.67, 76089.0, 76090.0, 76092.0, 76093.08, 76094.0, 76095.0, 76096.08, 76097.0, 76093.0, 76100.0, 76101.0, 76103.0, 76104.0, 76108.0, 76110.0, 76112.0, 76113.36, 76114.0, 76113.26, 76116.0, 76115.0, 600400.0, 76119.0, 76120.0, 76121.0, 76121.63, 76123.0, 76124.0, 76125.0, 76126.0, 76122.0, 76128.0, 76130.0, 76131.0, 76132.5, 76132.0, 76134.0, 76135.0, 76136.0, 76137.0, 76139.0, 76140.0, 76141.0, 76142.0, 76144.0, 76145.0, 76146.0, 76147.0, 76148.0, 76149.0, 76150.0, 76151.35, 76152.0, 76153.0, 76155.0, 338300.0, 76157.0, 76159.0, 76160.0, 76161.36, 76162.0, 76161.0, 76164.0, 76165.0, 76167.0, 76168.0, 76169.0, 76170.0, 76174.0, 76175.94, 76176.0, 76178.0, 76178.94, 76180.0, 76179.63, 76182.0, 76183.0, 76184.95, 76185.0, 76187.64, 76188.0, 76189.68, 76190.88, 76190.0, 76191.64, 76192.0, 76194.0, 76193.03, 76196.0, 76197.0, 76198.0, 76197.42, 76200.0, 76201.0, 76202.0, 76195.0, 76204.0, 76199.69, 76206.41, 76206.24, 76209.0, 76210.0, 76211.0, 76212.0, 76214.0, 76215.0, 76216.0, 76217.6, 76218.0, 76219.0, 76220.0, 76220.04, 76222.0, 76221.0, 76224.0, 76225.0, 76226.0, 76229.0, 76230.0, 76232.28, 76233.0, 76234.0, 76232.0, 76236.0, 76235.0, 76238.0, 76239.0, 76240.0, 76241.0, 76242.0, 76243.0, 76244.04, 76244.0, 76245.0, 76246.0, 76248.0, 76248.12, 76250.0, 76251.0, 76252.8, 67259.0, 76254.38, 76254.0, 76260.0, 76261.0, 76262.0, 76263.0, 76265.0, 76266.0, 76267.0, 76268.0, 76270.0, 76272.6, 76273.0, 76272.0, 76274.0, 76276.0, 76277.0, 76278.0, 76275.0, 76280.0, 76281.36, 76284.0, 76285.2, 76287.0, 76288.0, 76290.63, 76290.0, 76292.0, 76293.0, 76295.0, 76296.0, 76298.0, 76299.0, 76300.0, 76301.0, 76307.0, 76308.0, 76309.0, 76310.0, 76311.0, 76312.0, 76313.0, 76314.0, 76319.0, 76320.0, 76321.08, 76322.0, 76323.0, 76324.0, 76325.0, 76326.0, 76319.3, 76328.0, 76329.0, 76330.0, 76331.04, 76332.0, 76333.0, 76334.0, 76336.0, 76337.0, 76340.0, 76341.44, 76342.0, 76343.0, 76344.0, 76345.0, 76346.0, 76347.33, 76348.0, 76347.0, 76350.0, 76349.0, 76352.5, 76355.0, 76356.0, 76358.0, 76360.6, 76362.0, 76363.0, 76364.0, 76365.31, 76365.0, 76367.0, 76368.0, 76369.0, 76371.0, 76373.0, 76374.17, 76373.68, 76377.0, 76378.0, 76379.04, 76380.0, 76377.04, 76384.0, 76385.0, 76386.0, 76388.0, 76390.0, 76391.0, 76392.0, 76394.0, 76395.0, 76396.0, 76398.4, 76398.92, 76400.0, 76401.0, 76399.0, 76398.0, 76404.0, 76405.0, 76406.0, 338548.0, 76402.0, 76409.0, 76410.0, 76411.0, 76412.0, 76413.0, 76414.0, 67290.0, 76416.0, 76410.25, 76419.0, 76421.0, 76421.53, 76422.0, 76424.0, 1125000.0, 76426.0, 76427.0, 76428.0, 76429.0, 76425.0, 76431.0, 76432.0, 76433.0, 338579.0, 76436.0, 76437.0, 76438.0, 76440.0, 76441.0, 76442.0, 76443.6, 76444.0, 76445.0, 76446.0, 76447.0, 76448.0, 76450.0, 76452.0, 76453.0, 76454.0, 76455.0, 338600.0, 76456.0, 76457.0, 76460.0, 76461.0, 76462.0, 76460.8, 76464.0, 76465.0, 76466.0, 76467.0, 76468.0, 76469.0, 76470.0, 76471.0, 76472.0, 76472.52, 76475.0, 76476.0, 76478.0, 76480.0, 76481.0, 76482.0, 76483.0, 76484.48, 76484.0, 76485.0, 76486.08, 76488.0, 76489.56, 76490.0, 76491.0, 76492.0, 76490.96, 76494.0, 76493.0, 76496.0, 76497.0, 76498.0, 76497.5, 76500.0, 76501.0, 76502.0, 76502.4, 76504.0, 76501.67, 76506.0, 76508.0, 76510.0, 76512.0, 76513.0, 76514.0, 76515.0, 76519.82, 76520.0, 76522.0, 76523.0, 76524.0, 76526.0, 76528.37, 76529.0, 76530.0, 76530.13, 76533.0, 76534.0, 76536.0, 76539.0, 76540.0, 76541.0, 76542.0, 76543.0, 76544.0, 76545.0, 76545.04, 76546.08, 76549.19, 76550.0, 76552.0, 76554.0, 76555.0, 76556.0, 76557.0, 76558.0, 76560.0, 76564.8, 76564.0, 76565.0, 76567.66, 76570.25, 76571.92, 76572.0, 67323.0, 76576.0, 76577.0, 76578.0, 76580.0, 76583.57, 76584.0, 76585.0, 76586.0, 76588.0, 76590.0, 76591.0, 76592.0, 76594.0, 76595.0, 76596.0, 76597.0, 76598.64, 76598.0, 76600.0, 76601.0, 76602.0, 76603.0, 76604.0, 76605.0, 76606.4, 76606.0, 76608.0, 76610.0, 76611.33, 76614.0, 76616.0, 76617.0, 76617.24, 76619.16, 76620.0, 76621.92, 76622.0, 76623.84, 76619.0, 76623.0, 76626.0, 76619.26, 76628.0, 76631.0, 76632.0, 76634.0, 76635.24, 76636.0, 76637.0, 76639.68, 76640.0, 76642.0, 76643.0, 76644.0, 76647.0, 76648.0, 76650.0, 76651.0, 76653.0, 76654.0, 76656.0, 76657.0, 76658.0, 76659.24, 76659.0, 76661.0, 76660.0, 76663.0, 76664.0, 76665.0, 76666.0, 76667.0, 76668.0, 76669.0, 76670.05, 76669.44, 76672.0, 76670.0, 76674.0, 76675.0, 76673.0, 76677.0, 76680.0, 76682.0, 76683.0, 76684.0, 76685.0, 76686.0, 76688.0, 76690.0, 76691.0, 76692.0, 76693.74, 76695.84, 76696.36, 76697.0, 76698.0, 76695.0, 76700.0, 76699.0, 76702.0, 76704.0, 76705.0, 76706.0, 76709.0, 76710.0, 76714.0, 76715.0, 76716.0, 76716.56, 76719.0, 76720.0, 67352.52, 76723.0, 76724.0, 76725.0, 76726.0, 76727.0, 76727.76, 76728.0, 76726.68, 76731.0, 76732.0, 76733.0, 76734.0, 76735.0, 76736.0, 76739.0, 76740.0, 76741.0, 76742.0, 76742.4, 76744.0, 76746.0, 76748.0, 76750.0, 76752.0, 76755.21, 76756.84, 76757.0, 76758.0, 76760.0, 76762.0, 76763.81, 76764.0, 76763.0, 76766.0, 76767.0, 76765.0, 76770.0, 76771.0, 76772.0, 76772.8, 76774.0, 76775.0, 76776.0, 76771.72, 76777.0, 76780.0, 76781.0, 76784.0, 76785.0, 76788.0, 76790.0, 76791.0, 76792.0, 76793.0, 76794.0, 76796.0, 76797.0, 76799.0, 76800.0, 76801.08, 76801.0, 76803.0, 76804.0, 76805.0, 76804.77, 76809.0, 76810.0, 76812.0, 76813.0, 76814.0, 76815.0, 76818.0, 76819.0, 76820.0, 76822.26, 76824.0, 76830.0, 76831.32, 76832.0, 76831.0, 76834.0, 76835.0, 76836.0, 76835.63, 76838.0, 76839.0, 76840.0, 76842.0, 76843.0, 76844.0, 76845.0, 76846.0, 76848.0, 76849.0, 76850.0, 76848.84, 76852.0, 76849.2, 76854.0, 76854.88, 339000.0, 76856.0, 76859.0, 76860.0, 76861.44, 76862.0, 76863.0, 76864.0, 76865.0, 76866.11, 76867.0, 76868.0, 601157.0, 76870.0, 76866.0, 76872.0, 76873.0, 76874.0, 76875.0, 76876.8, 76876.0, 76871.0, 76879.0, 76880.0, 76885.0, 76886.0, 76887.0, 76888.0, 76889.0, 76890.0, 76891.0, 76892.0, 76894.0, 76895.0, 76896.72, 76897.0, 76898.0, 76899.0, 76900.0, 76901.0, 76896.0, 76903.0, 76904.0, 76905.98, 76902.0, 76907.0, 76909.0, 76910.0, 76911.0, 76914.15, 76914.48, 76914.8, 76918.0, 76919.0, 76920.0, 76921.0, 76923.0, 76924.0, 76927.0, 76928.0, 76929.62, 76932.0, 76935.2, 76936.0, 76937.08, 76938.0, 76939.2, 76939.0, 76941.0, 76937.0, 76944.0, 76948.0, 76950.0, 76955.0, 76956.0, 76957.53, 76958.0, 76957.0, 76960.0, 76959.48, 76962.0, 76963.0, 76964.0, 76965.0, 76961.0, 76968.0, 76970.0, 76972.56, 76973.0, 76974.0, 76975.0, 76972.0, 76977.0, 76978.0, 76979.0, 76980.0, 76977.14, 76983.0, 76984.0, 76985.0, 76987.95, 76987.0, 76990.0, 76992.0, 76993.0, 67406.0, 76995.0, 76996.0, 76998.0, 76999.92, 77000.0, 77001.0, 76999.0, 77004.0, 77005.25, 77006.0, 77005.0, 77010.0, 77012.0, 77013.92, 77016.0, 77017.0, 67411.0, 77019.5, 77020.0, 77021.0, 77022.4, 77022.36, 77022.0, 77025.0, 77023.0, 77019.0, 77028.0, 77032.0, 339180.0, 77036.0, 77040.0, 77043.0, 77044.0, 77045.0, 77047.0, 77048.0, 77049.16, 77050.0, 77049.0, 77052.7, 77052.0, 77054.0, 77053.0, 77058.0, 57118.4, 77060.0, 67419.0, 77063.0, 77064.0, 77065.0, 61677.2, 77067.0, 77067.42, 77068.0, 77069.0, 77072.0, 77073.0, 77074.0, 77075.0, 77077.0, 77078.0, 77079.46, 77078.47, 77081.0, 77083.0, 77084.0, 77085.0, 77086.0, 77087.0, 77088.0, 77090.0, 77091.0, 77092.0, 77093.0, 77094.0, 77096.0, 77098.0, 77100.0, 77103.0, 77105.0, 77106.0, 77107.0, 77109.0, 77110.0, 77111.0, 77112.0, 77110.74, 77114.0, 77115.0, 77116.0, 77117.0, 77118.0, 77120.0, 77121.0, 77122.0, 77123.52, 77125.67, 77126.4, 77126.0, 77128.87, 77125.0, 77130.0, 77125.24, 77132.0, 77133.0, 77134.12, 77136.0, 1650000.0, 77137.0, 77138.0, 77140.0, 77139.0, 77142.0, 77144.0, 77145.0, 77147.0, 77148.0, 77150.72, 77150.0, 77150.46, 77151.72, 77154.0, 77156.0, 77157.03, 77158.0, 77159.0, 77160.0, 77157.0, 77162.0, 77161.0, 77164.0, 77165.0, 77168.0, 77169.0, 77170.0, 77172.0, 77174.64, 77175.0, 77177.88, 77177.0, 77179.0, 77180.0, 77181.0, 77182.0, 77183.0, 77184.0, 77187.0, 77189.57, 77191.0, 77193.0, 77194.0, 77195.0, 77196.0, 77197.0, 77198.0, 77198.28, 77200.0, 77197.45, 77201.0, 77206.0, 77207.0, 77208.0, 77209.0, 77210.0, 77210.28, 77211.0, 77213.0, 77215.0, 77216.0, 77219.0, 77220.0, 67451.0, 77222.0, 77224.0, 77225.0, 77225.5, 77227.08, 77230.0, 77231.0, 77232.0, 77235.0, 77236.0, 77237.0, 77239.42, 77240.0, 77241.0, 77242.0, 77242.8, 77244.0, 77243.0, 77246.0, 77247.0, 77248.0, 77249.0, 77250.0, 77251.2, 77252.0, 77251.0, 77250.43, 77256.0, 77257.0, 77258.0, 77260.0, 77262.0, 77265.0, 77266.0, 77265.97, 77268.7, 77270.0, 77271.84, 77272.0, 77273.0, 77274.25, 77274.0, 77275.92, 77270.8, 77278.0, 77276.0, 77280.0, 77279.0, 77282.0, 77284.0, 77285.0, 67464.24, 77287.0, 77289.0, 77290.0, 77291.4, 77292.36, 77292.0, 77291.0, 77294.57, 77297.25, 77299.64, 77300.0, 77300.08, 77302.0, 77303.0, 77304.0, 77305.0, 77306.0, 77307.0, 77305.46, 77305.53, 77310.0, 77312.0, 77316.0, 77317.66, 77318.0, 77319.0, 77320.0, 77320.84, 77321.0, 77323.0, 77324.0, 77325.0, 77326.0, 77327.0, 77326.66, 77330.0, 77332.0, 77333.0, 77334.0, 77335.0, 77338.0, 77340.0, 67475.0, 77343.0, 77344.0, 77345.0, 77349.0, 77350.0, 77352.0, 77355.0, 77360.16, 77361.0, 77362.0, 77362.75, 77364.0, 77360.0, 77366.0, 77367.0, 77363.0, 77369.0, 77370.0, 77371.0, 77372.5, 77373.0, 77368.0, 77375.0, 77376.0, 77378.0, 77379.0, 77380.0, 77381.0, 77381.02, 77382.0, 77384.0, 77385.0, 77383.0, 77387.0, 77388.0, 77385.19, 77390.0, 77391.0, 77392.0, 77393.0, 77394.0, 77395.0, 77396.8, 77396.0, 77398.0, 77400.0, 77401.0, 77402.0, 77404.0, 77405.0, 77410.0, 77411.0, 77412.0, 77414.0, 77415.0, 77416.0, 77417.0, 77418.0, 77420.0, 77421.0, 77423.7, 77424.0, 77425.0, 77427.0, 77428.0, 77429.0, 77430.0, 77431.0, 77434.0, 77436.0, 77438.0, 77439.0, 77440.0, 77441.0, 77442.0, 77443.0, 77444.0, 77445.0, 77448.0, 77449.0, 77450.0, 77452.0, 77453.0, 77454.0, 77455.0, 77456.0, 77457.0, 77456.82, 77459.0, 77460.0, 77460.6, 77460.53, 77463.0, 77463.36, 77459.2, 77466.6, 77467.0, 77461.16, 77469.0, 77472.0, 77473.59, 77473.84, 77477.0, 77478.24, 77479.0, 77480.0, 77481.0, 77483.0, 77488.0, 77490.0, 77491.2, 77492.0, 77494.0, 77495.0, 77496.0, 77497.0, 77498.0, 77499.0, 77500.0, 77501.0, 77504.0, 77506.0, 77507.0, 77508.0, 77510.0, 77513.0, 77514.0, 77515.0, 77516.0, 77518.0, 77520.0, 77521.0, 77522.0, 77525.0, 77527.0, 77529.0, 77530.0, 339675.0, 77532.0, 77533.0, 77534.0, 77533.82, 77536.0, 77535.0, 77538.0, 77539.0, 77540.0, 77541.96, 77542.0, 77543.0, 77544.19, 77544.0, 77546.57, 77545.0, 77547.0, 77550.0, 77553.0, 77554.0, 77556.0, 77558.0, 77560.0, 77561.0, 77563.0, 77564.0, 77565.0, 77567.0, 77568.0, 864000.0, 77570.0, 77571.92, 77572.0, 77573.0, 67521.0, 77575.0, 77577.0, 77579.0, 77580.0, 77581.58, 77582.0, 77581.0, 77584.0, 77585.0, 77586.0, 77583.0, 77589.0, 77590.64, 77592.0, 77593.0, 77595.0, 77596.0, 77597.0, 77599.0, 77600.0, 77601.0, 77603.0, 77604.0, 77605.0, 77604.8, 77607.0, 77608.0, 77610.0, 77611.0, 77612.0, 77613.0, 77615.0, 77616.0, 77617.0, 77619.0, 77620.0, 339764.0, 77622.0, 77623.0, 77624.0, 77625.0, 77627.0, 77628.0, 77629.32, 77630.0, 77631.0, 77632.0, 77635.0, 77638.0, 77639.0, 77640.0, 77642.0, 77643.0, 77644.0, 77646.0, 77647.38, 77647.0, 77647.61, 77650.0, 77650.52, 77652.96, 77653.21, 77648.0, 77655.0, 77656.0, 77655.92, 77658.0, 77659.0, 77660.3, 77661.0, 77662.0, 77657.0, 77664.0, 77665.0, 77666.0, 77667.0, 77668.0, 77666.94, 77670.0, 77671.0, 77673.0, 77674.0, 77676.0, 77677.0, 77678.0, 77679.0, 77680.0, 77682.0, 77685.0, 77687.2, 77688.0, 77687.0, 77690.0, 77689.0, 77692.0, 77695.0, 77696.56, 77700.0, 77703.0, 77705.0, 77705.4, 77707.0, 77705.88, 77709.0, 77710.0, 77711.0, 77712.0, 77713.0, 77712.95, 77716.0, 77719.0, 77720.0, 77721.0, 77724.0, 77725.0, 77726.0, 77727.0, 77728.0, 77729.0, 77730.0, 77731.0, 77733.0, 77736.0, 77736.32, 77737.0, 77738.0, 77740.0, 77742.0, 77743.0, 77744.0, 77745.0, 77746.0, 77747.28, 77746.2, 77748.0, 77750.0, 77753.0, 77754.0, 77755.0, 77756.4, 77756.0, 77758.0, 77757.0, 77760.0, 77759.0, 77760.96, 77763.0, 77764.0, 77765.0, 77766.0, 77767.0, 77768.0, 77769.0, 77770.0, 77771.0, 77773.08, 77774.0, 77775.0, 77776.0, 77777.0, 77780.0, 77781.6, 77782.0, 77783.0, 77784.0, 77786.0, 77789.0, 77790.0, 77791.0, 77792.0, 77791.67, 77794.0, 77795.0, 77796.0, 77797.0, 77798.0, 77799.0, 77800.0, 77801.0, 77801.46, 77803.0, 77804.0, 67566.27, 77802.0, 77807.0, 67568.75, 77804.16, 77810.0, 77808.0, 77812.8, 77813.0, 77811.0, 77816.0, 77819.0, 77820.0, 77821.0, 77822.0, 77823.0, 77821.9, 77825.0, 77826.0, 77827.0, 77824.0, 77830.4, 77831.0, 77832.0, 77833.0, 77835.0, 77837.0, 77838.0, 67574.0, 77840.0, 602129.0, 77842.0, 77843.0, 77844.0, 77840.02, 77843.16, 77847.0, 67576.0, 77850.0, 77851.0, 77854.0, 340000.0, 77857.96, 77858.0, 77856.0, 77860.0, 77860.18, 77862.0, 77863.0, 77864.0, 77865.0, 77861.0, 77867.0, 77868.0, 77866.0, 77870.0, 77871.62, 77865.6, 77865.36, 77872.0, 77875.0, 77875.2, 77877.0, 77876.0, 77871.0, 77880.0, 77873.0, 77882.0, 77887.0, 77889.0, 77890.0, 77891.0, 77892.0, 77893.0, 77895.0, 77896.0, 77897.0, 77897.73, 77899.0, 77900.0, 77901.48, 77898.98, 77899.95, 77904.0, 77903.8, 77906.0, 77907.0, 77908.39, 77909.0, 77908.0, 77913.0, 77914.0, 77913.27, 77916.66, 77916.0, 77917.0, 77919.0, 77920.78, 77921.0, 77922.0, 77924.0, 77925.0, 77926.0, 77927.0, 77928.0, 77930.0, 77931.0, 77932.0, 77933.0, 77934.24, 77935.0, 77936.0, 77937.0, 77937.6, 77940.0, 77947.0, 77948.0, 77950.0, 77951.19, 77951.0, 77952.0, 77954.0, 77955.0, 77956.0, 77958.0, 77959.0, 77960.0, 77963.0, 77964.0, 77966.0, 77967.0, 77968.0, 77968.45, 77970.0, 77969.0, 77972.0, 77973.0, 77974.0, 77975.0, 77976.0, 77979.0, 77980.0, 77983.0, 77984.0, 77985.0, 77986.0, 77987.0, 77988.0, 77989.0, 77989.65, 77991.0, 77995.0, 77998.73, 77999.0, 78000.0, 78001.0, 78000.07, 78004.39, 78006.0, 78007.0, 78008.0, 78012.0, 78014.28, 78015.0, 78016.0, 78017.0, 78016.76, 78019.71, 78020.0, 78015.68, 78023.0, 78025.0, 78027.0, 78030.0, 78031.0, 78032.0, 78033.0, 78034.0, 78038.0, 78040.0, 78042.0, 78042.48, 78044.0, 78046.8, 78048.0, 78049.0, 78050.0, 78052.0, 78054.0, 78055.0, 78056.0, 78056.46, 78058.88, 78059.0, 78060.0, 78061.0, 78062.0, 78062.57, 78064.0, 78065.0, 78067.0, 78068.12, 78069.0, 78070.0, 78075.0, 78076.0, 78078.0, 78079.0, 78080.0, 78081.0, 78081.55, 78083.0, 78084.0, 78082.0, 78087.0, 78090.0, 78092.28, 78092.0, 78094.0, 78095.04, 78096.0, 78093.36, 78098.0, 78099.0, 78100.0, 78101.0, 78098.2, 78103.0, 78104.0, 78106.0, 78108.0, 78109.0, 78109.2, 78111.12, 78112.0, 78113.0, 78114.0, 78115.0, 78116.0, 78119.52, 78120.0, 78122.0, 78124.0, 78125.0, 78126.0, 78130.0, 78131.0, 78132.0, 78132.55, 78133.08, 78133.32, 78137.0, 78138.0, 78139.0, 78140.0, 78141.0, 78142.0, 78143.0, 78144.0, 78145.0, 78145.6, 78147.0, 78148.0, 78149.76, 78150.02, 78150.0, 78152.0, 78149.22, 78148.2, 78155.0, 78156.0, 78157.0, 78158.0, 78159.0, 78160.0, 78161.16, 78162.0, 78164.0, 78165.0, 78168.0, 78169.0, 78170.0, 78169.6, 78172.0, 78173.0, 78174.0, 78175.0, 67641.0, 78177.0, 78178.0, 78179.0, 78180.0, 78180.14, 78182.0, 78183.0, 78184.0, 78185.0, 78187.0, 78188.0, 78189.0, 78190.0, 78191.88, 78191.0, 78192.0, 78194.5, 78194.0, 78195.0, 78197.0, 78199.68, 78200.0, 78199.0, 67646.63, 78204.0, 78206.0, 78208.0, 78209.81, 78210.0, 78212.0, 78213.0, 78214.0, 78215.0, 78216.0, 78218.0, 78219.0, 78220.0, 78221.31, 78220.14, 78223.0, 78224.0, 78225.0, 78226.0, 78227.0, 78228.0, 78228.8, 78230.0, 78232.08, 78233.79, 78234.0, 78236.0, 78237.0, 78238.0, 78240.0, 78244.0, 78245.0, 78246.0, 78248.0, 78249.6, 78250.0, 78252.0, 78254.0, 78255.0, 78256.0, 78257.0, 78258.28, 78258.0, 78260.0, 78262.0, 78262.34, 78264.0, 78265.0, 78266.66, 78267.0, 78270.0, 78271.0, 78272.0, 78273.6, 78273.0, 78275.0, 78276.0, 78277.0, 78278.28, 78279.0, 78280.0, 78282.0, 78283.0, 78284.0, 78286.0, 78288.0, 78290.0, 78292.0, 78293.0, 78295.0, 78296.72, 78298.73, 78299.0, 78300.0, 78301.0, 78305.0, 78305.04, 78308.0, 78312.0, 78313.0, 78315.0, 78316.3, 78317.72, 78316.0, 78318.0, 78318.84, 78320.0, 78321.0, 78322.0, 78323.0, 78324.0, 78325.0, 78327.0, 78328.0, 78329.0, 78330.0, 78333.0, 78336.0, 78338.0, 78339.0, 78340.0, 78341.0, 78342.0, 78343.0, 78344.0, 78346.0, 78348.0, 78350.0, 78352.0, 78353.0, 78355.0, 602646.0, 78359.0, 78360.0, 78361.0, 78358.0, 78363.12, 78363.0, 78365.0, 78364.0, 67679.0, 78372.0, 78374.0, 78375.0, 78376.0, 78377.41, 78377.0, 78379.0, 78380.0, 78381.3, 78384.0, 78385.42, 78386.0, 78388.0, 78389.46, 78389.0, 78390.0, 78392.0, 78393.0, 78392.07, 78395.0, 78396.0, 78397.0, 78395.2, 78400.0, 78401.0, 78402.16, 78402.0, 78404.0, 78406.0, 78408.0, 78409.0, 78411.0, 78412.0, 78413.0, 78415.0, 78416.0, 78418.6, 78419.76, 78420.0, 78422.0, 78424.06, 78425.0, 78426.0, 78427.0, 78428.0, 78424.0, 78429.4, 78430.0, 78431.0, 78432.0, 78434.0, 78435.0, 78436.0, 78429.0, 78438.0, 602724.0, 78440.0, 78441.0, 78440.13, 78442.0, 78444.0, 78437.0, 78446.0, 78447.0, 78448.0, 78450.0, 78451.0, 78452.0, 78453.0, 78455.0, 78456.45, 78457.0, 78456.0, 78459.0, 78460.0, 78461.0, 78462.0, 78463.0, 78465.0, 78466.0, 78466.32, 78468.0, 78469.1, 78470.65, 78470.0, 78472.0, 78469.0, 78474.0, 78475.0, 78476.0, 78477.0, 78478.0, 78479.0, 78480.0, 78482.0, 78483.6, 78484.0, 78485.0, 78488.0, 78490.0, 78491.0, 78492.0, 78493.9, 78495.0, 78496.0, 78499.0, 78500.0, 78502.0, 78503.94, 78504.0, 78505.0, 78506.0, 78507.0, 78508.62, 78509.0, 78510.0, 78512.0, 78513.0, 78514.0, 78515.0, 78516.0, 78517.0, 78516.88, 78520.0, 78521.0, 78522.0, 78523.89, 78523.0, 78525.0, 78526.0, 78528.0, 78529.0, 78530.0, 78534.0, 78535.0, 78536.34, 78536.0, 78538.0, 78540.0, 78542.0, 78543.0, 78544.0, 78545.0, 78546.0, 78548.0, 78550.0, 78551.0, 78552.0, 78553.0, 78555.96, 78556.0, 78555.0, 78557.0, 2700000.0, 78560.0, 78562.0, 78562.8, 78564.0, 78565.0, 78566.28, 78561.0, 865000.0, 78569.0, 78571.0, 78572.0, 78573.0, 78575.0, 78576.0, 78577.0, 78578.0, 78580.0, 78581.0, 78582.0, 78584.0, 78585.0, 78586.0, 78587.0, 78588.0, 78589.0, 78590.0, 78586.16, 78592.0, 78595.0, 78597.0, 78598.0, 78598.99, 78600.0, 78602.0, 78603.0, 78604.0, 78608.04, 78610.4, 78611.0, 78612.0, 78613.0, 78614.0, 78615.0, 78610.0, 78617.98, 78617.0, 78618.0, 78620.0, 78619.0, 78622.0, 78623.4, 78624.0, 78625.0, 78623.0, 78626.0, 78628.0, 78630.0, 78632.0, 78633.0, 78634.0, 78635.0, 78636.0, 78640.0, 78641.0, 78643.0, 78644.44, 78645.0, 78646.44, 78646.0, 78647.0, 78648.0, 78645.41, 78650.0, 78649.0, 78651.0, 78654.0, 78652.0, 78656.0, 78658.32, 78659.0, 78660.0, 78663.8, 78665.0, 78666.36, 78666.0, 78667.68, 78669.0, 78670.0, 78673.0, 78675.0, 78676.0, 78678.31, 78678.0, 78680.0, 78681.0, 78683.0, 78684.0, 78685.0, 78686.0, 78689.0, 78690.0, 78691.0, 78692.0, 78693.0, 78694.32, 78694.0, 78696.0, 78697.0, 78695.0, 78699.0, 78700.0, 78702.0, 78703.0, 78704.0, 78707.0, 78708.0, 78709.55, 78707.88, 78711.0, 78712.0, 78713.0, 78714.0, 78715.0, 78716.97, 78717.16, 78718.0, 78720.0, 78721.0, 78722.0, 78723.0, 78724.0, 78725.0, 78728.0, 78728.35, 78730.0, 78732.0, 78733.56, 78734.0, 78732.96, 78736.0, 78737.0, 78735.0, 78739.0, 78740.0, 78741.0, 78742.0, 78743.0, 78744.0, 78742.68, 78740.48, 78739.57, 78748.8, 78749.0, 78750.0, 78748.0, 78752.0, 340898.0, 78755.0, 78756.74, 78757.0, 78760.0, 78762.0, 78764.0, 78765.0, 78767.0, 78768.0, 78772.0, 78774.0, 78775.0, 78776.0, 78777.0, 67761.88, 78780.0, 78782.0, 78784.0, 78786.0, 78787.0, 78788.19, 78790.0, 78792.0, 78793.0, 78795.0, 78796.0, 78799.0, 78800.0, 78802.0, 78804.0, 78806.0, 78810.0, 78811.0, 78812.0, 78813.96, 78813.0, 78815.0, 78816.0, 78817.0, 78814.0, 78819.0, 78820.0, 78822.0, 78825.0, 78828.0, 78832.0, 78833.0, 78834.0, 78835.0, 78836.0, 78838.0, 78839.72, 78840.0, 78839.0, 78844.0, 78845.0, 78847.0, 78849.0, 78850.0, 78852.4, 78853.0, 78852.0, 78855.0, 341000.0, 78858.0, 78860.0, 78861.0, 78862.0, 78863.0, 78864.0, 78865.44, 78866.0, 78867.0, 78868.0, 78870.0, 78872.0, 78873.6, 78875.0, 78876.0, 78875.4, 78878.0, 78877.0, 78880.0, 78881.0, 78882.0, 78883.0, 78884.0, 78885.0, 78886.0, 78887.0, 78888.0, 78889.0, 78890.0, 78892.0, 78894.0, 78895.0, 78896.0, 78897.0, 78899.0, 78900.0, 78903.0, 78905.0, 78906.0, 78909.0, 78910.0, 78912.0, 78915.0, 78916.0, 78917.0, 78918.0, 78920.0, 78921.0, 78924.0, 78925.0, 78927.0, 78929.0, 78930.0, 78931.0, 78932.0, 78931.2, 78935.0, 78936.0, 78940.0, 78943.0, 78944.0, 78945.0, 78946.0, 78948.0, 78950.0, 78954.6, 78954.0, 78956.0, 78957.0, 78956.8, 78955.0, 78960.0, 78962.0, 78964.0, 78965.0, 78966.0, 78967.0, 78968.0, 78967.2, 78971.0, 78972.0, 78973.0, 78974.0, 78975.0, 341120.0, 78977.0, 78978.0, 78972.92, 78980.0, 78981.0, 78982.0, 78984.0, 78985.0, 78986.0, 78987.0, 78988.0, 78988.37, 78990.0, 78993.0, 78995.0, 78996.0, 78997.0, 78999.0, 79000.0, 79001.0, 78999.96, 78999.24, 79007.0, 79008.0, 79010.0, 79011.0, 79015.0, 79018.0, 79019.0, 79020.0, 79021.0, 79021.67, 79025.0, 79027.0, 79030.0, 79031.79, 79032.52, 79033.27, 79032.0, 79038.0, 79040.0, 79041.0, 79043.0, 79044.0, 79045.0, 79047.0, 79049.0, 79050.0, 79054.0, 79056.0, 79057.0, 79058.44, 79060.8, 79060.0, 79062.0, 79061.0, 79065.0, 79066.0, 79066.44, 79068.0, 79073.71, 79077.0, 79079.0, 79080.0, 79081.0, 79084.74, 79084.0, 79087.0, 79088.0, 79089.0, 79090.0, 79092.0, 79097.76, 79098.0, 79099.0, 79100.0, 79102.91, 79104.0, 79105.0, 79107.39, 79108.0, 79109.0, 79110.0, 79111.0, 79112.0, 79113.0, 79112.04, 79114.0, 79117.5, 79118.0, 79119.0, 79120.0, 79121.0, 79122.0, 79123.0, 79124.0, 79125.0, 79126.0, 79130.0, 79131.0, 79133.0, 79134.0, 79135.0, 79136.0, 79137.24, 79139.0, 79140.0, 79143.0, 79144.0, 79145.0, 79145.28, 79147.5, 79148.0, 79146.0, 79150.0, 79152.0, 79154.0, 79154.4, 79157.0, 79160.0, 79163.0, 79164.0, 79165.0, 79166.01, 67838.0, 79168.0, 79167.0, 79170.0, 79172.0, 79176.0, 79178.0, 79179.0, 79180.0, 79183.0, 79185.0, 79186.0, 79188.0, 79190.0, 79191.16, 79192.0, 79194.0, 79196.0, 79197.6, 79198.0, 79199.0, 79200.0, 79203.0, 79206.0, 79207.0, 79209.52, 79210.0, 79212.0, 79213.0, 79214.0, 79217.0, 79220.0, 79221.6, 79222.0, 79224.0, 79225.0, 79227.0, 79228.0, 79230.0, 79231.0, 79232.14, 79232.0, 79235.0, 79236.0, 79237.0, 79239.0, 79240.0, 79242.0, 79243.0, 79244.0, 79248.0, 79248.55, 79250.0, 79251.0, 79252.0, 79253.0, 79254.51, 79255.0, 79251.62, 79257.0, 79258.0, 79260.0, 79262.0, 79264.0, 79268.0, 79269.0, 79270.0, 79272.0, 79274.0, 79275.0, 79276.0, 79278.0, 79279.0, 79280.0, 1390000.0, 79284.0, 79285.0, 79286.0, 79289.0, 79290.0, 79291.0, 79292.0, 79293.0, 79294.0, 79296.0, 79298.0, 79300.0, 79301.0, 79303.0, 79304.0, 79305.0, 79303.56, 79308.0, 79309.0, 79310.0, 79312.0, 79313.0, 79315.0, 79317.0, 79320.0, 79321.0, 79321.42, 79322.0, 79324.0, 79325.0, 79322.64, 79328.0, 79330.2, 79331.0, 79331.2, 79333.0, 79332.0, 79335.0, 79336.0, 79337.0, 79342.0, 79343.0, 79344.0, 79346.0, 79350.0, 79351.0, 79352.16, 79353.0, 79354.0, 79355.0, 79352.0, 79358.0, 79360.0, 79361.0, 79363.08, 79364.0, 79365.0, 79366.0, 79363.0, 79368.0, 79367.0, 79371.4, 79372.8, 79373.64, 79372.0, 79375.0, 79373.0, 79377.0, 79378.0, 341517.0, 79380.0, 79381.0, 79383.0, 79384.56, 79385.0, 79386.0, 79385.76, 79390.0, 79392.0, 79394.59, 79394.0, 79395.0, 79397.0, 79397.5, 79400.0, 79401.0, 79402.0, 79403.0, 79404.0, 79405.0, 79406.0, 79409.0, 79410.0, 79412.0, 79412.78, 79415.7, 79415.95, 79416.0, 79420.0, 79423.0, 79424.0, 79426.2, 79428.0, 79430.0, 865863.96, 79432.0, 79432.2, 79435.0, 79437.29, 79437.28, 79440.0, 79441.0, 79442.8, 79443.0, 79444.0, 79445.0, 79446.0, 79447.0, 79448.0, 79444.65, 79450.0, 79452.0, 79453.0, 79455.4, 341600.0, 79456.0, 79458.0, 79458.68, 79460.0, 79457.0, 79455.0, 79459.0, 79464.0, 79465.0, 79466.0, 79467.0, 79469.0, 79473.0, 79474.0, 79475.0, 79476.0, 79477.0, 79478.0, 79476.21, 79480.0, 79479.0, 79482.0, 79483.6, 79484.0, 79484.38, 79483.0, 79487.0, 79488.0, 79490.0, 79492.0, 79494.0, 79495.81, 79495.0, 79497.0, 79500.0, 79501.0, 79504.0, 79505.0, 79507.0, 79508.0, 79509.61, 79512.0, 79513.0, 79514.0, 79515.0, 79518.0, 79520.0, 79523.0, 79524.0, 79525.0, 79528.0, 79529.0, 79530.0, 79532.0, 79533.0, 79534.0, 79536.0, 79538.0, 79538.39, 79540.0, 79539.0, 79540.68, 79543.0, 79544.0, 79546.0, 79547.0, 79548.7, 341690.0, 79550.0, 79551.0, 79552.0, 79553.0, 79554.0, 79558.0, 79559.0, 79560.0, 79562.0, 79564.0, 79565.0, 79567.0, 866000.0, 79568.0, 79567.5, 79571.0, 79572.0, 79573.0, 79574.0, 79575.0, 79577.0, 79580.0, 79582.0, 79584.0, 79585.0, 79586.0, 79588.54, 79590.0, 79595.28, 79595.0, 79596.0, 79597.0, 79598.0, 341742.0, 79600.0, 79601.0, 79599.0, 79603.0, 79601.6, 79605.0, 79608.0, 79611.41, 79612.85, 79611.0, 79618.0, 79620.0, 79621.0, 79623.0, 79625.0, 79626.0, 79628.0, 79629.0, 79628.2, 79630.0, 79631.0, 79632.0, 79633.0, 79634.0, 79635.0, 79636.0, 79637.52, 79638.0, 79640.0, 79641.0, 79642.0, 79643.0, 79644.0, 79645.0, 79646.4, 79647.0, 79648.92, 79648.0, 79650.0, 79651.0, 67934.0, 79653.0, 79654.0, 79655.0, 79657.2, 79658.0, 79660.0, 79664.0, 79665.0, 79666.0, 79668.52, 79668.0, 79669.67, 79670.0, 79672.0, 79674.0, 79675.0, 79678.0, 1128255.0, 79680.0, 79681.0, 79679.0, 79684.0, 79685.0, 79686.0, 79687.0, 79688.0, 79689.2, 67942.55, 79691.0, 79692.0, 79693.0, 79694.0, 79695.0, 79696.8, 79697.0, 79695.4, 79699.0, 79700.0, 79701.0, 79704.0, 79705.0, 79706.0, 79708.0, 79710.0, 79711.0, 604000.0, 79712.0, 79715.0, 79716.0, 79717.0, 79718.0, 79717.68, 79720.0, 79725.0, 79726.4, 79727.0, 79728.0, 79730.0, 79732.0, 79734.0, 79738.14, 79740.0, 79741.0, 79742.4, 79743.0, 79744.0, 79747.0, 79748.0, 79747.2, 79750.0, 79751.64, 79752.0, 79753.0, 79756.0, 79758.0, 79760.0, 79762.0, 79763.45, 79764.72, 79764.0, 79766.0, 79767.36, 79768.0, 79768.33, 79769.0, 79772.7, 79773.0, 79774.0, 79772.39, 79776.0, 79777.68, 79778.0, 79779.0, 79780.0, 79780.86, 79777.0, 79783.0, 79783.08, 79785.0, 79786.0, 79787.0, 79788.0, 79789.0, 79791.0, 341937.0, 79794.0, 79795.0, 79796.0, 79795.32, 79793.0, 79799.0, 79800.0, 79801.0, 79802.0, 79803.0, 79804.0, 79805.0, 79800.03, 79808.0, 79810.0, 79813.0, 79815.0, 79816.2, 79816.0, 79818.0, 79819.0, 79820.0, 79821.0, 79822.0, 79824.96, 79824.25, 79826.0, 79824.0, 79825.0, 79829.0, 79830.0, 79832.0, 79834.0, 79835.0, 79836.0, 79837.0, 79840.0, 79841.0, 79842.0, 79843.0, 79845.0, 79846.56, 79845.72, 79847.0, 79849.0, 79848.0, 79847.39, 79850.0, 79851.0, 79846.0, 79852.0, 79854.0, 79855.0, 342000.0, 79856.0, 79857.0, 79859.0, 79860.0, 79862.0, 79863.0, 79864.0, 79865.4, 79865.0, 79866.0, 79869.0, 79870.0, 79872.48, 79873.0, 79872.0, 79874.0, 79875.0, 79876.0, 79877.82, 79878.0, 79879.0, 79880.0, 79881.0, 79882.0, 79877.0, 79884.0, 79886.0, 79887.5, 79888.0, 79889.0, 79890.0, 79882.37, 79892.24, 79892.0, 79894.0, 79895.0, 79895.96, 79897.0, 79896.0, 79898.0, 79900.0, 79900.8, 79899.0, 79898.28, 79904.64, 79904.0, 79906.0, 79901.0, 79908.0, 79909.0, 79910.0, 79911.0, 79912.0, 79913.6, 79914.0, 79913.0, 79915.0, 79917.0, 79919.46, 79920.0, 79920.6, 79924.0, 79925.0, 79926.0, 79930.0, 79931.0, 79932.0, 79933.0, 79934.39, 79934.0, 79936.0, 79937.0, 79935.0, 79940.0, 79941.0, 79942.0, 79942.13, 79944.0, 79946.0, 79949.0, 79950.0, 79952.0, 79952.24, 79954.0, 79955.0, 79960.0, 79964.0, 79968.0, 79969.0, 79970.0, 79971.0, 79972.0, 79973.0, 79974.0, 79975.0, 79976.0, 79978.0, 79979.0, 79980.0, 79981.0, 79982.0, 79984.0, 79985.0, 79986.0, 79987.0, 79988.0, 79990.0, 342135.0, 79992.0, 79991.0, 79994.0, 79995.0, 79996.0, 79998.0, 79999.0, 80000.0, 80001.0, 80002.0, 80003.0, 80004.0, 80005.0, 80006.0, 80007.0, 80008.0, 80009.0, 80010.46, 80011.0, 80010.0, 80012.0, 80013.0, 80014.0, 80015.0, 80016.0, 80015.26, 80016.84, 80019.0, 80017.0, 80023.0, 80024.0, 80026.2, 80028.0, 80030.0, 80032.0, 80033.0, 80034.0, 80038.0, 80038.4, 80040.0, 80043.0, 80045.0, 80047.44, 80049.0, 80050.0, 80051.0, 80052.0, 80050.56, 80054.0, 80055.0, 80056.0, 80049.47, 80059.2, 80059.0, 80060.0, 80062.21, 80063.0, 80064.0, 80067.0, 80068.0, 80069.0, 80070.0, 80071.0, 80072.0, 80073.0, 80074.0, 80075.0, 80076.0, 80077.0, 80079.0, 80080.0, 80082.0, 80083.0, 80087.0, 80088.0, 80089.0, 80092.0, 80093.0, 80095.0, 80099.0, 80100.0, 80101.0, 80102.0, 80104.0, 80106.0, 80107.0, 80108.0, 80110.0, 80111.0, 80112.0, 80114.0, 80115.0, 80116.0, 80117.0, 80120.0, 80121.0, 80122.0, 80122.8, 80124.8, 80124.0, 80125.0, 80126.0, 80128.24, 80127.0, 80128.66, 80130.96, 80130.0, 80132.0, 80135.0, 80136.0, 80137.0, 80140.0, 80142.4, 80142.0, 80144.0, 80145.0, 80147.88, 80148.0, 80149.0, 80150.0, 80151.0, 80152.0, 80154.0, 80160.0, 80161.0, 80163.0, 80164.0, 80165.0, 80167.0, 80169.0, 80170.97, 80171.0, 80172.0, 80174.0, 80175.0, 80177.0, 80178.0, 80179.0, 80180.0, 80181.0, 80184.0, 80185.0, 80186.0, 80188.0, 80190.88, 80192.0, 80196.0, 80198.0, 80199.0, 80200.0, 80201.0, 80203.0, 80204.0, 80205.52, 80206.0, 80205.0, 80208.0, 80203.92, 80211.0, 80212.0, 80213.28, 80214.0, 80215.98, 80216.0, 80217.0, 80218.74, 80219.0, 80220.0, 80223.48, 80224.0, 80225.6, 80225.0, 80227.0, 80226.0, 80223.0, 80230.0, 80231.0, 80232.0, 80233.0, 80234.0, 80236.0, 80237.0, 80238.6, 80240.0, 80242.0, 80243.0, 80244.0, 80245.0, 80246.0, 80247.0, 80248.0, 80249.0, 80250.0, 80252.0, 80253.48, 80253.0, 80254.0, 80256.0, 80258.0, 80259.0, 80260.0, 80262.0, 80263.0, 80264.0, 80265.0, 80266.0, 80267.44, 80268.0, 80270.0, 80274.0, 80276.0, 80276.73, 80278.0, 80280.0, 80287.2, 80288.0, 80289.1, 80287.0, 80290.0, 80292.0, 80291.0, 80295.0, 80296.32, 80297.0, 80298.0, 80299.0, 80300.0, 80301.0, 80302.0, 80303.0, 80304.0, 80307.0, 80308.8, 80309.0, 80310.0, 80308.0, 80312.37, 80313.0, 80312.0, 80315.0, 80316.0, 80314.62, 80318.0, 80320.0, 80322.0, 80323.0, 80324.0, 80325.0, 80322.84, 80328.0, 80329.0, 80329.6, 80331.0, 80332.0, 80333.0, 80334.48, 80336.0, 80339.0, 80340.0, 80341.0, 80340.33, 80340.37, 80341.87, 80342.0, 80343.0, 80347.0, 80345.0, 80350.0, 342496.0, 80353.0, 80355.0, 80356.0, 342500.0, 80358.17, 80358.0, 80360.0, 80361.0, 80363.0, 80364.0, 80365.0, 80367.0, 80369.0, 80370.0, 80371.0, 80375.0, 80376.0, 80380.0, 80384.0, 80386.44, 80387.0, 80388.0, 80390.0, 80392.0, 80393.0, 80396.0, 80397.0, 80398.0, 80400.0, 80402.66, 80402.0, 80405.0, 80406.0, 80409.0, 80410.0, 80412.0, 80413.0, 80414.0, 80415.0, 80418.0, 80419.56, 80420.0, 80422.0, 80423.0, 80424.0, 80427.0, 80428.36, 80429.0, 80430.0, 80431.0, 80433.0, 80436.0, 80438.0, 80439.0, 80440.0, 80441.0, 80443.0, 80445.0, 80448.0, 80449.0, 80450.0, 80451.0, 80452.0, 80453.0, 80454.0, 80455.0, 80456.0, 80458.0, 80459.0, 80460.0, 80462.0, 80463.0, 80463.8, 80464.0, 80465.0, 80468.0, 80469.28, 80470.0, 80471.0, 80472.0, 80472.31, 80475.0, 80476.97, 80477.0, 80478.0, 80480.0, 80481.0, 80483.0, 80484.0, 80485.0, 80487.0, 80489.28, 80490.0, 80492.0, 80494.0, 80495.0, 80496.0, 80497.0, 80499.0, 80500.0, 80502.0, 80503.0, 80505.0, 80505.55, 80507.0, 80508.0, 80506.0, 80511.87, 80513.0, 80516.0, 80517.0, 80516.8, 80519.0, 80520.0, 80521.0, 80522.0, 80518.0, 80525.0, 80528.0, 80530.0, 80532.0, 80533.27, 80534.0, 80537.0, 80538.66, 80539.0, 80540.0, 80538.0, 80541.57, 80543.28, 80544.0, 80548.0, 80549.66, 80550.0, 80552.0, 80553.0, 80555.0, 80556.0, 80559.36, 80560.0, 80559.0, 80562.0, 80563.0, 80564.0, 80565.0, 80563.78, 80567.0, 80568.0, 80569.0, 80570.0, 80571.0, 80576.0, 80577.0, 80578.0, 80579.0, 80580.0, 80581.0, 80582.4, 80583.0, 342727.0, 80584.56, 80587.0, 80590.0, 80591.0, 80592.0, 80593.0, 80594.0, 80593.8, 80597.0, 80598.0, 80599.0, 80600.0, 80601.0, 80602.8, 80603.0, 80603.12, 80605.0, 80604.0, 80607.0, 80608.0, 80610.0, 80611.0, 80612.7, 80615.0, 80616.0, 80620.02, 80620.0, 80620.68, 80623.0, 80621.0, 80625.0, 80628.0, 80630.0, 80632.0, 80634.0, 80635.0, 80636.0, 80638.0, 80639.0, 80640.0, 80641.0, 80642.0, 80643.0, 80644.0, 80646.0, 80648.0, 80649.0, 80650.0, 80651.0, 80649.87, 80653.0, 80654.0, 80655.0, 80652.0, 80659.0, 80660.28, 80660.0, 80662.0, 80662.4, 80664.0, 80665.0, 80666.0, 80667.0, 80668.0, 80669.0, 80669.48, 80671.0, 80672.0, 80676.0, 80678.0, 80679.0, 80679.74, 80681.4, 80683.0, 80684.0, 80685.0, 80686.24, 80687.0, 80688.0, 80689.0, 80690.0, 80691.2, 342835.0, 80692.0, 80689.8, 80695.0, 80696.0, 68142.0, 80698.0, 68141.17, 80700.0, 80697.0, 80704.0, 80705.0, 80706.0, 80707.0, 80710.0, 80711.0, 80712.0, 80713.0, 605000.0, 80714.0, 80716.0, 80710.8, 80718.0, 80720.0, 80721.0, 80722.0, 80724.8, 80725.0, 80726.0, 80730.0, 80731.0, 80734.0, 80736.0, 80737.0, 80739.0, 80741.0, 80742.0, 80744.0, 80745.0, 80746.0, 80747.0, 80748.0, 80750.0, 80752.0, 80755.0, 80755.65, 80757.0, 80758.0, 80759.0, 80760.0, 80757.37, 80763.0, 80764.0, 80766.0, 80767.0, 80768.0, 80770.0, 80772.0, 80774.0, 80776.8, 80777.0, 80776.0, 80779.0, 80779.36, 80780.0, 80784.0, 80787.0, 80788.0, 80789.0, 80789.28, 80792.0, 80793.0, 80795.0, 80796.0, 80798.0, 80799.0, 80800.0, 80799.29, 80801.0, 80805.0, 80806.34, 80808.0, 80812.0, 80813.0, 80815.0, 80816.0, 80817.0, 80817.41, 80820.0, 80821.0, 80827.0, 80828.0, 80830.0, 80831.0, 80832.0, 80833.44, 80834.0, 80835.0, 80833.0, 80839.0, 80840.0, 80841.0, 80842.0, 80843.0, 80844.25, 80844.0, 80845.0, 80847.0, 80848.0, 80849.6, 80850.0, 80851.0, 80852.0, 80851.86, 80854.0, 68173.54, 343000.0, 80856.0, 80859.92, 80860.0, 80863.0, 80863.8, 80865.0, 80866.0, 80867.0, 80868.0, 80870.4, 80870.0, 68177.37, 80873.0, 80874.0, 80880.0, 80882.0, 80888.0, 80890.0, 80891.2, 80892.0, 80895.0, 80896.8, 80897.0, 80898.08, 68182.0, 80900.0, 80901.0, 80898.0, 80899.0, 80904.0, 80905.0, 80906.0, 80907.0, 80912.0, 80914.0, 80915.0, 80916.96, 80916.0, 80920.0, 80922.0, 80924.0, 80925.0, 80927.0, 80928.0, 80930.0, 80932.0, 80932.8, 80935.0, 80937.0, 80939.0, 80940.0, 80941.73, 80942.0, 80943.0, 80949.0, 80950.0, 80952.0, 80953.0, 80953.63, 80956.0, 80957.0, 80958.0, 80956.32, 80960.0, 80964.0, 80965.0, 80966.0, 80967.0, 80968.0, 80969.94, 80970.0, 80971.17, 80969.0, 80973.0, 80974.42, 80971.07, 80976.0, 80977.0, 80980.0, 80982.0, 80983.0, 80984.0, 80985.0, 80986.0, 80987.0, 80988.0, 80989.0, 80990.0, 80993.0, 80994.0, 80995.0, 80998.0, 80999.0, 81000.0, 81004.0, 81006.0, 81007.0, 81008.0, 81011.0, 81012.0, 81015.0, 81016.0, 81017.0, 81019.0, 81020.0, 81021.48, 81024.0, 81025.0, 81031.0, 81036.0, 81040.0, 81042.0, 81043.0, 81042.52, 81045.0, 81048.0, 81050.0, 81051.0, 81054.85, 81055.0, 81056.0, 81057.0, 81057.02, 81060.0, 81061.0, 81064.0, 81065.0, 81066.0, 81067.0, 81070.0, 81071.0, 81072.0, 81074.0, 81076.0, 81077.0, 81078.0, 81079.0, 81080.0, 81083.54, 81084.0, 81085.0, 81089.0, 81090.0, 81092.0, 81093.0, 81094.0, 81095.0, 81096.99, 81096.0, 81098.0, 81099.24, 81099.0, 81100.0, 81102.84, 81102.0, 81103.0, 81098.84, 81106.0, 81105.0, 81099.2, 81109.0, 81101.0, 81111.0, 81112.5, 81112.0, 81114.0, 81115.0, 81116.0, 81113.0, 81117.0, 81118.56, 81118.0, 81120.0, 81121.0, 81122.0, 81123.0, 81124.0, 81125.22, 81129.0, 81130.7, 81135.0, 81136.0, 81138.84, 68230.24, 81140.0, 81144.0, 68231.21, 81149.0, 81150.0, 81151.0, 81153.0, 81156.0, 81157.0, 81158.0, 81159.0, 81160.0, 81160.8, 81162.0, 81162.46, 81167.0, 81168.0, 81170.0, 81173.0, 81174.0, 81175.0, 81176.0, 81178.0, 81180.0, 81182.0, 81183.0, 81184.0, 81185.0, 81187.18, 81187.13, 81188.08, 81192.0, 81193.0, 81193.2, 81195.0, 81194.0, 81197.0, 81198.0, 81200.0, 81202.0, 81203.0, 81204.0, 343350.0, 81207.0, 81208.0, 81210.0, 81211.0, 81213.0, 81214.0, 81215.0, 81216.0, 81217.0, 81220.0, 81221.0, 81222.0, 81224.0, 81225.0, 343368.0, 81227.0, 81226.0, 81230.0, 81231.0, 81232.0, 81233.0, 81234.0, 81235.0, 81236.0, 81238.0, 81240.0, 81241.0, 81242.0, 81244.8, 81245.0, 81246.0, 81247.0, 81248.0, 81249.76, 81250.0, 81249.0, 81248.84, 81253.0, 81254.0, 81255.0, 81257.8, 81258.0, 81257.0, 81260.0, 81262.0, 81263.0, 81264.0, 81265.0, 81264.15, 81266.86, 81268.0, 81269.0, 81270.0, 81267.0, 81274.0, 81275.0, 81276.0, 81277.0, 81280.0, 81281.0, 81282.0, 1392000.0, 81284.0, 81286.0, 81288.0, 81289.0, 81290.0, 81291.0, 81292.0, 81293.0, 81294.55, 81295.0, 81298.0, 81300.0, 81300.03, 81302.94, 81304.0, 81305.6, 81306.0, 81307.0, 81309.0, 81310.0, 81311.0, 81314.0, 81316.0, 81317.0, 81320.91, 81320.0, 81323.0, 81324.0, 81325.0, 81326.0, 81327.0, 81328.0, 81331.0, 81332.0, 81333.0, 81335.0, 81336.59, 81338.0, 81339.0, 81340.0, 81341.8, 81342.0, 81341.0, 81344.0, 81345.0, 81347.0, 81348.0, 81348.8, 81350.0, 81349.08, 81356.0, 81357.0, 81359.0, 81360.0, 81363.0, 81364.86, 81364.0, 81365.0, 81368.0, 81369.48, 81369.6, 81370.0, 81373.0, 81375.0, 81377.0, 81378.6, 81380.0, 81383.0, 81384.0, 81385.0, 81386.0, 81387.0, 68279.0, 81394.0, 81396.0, 81398.4, 81399.0, 81400.0, 81401.0, 81402.0, 81404.0, 81405.0, 4800000.0, 81408.0, 68284.84, 81411.2, 81411.0, 81414.0, 81420.0, 81422.0, 1130000.0, 81425.0, 81424.0, 81431.0, 81432.0, 81433.0, 81435.0, 68289.1, 81437.0, 81439.0, 81440.0, 81442.0, 81444.0, 81445.0, 81448.0, 81449.96, 81450.0, 81449.0, 81452.8, 81448.28, 81456.0, 81460.0, 81461.0, 81463.0, 81464.0, 81465.0, 81466.0, 81468.0, 81471.0, 81472.25, 81473.0, 81474.0, 81475.0, 81476.0, 81477.0, 81478.0, 81480.0, 81481.0, 81484.0, 81484.39, 81485.0, 81487.0, 81488.0, 81488.7, 81490.0, 81491.81, 81492.0, 81493.5, 81494.0, 81493.0, 81489.0, 81495.0, 81490.41, 81499.0, 81500.0, 81501.16, 81504.0, 81506.0, 81507.0, 81509.0, 81510.0, 81515.0, 81516.0, 81517.0, 81518.0, 81519.12, 81519.1, 81520.0, 81522.0, 81521.86, 81524.0, 81525.0, 81526.0, 81519.0, 81528.0, 81529.4, 81530.0, 81531.0, 81536.0, 81537.0, 81539.0, 81540.0, 81543.0, 81544.0, 81545.0, 81546.0, 81547.0, 81548.16, 81548.0, 81549.0, 81550.0, 81552.72, 81551.0, 81552.0, 81553.0, 81555.0, 81555.24, 81557.0, 81557.76, 81559.0, 81560.0, 81556.0, 81562.0, 81563.0, 81564.0, 81565.0, 81567.67, 81566.0, 81572.0, 81573.0, 81575.0, 81576.0, 81577.0, 81578.0, 81582.4, 81583.0, 81584.0, 81586.0, 81588.0, 81590.0, 81591.26, 343737.0, 81593.0, 81595.0, 81596.0, 343738.0, 81597.0, 81599.0, 81600.0, 81600.57, 81603.0, 81604.0, 81606.0, 81608.0, 81609.33, 81609.04, 81610.0, 81612.0, 81613.0, 81614.0, 81615.0, 81616.0, 81618.0, 81619.0, 81619.2, 81620.0, 81622.48, 81621.0, 81624.0, 81625.0, 81626.0, 81624.24, 81628.0, 81630.0, 81632.0, 81633.0, 81636.0, 81640.0, 81643.0, 81645.0, 81646.0, 81647.0, 81648.0, 81650.0, 81653.0, 81656.0, 81657.0, 81659.0, 81660.0, 81665.0, 81666.0, 81669.7, 81670.0, 81672.36, 81672.0, 81675.0, 81680.0, 81681.0, 81682.0, 81683.0, 81684.0, 81685.0, 81686.0, 81688.0, 81690.0, 81691.0, 81692.0, 81694.0, 81696.0, 81698.0, 81699.0, 81700.0, 81702.0, 81702.65, 81704.39, 81702.39, 81706.0, 81707.0, 81708.0, 81704.0, 81711.24, 81712.0, 606000.0, 81714.42, 81715.0, 81716.0, 81717.0, 81718.0, 81719.0, 81720.0, 81723.0, 81725.0, 81728.0, 81729.0, 81730.8, 81732.0, 68348.8, 81734.0, 81740.0, 81743.0, 81744.0, 81745.0, 81747.0, 81749.0, 81750.0, 81751.0, 81751.58, 81752.0, 81754.0, 81755.0, 81757.0, 81759.0, 81760.0, 81762.0, 81765.0, 81766.0, 81768.0, 81770.0, 81775.0, 81777.0, 81778.0, 81780.0, 81781.0, 81783.0, 81784.0, 81783.72, 81785.0, 81788.0, 81789.0, 81790.0, 81792.0, 81793.0, 81794.0, 81795.0, 81796.0, 81797.0, 81798.0, 68361.84, 81800.0, 81801.0, 81802.0, 81798.98, 81804.0, 81806.0, 81807.0, 81808.0, 81810.0, 81811.0, 81812.0, 81813.12, 81818.0, 68365.92, 81820.0, 81823.0, 81824.0, 81825.0, 81825.4, 81827.0, 81828.0, 81830.0, 81832.0, 81834.0, 81836.88, 81838.0, 81840.0, 81843.0, 81845.0, 81845.5, 81847.0, 81848.0, 81849.0, 81850.0, 81851.0, 81852.0, 81853.0, 81855.0, 344000.0, 81857.0, 81858.0, 81856.0, 81861.0, 81863.0, 81864.0, 81864.69, 81868.0, 81868.8, 81869.0, 81874.0, 81876.0, 81880.0, 81882.0, 81884.0, 81885.0, 81886.0, 81888.0, 81891.0, 81894.0, 81896.0, 81897.0, 81898.0, 81899.0, 81900.0, 81901.0, 81900.52, 81903.11, 81909.0, 81910.4, 81910.0, 81912.0, 81913.0, 81915.96, 81916.0, 81918.46, 81918.0, 81920.0, 81922.0, 81923.0, 81924.0, 81925.0, 81930.0, 81931.0, 81932.0, 81933.0, 81936.0, 81939.0, 81940.0, 81941.0, 81942.0, 81943.0, 81944.0, 81946.0, 81947.0, 81948.0, 81952.0, 81955.0, 81960.0, 81961.0, 81964.0, 81966.0, 81970.0, 81972.8, 81972.0, 81974.0, 81974.32, 81976.0, 81975.0, 81978.0, 81980.0, 81981.0, 81983.0, 81984.0, 81987.0, 81988.0, 81989.0, 81990.0, 81991.0, 81992.0, 81993.0, 81994.0, 81995.0, 81996.0, 81999.0, 82000.0, 82000.88, 82002.0, 82003.0, 81999.96, 81999.75, 82006.0, 82001.0, 82008.0, 82009.0, 82014.0, 82016.0, 82017.52, 82018.0, 82019.0, 82020.0, 82023.0, 82024.04, 82025.0, 82024.54, 82027.0, 82029.6, 82030.0, 82031.52, 82032.0, 82038.0, 82039.0, 82040.0, 82041.0, 82042.0, 82046.0, 82047.0, 82048.0, 82050.0, 82052.0, 82052.25, 82054.0, 82056.0, 82058.0, 82060.0, 82060.56, 82062.5, 82064.0, 82065.0, 82066.0, 82068.0, 82071.86, 82071.0, 82073.0, 82074.6, 82076.0, 82079.0, 82080.0, 82081.0, 82082.0, 82082.04, 82085.0, 82090.0, 82091.0, 82092.0, 82094.0, 82098.0, 82099.0, 82100.0, 82101.0, 82102.32, 82103.0, 82104.0, 82102.0, 68422.0, 82109.0, 82110.0, 82113.33, 82116.0, 82120.0, 82121.0, 82122.0, 82123.0, 82125.0, 82126.0, 82128.0, 82131.0, 82132.0, 82133.0, 82134.0, 82135.0, 82136.0, 82137.0, 82138.0, 82134.47, 82140.0, 82141.0, 82143.0, 82144.96, 82145.0, 82147.0, 82149.0, 82150.0, 82151.0, 82152.0, 82153.0, 82154.0, 82156.0, 82158.0, 82160.0, 82162.0, 82163.0, 82164.0, 82165.0, 82166.0, 82167.67, 82168.0, 82170.0, 82171.0, 82172.0, 82173.0, 82175.0, 82176.0, 82177.0, 82179.0, 82180.0, 82182.0, 82187.0, 82187.56, 82188.0, 82190.0, 82192.0, 82193.0, 82194.74, 82194.84, 82195.0, 82194.0, 82199.76, 82200.0, 68441.0, 82202.4, 82207.0, 82208.0, 82210.0, 68443.0, 82212.0, 82211.0, 82214.0, 82215.0, 82216.0, 82216.08, 82218.0, 82220.06, 82221.78, 82222.0, 82223.0, 82224.0, 82225.0, 82220.0, 82230.0, 82231.0, 82232.0, 82233.0, 82234.0, 82235.0, 82236.0, 82237.0, 82230.45, 82239.0, 82241.0, 82241.35, 82243.0, 82246.0, 82247.0, 82248.0, 82250.0, 82252.0, 82255.0, 82256.0, 82256.52, 82258.0, 82256.5, 82260.0, 82262.4, 82263.0, 82264.0, 82265.91, 82266.0, 82265.0, 82271.0, 82272.0, 82273.68, 82275.0, 82277.0, 82278.0, 82279.0, 82280.0, 82282.0, 82283.76, 82284.0, 82286.0, 82288.2, 82289.55, 82288.0, 82290.0, 82292.0, 82293.0, 82290.64, 82289.0, 82296.0, 82298.8, 82299.0, 82300.0, 82298.23, 82302.0, 82298.0, 82304.0, 82305.0, 82306.0, 82307.0, 82308.0, 82310.0, 82311.0, 82313.0, 82314.0, 82316.0, 82318.0, 82319.0, 82320.0, 82321.0, 82323.0, 82324.0, 82325.0, 82328.0, 82329.0, 82331.4, 82332.0, 82333.0, 82334.0, 82337.0, 82338.0, 82339.0, 82340.0, 82341.0, 82342.0, 82343.0, 82344.0, 82345.0, 82347.0, 82348.0, 82349.0, 82350.0, 82351.0, 82352.0, 82354.0, 82356.0, 344500.0, 82358.0, 82357.91, 82360.0, 82361.0, 82362.0, 82359.0, 82364.0, 82365.0, 82367.69, 82368.0, 82369.0, 68475.18, 82372.8, 82374.0, 82375.0, 82377.0, 82378.0, 82379.0, 82380.0, 82381.52, 82382.0, 82381.0, 82383.0, 82385.0, 82386.0, 82387.0, 82388.0, 82389.6, 82385.13, 82394.0, 82395.0, 82396.0, 82397.0, 82398.0, 82399.0, 82400.0, 82401.0, 82399.12, 82403.0, 82404.0, 82402.0, 82399.92, 82407.0, 82405.0, 82409.0, 82410.0, 82411.0, 82412.0, 82413.63, 82415.0, 82416.0, 82420.0, 82421.0, 82423.0, 82424.0, 82423.64, 82425.0, 82427.0, 82428.0, 82429.0, 82430.0, 82430.4, 82432.0, 82433.0, 82434.0, 82432.08, 82435.0, 82438.0, 82439.0, 82440.0, 82441.0, 82439.67, 82442.0, 82444.44, 82444.82, 82444.0, 82447.49, 82449.0, 82450.0, 82451.0, 82452.0, 82453.0, 82456.0, 82457.0, 82458.0, 82459.0, 82460.0, 82461.0, 82462.26, 82463.04, 82464.0, 82465.0, 82466.0, 82468.0, 82469.39, 82472.0, 82473.0, 82474.0, 82475.0, 82473.24, 82476.0, 82480.0, 82481.0, 82482.0, 82485.0, 82488.0, 82489.0, 82490.0, 82491.6, 82492.0, 82492.8, 82495.0, 82496.0, 82499.0, 82500.0, 82502.0, 82503.0, 82504.0, 82506.0, 82511.0, 82512.0, 82513.0, 82512.6, 82515.21, 82515.0, 82516.0, 82518.0, 82520.0, 82521.6, 82523.0, 82524.0, 82526.22, 82529.0, 82530.0, 82531.8, 82533.0, 82534.0, 82535.0, 82536.0, 82539.0, 82540.0, 82542.0, 82544.0, 82545.0, 82546.0, 82548.0, 82550.0, 82552.0, 82554.0, 82555.0, 82556.0, 82559.0, 82560.0, 82561.0, 82564.0, 82565.54, 82566.0, 82568.0, 82570.0, 82571.0, 82572.0, 82574.0, 82575.0, 82576.0, 82577.0, 82578.0, 82579.0, 82580.0, 82582.0, 82583.0, 82584.0, 82585.0, 82590.0, 82592.32, 82594.0, 82595.0, 82596.0, 82596.8, 82598.0, 82599.0, 82600.0, 82597.0, 82602.0, 82603.0, 82605.0, 82606.0, 82608.0, 82609.0, 82610.0, 82613.0, 82614.0, 82615.39, 82617.0, 82618.0, 82619.0, 82620.0, 82621.0, 82623.0, 82624.0, 82626.82, 82627.0, 82628.0, 82626.0, 82630.0, 82632.0, 82633.0, 82636.0, 82637.0, 82638.0, 82639.0, 82640.0, 82642.0, 82643.88, 82645.0, 82646.0, 82648.0, 82650.0, 82651.0, 82652.0, 82653.0, 82654.68, 82654.6, 82656.0, 82651.32, 82654.0, 82659.2, 82660.0, 82661.0, 82662.0, 82661.24, 82664.11, 82662.38, 82667.0, 82668.0, 82670.0, 82671.24, 82671.0, 82673.0, 82670.53, 82676.0, 82677.0, 82678.0, 82680.0, 82682.64, 82683.0, 82684.0, 82682.0, 82686.0, 82687.0, 82688.0, 82689.0, 82690.0, 82692.59, 82693.74, 82693.0, 82695.0, 82698.0, 82699.0, 82700.0, 82701.0, 82702.8, 82702.06, 82704.0, 82705.0, 82706.0, 82710.0, 82712.0, 82713.0, 82714.32, 82715.0, 82716.0, 607000.0, 82719.0, 82720.0, 82722.0, 82724.0, 82725.0, 82726.0, 82727.0, 82728.0, 82729.68, 82730.0, 82732.0, 82733.0, 82734.0, 82735.0, 82740.0, 82741.0, 82742.4, 82742.8, 82744.0, 82742.0, 82746.0, 82747.0, 82748.0, 82750.0, 82751.76, 82752.0, 82753.0, 82754.0, 82755.0, 82756.8, 82756.0, 82760.0, 82762.0, 82763.0, 82763.2, 82764.0, 82765.0, 82767.78, 82767.0, 82766.22, 82770.0, 82772.0, 82774.0, 82775.0, 82776.0, 82777.0, 82778.0, 82780.0, 82782.16, 82783.0, 82784.0, 82785.0, 82786.0, 82788.0, 82789.0, 82790.0, 82791.0, 82792.0, 82793.88, 82795.0, 82797.0, 82800.0, 82803.0, 82804.0, 82805.0, 82804.8, 82808.0, 82810.0, 82812.86, 82813.0, 82812.0, 82815.0, 82816.91, 82817.0, 82818.0, 82814.0, 82820.0, 82821.0, 82822.0, 82824.0, 82825.0, 82827.0, 82828.0, 82829.0, 82830.0, 82832.0, 82834.0, 82835.03, 82836.0, 82837.0, 82839.0, 82840.0, 82844.04, 82845.0, 82847.0, 82848.0, 82850.0, 82852.0, 82853.0, 345000.0, 82857.0, 82860.0, 82861.0, 82863.72, 82864.0, 82865.0, 82863.0, 82867.0, 82868.0, 82869.0, 82867.2, 82871.0, 82872.18, 82873.15, 82874.0, 82875.0, 82877.0, 82878.0, 82879.0, 82880.0, 82881.0, 82882.0, 82884.0, 82885.0, 82888.0, 82888.69, 82890.0, 82892.0, 82894.0, 82895.0, 82896.0, 82897.68, 82898.0, 82900.0, 82901.88, 82902.0, 82903.0, 82904.04, 82907.28, 82908.8, 82908.0, 82909.0, 82911.0, 82913.0, 82914.0, 82915.0, 82919.0, 82920.77, 82920.0, 82921.0, 82920.78, 82923.0, 82922.0, 82925.0, 82924.0, 82926.0, 82929.0, 345073.2, 82931.0, 82926.48, 82933.0, 82935.0, 82939.08, 82940.0, 82944.0, 82945.0, 82949.0, 82950.0, 82952.0, 82953.0, 82954.0, 82956.0, 82956.5, 82957.0, 82960.0, 82964.0, 68593.82, 82968.0, 82972.0, 82974.72, 82975.0, 82977.0, 82977.18, 82979.0, 82980.0, 82981.0, 82977.74, 82985.0, 82986.0, 82987.0, 82988.0, 82990.0, 82992.0, 82993.0, 82994.0, 82995.0, 82996.0, 82997.0, 82998.0, 82999.0, 83000.0, 82999.92, 83002.0, 83004.0, 83008.0, 83010.0, 83013.0, 83014.0, 83016.0, 83018.0, 83020.0, 83022.0, 83023.0, 83025.0, 83025.96, 83026.8, 83028.0, 83029.0, 83030.0, 83031.52, 83031.0, 83033.0, 83034.09, 83035.8, 83037.0, 83040.0, 83043.0, 83047.0, 83048.0, 83050.0, 83051.0, 83052.0, 83054.0, 83057.0, 83059.0, 83060.68, 83061.0, 83062.56, 83063.0, 83066.0, 83067.92, 83067.2, 83069.0, 345213.0, 83071.0, 83070.0, 83075.0, 83076.94, 83076.24, 83076.0, 83080.0, 83081.96, 83082.0, 83083.52, 83081.0, 83088.0, 345237.0, 83097.0, 83099.0, 83100.0, 83104.0, 83105.0, 83106.0, 83108.0, 83109.62, 83110.0, 83111.0, 83112.0, 83116.0, 83119.4, 83120.0, 83121.0, 83122.0, 83123.0, 83124.0, 83121.24, 83125.0, 83127.0, 83128.0, 83128.32, 83130.0, 83132.0, 83134.0, 83135.0, 83136.0, 83137.0, 1656000.0, 83139.0, 83137.6, 83140.0, 83143.0, 83144.0, 83146.0, 83147.0, 83148.0, 83149.0, 83150.0, 83150.86, 83156.0, 83157.0, 83158.0, 83159.0, 83160.0, 83160.09, 83163.0, 83164.0, 83165.0, 83168.0, 83169.0, 83170.0, 83172.0, 83173.0, 83175.0, 83176.0, 83179.0, 83180.0, 83181.0, 83182.0, 83183.0, 83184.0, 83185.0, 83187.0, 83188.0, 83190.0, 83191.0, 83193.6, 83195.0, 83196.0, 83199.96, 83200.04, 83199.0, 83200.0, 83203.68, 83204.0, 83200.82, 83202.0, 83203.0, 83200.08, 83205.0, 83206.0, 83207.0, 83208.0, 83210.0, 83211.0, 83210.4, 83213.64, 83214.0, 83212.0, 83216.64, 83216.0, 83220.0, 83221.0, 83223.0, 83225.0, 83226.0, 83227.68, 83230.0, 83231.0, 83232.0, 83233.8, 83234.0, 83235.0, 83233.0, 83236.0, 83237.0, 83240.0, 83241.0, 83242.0, 83243.0, 83241.6, 83245.0, 83245.2, 83246.0, 83248.92, 83249.0, 83250.0, 83251.0, 83252.0, 83253.0, 83256.0, 83258.0, 83259.0, 83260.0, 83261.0, 83262.4, 83262.0, 83264.06, 83265.0, 83266.0, 83263.0, 83269.0, 83270.0, 83272.0, 83275.0, 83276.96, 83276.0, 83278.0, 83277.0, 83280.0, 83277.6, 83282.0, 83283.0, 83281.0, 83285.0, 83286.0, 83286.4, 83288.0, 83290.0, 83292.0, 83294.0, 83297.0, 83299.0, 83300.0, 83303.0, 83304.0, 83306.0, 83307.0, 83309.0, 83312.0, 83316.0, 83318.0, 83319.0, 83320.0, 83324.0, 83325.0, 83327.0, 83328.0, 83330.0, 83333.98, 83334.0, 83333.0, 83336.0, 83338.0, 83339.0, 83340.0, 83343.0, 83344.0, 83345.0, 83345.79, 83349.0, 83350.0, 83352.0, 83352.02, 83354.0, 83355.67, 83356.0, 83357.0, 83359.0, 83360.0, 83361.0, 83364.0, 83365.0, 83366.0, 83365.03, 83369.0, 83370.0, 83372.0, 83373.36, 83373.2, 83375.0, 83376.0, 83374.0, 83378.0, 83379.0, 83380.0, 83381.0, 83382.0, 83379.84, 345529.0, 68677.92, 83389.0, 83390.0, 68677.0, 83394.0, 83395.0, 83397.0, 83400.0, 83401.0, 83402.0, 83404.88, 83405.0, 83404.0, 83408.72, 83408.0, 83410.0, 83409.0, 83412.0, 83417.0, 83420.0, 83421.0, 83424.0, 83425.0, 83427.0, 83428.05, 83429.0, 83430.0, 83428.0, 83432.0, 83433.0, 83434.0, 83435.91, 83438.0, 83440.0, 83441.0, 83442.21, 83443.46, 83444.0, 83445.0, 83446.0, 83447.0, 83448.0, 83449.0, 83450.0, 83451.0, 83452.0, 83452.93, 83456.0, 83458.0, 83459.0, 83460.0, 83462.0, 83465.0, 83468.0, 83468.12, 83469.0, 83471.0, 83472.0, 83471.68, 83470.08, 83475.0, 83476.0, 83470.0, 83470.17, 83480.0, 83480.96, 83483.0, 83484.0, 83485.0, 83486.0, 83487.0, 83488.0, 83490.0, 83491.0, 83490.89, 83494.0, 83496.0, 83499.0, 83500.0, 83502.0, 83503.0, 83506.0, 83508.0, 83512.0, 83513.0, 83514.0, 83517.0, 83518.0, 83520.0, 83526.0, 83527.0, 83528.0, 83529.0, 83530.0, 83531.0, 83532.96, 83532.0, 83533.15, 83535.0, 83536.0, 83537.0, 83533.0, 83542.0, 83543.0, 83544.56, 83544.0, 345686.5, 83546.0, 83548.0, 83549.67, 83550.0, 83547.0, 83552.0, 83553.0, 83553.6, 607843.0, 83556.0, 83555.0, 83560.0, 83561.0, 83562.0, 83564.0, 83566.0, 83568.0, 83569.0, 870000.0, 83571.0, 83571.48, 83573.0, 83574.0, 83575.0, 83576.0, 83579.0, 83580.0, 83581.0, 83582.0, 68715.0, 83584.0, 68716.96, 83590.0, 83591.0, 83592.0, 83594.0, 83595.0, 83594.16, 83597.0, 83598.0, 83599.0, 83600.0, 83601.64, 83602.0, 83603.0, 83604.0, 83603.04, 83606.0, 83599.88, 83607.0, 83609.0, 83609.58, 83611.0, 83610.0, 83605.0, 83615.22, 83616.0, 83618.0, 83620.0, 83621.91, 83622.0, 83621.0, 83624.0, 83625.0, 83626.67, 83626.0, 83627.0, 83628.0, 83627.17, 83630.0, 83631.0, 83632.0, 83634.0, 83633.0, 83636.0, 83637.0, 83636.8, 83640.0, 83642.0, 83644.75, 83645.0, 83646.0, 83649.0, 83650.0, 83651.0, 83652.0, 83653.0, 83654.0, 83655.0, 345800.0, 83656.0, 83657.0, 83656.32, 83660.0, 83661.0, 83654.32, 83664.0, 83665.0, 83666.0, 83664.8, 83665.8, 83669.0, 83673.0, 83675.0, 83676.0, 83677.0, 83678.4, 83679.0, 83680.0, 83678.0, 83681.0, 83682.0, 83684.0, 83682.56, 83687.0, 83688.0, 83689.0, 83690.0, 83691.0, 83692.0, 83693.95, 83694.0, 83695.0, 83696.0, 83693.0, 83698.0, 83699.0, 83700.0, 83704.0, 83705.0, 83706.0, 83709.0, 83711.0, 608000.0, 83712.0, 83714.0, 83715.0, 83713.0, 83718.0, 83719.0, 83720.0, 83721.0, 83722.0, 83725.0, 83726.0, 83727.0, 83730.0, 83731.0, 83732.0, 83735.0, 83736.0, 83736.72, 83739.0, 83740.0, 83741.06, 83742.0, 83743.0, 83741.03, 83739.24, 83747.0, 83748.0, 83750.0, 83751.0, 83752.32, 83752.0, 83753.13, 83753.0, 83751.83, 83754.92, 83759.0, 83760.0, 83761.0, 83762.8, 83763.0, 83765.89, 83766.35, 83768.0, 83769.0, 83770.0, 83772.0, 83773.0, 83773.08, 83775.0, 83777.0, 83781.0, 83782.0, 83784.0, 83785.0, 83788.0, 83790.0, 83791.63, 83792.0, 83795.0, 83796.0, 83797.0, 83798.0, 83799.0, 83800.0, 345946.0, 83803.0, 83804.0, 83805.6, 83806.0, 83805.0, 83808.0, 83809.0, 83808.92, 83811.0, 83807.0, 83813.04, 83807.4, 83819.67, 83820.0, 83819.0, 83822.0, 83823.5, 83824.0, 83827.0, 83828.0, 83830.0, 83831.0, 83832.0, 83833.0, 83835.0, 83836.0, 83837.0, 83835.95, 83839.0, 83840.0, 83842.0, 83844.0, 83845.0, 83848.0, 83850.0, 83855.0, 83856.12, 346000.0, 83857.0, 83858.0, 83860.74, 83860.0, 83856.0, 83861.0, 83864.0, 83865.0, 83867.0, 83870.0, 83873.0, 83876.0, 83877.0, 83878.0, 83879.0, 83880.0, 83881.0, 83876.29, 83883.0, 83884.0, 83885.0, 83886.4, 83888.0, 83892.0, 83895.0, 83897.0, 83899.84, 83900.0, 68779.0, 83904.0, 83906.0, 83910.0, 83911.0, 83913.22, 83914.0, 83915.0, 83916.0, 83918.0, 83919.0, 83919.48, 83918.4, 83920.0, 83923.55, 83924.0, 83925.74, 83925.0, 5588951.0, 83928.0, 83929.5, 83930.0, 83931.24, 83938.0, 83940.0, 83942.0, 346088.0, 83946.0, 83946.16, 83948.0, 83949.0, 83950.0, 83954.0, 83957.0, 83958.0, 83959.0, 83960.0, 83961.0, 83959.32, 83964.0, 83965.0, 83967.78, 83969.6, 83974.69, 83975.0, 83976.0, 83978.0, 83980.0, 83982.0, 83983.32, 83984.0, 83988.0, 83989.0, 83990.0, 83991.0, 83993.0, 83995.0, 83995.82, 83999.0, 84000.0, 84002.0, 84003.0, 84004.0, 84005.52, 84008.0, 84009.0, 84010.0, 84012.0, 84014.0, 84015.0, 84016.0, 84017.0, 68803.86, 84023.0, 84024.0, 84025.0, 84027.0, 84030.0, 84032.0, 84035.0, 84036.0, 84039.0, 84040.0, 84042.0, 84043.0, 84044.0, 84046.0, 84047.0, 84048.0, 84050.0, 84050.16, 84054.0, 84056.0, 84057.0, 84058.0, 84060.0, 84060.91, 84064.0, 84067.0, 84068.0, 84069.48, 84070.0, 84071.0, 84072.0, 84075.0, 84076.0, 84077.0, 84078.0, 84081.0, 84082.18, 84084.0, 84085.0, 84087.0, 84092.8, 84094.0, 84095.0, 84096.0, 84098.0, 84099.0, 84100.0, 84103.0, 84108.0, 84109.0, 84110.0, 84111.0, 84112.0, 84115.0, 84116.0, 84117.0, 870550.0, 84120.0, 84120.5, 84123.0, 84124.0, 84125.0, 84128.0, 84130.0, 84132.0, 84133.0, 84135.0, 84136.0, 84137.14, 84140.0, 84142.0, 84143.0, 84143.6, 84143.92, 84146.0, 84147.0, 84144.0, 84150.0, 84152.76, 84153.0, 84156.0, 84157.0, 84159.0, 84160.0, 84162.0, 84166.68, 84167.0, 84168.0, 84169.0, 84170.0, 84171.0, 84175.0, 84178.0, 84180.0, 84181.0, 84181.32, 84183.0, 84183.24, 84182.0, 84181.92, 84187.0, 84188.0, 84189.0, 84190.0, 84188.23, 84192.0, 84191.0, 84196.0, 84197.0, 84198.4, 84198.0, 84200.0, 84201.0, 84202.0, 84199.0, 84204.84, 84204.0, 84206.0, 84205.0, 84209.0, 84210.0, 84211.0, 84212.0, 84213.0, 84214.0, 84214.5, 84216.0, 84217.94, 84218.0, 84219.0, 84220.0, 84224.0, 84225.0, 84227.0, 84228.0, 84229.0, 84230.0, 84232.0, 84234.0, 84235.0, 84235.16, 84234.14, 84238.0, 84237.0, 84240.0, 84237.9, 84242.0, 84243.0, 84241.0, 84245.0, 84248.93, 84249.99, 84250.0, 84250.12, 84252.0, 84250.06, 84254.0, 84255.0, 84256.0, 84258.0, 84259.24, 84260.0, 84261.0, 84262.62, 84263.0, 84264.0, 84266.0, 84271.0, 84272.0, 84273.0, 84275.0, 84276.0, 84277.0, 84279.78, 84279.0, 84281.0, 84281.36, 84283.0, 84285.0, 84289.0, 84290.0, 84292.0, 84293.0, 84294.0, 84295.0, 84297.97, 84298.0, 84300.0, 84302.0, 84302.4, 84303.0, 84307.0, 84308.0, 84309.0, 84310.0, 84311.0, 84312.0, 84313.0, 84315.0, 84316.0, 84317.0, 84319.0, 84320.0, 84321.0, 84322.13, 84323.0, 84324.0, 84327.0, 84328.0, 84329.0, 84331.1, 84332.0, 84333.0, 84334.0, 84334.54, 84332.87, 84337.0, 84337.51, 84339.0, 84340.0, 84334.9, 84342.0, 84343.0, 84345.0, 84346.0, 84347.0, 84349.0, 84350.0, 84353.0, 84355.0, 84356.0, 84357.0, 84358.0, 84360.0, 84362.04, 84362.3, 84362.0, 84365.0, 84363.13, 84364.8, 84363.0, 84369.48, 84364.0, 84372.0, 84373.0, 84374.0, 84375.0, 84376.0, 84377.0, 84378.0, 84380.0, 84381.0, 84382.0, 84384.0, 84385.0, 84386.0, 84387.0, 84388.0, 84385.8, 84390.0, 84395.0, 84396.0, 84399.0, 84400.0, 84402.0, 84403.0, 84404.0, 84405.0, 84406.0, 84406.8, 84408.0, 84409.74, 84410.0, 84415.0, 84416.0, 84418.0, 84419.0, 84420.0, 84422.0, 84423.0, 84424.0, 84425.0, 84428.0, 346573.0, 84430.0, 84429.0, 84432.0, 84433.0, 84434.0, 84428.86, 84436.0, 84437.0, 84438.0, 84439.96, 84440.0, 84441.0, 84442.0, 84443.0, 84444.0, 84445.0, 84448.25, 84448.0, 84450.0, 84455.0, 84456.0, 84460.0, 84461.0, 84463.0, 84466.0, 84467.0, 84468.0, 84470.0, 84471.84, 84471.0, 84474.0, 84475.0, 84477.0, 84479.0, 84480.0, 84481.0, 84482.0, 84484.0, 84486.0, 84488.0, 84489.0, 84491.64, 84492.0, 84495.56, 84498.0, 84500.0, 84501.0, 84503.5, 84504.0, 84503.52, 84507.0, 84508.0, 84512.0, 84513.0, 84520.0, 84522.0, 84523.0, 84525.0, 84527.0, 84528.0, 84530.0, 84531.0, 84532.0, 84534.8, 84537.0, 84540.0, 84541.0, 84544.68, 84546.0, 84550.0, 84551.28, 84552.0, 84554.0, 84555.0, 84556.0, 84557.0, 84558.0, 84559.0, 84554.16, 84560.0, 84562.0, 84563.0, 84564.0, 84561.0, 84567.66, 84567.0, 84569.0, 84570.0, 84568.0, 84572.8, 84574.0, 84575.0, 84576.0, 84578.0, 84579.0, 84580.0, 84582.0, 84583.0, 84585.0, 84587.0, 84588.0, 84590.0, 84590.64, 84593.0, 84595.0, 84597.0, 84598.0, 84599.0, 84600.0, 84601.0, 84598.08, 84603.0, 84608.0, 84609.0, 84610.0, 84609.55, 84614.87, 84614.0, 84615.0, 84617.0, 84618.0, 84620.0, 84621.0, 84622.02, 84622.0, 84624.0, 84628.0, 84629.0, 84632.0, 84633.0, 84636.0, 84637.0, 84637.06, 84640.0, 84645.0, 84646.0, 84649.56, 84650.0, 84651.84, 84653.0, 84655.0, 84656.0, 84660.0, 84662.0, 84663.0, 84666.0, 84670.0, 84672.0, 84675.0, 84676.0, 84680.0, 84682.0, 84685.08, 84686.0, 84688.0, 84689.0, 84690.0, 84690.84, 84692.0, 84693.0, 84694.0, 84695.0, 84696.0, 84697.6, 84697.0, 84699.0, 84700.0, 84702.0, 84703.0, 84704.0, 84707.0, 84708.0, 84709.0, 609000.0, 84714.0, 84717.73, 84718.0, 84720.0, 84721.0, 84724.0, 84725.0, 84726.0, 84727.0, 84728.0, 84729.0, 84730.0, 84731.0, 84732.96, 84734.0, 84735.0, 84736.78, 84738.0, 84739.0, 84742.0, 84744.0, 68947.2, 68947.53, 84748.0, 84749.94, 84750.0, 84752.3, 84752.0, 84753.0, 84754.0, 84756.0, 84756.92, 84758.51, 84753.85, 84760.0, 84756.24, 346900.0, 84765.0, 84765.25, 84766.58, 84768.0, 84770.0, 84771.0, 84777.0, 84777.25, 84779.0, 84780.0, 84784.0, 84785.0, 84787.0, 84788.0, 84790.0, 84792.0, 84793.0, 84799.0, 84800.0, 84803.0, 84804.32, 84805.76, 84804.0, 84807.0, 84812.0, 84814.0, 84815.0, 84816.0, 84816.16, 84818.75, 84818.0, 84820.0, 84822.0, 84824.0, 84825.0, 84828.0, 84836.0, 84838.0, 84839.0, 84840.0, 84839.5, 84842.0, 84841.0, 84845.0, 84846.0, 84847.43, 84847.0, 84850.0, 84851.0, 84852.0, 84854.0, 84855.0, 347000.0, 84856.0, 84858.0, 84859.0, 84860.0, 84862.0, 84863.0, 84864.0, 84865.36, 84866.0, 84867.64, 84868.0, 84869.0, 84870.0, 84871.0, 84872.0, 84873.0, 84874.0, 84875.0, 84877.0, 84879.0, 84880.0, 84885.0, 84886.0, 84887.0, 84888.0, 84889.0, 84891.0, 84892.0, 84894.0, 84898.0, 84899.0, 84900.0, 84901.0, 84906.0, 84908.0, 84911.0, 84913.0, 84913.44, 84914.0, 84916.0, 84924.0, 84926.0, 84930.92, 84932.0, 84934.0, 84935.0, 84939.0, 84943.0, 84944.0, 84945.0, 84947.0, 84948.0, 84950.0, 84952.0, 84953.66, 84953.0, 84955.0, 84956.0, 84954.0, 84957.0, 84959.0, 84960.0, 84962.0, 84966.0, 84968.0, 84969.0, 84970.0, 84971.0, 84972.0, 84973.0, 84970.6, 84975.0, 84976.56, 84968.04, 84979.0, 84980.0, 84981.0, 84982.0, 84984.0, 84988.0, 84990.0, 84992.0, 1920000.0, 84994.0, 84995.04, 84996.0, 84998.64, 84999.92, 84999.61, 85001.0, 85002.0, 84998.0, 84999.0, 85000.0, 84999.98, 85000.24, 85003.0, 85004.92, 85005.0, 85007.0, 85008.0, 85009.6, 85009.0, 85016.28, 85017.59, 85017.0, 85019.0, 85020.0, 85021.0, 85017.2, 85027.0, 85028.0, 85030.0, 85032.0, 85035.0, 85039.0, 85040.0, 85041.0, 85044.0, 85047.0, 85048.0, 85049.0, 85050.0, 85051.2, 85054.0, 85055.0, 85056.0, 85058.06, 85060.0, 85061.0, 85062.0, 85064.0, 85068.0, 85070.0, 85072.0, 85073.0, 85075.0, 85076.0, 85077.0, 85079.0, 85080.0, 85081.53, 85083.0, 85084.0, 85085.0, 85086.0, 85087.0, 85085.45, 85089.0, 85090.0, 85092.0, 85093.0, 85100.0, 85101.0, 85104.0, 85106.72, 85107.0, 85109.0, 85110.0, 85111.0, 85115.0, 85116.8, 85119.0, 85120.0, 85124.0, 85125.0, 85126.0, 85128.0, 85129.13, 85131.88, 85133.0, 85134.0, 85133.67, 85137.0, 85138.0, 85140.0, 85140.61, 85141.0, 85147.0, 85150.0, 85151.0, 85152.0, 85153.0, 85155.0, 85155.29, 85158.0, 85158.92, 85160.0, 85163.0, 85164.0, 85165.0, 85168.0, 85169.0, 85168.08, 85171.0, 85172.0, 85173.08, 85175.0, 85176.0, 85175.04, 85177.0, 85179.0, 85179.91, 85181.0, 85180.0, 85183.0, 85184.0, 85185.0, 85186.0, 85190.0, 85192.0, 85193.0, 85195.0, 85196.88, 85196.0, 85198.0, 85197.0, 85200.0, 85204.0, 85205.0, 85207.0, 85208.71, 85209.0, 85210.0, 85212.0, 85214.0, 85215.0, 347360.0, 85217.0, 85216.0, 85220.0, 85221.12, 85224.0, 85225.79, 85225.0, 85227.0, 85228.0, 85229.0, 85230.0, 85233.82, 85234.0, 85235.0, 85236.0, 85237.0, 85238.0, 85240.0, 85241.5, 85242.0, 85244.9, 85245.0, 85248.0, 85249.0, 85250.0, 85251.0, 85253.87, 85255.0, 85256.0, 85258.01, 85259.0, 85260.0, 85264.0, 85265.0, 85268.0, 85269.0, 85270.0, 85270.12, 85272.0, 85272.68, 85271.0, 85275.0, 85280.0, 85282.0, 85284.0, 85288.0, 85289.04, 85289.0, 85291.0, 85294.0, 85295.0, 85296.0, 85297.0, 85298.2, 85298.0, 85300.0, 85302.0, 85304.0, 85305.0, 85306.72, 85306.0, 85309.0, 85311.0, 85312.2, 85313.37, 85313.0, 85318.0, 85319.0, 85320.24, 85321.0, 85320.0, 85323.42, 85324.0, 85325.0, 85326.0, 85328.0, 85330.0, 85332.0, 85333.0, 85335.0, 85337.0, 85338.0, 85339.0, 85340.0, 85342.0, 85343.0, 85344.0, 85346.0, 85347.22, 85348.0, 85350.0, 85351.0, 85352.0, 85353.0, 85354.0, 85355.0, 85356.0, 347500.0, 85360.28, 85360.0, 85363.0, 85364.0, 85365.0, 85366.0, 85367.0, 85368.0, 85369.0, 85370.0, 85369.08, 85368.92, 85373.0, 85374.0, 85376.69, 85377.0, 85379.02, 85379.0, 85380.0, 85383.0, 85384.0, 85388.0, 85389.0, 85391.28, 85392.0, 85391.48, 85394.0, 85394.7, 85396.0, 85391.0, 85398.0, 85399.0, 85400.0, 85401.0, 85402.0, 85401.6, 85404.0, 85405.0, 85410.0, 85412.0, 85416.0, 85420.0, 85423.0, 85424.0, 85425.0, 85426.0, 85427.0, 85428.0, 85430.91, 85432.0, 85433.0, 85434.0, 85435.0, 85436.0, 85438.0, 85439.0, 85440.0, 85441.0, 85444.0, 85446.0, 85447.0, 85448.0, 85449.0, 85450.0, 85451.0, 85452.0, 85453.0, 347601.0, 85460.0, 85461.5, 85463.0, 85464.0, 85465.0, 85467.0, 85468.0, 85470.0, 85472.0, 85473.0, 85474.0, 85480.0, 85482.0, 85484.0, 85485.0, 85486.0, 85488.0, 85490.0, 85492.0, 85494.0, 85496.0, 85498.0, 85500.0, 85501.0, 85505.0, 85508.0, 85509.0, 85510.0, 85511.0, 85512.0, 85511.66, 85514.0, 85515.0, 85513.0, 85518.16, 85520.0, 85523.0, 85524.0, 85526.0, 85527.0, 85528.0, 85529.0, 85530.0, 85531.0, 85535.0, 85536.0, 85537.0, 85540.0, 85541.0, 85544.0, 85548.0, 85549.82, 85550.0, 85551.0, 85549.93, 85553.0, 85555.0, 85556.0, 85558.0, 85559.0, 85560.0, 85562.0, 85562.36, 85563.0, 85566.0, 85567.0, 85570.2, 85570.39, 85572.0, 85574.0, 85575.0, 85579.3, 85580.0, 85581.56, 85582.0, 85583.0, 85586.0, 85587.89, 85588.0, 85590.0, 85590.07, 85592.0, 85594.0, 85595.0, 85596.0, 85600.0, 85601.0, 85602.0, 85603.0, 85604.0, 85605.0, 85607.0, 85609.0, 85610.0, 85612.0, 85613.0, 85614.0, 85617.0, 85618.0, 85619.0, 85620.0, 85621.0, 85622.0, 85624.0, 85626.06, 85628.31, 85630.0, 347775.0, 85633.0, 85634.0, 85635.0, 85638.0, 85640.0, 85641.0, 85643.0, 85644.0, 85645.0, 347789.0, 85648.0, 85650.0, 85652.0, 85652.65, 85655.96, 85656.0, 85656.48, 85658.0, 85658.84, 85660.0, 85655.0, 85662.0, 85666.0, 85667.0, 85668.0, 85669.0, 85670.0, 85673.0, 85674.04, 85675.0, 85676.0, 85678.0, 85680.0, 85684.0, 85685.6, 85686.0, 85688.0, 85689.0, 85690.0, 85692.0, 85693.0, 85696.0, 85697.0, 85700.0, 85702.0, 85703.0, 85704.0, 85705.0, 85704.5, 85707.0, 85711.0, 610000.0, 85712.0, 85714.0, 85715.14, 85716.0, 85717.0, 85715.0, 85719.0, 85720.0, 85722.3, 85723.0, 85724.0, 85722.0, 85725.0, 85723.88, 85728.0, 85728.24, 85731.0, 85732.0, 85734.0, 85737.0, 85738.0, 85739.0, 85740.0, 85740.84, 85742.0, 85748.0, 85749.85, 85750.0, 85752.0, 85753.0, 85753.8, 85755.0, 85756.0, 85757.0, 85758.0, 85757.5, 85760.0, 85761.0, 85765.0, 85766.0, 85767.0, 85768.0, 85770.0, 85771.0, 85772.0, 85777.0, 85778.0, 85779.0, 85780.0, 85782.0, 85786.0, 85788.0, 85789.0, 9522972.0, 85792.0, 85793.0, 85795.0, 85797.96, 85798.0, 85799.0, 85800.0, 85801.0, 85802.0, 85802.06, 85806.0, 85808.0, 85809.0, 85811.0, 85813.0, 85814.0, 85816.0, 85820.0, 85823.0, 85826.0, 85827.0, 85828.0, 85830.0, 85831.0, 85833.0, 85834.0, 85835.0, 85836.0, 85838.0, 85839.0, 85840.0, 85841.0, 85842.0, 85843.0, 85845.0, 85847.0, 85848.0, 85849.0, 85850.0, 85851.0, 85852.66, 85852.0, 85854.0, 85855.0, 348000.0, 85854.52, 85859.0, 85860.0, 85862.4, 85863.0, 85864.0, 85862.0, 85866.0, 85865.0, 85868.0, 85869.0, 85870.2, 85871.0, 85874.0, 85875.0, 85876.0, 85878.49, 85879.0, 85880.0, 85881.0, 85883.0, 85884.0, 85884.7, 85885.0, 85886.0, 85888.0, 85890.0, 85891.0, 85895.68, 85897.0, 85899.0, 85900.0, 85902.0, 85904.0, 85906.78, 85906.0, 85908.0, 85909.0, 85915.0, 85918.0, 85920.0, 85920.48, 85922.2, 85922.0, 85923.0, 85924.8, 85924.0, 85922.16, 85921.0, 85928.0, 85930.0, 85931.0, 85932.0, 85926.0, 85936.0, 85940.0, 85942.0, 85944.0, 85945.0, 85946.0, 85947.0, 85948.0, 85949.35, 85950.0, 85945.6, 85953.0, 85956.0, 85958.64, 85958.0, 85960.0, 85959.0, 85959.96, 85959.6, 85964.62, 85965.0, 85966.0, 85962.0, 85968.0, 85970.0, 85971.0, 85971.81, 85975.0, 85980.0, 85981.0, 85982.0, 85983.56, 85986.0, 85987.0, 85992.0, 85993.0, 69195.0, 85995.0, 85996.27, 85996.0, 85999.0, 85999.68, 86000.0, 86001.0, 86002.0, 86003.0, 86004.0, 86000.04, 86006.88, 86006.0, 86008.0, 69197.0, 86010.0, 86015.0, 86016.0, 86018.0, 86019.61, 86020.0, 86021.0, 86020.48, 86023.0, 86024.0, 86025.0, 86026.0, 86027.0, 86028.8, 86029.17, 86030.0, 86031.12, 86032.0, 86028.0, 86037.0, 86040.0, 86042.0, 86044.0, 86048.0, 86050.0, 86052.0, 86056.0, 86057.76, 86058.0, 86060.0, 86064.0, 86065.0, 86067.0, 86068.0, 86069.0, 86072.98, 86073.72, 86075.0, 86076.0, 86084.0, 86086.0, 86088.0, 86089.0, 86090.0, 86094.0, 86095.0, 86096.0, 86099.0, 86100.0, 86101.0, 86103.0, 86104.0, 86107.0, 86110.0, 86111.0, 86112.0, 86113.32, 86114.0, 86114.16, 86116.0, 86117.0, 86117.04, 86120.0, 86121.0, 86124.0, 86125.0, 86126.0, 86127.0, 86128.0, 86130.0, 86132.0, 86135.28, 86136.0, 86138.0, 86139.0, 86140.0, 86143.0, 86144.64, 86145.0, 86146.0, 86147.0, 86146.92, 86149.8, 86150.0, 86151.0, 86152.56, 86153.6, 86154.4, 86155.0, 348300.0, 86156.0, 86155.81, 86159.0, 86160.0, 86153.0, 86162.0, 86156.2, 86164.0, 86166.0, 86169.0, 86170.0, 86171.0, 86172.0, 86174.0, 86175.0, 86176.0, 86180.0, 86182.0, 86184.0, 86186.0, 86187.0, 86188.0, 86189.0, 86190.0, 86191.0, 86194.0, 86195.0, 86196.24, 86196.0, 86197.0, 86199.82, 86200.0, 86201.0, 86202.97, 86203.11, 86204.75, 86199.0, 86205.0, 86206.0, 86207.5, 86208.0, 86209.0, 86210.0, 86210.21, 86212.0, 86212.54, 86214.0, 86215.0, 86216.0, 86217.0, 86218.0, 86219.0, 86220.0, 86222.0, 86223.0, 86224.0, 86227.0, 86229.6, 86230.0, 86231.0, 86229.0, 86233.0, 86235.0, 86236.0, 86237.0, 86238.0, 86239.0, 86240.0, 86241.0, 86242.0, 86243.0, 86244.0, 86245.0, 86243.88, 86246.0, 86248.0, 86250.0, 86251.0, 86252.0, 86254.0, 86255.0, 86256.0, 86258.0, 86259.0, 86260.0, 86261.0, 86262.0, 86265.0, 86268.0, 86272.0, 86273.0, 86274.0, 86275.0, 86277.0, 86278.0, 86280.0, 86281.0, 86283.0, 86284.0, 86285.0, 86286.0, 86287.5, 86288.0, 86290.0, 86291.0, 86292.0, 86294.0, 86295.0, 86296.24, 86297.0, 86298.0, 86296.0, 86300.0, 86301.0, 86302.0, 86299.0, 86304.0, 86305.0, 86308.0, 86309.0, 86310.0, 86312.81, 86312.0, 86313.0, 86315.0, 86316.0, 86317.68, 86318.0, 86320.0, 86321.88, 86322.0, 86324.0, 86325.0, 86327.0, 86328.0, 86329.0, 86330.0, 86335.0, 86337.0, 86338.0, 86339.0, 86340.0, 86341.0, 86342.0, 86343.0, 86345.0, 86346.0, 86347.0, 86348.0, 86350.0, 86351.0, 86352.0, 86353.0, 86355.0, 86358.0, 86359.0, 86360.0, 86361.0, 86363.0, 86364.0, 86365.51, 86365.0, 86366.0, 86372.0, 86373.94, 86374.0, 86375.0, 86376.0, 86380.0, 69272.6, 86382.0, 86383.0, 86384.0, 86384.2, 86386.0, 86387.0, 86388.0, 86389.88, 86390.0, 86391.0, 86391.48, 86392.0, 86394.0, 86398.0, 86399.0, 86400.0, 86403.0, 86405.0, 86406.3, 86407.0, 86405.89, 86409.0, 86410.0, 86411.0, 86412.0, 86413.0, 86414.0, 86419.0, 86420.0, 86421.0, 86424.0, 86425.0, 86426.0, 86427.0, 86428.0, 86430.0, 86432.0, 86433.0, 86435.0, 86436.0, 86437.0, 86439.0, 86440.0, 86441.0, 86442.0, 86444.0, 86446.2, 86447.68, 86448.0, 86450.0, 86452.0, 86453.0, 86454.0, 86456.0, 86457.0, 86458.0, 86459.0, 86460.0, 86464.0, 86465.0, 86467.0, 86468.0, 86469.0, 86470.0, 86471.0, 86472.0, 86473.0, 86475.0, 86476.0, 86477.0, 86478.0, 86479.0, 86480.0, 86482.8, 86483.0, 86484.0, 86485.0, 86486.4, 86487.0, 86488.0, 86489.0, 86490.0, 86490.76, 86486.0, 86493.0, 86495.0, 86496.0, 86499.0, 86500.0, 610787.0, 86502.0, 86505.0, 86506.0, 86508.0, 86510.0, 86512.0, 86513.0, 86514.0, 86515.0, 86517.46, 86518.0, 86519.0, 86520.0, 86521.0, 86523.12, 86524.0, 86525.0, 86523.0, 86524.8, 86528.0, 86529.1, 86530.0, 86531.0, 86532.0, 86529.0, 86534.0, 86536.0, 86538.0, 86540.0, 86541.0, 86542.0, 86543.0, 86544.0, 86545.0, 86547.0, 86549.0, 86550.0, 86551.0, 86554.0, 86555.0, 86557.0, 86559.0, 86560.0, 86561.0, 86562.0, 86563.0, 86564.0, 86568.63, 86569.0, 86570.0, 86574.0, 86575.0, 86577.0, 86578.49, 86578.0, 86580.0, 86582.0, 86584.0, 86585.0, 86586.0, 86587.2, 86587.0, 86589.0, 86584.38, 86590.73, 86591.0, 86590.0, 86593.0, 86592.0, 86595.0, 86596.7, 86597.0, 86599.0, 86600.0, 86594.0, 86603.0, 86604.0, 86606.0, 86607.0, 86608.86, 86609.0, 86610.0, 86611.0, 86612.8, 86616.0, 86620.0, 86623.0, 86624.0, 86625.0, 86628.0, 86630.0, 86631.0, 86632.0, 86633.0, 86634.0, 86636.0, 86638.0, 86640.0, 86641.0, 86642.0, 86644.0, 86645.0, 86647.0, 86648.0, 86650.0, 86651.0, 86652.0, 86653.0, 86652.8, 86655.0, 86658.53, 86658.0, 86660.0, 86659.0, 86663.0, 86665.0, 86666.0, 86667.0, 86668.0, 86670.0, 86671.0, 86673.0, 86674.0, 86676.0, 86679.0, 86680.0, 86684.0, 86686.0, 86687.0, 86689.0, 86690.0, 86689.14, 86694.0, 86695.0, 86696.0, 86697.0, 86698.0, 86699.0, 86700.0, 86702.22, 86706.0, 86707.0, 86708.0, 86709.0, 86712.0, 86714.67, 86715.0, 86716.0, 86720.0, 86721.0, 86722.8, 86725.08, 69341.36, 86729.5, 86730.0, 86731.0, 86732.0, 86733.0, 86736.0, 86737.0, 86738.0, 86740.0, 86741.0, 86742.0, 86745.0, 86746.0, 86748.0, 86750.0, 86752.0, 86754.0, 86757.77, 86760.0, 86761.0, 86762.0, 86765.0, 86766.0, 86767.0, 86768.0, 69349.0, 86769.08, 86771.0, 86774.0, 86775.0, 86778.0, 86779.0, 86780.0, 86783.0, 86788.0, 86789.0, 86790.0, 86790.56, 86795.0, 86796.0, 86797.0, 86798.0, 86800.0, 86805.0, 86807.0, 86808.0, 86810.0, 86812.0, 86813.0, 86815.0, 86818.0, 86820.0, 86821.0, 86822.0, 86826.0, 86829.0, 86830.0, 86832.0, 86835.0, 86839.0, 86840.0, 86842.0, 86843.0, 86844.0, 86845.0, 86846.0, 86847.0, 86850.0, 86852.0, 86854.0, 86856.0, 349000.0, 86859.0, 86860.0, 86861.0, 86860.8, 86863.0, 86864.28, 86865.0, 86867.72, 86867.0, 86868.0, 86869.0, 86870.0, 86872.0, 86872.95, 86874.0, 86875.0, 86873.0, 86877.36, 86878.0, 86877.0, 86880.0, 86881.0, 86882.0, 86883.0, 86884.0, 86881.6, 86886.0, 86888.0, 86890.4, 86891.0, 86892.0, 86893.0, 86890.0, 86895.0, 86896.42, 86897.0, 86900.0, 86903.0, 86904.0, 86905.0, 86904.73, 86907.0, 86908.0, 86909.0, 86910.0, 86918.0, 86919.0, 86923.98, 86923.0, 86925.0, 86928.0, 86929.0, 86929.5, 86931.48, 86931.0, 86933.0, 86932.0, 86935.0, 86937.0, 86938.0, 86939.04, 86940.0, 86941.0, 86941.93, 86944.0, 86945.0, 86946.0, 86947.0, 86950.0, 86952.0, 86954.0, 86955.0, 86958.0, 86960.0, 86961.0, 86963.0, 86964.0, 86965.0, 86966.0, 86968.0, 86969.0, 86970.0, 86972.0, 86972.4, 86973.0, 86976.0, 86977.0, 86980.0, 86984.0, 86985.0, 86985.6, 86987.75, 86988.0, 86987.04, 86990.0, 86992.0, 86994.0, 86995.0, 86997.0, 86998.0, 86999.0, 87000.0, 86999.9, 87002.0, 87003.0, 87004.0, 87005.0, 87006.0, 87008.0, 87009.68, 87011.0, 87012.0, 87013.0, 87014.0, 87015.96, 87016.0, 87019.0, 87020.0, 87021.0, 87024.0, 87025.0, 87027.0, 87028.0, 87034.0, 87035.0, 87036.0, 87039.0, 87040.0, 87043.53, 87043.0, 87044.0, 87046.54, 87048.0, 87050.0, 87054.72, 87055.0, 87056.0, 87058.0, 87060.0, 87061.0, 87065.0, 87069.0, 87070.0, 87071.0, 87072.0, 87070.33, 87077.0, 87078.0, 87078.62, 87077.5, 87080.0, 87082.92, 87083.61, 87084.0, 87085.0, 87087.0, 87089.0, 87091.0, 87093.0, 87095.0, 87096.0, 87100.0, 87101.0, 9000000.0, 87104.0, 87106.0, 87108.49, 87108.0, 87110.0, 87111.0, 87112.0, 87113.84, 87115.0, 87117.0, 87119.0, 87120.0, 87122.18, 87122.56, 87124.0, 87125.0, 87126.0, 87123.0, 87128.0, 87129.0, 87130.0, 87129.22, 87132.0, 87133.0, 87131.0, 87135.0, 87136.0, 87132.24, 87138.0, 87143.04, 87144.92, 87145.0, 87146.0, 87147.0, 87144.0, 87149.0, 87150.0, 87151.0, 87152.0, 87156.0, 87157.0, 87158.0, 87159.0, 87160.0, 87162.0, 87168.0, 87174.0, 87177.0, 87178.0, 87179.64, 87180.0, 87183.0, 87184.5, 87186.0, 87190.0, 87192.0, 87193.0, 87194.4, 87195.0, 87196.0, 87197.0, 87198.0, 87199.0, 87200.0, 87201.0, 87202.0, 87194.0, 87204.0, 87207.0, 87208.0, 87209.0, 87210.0, 611500.0, 87213.0, 87214.0, 87216.0, 87217.0, 87216.91, 87219.0, 87220.12, 87221.0, 87225.0, 87226.0, 87229.0, 87230.0, 87231.0, 87234.0, 87235.0, 87236.0, 87238.0, 87240.0, 87241.0, 87242.0, 87243.0, 87244.0, 87246.0, 87249.0, 87250.0, 87252.0, 87253.0, 87254.0, 87255.0, 87256.0, 87257.0, 87258.0, 87261.0, 87263.0, 87264.0, 87264.48, 87265.0, 87267.0, 87268.0, 87269.0, 87275.0, 87276.0, 87276.68, 87277.0, 87278.0, 87280.0, 87281.55, 87282.0, 87283.0, 87284.0, 87285.0, 87286.0, 87287.0, 87289.0, 87290.0, 87292.0, 87295.0, 87297.0, 87298.0, 69454.0, 87300.0, 349444.0, 87306.0, 87307.92, 87308.0, 87312.0, 69457.92, 87315.0, 87316.0, 87319.0, 87320.0, 87322.0, 87323.0, 87322.76, 87327.0, 87328.0, 87330.0, 87334.0, 87335.0, 87337.68, 87338.0, 87339.0, 87340.0, 87341.0, 87343.0, 87344.0, 87345.0, 87346.0, 87347.0, 87348.0, 87349.0, 87350.0, 87352.0, 87355.0, 87359.64, 87360.0, 87363.0, 87364.0, 87365.0, 87367.0, 87370.0, 87372.0, 87373.0, 87374.0, 87375.0, 87377.0, 87378.0, 87379.0, 87380.0, 87381.0, 87384.0, 87386.0, 87388.0, 87392.0, 87394.0, 87396.0, 87397.0, 87399.0, 87400.0, 87401.6, 87402.0, 87401.0, 87404.0, 87405.0, 87410.0, 87412.0, 87413.0, 87414.0, 87415.0, 87417.0, 87418.0, 87420.0, 87421.0, 87423.0, 87424.0, 87425.0, 87427.0, 87428.0, 87430.0, 87431.0, 87434.0, 87435.0, 87436.0, 87437.0, 87438.0, 87439.0, 87440.0, 87441.0, 87442.76, 87443.0, 87444.0, 87442.0, 87445.0, 87447.0, 87448.0, 87449.2, 87450.0, 87448.56, 87451.0, 87453.0, 87452.0, 87456.0, 87456.8, 87458.0, 87459.0, 87460.0, 87463.0, 87464.0, 87465.0, 87467.0, 87468.0, 87470.0, 87471.0, 87473.0, 87475.0, 87476.0, 87478.0, 87480.0, 87483.0, 87485.93, 87488.0, 87489.0, 87490.0, 87491.0, 87492.0, 87495.0, 87496.0, 87497.0, 87498.0, 87500.0, 87501.0, 87500.09, 87503.0, 87504.0, 87505.0, 87507.0, 87508.0, 87511.0, 87512.0, 87515.04, 87516.0, 87518.0, 87520.0, 87521.0, 87522.0, 87524.0, 87525.0, 87527.0, 87530.0, 87531.0, 87535.0, 87537.0, 87539.0, 87540.0, 87542.0, 87543.0, 87544.0, 87545.0, 87546.0, 87547.0, 87547.44, 87550.0, 87552.0, 87553.0, 87553.62, 87554.0, 87556.0, 87557.0, 87553.6, 87559.2, 87560.2, 87560.0, 87561.0, 87562.0, 87565.0, 87567.0, 87568.0, 87569.0, 87570.0, 87571.0, 87572.0, 87576.0, 87577.0, 87578.0, 87579.0, 87576.27, 87580.0, 87582.0, 87585.0, 87587.0, 87588.0, 87589.0, 87590.0, 87591.0, 87592.0, 87595.0, 87597.18, 87598.27, 87599.0, 87600.0, 87597.0, 87602.84, 87604.0, 87606.0, 87607.0, 87609.0, 87610.0, 87611.0, 87612.0, 87610.8, 87609.6, 87615.0, 87619.0, 87620.0, 87621.39, 87622.0, 87623.0, 87624.0, 87625.0, 87626.0, 87627.0, 87630.0, 87631.0, 87635.0, 87636.0, 87637.0, 87639.0, 87640.0, 87641.0, 87642.88, 87643.0, 87645.0, 87646.0, 87646.53, 87648.0, 87650.0, 87651.2, 87651.0, 87653.0, 87654.0, 87655.0, 87656.0, 87657.0, 87660.0, 87661.0, 87663.0, 87665.0, 87668.0, 87669.0, 87670.0, 87671.0, 87672.0, 87673.0, 87675.0, 87676.0, 87677.0, 87678.24, 87677.41, 87680.0, 87684.0, 87687.0, 87688.0, 87690.0, 87692.0, 87693.0, 87694.48, 87695.0, 87696.0, 87698.0, 87699.0, 87700.0, 87699.96, 87702.0, 87703.0, 69535.0, 87705.0, 87706.0, 349847.0, 87708.0, 87710.0, 87712.0, 87713.0, 87714.0, 87713.7, 612000.0, 87716.0, 87720.0, 87721.0, 87722.0, 87725.0, 87727.0, 87729.0, 87730.0, 87732.0, 87734.0, 87737.0, 87739.0, 87740.0, 87740.04, 87742.0, 87745.0, 87747.0, 87747.96, 87749.0, 87750.0, 87748.0, 87752.0, 87753.0, 87754.0, 87755.0, 87756.42, 87756.0, 87756.9, 349903.0, 87757.0, 87762.0, 87763.0, 87763.03, 87767.0, 87768.0, 87769.76, 87769.0, 87771.0, 87767.85, 87774.0, 87775.0, 87776.0, 87779.0, 87780.0, 87781.0, 87782.0, 87784.69, 87785.88, 87786.0, 87785.0, 87788.0, 87789.0, 87790.0, 87791.0, 87792.0, 87794.0, 87795.0, 87796.8, 87796.0, 87800.0, 87802.0, 87804.0, 87805.0, 87806.07, 87807.0, 87808.0, 87809.0, 87806.0, 87811.0, 87812.0, 69555.0, 87815.0, 87816.0, 87818.0, 87818.84, 87819.0, 87821.0, 87828.0, 87830.0, 87832.0, 87838.0, 87840.0, 87841.83, 87844.0, 87845.0, 87846.0, 87847.62, 349992.0, 87849.0, 87850.0, 87851.88, 87852.0, 87853.0, 87854.0, 87855.0, 350000.0, 87857.0, 87858.0, 87859.54, 87860.0, 87859.0, 350004.0, 87864.0, 87867.0, 87869.3, 87870.0, 87871.0, 87872.0, 87873.0, 87875.0, 87876.0, 87877.0, 87879.94, 87880.0, 87879.0, 87882.0, 87884.0, 87885.48, 87885.0, 87887.0, 87888.0, 87885.2, 87890.0, 87890.55, 87892.0, 87894.0, 87895.0, 87896.0, 87900.0, 87900.8, 87903.0, 87904.0, 87905.0, 87906.0, 87907.0, 87909.0, 87910.0, 69576.8, 87912.0, 87914.0, 87915.0, 87916.0, 87917.0, 87918.0, 87920.0, 87921.0, 87922.0, 87924.0, 87926.0, 87927.0, 87929.0, 87936.0, 87937.0, 87938.0, 87939.0, 87940.0, 87942.0, 87943.0, 87944.0, 87945.0, 87942.4, 87947.0, 87948.0, 87946.0, 87950.0, 87950.2, 87954.0, 87956.0, 87957.0, 87958.0, 87960.0, 87960.24, 87962.0, 87963.0, 87964.0, 87966.24, 87966.0, 87968.0, 87966.96, 87970.43, 87971.11, 87971.0, 87971.28, 87972.0, 87975.0, 87970.0, 87977.0, 87979.0, 87980.0, 87981.0, 87981.97, 87983.0, 87984.0, 87983.29, 87986.08, 87986.0, 87988.32, 87988.0, 87990.0, 87991.0, 87987.0, 87996.0, 87998.0, 88000.0, 88003.0, 88004.0, 88005.0, 88007.0, 88008.0, 88010.0, 88012.0, 88015.76, 88017.0, 88018.0, 88020.0, 88022.0, 88023.0, 88025.0, 88026.0, 88027.68, 88028.0, 88030.0, 88032.0, 88033.0, 88036.0, 88041.0, 88042.0, 88044.0, 88045.25, 88045.0, 88046.0, 88050.0, 88053.0, 88054.0, 88055.0, 88056.0, 88057.0, 88058.0, 88060.0, 88061.0, 88062.0, 88065.0, 88066.5, 88066.0, 88068.0, 88074.0, 88075.0, 88076.0, 88078.0, 88080.0, 88082.21, 88083.0, 88085.0, 88086.0, 88087.0, 88088.0, 88090.0, 88092.0, 88096.0, 88099.44, 88100.0, 88104.0, 88109.0, 88110.0, 88113.63, 88114.0, 88115.0, 88116.0, 88119.0, 88120.0, 88122.0, 88124.76, 88125.0, 88127.0, 88128.0, 350272.0, 88132.0, 88133.0, 88137.0, 88140.0, 88141.0, 88142.0, 88141.08, 88144.0, 88145.72, 88145.08, 88147.0, 88148.0, 88150.0, 88151.0, 88151.33, 88151.99, 88154.0, 88155.0, 88157.0, 88160.0, 88161.0, 88162.0, 88163.0, 88164.0, 88165.0, 88166.0, 88167.0, 88169.0, 88170.0, 88171.0, 88172.0, 88174.0, 88176.0, 88177.0, 88181.0, 88182.0, 88183.0, 88183.93, 88185.0, 88186.0, 88184.34, 88188.0, 88184.0, 88192.0, 88193.0, 88196.0, 88199.0, 88200.0, 88202.0, 88204.0, 88205.0, 88207.0, 88209.0, 88210.0, 88212.0, 88213.0, 88214.0, 88215.0, 88217.0, 88221.0, 88224.0, 88225.0, 88226.0, 88229.0, 88230.0, 88231.0, 88233.0, 88234.69, 88234.92, 88234.0, 88237.33, 88235.0, 88236.0, 88238.0, 88240.0, 88242.0, 88245.0, 88246.0, 88247.0, 88248.0, 88249.0, 88250.0, 88251.0, 88252.0, 88253.0, 88249.79, 88255.0, 88257.0, 88258.0, 88259.0, 88260.0, 88261.0, 88262.0, 88265.0, 88266.0, 88268.0, 88269.0, 88270.0, 88272.0, 88274.0, 88275.0, 88275.91, 88277.0, 88278.0, 88280.0, 88282.0, 88284.0, 88286.0, 88288.0, 88292.0, 88295.0, 88296.0, 88300.0, 88302.0, 88303.3, 88305.0, 88306.0, 88309.0, 88312.05, 88313.0, 88317.0, 88318.84, 88319.0, 88320.0, 88323.0, 88324.0, 88325.0, 88326.0, 88325.4, 88331.0, 88332.42, 88337.0, 88338.0, 88340.0, 88341.0, 88342.0, 88344.0, 88349.0, 88350.0, 88353.38, 88354.0, 88355.52, 88356.0, 88360.0, 88363.36, 88364.0, 88365.0, 88367.0, 88368.0, 88369.0, 88370.04, 88371.0, 88373.0, 88375.0, 88376.0, 88380.0, 88381.0, 88382.0, 88385.0, 88386.0, 88386.37, 88388.0, 88391.0, 88392.0, 88393.0, 88391.65, 88396.0, 88397.76, 88399.0, 88400.0, 88399.48, 88403.0, 88404.0, 88405.0, 88406.0, 88408.0, 88414.0, 88415.0, 88416.0, 88417.0, 88418.0, 88420.0, 88423.0, 88424.0, 88428.48, 88430.0, 88431.0, 88432.0, 88434.0, 88438.71, 88440.0, 88441.0, 88442.0, 88445.0, 88450.0, 88451.0, 88452.0, 88453.0, 88454.0, 88456.61, 88457.0, 88458.0, 88457.02, 88460.33, 88462.0, 88463.0, 88464.0, 88466.0, 88472.0, 88475.0, 88476.0, 88475.92, 88480.0, 88482.0, 88483.0, 88484.0, 88487.0, 88488.0, 88490.0, 88491.0, 88492.0, 88494.45, 88497.0, 88498.2, 88499.0, 88500.0, 88501.0, 88502.0, 88504.98, 88504.0, 88506.0, 88508.0, 88510.0, 88511.0, 88512.0, 88513.0, 88512.89, 88515.6, 88516.0, 88517.0, 88518.0, 88510.04, 88520.0, 88521.0, 88522.0, 88524.0, 88525.0, 88526.0, 88529.22, 88530.0, 88531.0, 88532.0, 88533.0, 88529.0, 88535.0, 88536.95, 88537.0, 88538.0, 88536.0, 88540.0, 88541.0, 88543.0, 350688.0, 88545.0, 88548.0, 88550.0, 88552.0, 88553.0, 88554.77, 88554.0, 88556.0, 88558.0, 88559.0, 88560.0, 88560.31, 88562.0, 88565.0, 88566.96, 88567.0, 875000.0, 88568.0, 88570.0, 88571.0, 88572.0, 88566.0, 88575.0, 350720.0, 88578.0, 88579.0, 88580.0, 88581.0, 88582.0, 88583.0, 88584.0, 88587.0, 88588.0, 88590.0, 88593.84, 88596.0, 88598.0, 88600.0, 88605.0, 88607.0, 88608.0, 88613.0, 88615.0, 88616.0, 88617.73, 88620.0, 88621.0, 88622.0, 88625.0, 88628.8, 88629.0, 88632.0, 88634.49, 88636.0, 88638.26, 88639.0, 88640.0, 88641.0, 88639.85, 88643.0, 88645.0, 88646.0, 88648.0, 88649.0, 88650.0, 88648.37, 88652.0, 88654.0, 88655.0, 88656.0, 88657.0, 88658.0, 88660.0, 88661.0, 88666.0, 88668.0, 88670.0, 88671.0, 88672.0, 88670.36, 88675.0, 88676.64, 88678.0, 88680.0, 88681.08, 88683.0, 88686.81, 88686.0, 88688.0, 88690.0, 88691.0, 88692.0, 88693.0, 88695.0, 88696.0, 88698.24, 88700.0, 88701.0, 88702.0, 88704.0, 88706.5, 88709.0, 88710.0, 88712.0, 88713.0, 88714.96, 88714.0, 88713.16, 88715.0, 88718.0, 88720.0, 88721.0, 88723.0, 88724.0, 88725.0, 88727.0, 88728.38, 88728.0, 88732.0, 88733.0, 88735.0, 88736.1, 69740.88, 88736.96, 88740.0, 6118054.0, 88743.0, 88746.0, 88747.58, 88749.6, 88750.0, 88749.0, 88752.0, 88753.0, 88755.0, 88756.0, 88758.0, 88760.0, 88761.0, 88764.0, 88765.0, 88766.0, 88768.0, 88769.0, 88770.0, 88771.0, 88775.0, 88776.0, 88777.0, 88780.0, 88783.0, 88785.0, 88786.0, 88787.0, 88787.53, 88789.0, 88791.0, 88793.0, 88794.0, 88795.0, 88797.0, 88799.36, 88800.0, 88802.0, 88804.0, 88808.0, 88809.0, 88810.0, 88812.0, 88812.93, 88815.0, 88816.0, 88817.0, 88819.0, 88820.0, 88821.0, 88824.0, 88825.0, 88826.0, 88828.0, 88830.0, 88831.0, 88835.0, 88836.0, 88838.0, 88840.0, 88841.0, 88843.44, 88845.6, 88845.0, 88848.0, 88850.0, 88853.6, 351000.0, 88859.0, 88860.0, 88863.0, 88864.0, 88868.0, 88872.0, 88873.0, 88874.0, 88875.0, 88876.0, 88877.0, 88880.0, 88881.0, 88882.08, 88884.0, 88886.0, 88888.0, 88890.0, 88892.0, 88893.0, 88894.8, 88896.0, 88897.0, 88896.6, 88899.09, 88900.0, 88901.0, 88902.0, 88903.0, 88904.0, 88905.0, 88898.0, 88908.0, 88910.0, 88914.0, 88915.0, 88920.0, 88924.0, 88925.0, 88926.0, 88926.23, 88928.62, 88928.0, 88928.84, 88932.0, 88932.22, 88935.0, 88936.0, 88937.0, 88938.0, 88940.0, 88943.0, 88944.0, 88946.0, 88949.28, 88950.0, 88951.0, 88950.32, 88953.6, 88954.37, 88953.0, 88956.0, 88952.0, 88960.0, 88961.0, 88962.0, 88961.6, 88963.0, 88965.0, 88966.0, 88969.0, 88970.0, 88972.0, 88975.0, 88977.0, 88978.0, 88980.0, 88982.0, 88983.0, 88986.0, 88989.0, 88990.0, 88992.0, 88993.0, 88995.0, 88996.0, 88998.0, 88999.0, 89000.0, 89001.0, 89002.08, 58895.04, 89004.0, 89006.0, 89007.0, 89008.0, 89009.0, 89010.0, 89011.0, 89012.0, 89015.0, 89016.0, 89017.0, 89018.24, 89018.0, 89020.0, 89022.0, 89024.0, 89027.0, 89028.0, 89030.0, 89032.0, 89033.0, 89034.0, 89036.0, 89038.0, 89039.0, 89040.0, 89044.0, 89045.0, 89046.0, 89047.0, 89049.0, 89050.0, 89051.0, 89052.0, 89053.0, 89058.0, 89059.0, 89060.0, 89062.0, 89063.28, 89064.0, 89063.0, 89065.0, 89067.25, 89068.0, 89072.0, 89076.0, 89079.0, 89079.12, 89080.0, 89082.0, 89084.0, 89085.0, 89086.0, 89088.0, 89089.0, 89090.0, 89092.0, 89093.0, 89098.0, 89099.92, 89100.0, 89101.0, 89102.0, 89104.0, 89105.0, 89106.0, 89107.0, 89110.0, 89111.0, 89111.28, 89117.0, 89120.0, 89122.0, 89123.32, 89124.0, 89125.0, 89126.0, 89128.0, 89130.0, 89131.0, 89133.0, 89134.0, 89136.0, 89137.0, 89138.0, 89141.0, 89142.0, 89143.0, 89148.0, 89150.0, 89154.0, 89155.0, 89156.0, 89157.0, 89160.0, 89161.0, 89162.0, 89164.0, 89168.0, 89171.0, 89174.0, 89175.0, 89175.32, 89177.0, 89178.0, 89179.0, 89180.0, 89181.0, 89176.0, 89184.0, 89185.0, 89188.0, 89190.0, 89196.0, 89200.0, 89202.0, 89204.4, 89205.0, 89206.0, 89207.0, 89208.0, 89210.0, 89214.0, 89216.88, 89218.0, 89219.0, 89220.0, 89221.0, 89222.0, 89225.0, 89230.0, 89232.0, 89233.0, 89234.0, 89235.0, 89240.0, 89242.0, 89243.0, 89245.0, 89246.0, 89247.0, 89248.28, 89250.0, 89251.0, 89252.0, 89251.03, 89253.0, 89255.0, 89256.0, 89257.0, 89259.0, 89260.32, 89260.0, 89262.0, 89261.52, 89268.0, 89269.0, 89270.0, 89272.0, 89273.0, 89274.0, 89275.0, 89276.0, 89279.0, 1400000.0, 89280.0, 89284.0, 89285.0, 1400006.0, 89287.0, 89289.0, 89292.0, 89294.0, 89294.4, 89296.0, 89297.0, 89298.0, 89300.0, 89303.0, 89304.0, 89305.0, 89303.28, 89307.0, 89308.0, 89306.0, 89311.0, 89312.0, 89319.66, 89320.0, 89321.0, 89325.0, 89325.56, 89328.0, 89329.2, 89329.0, 89331.0, 89332.48, 89333.0, 89334.0, 89335.0, 89336.0, 89337.0, 89331.48, 89340.0, 89341.0, 89342.0, 89346.0, 89348.0, 89350.0, 89352.0, 89354.0, 89354.8, 89359.0, 89360.0, 89362.0, 89363.0, 89364.0, 89365.2, 89365.0, 89366.0, 89367.2, 89370.0, 89371.0, 89371.93, 89373.0, 89372.0, 89373.12, 89376.0, 89375.52, 89378.0, 89375.0, 89380.0, 89382.0, 89385.0, 89386.0, 89388.0, 89390.0, 89393.0, 89396.0, 89397.0, 89398.0, 89399.0, 89400.0, 89401.0, 89401.88, 89404.0, 89406.0, 89408.0, 89409.0, 89411.28, 89412.0, 89414.0, 89415.0, 89417.0, 89420.0, 89423.1, 89424.0, 89425.0, 89427.0, 89433.0, 89434.0, 89435.16, 89435.0, 89439.36, 89440.0, 89441.0, 89442.0, 89448.0, 89450.55, 89450.0, 89451.4, 89451.0, 89454.0, 89455.0, 89458.0, 89459.0, 89460.05, 89460.0, 89461.0, 89463.0, 89464.0, 89465.0, 89466.0, 89464.37, 89467.0, 89470.0, 89472.0, 89472.25, 89476.0, 89477.0, 89478.0, 89480.0, 89481.6, 89482.0, 89481.0, 89484.0, 89485.0, 89486.0, 89487.0, 89488.0, 89489.0, 89489.79, 89492.0, 89496.0, 89498.94, 89498.0, 89500.0, 89500.01, 89501.0, 89505.0, 89506.0, 89507.0, 89509.0, 89510.0, 89512.0, 89517.84, 89519.0, 89520.0, 89521.0, 89522.0, 89523.0, 89524.0, 89525.0, 89528.0, 89530.0, 89532.0, 89533.0, 89535.0, 89538.0, 89539.0, 89540.0, 89542.0, 89543.0, 89544.0, 89550.0, 89552.0, 89555.0, 89556.0, 351699.96, 89560.0, 89561.28, 89563.0, 89564.0, 89568.0, 89570.0, 89571.0, 89576.0, 89577.0, 89579.0, 89580.0, 89581.68, 89581.0, 89583.0, 89584.0, 89585.0, 89587.96, 89588.0, 89589.0, 89590.0, 89593.0, 89596.0, 89599.13, 89600.0, 89601.0, 89602.0, 89606.4, 89607.0, 89606.0, 89609.0, 89610.0, 89608.0, 89614.0, 89616.0, 89620.0, 89623.0, 89624.0, 89625.39, 89626.0, 89627.0, 89628.0, 89629.56, 89625.0, 89627.2, 89633.0, 89634.76, 89634.0, 89635.8, 89637.0, 89640.51, 89640.0, 89642.0, 89643.0, 89643.3, 89647.0, 89648.0, 89649.0, 89650.0, 89652.0, 89653.08, 89653.0, 89656.0, 89657.0, 89658.24, 89657.4, 89660.0, 89661.71, 89662.0, 89658.0, 89664.0, 89665.0, 89666.0, 89663.0, 89668.0, 89670.0, 89673.0, 89674.0, 89675.0, 89676.0, 89677.0, 89678.0, 89680.0, 89682.0, 89683.0, 89685.0, 89686.0, 89688.0, 89689.0, 89690.0, 89690.64, 89691.0, 89693.0, 89696.0, 89697.0, 89699.76, 89700.0, 89703.0, 89708.0, 89710.0, 89712.0, 89714.0, 89716.0, 89717.0, 89720.0, 89721.0, 89722.88, 89722.0, 89724.96, 89724.0, 89726.0, 89727.0, 89728.0, 89725.0, 89730.0, 89732.0, 89733.0, 89734.0, 89735.0, 89736.0, 89738.0, 89740.0, 89742.0, 89743.0, 89744.55, 89744.0, 89746.0, 89748.0, 89749.44, 89750.0, 89750.99, 89748.12, 89753.0, 89752.0, 89755.0, 89758.0, 89760.0, 89761.0, 89762.0, 89763.0, 89764.0, 89761.11, 89766.0, 89765.0, 89767.0, 89769.0, 89770.0, 89771.82, 89772.0, 89773.0, 89772.97, 89775.0, 89775.14, 89777.16, 89780.0, 89781.0, 89784.0, 89786.0, 89787.0, 89790.0, 89791.61, 89792.0, 89793.0, 89795.88, 89795.0, 89796.0, 89797.74, 89797.0, 89799.0, 89800.0, 89799.09, 89802.0, 89803.0, 89803.5, 89805.0, 351950.0, 89809.0, 89810.0, 89811.8, 89813.0, 89814.0, 89815.0, 89816.0, 89815.65, 89820.0, 89821.0, 89822.0, 89823.0, 89824.54, 89824.44, 89826.0, 89828.0, 89830.0, 89831.0, 89832.0, 89833.66, 89834.0, 89835.0, 89836.0, 89833.0, 89838.0, 89839.8, 89837.0, 89840.0, 89842.0, 89844.0, 89846.0, 89849.0, 89850.0, 89851.25, 89852.0, 89853.0, 352000.0, 89856.0, 89859.0, 89860.24, 89860.0, 89862.0, 89863.0, 89859.84, 89865.0, 89866.0, 89867.0, 89868.0, 89870.0, 89871.0, 89872.0, 89870.28, 89874.0, 89875.0, 89873.0, 89877.0, 89880.0, 89881.0, 89885.0, 89888.24, 89890.0, 89891.0, 89892.0, 89893.0, 89898.0, 89900.0, 89901.0, 89904.0, 89908.0, 89911.0, 89913.0, 89914.0, 89916.0, 89917.0, 89918.0, 89919.0, 89920.0, 89923.0, 89924.0, 89926.0, 89927.96, 89928.0, 89927.0, 89930.0, 89931.0, 89932.8, 89931.96, 89929.0, 89934.0, 89938.0, 89940.0, 89943.0, 89945.0, 89948.0, 89949.0, 89950.0, 89951.0, 89952.0, 89957.0, 89958.0, 89959.0, 89960.0, 89962.0, 89963.0, 89965.0, 89966.0, 89974.0, 89975.0, 89976.0, 89978.0, 89980.0, 89981.0, 89984.0, 89986.0, 89988.0, 89989.0, 89990.0, 89994.0, 89995.0, 89996.0, 89997.0, 89999.0, 90000.0, 90001.0, 89999.52, 90003.0, 90005.0, 90006.0, 90008.0, 90009.0, 90010.0, 90010.83, 90012.0, 69993.26, 90011.0, 90009.6, 90020.0, 90022.4, 90024.24, 90024.0, 90026.0, 90027.0, 90028.0, 90029.0, 90025.0, 90035.0, 90036.0, 90038.0, 69999.48, 90040.0, 90041.0, 90042.0, 90043.0, 90044.0, 90045.0, 90045.28, 90050.0, 90052.0, 90054.0, 90055.0, 90058.0, 90060.0, 90061.0, 90062.0, 90063.0, 90064.0, 90065.0, 90068.07, 90070.0, 90071.0, 90072.0, 90073.0, 90075.0, 90076.0, 90080.61, 90083.0, 90084.0, 90085.0, 90087.0, 90088.0, 90089.0, 90090.0, 90091.0, 90093.0, 90095.0, 90096.0, 90095.98, 90100.0, 90101.0, 90102.0, 90101.92, 90105.0, 90106.0, 90107.0, 352250.0, 90106.08, 90108.0, 90111.0, 90112.0, 90113.21, 90115.0, 90116.0, 90120.0, 90122.0, 90123.0, 90124.0, 90125.0, 90126.0, 90127.0, 90128.0, 90129.0, 90130.0, 90131.0, 90132.0, 90131.82, 90133.0, 90135.0, 90137.0, 90141.74, 90142.0, 90144.0, 90145.0, 90146.0, 90147.0, 90148.0, 90146.68, 90150.0, 90151.0, 90152.0, 90152.88, 90154.0, 90153.0, 90156.0, 90157.8, 90160.0, 90161.0, 90164.0, 90166.0, 90168.0, 90170.0, 90172.0, 90173.0, 90174.0, 90175.0, 90176.0, 90177.0, 90176.46, 90179.0, 90180.0, 90181.0, 90182.0, 90183.0, 90184.0, 90186.0, 90187.0, 90188.0, 90189.96, 90190.0, 90192.0, 90195.0, 90196.0, 90196.21, 90200.0, 90201.0, 90202.0, 90203.0, 90204.0, 90205.0, 90208.0, 90208.28, 614496.0, 90210.0, 90211.0, 90212.0, 90209.6, 90214.0, 90215.0, 90216.0, 90218.0, 90220.0, 70035.86, 90223.0, 90224.0, 90225.0, 90226.0, 90228.0, 90229.0, 90230.0, 90231.0, 614517.0, 90233.0, 90234.0, 90235.0, 90236.71, 90240.0, 90244.0, 90246.0, 90247.0, 90246.38, 90249.0, 90250.0, 90248.4, 90252.0, 90253.0, 90254.0, 90256.0, 90257.0, 90260.0, 90262.0, 90264.0, 90265.0, 90266.0, 90268.0, 90269.0, 90271.0, 90272.0, 90274.0, 90275.0, 90276.0, 90277.0, 90278.0, 90279.0, 90280.0, 90281.0, 90282.0, 90283.0, 90288.0, 90289.0, 90290.0, 90293.0, 90295.0, 90296.0, 90297.0, 90300.0, 90301.39, 90302.0, 90301.0, 90303.0, 90305.0, 90307.0, 90308.08, 90310.0, 90311.0, 90312.0, 90313.0, 90314.0, 90318.0, 90320.0, 90321.0, 90323.0, 90324.0, 90325.0, 90326.09, 90327.0, 90329.15, 90331.79, 90332.0, 90334.0, 90335.0, 90336.0, 90340.0, 90343.0, 90344.0, 90350.0, 90352.0, 90353.0, 90355.0, 90355.2, 90356.0, 90360.0, 90361.0, 90366.0, 90369.0, 90372.0, 90373.0, 90376.0, 90377.0, 90380.0, 90382.0, 90383.0, 90384.0, 90387.0, 90388.0, 90390.0, 90391.0, 90394.0, 90396.0, 90397.0, 90399.0, 90400.0, 90403.0, 90404.0, 90403.5, 90407.0, 90408.0, 90409.2, 90411.0, 90413.0, 90414.0, 90415.0, 90416.0, 90417.12, 90419.0, 90420.0, 90421.68, 90422.0, 90426.0, 90429.0, 90430.0, 90432.0, 90434.0, 90434.23, 90438.4, 90440.0, 90441.0, 90443.0, 90444.0, 90445.0, 90449.0, 90450.0, 90451.0, 90453.0, 90455.81, 90456.0, 90457.0, 90459.0, 90460.0, 90459.6, 90464.0, 90467.0, 90468.0, 90469.0, 90471.0, 90472.0, 90474.0, 90475.0, 90480.0, 90482.0, 90484.0, 90485.0, 90488.0, 90492.0, 90495.0, 90496.0, 90498.0, 90500.0, 90501.0, 90502.0, 90503.0, 90504.0, 90506.0, 90508.0, 90509.0, 90510.0, 90511.0, 90515.0, 90516.0, 90521.0, 90522.0, 90523.0, 90525.0, 90532.0, 90533.0, 90534.0, 90535.0, 90537.0, 90539.92, 90540.0, 90546.0, 90547.0, 90548.0, 90549.0, 90550.0, 90551.0, 90552.0, 90553.0, 90555.0, 90557.0, 90559.0, 90560.0, 90561.0, 90563.0, 90564.0, 90566.44, 90570.0, 90573.0, 90575.0, 90576.0, 90577.83, 90579.6, 90580.0, 90582.0, 90583.0, 90584.0, 90587.64, 90588.0, 90593.0, 90594.6, 90598.0, 90599.0, 90600.0, 90604.0, 90604.8, 90610.0, 90611.0, 90612.0, 90613.0, 90614.25, 90615.0, 90616.0, 90614.0, 90618.0, 90614.42, 90620.0, 90621.17, 90621.7, 3760640.0, 90625.0, 90624.0, 90626.0, 90628.0, 90627.0, 90630.0, 90631.0, 90634.0, 90636.0, 90637.0, 90638.0, 90639.0, 90640.0, 90641.0, 90640.16, 90643.0, 90644.0, 90645.0, 90647.0, 90648.0, 90650.0, 90651.0, 90652.0, 90650.85, 90652.64, 90655.0, 90656.0, 90657.0, 70122.0, 90660.0, 90663.0, 90665.0, 90666.0, 90667.0, 90670.0, 90672.0, 70125.01, 90675.0, 90676.0, 90677.0, 90679.0, 90680.0, 90682.0, 90683.0, 90684.54, 90685.0, 90688.0, 90690.6, 90691.39, 90691.0, 90690.0, 90694.0, 90695.0, 90696.0, 90697.0, 90700.0, 90701.0, 90705.0, 90706.0, 90707.0, 90710.0, 90711.0, 615000.0, 90713.0, 90715.0, 90716.0, 90719.0, 90720.0, 90722.32, 90722.0, 90725.0, 90726.0, 90727.0, 90728.0, 90730.0, 90731.0, 90732.0, 90732.12, 90734.0, 90732.96, 90736.0, 90737.0, 90739.0, 90740.0, 90741.0, 90742.08, 90743.0, 90744.0, 90743.31, 90742.0, 90748.0, 90749.0, 90750.0, 90751.0, 90754.0, 90756.0, 90757.0, 90760.0, 90765.0, 90766.4, 90771.0, 90776.0, 90777.9, 90778.0, 90779.0, 90780.0, 90781.0, 90784.68, 90785.0, 90786.0, 90787.0, 90789.0, 90790.0, 90791.91, 90792.0, 90793.0, 90795.12, 90796.0, 90797.0, 90798.0, 90799.0, 90800.0, 90804.0, 90812.0, 90813.0, 90814.0, 90821.56, 90822.0, 90823.0, 90825.0, 90827.0, 90828.0, 90829.0, 90830.0, 90833.0, 90837.0, 90838.0, 90839.0, 90840.0, 90843.0, 90844.0, 90845.0, 90848.0, 90850.0, 90851.0, 90852.0, 90854.0, 353000.0, 90857.0, 90856.0, 90858.0, 90862.0, 90864.0, 90865.0, 90866.0, 90867.5, 90867.0, 90869.0, 90870.0, 90871.0, 90868.53, 90873.0, 90875.0, 90876.0, 90879.0, 90880.0, 90886.0, 90887.0, 90888.0, 90892.0, 90894.0, 90895.0, 90896.0, 90897.0, 90898.0, 90899.43, 90900.0, 90899.02, 90899.76, 90899.0, 90904.0, 90905.0, 90906.0, 90908.0, 90910.0, 90912.0, 90913.0, 90914.38, 90915.0, 90918.96, 90919.0, 90924.0, 90927.0, 90928.0, 90930.0, 90931.17, 90935.0, 90935.04, 90937.0, 90938.0, 90936.0, 90939.0, 90941.0, 90945.0, 90946.0, 90947.0, 90948.0, 90949.0, 90954.0, 90955.0, 90957.0, 90958.15, 90960.0, 90964.0, 90967.0, 90969.0, 90974.0, 90975.06, 90976.0, 90977.0, 90978.0, 90979.0, 90980.0, 90983.0, 90984.0, 90986.0, 90987.0, 90993.0, 90994.44, 90994.0, 90996.0, 90997.0, 90998.0, 90999.0, 91000.0, 91000.89, 91002.89, 91004.0, 91008.0, 91011.0, 91016.0, 91018.0, 91020.0, 91022.0, 91023.0, 91026.0, 91027.0, 91028.0, 615322.0, 91034.0, 91034.94, 91039.0, 91045.0, 91047.0, 91050.0, 91052.56, 91053.0, 91054.0, 91052.0, 91058.0, 91060.0, 91061.0, 91063.0, 91065.17, 91066.0, 91067.0, 91068.0, 91072.0, 91075.0, 91077.0, 91078.27, 91079.0, 91080.0, 91080.72, 91083.0, 91084.0, 91087.0, 91090.0, 91091.64, 91092.0, 91100.0, 91103.0, 91104.0, 91106.0, 91108.39, 91109.0, 91108.0, 91111.0, 91115.0, 91116.0, 91117.2, 91119.0, 91120.0, 91125.0, 91128.0, 91130.0, 91132.0, 91135.0, 91136.0, 91139.0, 91140.0, 91141.0, 91144.0, 91147.0, 91149.0, 91152.0, 91153.0, 91154.72, 91155.0, 91154.0, 91157.0, 91159.0, 91161.0, 91162.0, 91166.4, 91169.0, 91175.0, 91176.12, 91176.0, 91178.36, 91177.0, 91178.0, 91175.28, 91182.0, 91185.0, 91185.48, 91186.0, 91188.0, 91185.15, 91190.0, 91193.0, 91196.0, 91200.0, 91203.0, 91207.0, 91209.0, 91210.0, 91211.72, 91212.0, 91209.63, 91214.0, 91215.0, 91216.0, 91220.0, 91223.0, 91224.0, 91225.0, 91227.0, 91228.0, 91229.0, 91230.0, 91231.0, 91232.0, 91233.0, 91234.2, 91235.0, 91236.0, 91238.0, 91239.0, 91240.0, 91241.64, 91241.0, 91242.0, 91244.0, 91245.0, 91243.0, 91248.0, 91249.0, 91250.0, 91253.0, 91255.0, 91259.0, 91260.0, 91262.0, 91264.0, 91266.0, 91267.0, 91270.0, 91271.0, 91272.0, 91273.0, 91274.94, 91275.0, 91276.84, 91277.0, 91278.32, 91278.0, 91280.0, 91281.0, 91277.76, 91281.21, 91284.0, 70245.0, 91286.0, 91287.0, 91288.0, 91290.0, 91291.0, 91292.0, 91295.0, 91296.0, 91297.0, 91298.0, 91298.01, 91300.0, 91299.0, 91305.0, 91306.0, 91313.0, 91315.0, 91316.0, 91317.0, 91315.08, 91318.0, 91320.0, 91321.0, 91324.0, 91326.0, 91327.0, 91328.0, 91333.0, 91336.0, 91337.0, 91339.44, 91340.11, 91342.0, 91343.0, 91344.0, 91345.0, 91347.0, 91348.3, 91348.0, 91350.0, 91352.0, 91353.0, 91354.0, 91355.0, 91356.0, 91360.0, 91360.68, 91360.43, 91362.0, 91364.0, 91361.0, 91365.0, 91368.0, 91370.0, 91371.77, 91375.0, 91376.0, 91377.0, 91380.0, 91383.0, 91384.08, 91384.0, 91385.0, 91386.45, 91386.0, 91388.0, 91389.0, 91390.0, 91387.0, 91392.0, 91393.0, 91395.0, 91396.0, 91393.54, 91398.81, 91399.0, 91400.0, 91401.0, 91402.0, 91402.41, 91404.0, 91405.0, 91409.0, 91411.92, 91412.88, 91413.0, 91412.0, 91415.0, 70273.84, 91419.0, 91420.0, 91421.0, 1140000.0, 91425.0, 91424.88, 91427.0, 91428.0, 91426.0, 91430.0, 91431.0, 91433.0, 91435.5, 91435.0, 91439.0, 91440.0, 91442.0, 91443.0, 91444.0, 91445.0, 91447.0, 91448.0, 91447.79, 91450.0, 91455.0, 91456.0, 91457.0, 91458.0, 91460.0, 91461.0, 91462.0, 91464.0, 91467.0, 91468.0, 91470.97, 91470.0, 91474.0, 91475.28, 91476.0, 91475.0, 91478.0, 91480.0, 91481.0, 91482.84, 91484.0, 91485.0, 91487.0, 91488.0, 91489.0, 91491.0, 91493.0, 91496.0, 91500.0, 91505.0, 91506.0, 91507.0, 91508.0, 91509.0, 91510.0, 91511.0, 91512.0, 91513.0, 91514.0, 353660.0, 91518.0, 91519.0, 91520.0, 91523.59, 91524.0, 91532.0, 91533.0, 91535.83, 91537.0, 91540.0, 91541.0, 91542.0, 91546.4, 91548.0, 91549.0, 91550.0, 91553.0, 91559.0, 91560.0, 91562.0, 91563.0, 91566.0, 91567.0, 91572.0, 91573.0, 91575.0, 91576.0, 91580.0, 91582.56, 91584.0, 91585.0, 91586.0, 91587.0, 91588.0, 91593.0, 91596.0, 91598.0, 91600.0, 91608.0, 91609.0, 91610.0, 91611.56, 91612.6, 91612.0, 91614.0, 91615.0, 91616.0, 91617.0, 91619.0, 91620.0, 91620.07, 91622.0, 91624.0, 91624.28, 91629.0, 91630.0, 91632.25, 91632.0, 91633.0, 91634.0, 91637.0, 91638.0, 91640.0, 91642.0, 91644.0, 91644.8, 91646.0, 91647.0, 91650.0, 91652.0, 91653.0, 91656.0, 91657.0, 91658.0, 91660.0, 91661.0, 91663.0, 91665.0, 91666.0, 91667.0, 91670.0, 91673.0, 91676.0, 91677.0, 91680.0, 91685.0, 91686.0, 91688.0, 91689.0, 91690.0, 91691.0, 91692.0, 91696.0, 91698.0, 91700.0, 91704.0, 91705.08, 91705.0, 91708.0, 91711.0, 616000.0, 91714.0, 91716.0, 91726.0, 91727.79, 91728.0, 91731.0, 91733.0, 91734.0, 91735.0, 91740.0, 91741.64, 91745.0, 91746.0, 91748.0, 91749.0, 91750.0, 91752.0, 91754.0, 91755.0, 91756.0, 91757.0, 91758.0, 91758.24, 91760.0, 91754.88, 91761.63, 91768.0, 91769.0, 91770.0, 91775.0, 91776.0, 91780.0, 91782.0, 91783.0, 91785.0, 91788.0, 91790.0, 91793.28, 91794.0, 91793.52, 91797.96, 91798.0, 91800.0, 91801.0, 91800.18, 91801.6, 91805.0, 91806.0, 91808.0, 91809.61, 91810.72, 91811.0, 91812.0, 91809.0, 91814.0, 91820.0, 91822.0, 91824.0, 91825.0, 91826.0, 91827.0, 91829.92, 91831.27, 91836.0, 91836.43, 91840.0, 91841.0, 91842.0, 91844.0, 91845.44, 91848.0, 91849.0, 91850.0, 91851.84, 91852.0, 91855.0, 91856.0, 354000.0, 91859.0, 91860.0, 91864.0, 91865.0, 91870.0, 91872.0, 91873.0, 91874.0, 91875.0, 91876.0, 91877.0, 91878.0, 91880.0, 91882.0, 91884.0, 91885.0, 91886.0, 70366.67, 91888.0, 91890.0, 91891.0, 91892.0, 91894.0, 91896.0, 91899.0, 91900.0, 91902.0, 91908.0, 91909.91, 91909.0, 91910.28, 91912.0, 91910.0, 91914.0, 91915.0, 91915.02, 91916.0, 91915.92, 91919.0, 91920.0, 91923.0, 91925.0, 91927.0, 91930.0, 91932.0, 91933.72, 91936.0, 91939.0, 91939.12, 91940.0, 91939.85, 91943.0, 91944.0, 91945.0, 91946.0, 91945.84, 91948.0, 91949.0, 91950.0, 91947.96, 91953.0, 91954.0, 91955.0, 91956.0, 91960.0, 70381.0, 91963.0, 91964.0, 91967.0, 91968.0, 91969.0, 91970.0, 91971.0, 91977.0, 91978.0, 91979.0, 91980.0, 91981.0, 91983.0, 91985.0, 91986.76, 91986.0, 91988.0, 91989.0, 91990.0, 91985.28, 91992.0, 91987.98, 91997.0, 91998.4, 91999.0, 92000.0, 91998.0, 91999.32, 92001.0, 92004.0, 92010.0, 92011.0, 92012.0, 92014.52, 92015.0, 92016.0, 92017.0, 92019.0, 92020.0, 92024.0, 92025.0, 92025.25, 92027.0, 92028.0, 92029.0, 92030.0, 92032.0, 92036.0, 92037.0, 92039.0, 92040.0, 92043.0, 92045.0, 92046.0, 92048.0, 92050.0, 92051.0, 92052.0, 92054.0, 92055.0, 92056.48, 92057.0, 92058.0, 92059.42, 92060.26, 92061.0, 92064.0, 92066.0, 92070.0, 92071.0, 92074.0, 92076.0, 92078.0, 92080.49, 92080.0, 92081.0, 92080.8, 92083.0, 92084.0, 92085.0, 92086.0, 92087.0, 92088.0, 92090.0, 92091.59, 92092.0, 92091.0, 92094.0, 92090.8, 92096.64, 92097.0, 92096.0, 92099.78, 92100.0, 92101.0, 92101.76, 92102.0, 92098.0, 92108.0, 92110.0, 92112.0, 92114.88, 92116.0, 92117.76, 92121.0, 92123.0, 92127.88, 92130.0, 92132.89, 92133.0, 92135.0, 92136.0, 92138.0, 92139.12, 92140.0, 92141.0, 92142.0, 3500015.0, 92144.0, 92145.0, 92146.0, 92147.0, 92148.0, 92149.0, 92150.0, 92151.0, 92154.0, 92156.0, 92158.0, 92160.0, 92160.64, 92162.0, 92163.0, 92165.0, 92170.0, 92172.0, 92175.0, 92177.0, 92180.0, 92182.0, 92183.94, 92184.0, 92185.0, 92188.46, 92188.45, 92191.0, 92192.0, 92193.56, 92193.0, 92196.0, 92197.37, 92197.0, 92197.44, 92200.0, 92198.66, 92202.0, 92203.0, 92204.0, 92205.0, 92206.0, 92207.0, 92208.0, 92209.0, 92210.0, 92208.37, 92212.0, 92213.0, 92214.0, 92210.82, 92212.01, 92217.0, 92217.6, 92220.0, 92222.4, 92223.0, 92224.0, 92225.0, 92226.0, 92222.0, 92230.6, 92230.0, 92232.0, 92233.0, 92234.0, 92235.0, 92236.0, 92240.0, 92243.0, 92244.0, 92245.0, 66188.0, 92247.0, 92248.0, 92250.0, 92253.84, 92256.0, 92257.0, 92258.0, 92259.0, 92260.0, 92265.0, 92268.0, 92268.8, 92270.0, 92269.0, 92274.0, 92275.0, 92276.0, 92274.24, 92279.05, 92280.0, 92281.0, 70445.0, 92284.0, 92286.0, 70446.2, 92289.0, 92290.0, 92291.0, 92292.0, 92288.0, 92294.0, 92295.0, 92298.0, 92300.0, 92301.0, 92304.0, 92306.0, 92307.41, 92308.0, 92309.0, 92310.0, 92312.0, 92315.0, 92316.0, 92318.0, 92320.0, 92320.95, 92323.0, 92324.0, 92325.0, 92326.68, 92326.0, 92330.0, 92331.0, 92333.0, 92334.24, 92334.0, 92340.0, 92341.11, 92341.0, 92345.0, 92347.98, 92349.0, 92350.0, 92352.0, 92354.0, 92355.0, 92360.56, 92360.0, 92361.0, 92363.6, 92364.0, 92365.0, 92366.0, 92367.0, 92368.0, 92369.0, 92371.0, 92372.0, 92371.91, 92374.0, 92375.0, 92374.13, 92378.0, 92379.03, 92378.27, 92380.0, 92383.0, 92384.0, 92387.0, 92388.0, 92390.0, 92391.0, 92392.0, 92393.0, 92399.0, 92400.0, 92402.0, 92404.0, 92405.41, 92406.0, 92407.0, 92408.0, 92410.0, 92414.0, 92418.21, 92418.0, 92420.0, 92421.0, 92422.0, 92423.0, 92424.0, 92426.0, 92428.18, 92429.0, 92430.0, 92433.0, 92435.0, 92436.0, 92437.0, 92436.99, 92440.0, 92441.0, 92444.0, 92446.0, 92448.0, 92450.0, 92451.0, 92452.0, 92456.0, 92458.0, 92459.0, 92460.0, 92463.0, 92464.0, 92465.0, 92466.0, 92469.0, 92470.0, 92472.0, 92473.0, 92475.0, 92477.0, 92478.0, 92480.0, 92482.0, 92484.0, 92485.0, 92487.0, 92488.0, 92489.0, 92490.0, 92491.0, 92495.0, 92496.0, 92497.0, 92500.0, 92502.0, 92504.0, 92506.0, 92507.0, 92508.0, 92510.0, 92514.0, 92517.0, 92518.0, 92520.0, 92521.0, 92525.0, 92529.0, 92530.0, 92531.04, 92532.62, 92533.0, 92532.0, 92535.0, 92534.0, 92539.0, 92540.0, 92541.0, 92542.0, 92543.0, 92544.0, 92548.0, 92550.0, 92551.0, 92552.0, 92554.49, 92555.0, 92557.0, 92559.0, 92560.0, 92561.0, 92562.0, 92563.0, 92564.0, 92565.0, 92564.42, 92564.03, 92568.0, 92570.0, 92571.0, 92575.0, 92576.0, 92577.03, 92580.0, 92581.0, 92581.68, 92584.0, 354728.0, 92586.0, 92588.0, 92589.0, 92592.0, 92593.0, 92595.0, 92599.0, 92600.0, 92601.6, 92604.0, 92607.0, 92608.0, 92614.0, 92615.0, 92616.0, 92620.0, 92622.0, 92628.0, 92632.0, 92633.0, 92636.0, 92637.48, 92638.0, 92640.0, 92641.0, 92641.34, 92643.0, 92644.0, 92643.38, 92645.0, 92647.0, 92648.0, 92650.0, 92652.0, 92661.0, 92662.8, 92664.0, 92665.0, 354808.0, 92666.08, 92668.0, 92666.0, 92670.0, 92671.0, 92673.0, 92674.0, 92675.0, 92676.0, 92678.0, 92679.0, 92680.0, 92680.85, 92682.0, 92683.0, 92685.0, 92686.0, 92687.0, 92688.0, 92690.0, 92691.0, 92693.0, 92694.0, 92695.0, 92697.0, 92699.88, 92700.0, 92704.0, 92705.0, 92706.0, 92707.0, 92708.0, 92709.0, 92710.0, 92712.79, 92712.0, 617000.0, 92715.0, 92716.0, 92721.0, 92722.0, 92725.0, 92726.0, 92728.0, 92729.0, 92730.0, 92732.0, 92733.0, 92734.0, 92735.85, 92736.0, 92737.0, 92738.0, 92740.0, 92742.0, 92744.0, 92745.0, 92747.0, 92748.0, 92749.0, 92750.0, 92750.52, 92751.0, 92754.0, 92758.0, 92760.0, 92761.0, 92763.0, 92763.84, 92767.0, 92768.0, 92772.0, 92774.0, 92775.0, 92777.0, 92778.0, 92780.0, 92781.0, 92783.0, 92784.0, 92788.0, 92789.04, 92791.0, 92792.0, 92794.0, 92795.0, 92796.0, 92798.0, 92800.0, 92801.73, 92805.0, 92808.0, 92813.0, 92815.0, 92816.0, 92818.0, 92819.0, 92820.0, 92823.0, 92824.56, 92823.96, 92824.0, 92827.44, 92829.0, 92832.0, 92833.0, 92833.48, 92840.0, 92843.0, 92844.0, 92845.0, 92847.0, 92850.0, 92851.2, 92852.0, 92853.0, 92851.0, 355000.0, 92856.0, 92858.0, 92859.0, 92862.0, 92863.0, 92866.1, 92866.0, 92868.0, 92867.0, 92870.0, 92871.0, 92872.0, 92866.63, 92869.0, 92876.0, 92876.62, 92878.0, 92879.0, 92880.0, 92881.0, 92882.0, 92885.0, 92888.46, 92889.0, 92889.08, 92892.0, 92893.0, 92892.8, 92895.44, 92897.0, 92898.0, 92899.0, 92900.0, 92901.0, 92902.0, 59050.54, 92904.0, 92904.76, 92906.0, 92907.0, 92908.8, 92907.62, 92914.0, 92916.0, 92917.0, 92920.0, 92922.0, 92925.0, 92926.41, 92926.0, 92928.0, 92934.0, 92934.4, 92935.0, 92937.0, 92940.0, 92940.07, 92942.0, 92944.0, 92946.96, 92947.0, 92949.0, 92951.28, 92953.71, 92954.0, 92955.0, 92957.0, 92961.0, 92962.0, 92964.0, 92966.0, 92968.0, 92970.0, 92973.0, 92974.0, 92975.0, 92976.0, 92974.96, 92978.0, 92979.0, 92980.0, 92981.0, 92984.0, 92985.0, 92987.0, 92988.0, 92996.0, 92997.0, 92999.0, 93000.0, 93003.0, 93004.0, 93006.0, 93007.0, 93008.0, 93010.0, 93012.0, 93014.0, 93019.0, 93020.0, 93024.0, 93025.0, 93027.0, 93028.0, 93030.0, 93031.64, 93032.04, 93033.0, 93036.0, 93040.0, 93042.0, 93044.0, 93046.08, 93048.0, 93049.21, 93050.0, 93053.0, 93054.0, 355200.0, 93059.3, 93060.0, 93062.0, 93065.0, 93066.0, 93069.0, 93070.08, 93071.0, 93072.0, 93075.0, 93076.0, 93077.0, 93078.0, 93079.0, 93080.0, 93081.6, 93083.12, 93084.0, 93085.0, 93086.0, 93087.0, 93090.0, 93091.0, 93093.0, 93094.0, 93096.0, 93098.0, 93100.0, 93100.8, 93102.0, 93103.0, 93101.0, 93105.0, 93106.0, 93108.0, 93109.0, 93113.0, 93114.0, 93116.0, 93120.0, 93121.0, 93123.0, 93125.0, 93126.0, 93127.0, 93129.0, 93130.0, 93132.0, 93135.0, 93136.0, 93138.0, 93140.0, 93141.0, 93142.0, 93143.0, 93145.0, 93145.56, 93147.0, 93147.37, 93149.0, 93150.0, 93151.0, 93152.0, 93152.8, 93155.0, 93156.0, 93157.0, 93158.0, 93160.0, 93168.0, 93169.0, 93172.0, 93174.0, 93177.0, 93181.0, 93182.0, 93184.0, 93185.0, 93186.33, 93186.0, 93189.6, 93189.0, 93190.0, 93192.0, 93198.0, 93200.0, 93201.0, 93202.0, 93204.0, 93210.0, 93211.0, 93215.0, 93216.0, 93217.0, 93218.32, 93219.0, 93220.0, 93221.0, 93218.0, 93224.0, 93225.0, 93226.0, 93232.0, 93233.0, 93234.0, 93238.0, 93239.0, 93240.0, 93246.0, 93250.0, 93252.0, 93254.06, 93255.0, 93256.0, 93257.0, 93259.0, 93260.0, 93262.21, 93264.0, 93266.98, 93269.0, 93270.0, 93271.0, 93275.0, 93280.0, 93284.0, 93285.0, 93287.58, 93288.0, 93289.0, 93290.0, 93291.0, 93293.0, 93294.0, 93295.0, 93298.0, 93299.0, 93300.0, 93300.48, 93302.0, 93303.0, 93309.0, 93310.0, 93310.08, 93312.0, 93315.0, 93318.0, 93321.0, 93321.25, 93322.0, 93324.0, 93325.0, 93328.0, 93330.0, 93331.0, 93332.0, 93333.0, 93331.8, 93336.0, 93340.0, 93341.0, 93343.0, 93349.0, 93350.0, 93352.0, 93353.44, 93354.16, 93354.0, 93356.0, 93355.0, 93360.0, 93363.0, 93365.0, 93366.0, 93370.0, 93372.0, 93375.0, 93376.0, 93380.0, 93382.0, 93383.0, 93382.32, 93385.0, 93384.0, 93386.0, 93388.0, 93389.28, 93387.0, 93392.0, 93394.0, 93396.78, 93396.0, 93399.0, 93400.0, 93403.0, 93408.0, 93410.0, 93411.0, 93412.0, 93413.0, 93415.0, 93416.0, 93418.0, 93419.0, 93420.0, 93421.0, 93423.0, 93424.0, 93425.0, 93423.45, 93426.73, 93428.0, 93429.0, 93430.88, 93430.0, 93432.0, 93433.0, 93434.0, 93435.0, 93438.0, 93440.0, 93447.0, 93450.0, 93451.0, 93450.32, 93452.0, 93454.0, 93455.04, 93456.0, 93457.0, 93458.0, 93459.0, 93460.0, 93453.0, 93462.0, 93463.0, 93464.0, 93466.0, 93468.0, 93470.0, 93471.0, 93472.0, 93473.0, 93475.0, 93475.07, 93476.0, 93479.0, 93480.0, 93481.0, 93482.0, 93484.0, 93489.0, 93490.0, 93492.0, 93493.0, 93496.0, 93497.0, 93498.0, 93499.9, 93500.0, 93501.0, 93503.0, 93504.0, 93505.0, 93509.0, 93510.0, 93511.0, 93512.0, 93516.0, 93518.0, 93520.0, 93523.0, 93530.0, 93534.0, 93536.0, 93540.0, 93542.0, 93545.0, 93547.0, 93548.0, 93549.0, 93550.0, 93552.0, 93553.0, 93558.28, 93558.66, 93560.0, 93561.0, 93562.0, 93563.0, 93564.0, 93565.33, 93558.0, 93567.36, 880000.0, 93569.0, 93570.0, 93573.0, 93574.0, 93575.0, 93576.0, 93580.0, 93581.0, 93584.0, 93585.0, 93586.0, 93588.0, 93590.0, 93591.0, 93594.0, 93597.0, 93598.22, 93599.0, 93600.0, 880030.0, 93602.0, 93606.0, 93607.0, 93610.08, 93611.0, 93611.76, 93613.0, 93611.44, 93615.12, 93610.0, 93618.0, 93618.72, 93620.0, 93621.0, 93625.0, 93629.0, 93632.0, 93634.0, 93635.0, 93636.0, 93639.0, 93641.0, 93642.0, 93643.0, 93645.0, 93646.0, 93647.0, 93648.0, 93650.0, 93652.0, 93653.18, 93654.0, 93655.0, 93656.0, 93657.5, 93658.0, 93659.28, 93660.0, 93661.8, 93661.92, 93663.0, 93664.0, 93665.0, 93667.0, 93669.0, 93672.0, 93673.0, 93672.32, 93675.0, 93676.0, 93677.0, 93674.56, 93679.92, 93680.0, 93681.0, 93682.0, 93685.0, 93686.0, 70724.7, 93690.0, 93691.0, 93692.0, 93694.0, 93695.0, 93696.0, 93699.0, 93700.0, 93704.0, 93706.16, 93707.0, 93710.0, 93715.0, 93716.0, 93717.76, 93718.0, 93719.0, 93720.0, 93721.0, 93722.0, 93723.0, 93730.0, 93731.32, 93734.0, 93736.0, 70734.0, 93738.56, 93739.0, 93740.0, 93741.0, 93737.0, 93743.26, 93744.0, 93745.0, 93748.0, 93750.0, 93752.0, 93755.94, 93756.0, 93758.0, 93760.0, 93762.0, 93763.0, 93764.0, 93765.08, 93765.12, 93767.0, 93768.0, 93770.0, 93772.0, 93773.0, 93772.21, 93775.0, 93776.0, 93776.99, 93777.0, 93778.0, 93779.0, 93780.0, 93783.0, 93784.0, 93787.0, 93788.0, 93789.0, 93790.0, 93791.0, 93792.0, 93797.0, 93800.0, 93804.0, 93805.0, 93806.0, 93805.56, 93808.0, 93811.0, 93812.0, 93815.0, 93818.0, 93821.0, 93822.0, 93823.0, 93824.62, 93827.0, 93828.0, 93830.0, 93832.0, 93833.0, 93835.0, 93840.0, 93842.0, 93844.0, 93847.0, 93848.0, 93850.0, 93851.0, 93852.0, 93851.55, 93851.72, 356000.0, 93859.0, 93860.0, 93861.0, 93862.2, 93864.0, 93865.0, 93866.0, 93868.0, 93870.4, 3239600.0, 93872.0, 93874.0, 93875.0, 93877.0, 70762.88, 93885.0, 93888.0, 93891.0, 93892.0, 93893.0, 93894.0, 93892.88, 93900.0, 93903.0, 93904.0, 70767.0, 93906.4, 93911.0, 93912.0, 93914.0, 93915.0, 93916.0, 93918.0, 93921.0, 93922.0, 93924.0, 93926.0, 93928.0, 93930.0, 93932.0, 93933.2, 93935.0, 93936.0, 93938.0, 93940.0, 93941.0, 93942.0, 93944.0, 93948.0, 93949.0, 93951.52, 93952.8, 93953.0, 93954.0, 93955.0, 93956.0, 93957.0, 93959.0, 93960.0, 93962.0, 93963.0, 93965.0, 93968.0, 93969.0, 93970.0, 93972.53, 93977.0, 93979.0, 93980.0, 93981.0, 93983.0, 93984.0, 93986.0, 93988.0, 93989.0, 93990.0, 93991.0, 93992.0, 93995.0, 93996.0, 93997.0, 93997.56, 93999.88, 94000.0, 93996.96, 94004.0, 94004.82, 94007.0, 94010.0, 94011.0, 94013.68, 94016.0, 94018.0, 94020.0, 94022.04, 94023.0, 94025.4, 94031.0, 94032.0, 94035.0, 94036.0, 94037.0, 94035.6, 94039.0, 94038.0, 94044.0, 94048.0, 94049.0, 94050.0, 94052.52, 94052.0, 94055.0, 94056.0, 94057.0, 94060.0, 94067.0, 94068.0, 94069.0, 94070.0, 94074.06, 94075.0, 94078.0, 94080.0, 94081.0, 94086.0, 94090.0, 94092.0, 94095.0, 94096.08, 94096.0, 94097.61, 94099.0, 94100.0, 94102.0, 94103.95, 94104.0, 94107.0, 94109.0, 94110.0, 94111.0, 94112.0, 94114.0, 94115.0, 94116.25, 94116.0, 94116.65, 94118.0, 94120.0, 94121.0, 94122.0, 94123.0, 94125.0, 94126.0, 94128.0, 94130.0, 94133.8, 94134.0, 94135.0, 94137.0, 94137.84, 94137.24, 94140.0, 94141.0, 94144.0, 94145.0, 94158.0, 94160.0, 94161.0, 94166.0, 94168.0, 94169.0, 94171.0, 94173.0, 94175.0, 94176.0, 94178.0, 94179.0, 94180.0, 94181.0, 94182.0, 94184.0, 94185.0, 70824.24, 94191.0, 94198.0, 94199.0, 94200.0, 94203.0, 94203.2, 94204.8, 94206.0, 94208.0, 94209.0, 94211.0, 94212.0, 94215.0, 94218.0, 94220.0, 94221.0, 94222.0, 94223.0, 94224.0, 94225.0, 94229.0, 94230.0, 94232.0, 94233.0, 94233.48, 94235.0, 94236.0, 94238.0, 94240.0, 94243.0, 94244.0, 94245.0, 94248.0, 94248.96, 94250.0, 94253.0, 94254.0, 94256.0, 94257.0, 94260.0, 94261.0, 94262.0, 94263.0, 94265.0, 94265.09, 94268.0, 94271.0, 94272.0, 94273.0, 94276.0, 94279.0, 94281.71, 94284.44, 94287.0, 94290.0, 94293.0, 94296.0, 94297.0, 94298.0, 94299.0, 94300.0, 94303.0, 94304.0, 94308.0, 94318.0, 94320.0, 94321.0, 94323.0, 94325.0, 94326.18, 94327.0, 94328.0, 94326.0, 94330.0, 94333.0, 94333.8, 94335.0, 94334.28, 94337.0, 94338.35, 94340.0, 94341.0, 94342.0, 94344.0, 94345.0, 94348.0, 94350.0, 94351.0, 94354.0, 94356.0, 70857.47, 94360.0, 94363.0, 94365.0, 94368.0, 94372.0, 94374.0, 94375.0, 94376.0, 94377.0, 94380.0, 94382.0, 94384.0, 94385.0, 94387.37, 94388.1, 94389.0, 94392.0, 94393.0, 94397.76, 94398.0, 94400.0, 94401.0, 94404.0, 94405.0, 94406.0, 94407.0, 94408.0, 94409.0, 94410.0, 94411.0, 94409.32, 94414.0, 94415.0, 94420.0, 94423.0, 94426.0, 94428.0, 94430.0, 94432.0, 94433.76, 94438.0, 94440.0, 94441.0, 94442.0, 94444.0, 94450.0, 94452.0, 94454.0, 94456.0, 94457.0, 94458.0, 94459.0, 94460.0, 94461.0, 94462.8, 94463.0, 94464.0, 94468.0, 94469.0, 94470.0, 94472.0, 94475.0, 94476.0, 94477.5, 94480.0, 94481.0, 94482.0, 94484.0, 94485.0, 94487.0, 94488.0, 94490.0, 94493.04, 94499.0, 94500.0, 94501.0, 94502.95, 94502.0, 94507.0, 94509.0, 94512.0, 94513.0, 94514.0, 94515.0, 94516.0, 94518.0, 94520.0, 94521.0, 94523.0, 94526.0, 94527.68, 94528.0, 94527.0, 94530.0, 94529.83, 70891.0, 94533.0, 94534.2, 94536.0, 94536.52, 94541.0, 94542.0, 94544.0, 94545.0, 94547.0, 94548.0, 94550.0, 94551.0, 94552.0, 94554.0, 94555.0, 94556.0, 94558.1, 94559.0, 94560.0, 94563.0, 94564.0, 94567.0, 94568.0, 94570.0, 94571.0, 94572.0, 94574.0, 94575.0, 94577.0, 70901.44, 94579.0, 94580.0, 94582.0, 94583.0, 94584.0, 94585.32, 94585.0, 94586.0, 94588.0, 94590.0, 94592.0, 94593.0, 94596.0, 94597.0, 94598.16, 94599.55, 94600.0, 94599.0, 94604.0, 94605.0, 94606.0, 94608.0, 94611.0, 94613.0, 94615.0, 94618.0, 94620.0, 94621.0, 94622.76, 94623.46, 94624.0, 94625.0, 94626.0, 94622.0, 94628.0, 94630.0, 94631.0, 94632.0, 94635.12, 94635.0, 94637.0, 94636.0, 94639.0, 94640.0, 94644.0, 94645.0, 94645.49, 94647.0, 94648.0, 94649.0, 94650.0, 94652.0, 94653.27, 94655.0, 94656.0, 94657.0, 94659.0, 94660.0, 94661.69, 94661.0, 94665.0, 94665.96, 94666.0, 94670.0, 94672.0, 94674.0, 94677.48, 94677.0, 94678.0, 94680.0, 94681.6, 94681.0, 94683.0, 94684.0, 94685.0, 94686.0, 94687.0, 94688.0, 94691.0, 94692.0, 94695.0, 94695.81, 94697.0, 94698.0, 94700.0, 94701.0, 94702.4, 356848.0, 94705.0, 94704.0, 94707.0, 94710.0, 94712.0, 94713.12, 619000.0, 94717.0, 94720.0, 94722.0, 94723.0, 94724.0, 94728.0, 94728.4, 94732.0, 94736.0, 94736.98, 94738.0, 94739.0, 94740.0, 94744.0, 94748.0, 94750.0, 94752.0, 94753.08, 94755.0, 94756.0, 94760.0, 94765.0, 94768.0, 94770.0, 94771.0, 94773.24, 94776.19, 94776.0, 94778.0, 94780.0, 94784.0, 94785.0, 94786.0, 94787.0, 94788.0, 94789.0, 94791.0, 94792.0, 94793.0, 94794.0, 94796.0, 94796.25, 94798.0, 94799.0, 94800.0, 94801.0, 94800.16, 94803.0, 94807.0, 94813.0, 94815.0, 94819.66, 94820.0, 94820.8, 94823.56, 94826.0, 94827.0, 94830.0, 94832.0, 94833.0, 94834.0, 94836.56, 94837.0, 94838.0, 94840.0, 94844.0, 94846.0, 94848.0, 94850.0, 94852.0, 94855.0, 357000.0, 94857.59, 94856.0, 94860.0, 94864.0, 94866.0, 94868.0, 94870.0, 94871.0, 94872.0, 94875.0, 94876.0, 94880.0, 94882.8, 94884.0, 94885.0, 94888.0, 94889.0, 94890.0, 94895.0, 94896.33, 94897.0, 94898.0, 94896.0, 94900.0, 94902.0, 357047.0, 94904.0, 94910.0, 94911.0, 94913.0, 94917.0, 94919.0, 94920.0, 94922.0, 94924.0, 94925.0, 94929.12, 94930.0, 94934.0, 94937.16, 94937.0, 94939.0, 94940.0, 94944.0, 94946.0, 94950.0, 94952.0, 94953.0, 94956.0, 94957.0, 94960.0, 94965.0, 94966.0, 94968.0, 94969.0, 94970.0, 94976.0, 94977.0, 94978.0, 94979.0, 94980.0, 94987.0, 94988.0, 94988.89, 94989.29, 94991.0, 94992.0, 94996.0, 94998.0, 94999.0, 95000.0, 95000.09, 95001.0, 95000.16, 95004.0, 95006.16, 95008.0, 95009.0, 95010.24, 95012.0, 95013.0, 95014.0, 95015.0, 95016.0, 95014.33, 95019.0, 95021.0, 95025.0, 95028.0, 95030.0, 95031.0, 95032.0, 95033.0, 95035.0, 95036.0, 95040.0, 95045.0, 95047.0, 95048.0, 95050.0, 95051.0, 95052.0, 357202.0, 95059.0, 95060.0, 95062.0, 95063.0, 95064.0, 95065.0, 95066.0, 95067.0, 95070.0, 95074.0, 95077.0, 95078.0, 95080.0, 95083.0, 95087.0, 95088.0, 95092.0, 95094.0, 95095.0, 95096.0, 95097.0, 95099.0, 95100.0, 95103.0, 95105.28, 95107.0, 95111.0, 95113.0, 95116.0, 95118.0, 95119.0, 95120.0, 95121.37, 95122.0, 95123.0, 95122.2, 95124.0, 95120.7, 95121.12, 95130.0, 95131.0, 95132.0, 95134.0, 95136.0, 95138.0, 95139.0, 95140.0, 95141.0, 95142.0, 95145.0, 95148.0, 95149.0, 95150.0, 71015.68, 95152.0, 95154.0, 357301.4, 95157.07, 95158.0, 95160.0, 95161.0, 95166.0, 95168.0, 95170.0, 95171.86, 95172.0, 95173.0, 95174.0, 95175.0, 95176.38, 95174.4, 95178.0, 95176.0, 95180.0, 95184.0, 95187.0, 95189.0, 95190.0, 95191.0, 95192.0, 95192.1, 95195.28, 95197.0, 95198.0, 95199.0, 95200.0, 95202.0, 95204.0, 95210.0, 95211.0, 95213.72, 95216.0, 95217.0, 95219.4, 95220.0, 95221.44, 95219.0, 95223.0, 95224.0, 95226.0, 95228.0, 59794.8, 95234.0, 95235.0, 95237.0, 95239.0, 95240.0, 357384.75, 95242.0, 95243.0, 95244.0, 95246.0, 95248.0, 95250.0, 95255.0, 95256.0, 95259.0, 95260.0, 95261.0, 95263.0, 95264.0, 95265.0, 357410.0, 95268.0, 95269.0, 95270.0, 95271.0, 95275.0, 95280.0, 95284.8, 95284.0, 95287.0, 95288.0, 95289.0, 95290.0, 95292.0, 95296.0, 95298.0, 95300.0, 95301.0, 95302.78, 95304.0, 95310.0, 95313.3, 95315.0, 95316.0, 95320.0, 95321.0, 71049.0, 95324.0, 95325.0, 95326.0, 95327.0, 95328.0, 95329.92, 95327.44, 95324.07, 95330.0, 95334.0, 95335.5, 95335.0, 95337.5, 95337.45, 95339.0, 95340.0, 95341.0, 95342.0, 95343.0, 95340.44, 95350.0, 95351.0, 95352.0, 95353.0, 95357.0, 95359.0, 95360.0, 95362.0, 95364.0, 95365.0, 95366.0, 95367.0, 95368.0, 95369.9, 95370.0, 95373.0, 95374.0, 95377.0, 95379.0, 95380.0, 95381.0, 95382.0, 95383.0, 95385.0, 95387.0, 95388.0, 95389.0, 95390.0, 95392.0, 95393.0, 95396.0, 95398.0, 95400.0, 95404.0, 95405.0, 95406.0, 95407.0, 95409.0, 95410.0, 95412.0, 95415.0, 95416.0, 95420.0, 95424.0, 95428.0, 95430.0, 95431.0, 95436.0, 95437.0, 95438.0, 95440.0, 95444.0, 95446.0, 95449.0, 95450.0, 95452.0, 95453.0, 95455.0, 95456.0, 95458.0, 95459.0, 95460.0, 95459.67, 95462.0, 95463.0, 95467.0, 95469.0, 95470.0, 95471.0, 95472.0, 95474.0, 95477.0, 95478.75, 95479.0, 95480.0, 95481.0, 95481.31, 95483.0, 95479.4, 95488.0, 95492.0, 95494.44, 95495.0, 95496.0, 95498.0, 95500.0, 95502.0, 95503.0, 95504.0, 95509.0, 95511.0, 95512.0, 95513.0, 95514.0, 95516.0, 95517.0, 95518.0, 95519.0, 95520.0, 95521.0, 95524.0, 95525.0, 95526.87, 95528.0, 95529.0, 95530.0, 95532.0, 95533.0, 95534.0, 95535.2, 95535.0, 95542.0, 95543.0, 95544.0, 95546.0, 95547.0, 95546.12, 95550.0, 95551.0, 95555.2, 95557.0, 95561.0, 95563.0, 95565.0, 95567.0, 95568.0, 95570.0, 95571.0, 95573.0, 95576.0, 95578.0, 95579.0, 95580.0, 95581.0, 95583.0, 95584.0, 95585.0, 95588.0, 95590.0, 95591.0, 95592.0, 95596.0, 95598.0, 95600.0, 95602.0, 95604.0, 95610.0, 95611.0, 95610.68, 95613.0, 95612.0, 95615.0, 95616.0, 95617.0, 95614.0, 95620.0, 95622.0, 95623.0, 95624.0, 95625.0, 95622.73, 95628.0, 95630.0, 95632.0, 95633.0, 95634.0, 95635.0, 95637.0, 95638.0, 95639.0, 95640.0, 95641.0, 95644.0, 95645.0, 95650.0, 95651.0, 95652.0, 95653.0, 95654.0, 95655.0, 95657.0, 95658.0, 95660.0, 95661.0, 95662.0, 95663.0, 95664.0, 95666.0, 95667.0, 95668.0, 95669.0, 95670.0, 95673.0, 95675.11, 95678.0, 95678.15, 95680.0, 95684.0, 95687.0, 95688.0, 95690.0, 95692.0, 95695.0, 95700.0, 95700.15, 95703.0, 95704.0, 95706.0, 95708.0, 620000.0, 95713.0, 95714.32, 95716.92, 95717.0, 95719.0, 95720.0, 95724.0, 95728.0, 95729.0, 95730.0, 95731.0, 95732.0, 95733.0, 71131.0, 95736.0, 95737.0, 95739.36, 95740.0, 95742.0, 95743.0, 95745.0, 95747.0, 95748.0, 71134.59, 95750.0, 71134.0, 357900.0, 95757.0, 95758.0, 95759.0, 95760.0, 95760.11, 95762.0, 95764.0, 95768.0, 95770.0, 95772.0, 95773.0, 95775.0, 95777.0, 95778.0, 95779.0, 95780.0, 95783.0, 95784.0, 95785.0, 95786.88, 95790.0, 95792.0, 95793.88, 95796.0, 95798.0, 95800.0, 95802.0, 95804.0, 95805.0, 95806.0, 95809.0, 95812.0, 95815.0, 95816.0, 95819.99, 95820.0, 95822.0, 95823.07, 95826.0, 95826.13, 95828.51, 95827.0, 95829.0, 95832.0, 95833.0, 95835.0, 95838.0, 95840.0, 95841.0, 357985.0, 95843.0, 95846.0, 95847.48, 95850.0, 95852.0, 95854.0, 358000.0, 95857.0, 95856.0, 95859.0, 95860.0, 95861.0, 95862.0, 95863.0, 95858.0, 95868.0, 95870.0, 95871.0, 95872.0, 95873.0, 95874.0, 95875.0, 95876.0, 95880.0, 95882.0, 95883.0, 95885.0, 95886.0, 95888.0, 95892.0, 95894.0, 95896.0, 95900.0, 95903.0, 95904.0, 95905.0, 95908.0, 95912.0, 95913.0, 95917.0, 95919.0, 95920.0, 95922.0, 95924.34, 95924.0, 95928.0, 95929.0, 95930.0, 95931.0, 95934.48, 95940.0, 95944.0, 95945.0, 95950.0, 95952.74, 95956.0, 95958.0, 95960.0, 95960.04, 95962.0, 95961.8, 95966.0, 95967.0, 95968.0, 95969.0, 95970.0, 95975.0, 95976.0, 95977.0, 95977.15, 95978.12, 95980.0, 95982.0, 95982.38, 95983.0, 95985.0, 95986.6, 95987.0, 95988.0, 95990.0, 95992.0, 95997.0, 95998.0, 95999.0, 96000.0, 96002.0, 96004.0, 96005.0, 96009.0, 96010.0, 96011.0, 96012.6, 96012.0, 96016.0, 96017.0, 96020.0, 96023.0, 96024.0, 96025.0, 96031.0, 96035.0, 96036.0, 96040.0, 96041.0, 96044.0, 96045.0, 96046.6, 96047.0, 96048.0, 96050.0, 96053.0, 96054.0, 96057.0, 96059.0, 96060.0, 96063.0, 96066.0, 96069.0, 96070.0, 96070.82, 96072.0, 96080.0, 96082.0, 96085.31, 96088.0, 96090.0, 96092.0, 96094.0, 96096.0, 96098.0, 96100.0, 96102.24, 96105.0, 96106.0, 96106.04, 96110.0, 96111.0, 96111.53, 96113.0, 96112.0, 96110.56, 96116.0, 96117.0, 96115.0, 96120.0, 96121.0, 96121.73, 96123.0, 96120.12, 96125.0, 96124.0, 96128.0, 96129.0, 96132.0, 96133.0, 96134.76, 96136.0, 96138.0, 96140.0, 96141.81, 96142.0, 96143.65, 96144.0, 96147.6, 96150.0, 96152.0, 96156.0, 96158.0, 96159.0, 96160.0, 96163.0, 96165.96, 96166.78, 96168.0, 96170.0, 96171.0, 96174.0, 96178.0, 96179.0, 96180.0, 96181.28, 96181.0, 96184.0, 96185.76, 96186.0, 96185.0, 96188.0, 96191.0, 96192.0, 96200.0, 96202.0, 96204.0, 96208.0, 96210.0, 96212.0, 96212.27, 96214.0, 96215.0, 96216.0, 96218.0, 96220.0, 96223.0, 96224.0, 96225.0, 96226.0, 96227.0, 96230.0, 96234.0, 96235.0, 96236.0, 96238.31, 96240.0, 96241.0, 96242.69, 96243.0, 96245.0, 96248.0, 96249.0, 96250.0, 96252.0, 96253.0, 96259.0, 96260.0, 96262.0, 96264.0, 96266.0, 96267.0, 96270.0, 96271.0, 96273.0, 96274.15, 96275.0, 96280.0, 96281.0, 96284.0, 96285.0, 96284.16, 96288.5, 96288.0, 96290.0, 96295.0, 96297.0, 96299.0, 96300.0, 96301.2, 96303.0, 96304.0, 96305.0, 96307.14, 96308.0, 96309.0, 96310.0, 96311.0, 96312.0, 96313.16, 96314.0, 96316.0, 96317.18, 96320.0, 96325.0, 96327.96, 96328.0, 96327.0, 96330.0, 96335.0, 96336.0, 96337.54, 96338.0, 96339.0, 96340.0, 96341.0, 96342.0, 96343.0, 96344.0, 96345.0, 96337.0, 96350.0, 96352.0, 96353.0, 96354.0, 96355.33, 96356.0, 96355.0, 96359.09, 96360.0, 96363.02, 96365.0, 96366.0, 96368.0, 96369.0, 96371.0, 96372.0, 96377.0, 96380.0, 96382.0, 96383.0, 96384.0, 96387.0, 96388.0, 96389.0, 96390.0, 96391.0, 96393.0, 96396.0, 96398.0, 96400.0, 96403.0, 96404.0, 96405.0, 96408.0, 96410.0, 96411.0, 96415.0, 96416.0, 96420.0, 96421.0, 96421.36, 96423.0, 96425.0, 96428.0, 96429.0, 96430.0, 96432.0, 96433.0, 96434.0, 96437.0, 96438.0, 96440.0, 96444.0, 96446.0, 96447.0, 96449.0, 96450.0, 96452.0, 96453.0, 96454.0, 96455.0, 96456.0, 96458.0, 96459.0, 96460.0, 96458.87, 96462.37, 96463.0, 96465.0, 96467.0, 96467.28, 96470.0, 96471.0, 96472.0, 96474.0, 96475.0, 96477.0, 96478.0, 96479.0, 96480.0, 96483.0, 96484.0, 96485.0, 96486.0, 96488.0, 96489.0, 96491.2, 96492.0, 96492.77, 96495.0, 96499.53, 96500.0, 96501.0, 96502.0, 96503.0, 96504.0, 96510.0, 96512.0, 96512.68, 96516.87, 96520.92, 96521.0, 96524.0, 96525.0, 96528.0, 96529.0, 96530.0, 96532.8, 96532.0, 96534.08, 96535.0, 358680.0, 96537.0, 96539.0, 96540.0, 96544.34, 96544.0, 96546.0, 96545.0, 96548.0, 96550.0, 96552.0, 96557.0, 96558.0, 96559.0, 96560.0, 96562.0, 96563.0, 96567.0, 96568.0, 96569.0, 96571.0, 96572.0, 96573.0, 96575.0, 96576.0, 96577.0, 96578.0, 96579.0, 96584.0, 96585.0, 96586.0, 96587.0, 96588.0, 96590.0, 96595.0, 96596.0, 96595.44, 96598.0, 96599.0, 96600.0, 96601.0, 96604.0, 96609.48, 96609.0, 96613.0, 96615.0, 96616.0, 96617.0, 96620.0, 96623.0, 96624.0, 96625.0, 96623.87, 96626.0, 96628.0, 96630.0, 96631.0, 96632.0, 96634.0, 96635.0, 96636.8, 96636.0, 96638.92, 96639.0, 96640.0, 96641.0, 96637.0, 96643.0, 96644.0, 96648.0, 96649.92, 96650.32, 96649.0, 96650.0, 96652.0, 96654.0, 96656.0, 96656.4, 96660.0, 96661.0, 96663.0, 96664.0, 96663.64, 96666.0, 96667.0, 96669.0, 96669.76, 96670.0, 96672.0, 96673.0, 96674.0, 96675.0, 96676.0, 96677.0, 96678.0, 96680.96, 96681.0, 96682.0, 96680.0, 96685.0, 96687.0, 96690.0, 96693.0, 96696.0, 96700.0, 96701.0, 96702.0, 96706.0, 96707.0, 96712.0, 621000.0, 96714.0, 96713.32, 96717.0, 96718.0, 96719.0, 96720.0, 96721.0, 96722.0, 96725.0, 96732.0, 96734.0, 96735.0, 96738.43, 96740.0, 96742.0, 96744.0, 96749.0, 96750.0, 96751.0, 96752.0, 96753.0, 96754.0, 96756.0, 96757.0, 96758.0, 96760.0, 96761.0, 96762.0, 96765.0, 96766.0, 96770.0, 96771.0, 96772.0, 96774.0, 96775.0, 96774.78, 96777.0, 96780.0, 96781.5, 96782.0, 96783.0, 96784.0, 96786.0, 96786.83, 96789.0, 96790.0, 96792.0, 96795.0, 96796.0, 96798.0, 96800.0, 96801.0, 96800.04, 96804.0, 96806.4, 96808.0, 96811.0, 96812.43, 96814.0, 96816.0, 96817.27, 96818.36, 96818.0, 96820.4, 96820.0, 96821.31, 96823.0, 96824.0, 96825.24, 96826.0, 96825.0, 96828.0, 96821.0, 96829.0, 96831.0, 96835.0, 96836.0, 96837.0, 96839.34, 96840.0, 96844.0, 96846.0, 96847.0, 96850.0, 96851.0, 96854.22, 96855.0, 359000.0, 96857.0, 96858.0, 96860.0, 96861.0, 96862.0, 96865.0, 96866.0, 96867.0, 96870.0, 96871.2, 96872.0, 96874.0, 96875.0, 96876.0, 96877.0, 96878.0, 96880.0, 96882.54, 96883.0, 96888.0, 96890.0, 96891.0, 96893.0, 96895.0, 96898.0, 96899.0, 96900.0, 96902.0, 96904.0, 96906.0, 96907.0, 96910.0, 96912.0, 96913.0, 96914.0, 96920.0, 96921.0, 96922.0, 96924.0, 96926.0, 96928.0, 96933.62, 96933.0, 96935.0, 96937.0, 96939.0, 96940.0, 96942.0, 96944.43, 96944.0, 96948.0, 96950.0, 96954.0, 96956.0, 96957.0, 96960.0, 96963.0, 96964.0, 96966.0, 96967.0, 96969.0, 96970.0, 96971.0, 96972.0, 96976.0, 96980.0, 96984.0, 96987.0, 96989.0, 96990.0, 96995.0, 96996.0, 96999.0, 97000.0, 97001.0, 97003.0, 97004.0, 97005.0, 97006.0, 97008.0, 97010.0, 97010.1, 97012.0, 97011.0, 97015.0, 97017.0, 97019.0, 97020.0, 97019.59, 97023.0, 97025.0, 97027.0, 97028.0, 97032.0, 97033.0, 97035.0, 97037.0, 97038.0, 97039.05, 97040.0, 97043.0, 97044.0, 97045.82, 97045.0, 97047.0, 97048.0, 97050.0, 97056.0, 97060.0, 97064.0, 97067.0, 97073.0, 97075.0, 97080.0, 97083.0, 97086.24, 97087.44, 97088.0, 97089.0, 97090.0, 97092.0, 97093.0, 97094.0, 97099.2, 97100.0, 97101.0, 97102.0, 97104.0, 97105.0, 97107.0, 97108.0, 97110.0, 97111.0, 97112.0, 97113.0, 97114.0, 97115.0, 97116.0, 97118.0, 97120.0, 97123.0, 97124.0, 97125.0, 97126.0, 97127.0, 97128.0, 97129.0, 71408.48, 97132.0, 97133.0, 97135.0, 97136.0, 97137.0, 97138.0, 97140.0, 97141.0, 97143.0, 97144.0, 97145.33, 97145.0, 97149.0, 97152.0, 97156.0, 97158.0, 71414.26, 97160.0, 97162.0, 97164.0, 97165.0, 97167.0, 97172.0, 97175.64, 97176.0, 97175.0, 97179.0, 97180.0, 97181.0, 97186.0, 97187.0, 97188.0, 71420.95, 97190.0, 97192.0, 97193.0, 97198.0, 97199.0, 97200.0, 97201.5, 97202.0, 97199.02, 71423.04, 97208.0, 97210.0, 97212.0, 97213.0, 97214.0, 97215.0, 97216.0, 97218.0, 97220.0, 97224.0, 97227.0, 97228.0, 97230.0, 97231.92, 97234.0, 97235.0, 97236.0, 71430.0, 97240.0, 97243.52, 97248.0, 97250.0, 97251.0, 97252.0, 97253.0, 97255.0, 97257.16, 97258.0, 97260.0, 97261.0, 97262.0, 97265.0, 97266.0, 97272.0, 97278.79, 97280.0, 97282.0, 97285.0, 97287.0, 97288.0, 97291.0, 97292.0, 97293.0, 97294.0, 97296.0, 97298.0, 97300.0, 97301.0, 97304.0, 97305.0, 97307.0, 97308.0, 97309.0, 97310.0, 97314.0, 97320.0, 97321.0, 97321.26, 97324.0, 97326.0, 97327.0, 97330.0, 97331.0, 97332.0, 97333.0, 97334.0, 97335.0, 97338.0, 97340.0, 97341.0, 97344.0, 97345.2, 97346.0, 97347.0, 97345.0, 97349.0, 97350.0, 97350.9, 97352.0, 97352.92, 97354.0, 97352.64, 97356.0, 97358.0, 97360.0, 97362.0, 97365.0, 97367.0, 97368.0, 97369.0, 97370.0, 97371.6, 97375.0, 97376.0, 97377.0, 97378.0, 97379.0, 97380.0, 97381.0, 97385.0, 359530.0, 97386.0, 97392.0, 97393.0, 97395.0, 97397.56, 359542.0, 97397.0, 97400.0, 97404.0, 97407.76, 97408.0, 359556.0, 97413.0, 97414.0, 97415.0, 97416.0, 97417.0, 359564.0, 97421.0, 97422.0, 97420.0, 97424.0, 97425.0, 97427.0, 97427.16, 97429.0, 97430.0, 97431.0, 97432.0, 97434.0, 97435.44, 97440.0, 97442.0, 97444.47, 97447.0, 97448.0, 97449.0, 97450.0, 97450.39, 97452.0, 97454.0, 97455.24, 97458.0, 97461.0, 97462.8, 97462.2, 97464.0, 97463.0, 97467.0, 97470.0, 97472.0, 97473.0, 97474.0, 97475.0, 97476.0, 97477.0, 97479.0, 97480.0, 97480.8, 97482.15, 97483.0, 97484.0, 97485.0, 97487.0, 97488.0, 97494.0, 97498.0, 97499.64, 97500.0, 97501.0, 97503.0, 97504.0, 97505.0, 97508.0, 97509.0, 97510.0, 97511.0, 97513.0, 97515.0, 97519.0, 97520.0, 97522.0, 97523.0, 97524.0, 97524.6, 97525.0, 97523.16, 97530.0, 97531.0, 97532.0, 97533.0, 97534.36, 97536.0, 97539.0, 97540.0, 97545.0, 97546.0, 97548.0, 97550.0, 97551.0, 97552.0, 97553.0, 97555.0, 97560.0, 97561.0, 97563.0, 97564.0, 97565.0, 97567.34, 97569.0, 97571.0, 97572.0, 97572.78, 97575.0, 97576.0, 97579.0, 97580.0, 97582.0, 97583.0, 97584.0, 97588.0, 97590.0, 97593.0, 97594.0, 97595.0, 97598.0, 97600.0, 97602.0, 97602.8, 97605.0, 97606.0, 97607.0, 97608.0, 97612.0, 97614.0, 97616.0, 97618.0, 97620.0, 97624.0, 97628.0, 97629.0, 97632.0, 97633.0, 97634.0, 97635.0, 97638.7, 97639.0, 97640.0, 97641.0, 97638.0, 97643.0, 97644.0, 97645.0, 97647.0, 97648.0, 97649.0, 97650.0, 97653.0, 97654.0, 97655.0, 97656.0, 97657.0, 97660.0, 97662.0, 97663.0, 97664.0, 97665.0, 97666.07, 97667.0, 97668.0, 97671.0, 97674.0, 97676.8, 97676.0, 97679.0, 97680.0, 97685.0, 97688.0, 97689.0, 97690.0, 97692.0, 97693.0, 97695.0, 97696.0, 97700.0, 97704.0, 97706.0, 97707.0, 97708.32, 97708.0, 622000.0, 97714.0, 97715.0, 97717.0, 97718.0, 97718.4, 97720.0, 97722.0, 97726.0, 97728.13, 97729.0, 97728.0, 97732.0, 97735.0, 359880.0, 97738.44, 97739.0, 97740.0, 97741.0, 97745.52, 97750.0, 97754.0, 97759.0, 97760.0, 97761.0, 97763.0, 97764.0, 97765.0, 97766.0, 97767.0, 97769.52, 97770.0, 97772.0, 97773.0, 97775.0, 97776.0, 97777.0, 97778.0, 97780.0, 97781.0, 97782.36, 97784.0, 97787.0, 97788.0, 97789.0, 97790.0, 97795.8, 97795.0, 97797.6, 97798.0, 97800.0, 97803.0, 97804.0, 97806.0, 359950.0, 97811.0, 97812.0, 97814.0, 97816.0, 97820.0, 97821.0, 97825.0, 97826.0, 97827.0, 97828.0, 97830.0, 97834.8, 97835.0, 97836.0, 97834.0, 97843.0, 97843.2, 97845.0, 97848.0, 97850.0, 97853.0, 97854.0, 360000.0, 97857.0, 97858.0, 97860.0, 97861.5, 97862.0, 97861.7, 97864.0, 97866.0, 97867.0, 97867.45, 97869.75, 97869.0, 97871.0, 97872.0, 97873.0, 97874.0, 97870.0, 97876.0, 97868.0, 97878.0, 97879.0, 97880.0, 97882.0, 97884.0, 97885.0, 97887.0, 97888.0, 97889.0, 97890.0, 97891.0, 97892.0, 97895.0, 97896.0, 97897.0, 97898.0, 97900.0, 97901.0, 97902.0, 97903.0, 97905.0, 97908.0, 97920.0, 97921.64, 97922.0, 97923.0, 97921.0, 97930.0, 97934.0, 97936.0, 97937.0, 97938.0, 97942.0, 97950.0, 97951.0, 97955.0, 97956.0, 97959.0, 97968.0, 97969.0, 97970.0, 97971.0, 97972.0, 97975.42, 97976.0, 97977.0, 97975.0, 97980.0, 97983.0, 97984.0, 97988.0, 97989.0, 97990.0, 97992.0, 97993.0, 97994.4, 97995.8, 97995.0, 97996.0, 97998.0, 97999.0, 98000.0, 98001.0, 98003.0, 98004.0, 98005.0, 98007.8, 98007.0, 98009.0, 98010.0, 98013.0, 98014.0, 98016.0, 98017.0, 98018.0, 98020.0, 98021.0, 98022.0, 98024.0, 98025.0, 98027.0, 98030.16, 98039.0, 98040.0, 98044.0, 98048.0, 98050.0, 98051.0, 98052.0, 98055.21, 98056.0, 98057.0, 98055.0, 98060.0, 98067.0, 98068.0, 98069.0, 98074.0, 71596.18, 98076.0, 98077.08, 98079.2, 98080.0, 98084.0, 98086.0, 98087.0, 98087.86, 98088.0, 98091.0, 98092.0, 98096.0, 98099.0, 98100.0, 98102.0, 98106.0, 98110.84, 98113.6, 98113.0, 98115.0, 98116.0, 98118.0, 98120.0, 98123.0, 98124.0, 98125.0, 98126.0, 98127.0, 98130.0, 98131.0, 98132.0, 98133.0, 98134.0, 98136.0, 98140.0, 98143.0, 98147.97, 98148.0, 98150.0, 98153.0, 98154.0, 98155.0, 622445.0, 98159.0, 98160.0, 98161.0, 98162.0, 98163.0, 98164.0, 98167.0, 98169.0, 98171.0, 98172.0, 98174.0, 98176.0, 98182.0, 98184.0, 98185.0, 98187.0, 98189.0, 98191.55, 98192.0, 98194.04, 98196.0, 98199.0, 98200.0, 98201.0, 98202.0, 98206.0, 98208.0, 98209.0, 98210.0, 98211.0, 98215.52, 98215.0, 98217.0, 98219.0, 98220.0, 98221.0, 98220.72, 98225.0, 98226.0, 98227.0, 98228.0, 98230.0, 98232.0, 98233.0, 98235.0, 98240.0, 98241.0, 98243.0, 98246.28, 98247.0, 98246.0, 98248.0, 98250.0, 98249.0, 98252.0, 98253.0, 98254.0, 98255.0, 98256.0, 98257.0, 98259.0, 98260.0, 98261.0, 98262.0, 98264.0, 98265.42, 98265.0, 98267.0, 98268.0, 98269.0, 98266.0, 98265.23, 98272.0, 98273.0, 98274.0, 98276.0, 98279.0, 98280.0, 98281.0, 98282.0, 98281.95, 98285.0, 98288.0, 98289.0, 98290.0, 98292.0, 98293.44, 98295.0, 360441.0, 98299.0, 98300.0, 98303.0, 98304.0, 98305.0, 98308.8, 98309.76, 98312.0, 98313.5, 98314.91, 98315.0, 98319.0, 98320.0, 98321.0, 98323.0, 98325.0, 98328.0, 98329.0, 98331.0, 98332.0, 98333.0, 98334.0, 98338.0, 98341.0, 98343.0, 98345.0, 98346.0, 98347.0, 98348.0, 98349.93, 98350.0, 98352.0, 98353.0, 98355.0, 98357.0, 98360.0, 98362.68, 98364.0, 98365.0, 98366.0, 98367.36, 98369.0, 98370.0, 98373.0, 98374.0, 98376.0, 98378.0, 98383.0, 98385.0, 98386.0, 98387.0, 98390.0, 98393.0, 98394.0, 98395.0, 98396.74, 98395.05, 98398.0, 98396.0, 98400.0, 98401.0, 98404.0, 98404.8, 98406.0, 98407.84, 98409.12, 98410.0, 98411.0, 98409.0, 98412.0, 98416.0, 98418.0, 98419.0, 98420.0, 98423.0, 98424.0, 98426.0, 98428.0, 98430.0, 98431.0, 98434.0, 98438.86, 98440.0, 98444.17, 98445.0, 98446.0, 98447.0, 98448.0, 98450.0, 98451.83, 98451.54, 98451.0, 98454.0, 98455.0, 98452.0, 98457.0, 98458.0, 98453.0, 98460.0, 98461.0, 98462.0, 98463.0, 98464.0, 98465.0, 98467.0, 98470.0, 98471.0, 98472.0, 98473.0, 98474.0, 98476.0, 98477.0, 98478.0, 98480.0, 98484.0, 98485.0, 98488.0, 98493.0, 98495.0, 98496.0, 98497.0, 98498.0, 98500.0, 98503.0, 98505.0, 98508.0, 98510.58, 98511.0, 98515.0, 98518.0, 98519.0, 98520.0, 98521.0, 98522.0, 98524.0, 98525.0, 98527.0, 98528.0, 98529.0, 98530.0, 98531.0, 98533.0, 98535.0, 98537.0, 98538.0, 98539.65, 98540.0, 98541.0, 98539.0, 98543.0, 98544.0, 98545.0, 98546.0, 98541.24, 98548.76, 98549.0, 98550.0, 98548.0, 98552.0, 98553.0, 98554.0, 98555.0, 98556.0, 98559.39, 98560.0, 98562.0, 98564.0, 98565.0, 98566.0, 98567.0, 885000.0, 98568.0, 98571.0, 98574.1, 98575.0, 98576.0, 98578.0, 98580.0, 98582.68, 98583.0, 98585.0, 98586.0, 98587.0, 98588.0, 98590.0, 98593.24, 98594.0, 98596.0, 98599.02, 98600.0, 98601.0, 98599.0, 98603.0, 98603.6, 98602.3, 98607.0, 98608.0, 98616.0, 98618.0, 98620.0, 98621.47, 98624.0, 98625.0, 98626.0, 98628.0, 98630.0, 98633.0, 98633.6, 98635.0, 98639.0, 98640.0, 98642.0, 98643.0, 98644.0, 98645.0, 98646.0, 98649.0, 98650.0, 98651.0, 98652.0, 98653.58, 98654.84, 98655.0, 98656.0, 98657.2, 98655.7, 98657.0, 98660.0, 98661.0, 98658.0, 98663.0, 98664.0, 98665.0, 98666.89, 98662.0, 98668.0, 98668.16, 98670.0, 98672.0, 98673.0, 98675.0, 98676.0, 98677.0, 98678.0, 98679.0, 98680.0, 98682.0, 98688.0, 98689.0, 98690.0, 98692.0, 98694.0, 98695.0, 98696.0, 98700.0, 98702.0, 98703.0, 98705.0, 98707.0, 98709.0, 98710.0, 98712.0, 623000.0, 98720.0, 98721.0, 98722.0, 98725.0, 98728.0, 98729.0, 98730.0, 98731.0, 98733.0, 98734.0, 98735.0, 98736.0, 98740.0, 98742.0, 98744.0, 98748.0, 98749.0, 98750.0, 98752.0, 98754.0, 98756.0, 98758.0, 98759.0, 98760.0, 98758.4, 98762.0, 98763.0, 98764.0, 98765.0, 98766.0, 98768.0, 98769.0, 98771.0, 98775.11, 98775.0, 98777.0, 98780.0, 98784.0, 98785.0, 98786.0, 98787.0, 98788.0, 98789.0, 98790.0, 98791.0, 98792.0, 98793.0, 98794.08, 98795.0, 98796.0, 98797.0, 98798.0, 98800.0, 98802.0, 98803.0, 98805.6, 98808.0, 98810.96, 98810.0, 98814.0, 98815.0, 98819.0, 98820.0, 98821.0, 98823.56, 98824.0, 98823.0, 98826.0, 98828.0, 98829.0, 98831.0, 98832.0, 98838.0, 98839.0, 98840.0, 98841.0, 98843.0, 71749.8, 98847.0, 98850.0, 98852.0, 98853.0, 361000.0, 98856.0, 98859.0, 98860.0, 98861.0, 98864.02, 98865.0, 98871.58, 98874.0, 98875.0, 98876.0, 98877.79, 98879.0, 98880.0, 98883.0, 98885.0, 98886.0, 98888.0, 98889.0, 98890.0, 98892.0, 98893.0, 98895.0, 98898.0, 98899.39, 98900.0, 98901.0, 98906.0, 98908.0, 98912.0, 98916.0, 98920.0, 98921.0, 98922.0, 98925.0, 98926.0, 98928.0, 98929.0, 98929.44, 98933.0, 98935.0, 98938.0, 98938.2, 98940.0, 98942.0, 98943.0, 98944.0, 98945.0, 98948.0, 98950.0, 98951.0, 98952.0, 98954.0, 98955.0, 98960.0, 98963.0, 98965.0, 98966.0, 98967.0, 98968.0, 98972.0, 98974.0, 98975.0, 98976.0, 98978.0, 98980.0, 71776.84, 98982.0, 98981.0, 98984.0, 71777.0, 98986.0, 98987.0, 71777.6, 98990.0, 98991.0, 71778.48, 98994.0, 98995.0, 98996.0, 98997.0, 98998.0, 98999.0, 99000.0, 99001.0, 98999.89, 99006.0, 99008.0, 99009.0, 99010.0, 99015.0, 99017.0, 99019.0, 99024.0, 99025.0, 99031.0, 99036.0, 99037.0, 99044.0, 99048.0, 99049.6, 99050.0, 99049.56, 99049.0, 99056.0, 99057.0, 99059.0, 99060.0, 99061.88, 71792.0, 99062.0, 99065.0, 99068.0, 99068.03, 99070.0, 99072.0, 99075.0, 99076.0, 99084.0, 99085.0, 99086.0, 99091.0, 99092.0, 99095.0, 99096.0, 99097.0, 99098.0, 99099.0, 99100.0, 99101.0, 99102.0, 99103.0, 99104.0, 99105.0, 99107.0, 99108.0, 99109.0, 99110.0, 99111.0, 99114.0, 99118.0, 99120.0, 99122.0, 99123.0, 99124.79, 99125.0, 99127.0, 99128.0, 99129.0, 99131.0, 99132.0, 99135.0, 99138.0, 99139.92, 99140.0, 99142.0, 99144.0, 99145.0, 99146.0, 99148.0, 99149.0, 99150.38, 99151.0, 99152.0, 99153.0, 99155.0, 99156.0, 99158.81, 99158.0, 99162.0, 99163.56, 99164.0, 99165.0, 99167.0, 99168.0, 99170.0, 99173.0, 99174.0, 99175.0, 99176.0, 99180.0, 99187.0, 99190.0, 99192.0, 99194.0, 99195.0, 99197.0, 99198.0, 99199.0, 99200.0, 99201.0, 99203.0, 99205.0, 99207.0, 99208.0, 99213.0, 99215.0, 99216.0, 99219.0, 99220.0, 99222.43, 99223.0, 99224.88, 99225.6, 99226.0, 99225.0, 99228.0, 99227.0, 99224.0, 99231.0, 99233.0, 99234.0, 99235.0, 99234.11, 99238.0, 99240.0, 99241.0, 99244.0, 99247.0, 99248.91, 99248.0, 99250.0, 99249.0, 99254.0, 99255.0, 99256.0, 99258.0, 99260.0, 99261.95, 99263.0, 99264.0, 99265.0, 99267.0, 99268.0, 99270.0, 99273.0, 99274.0, 99275.0, 99276.0, 99277.0, 99279.0, 99281.0, 99282.0, 99283.0, 99283.69, 99285.0, 99288.0, 99289.4, 99289.0, 99295.0, 99296.0, 99296.7, 99297.0, 99300.0, 99303.0, 99305.0, 99310.0, 99316.0, 99317.0, 99319.0, 99320.0, 99321.04, 99324.0, 99326.0, 99330.0, 99331.0, 99333.0, 99336.74, 99337.0, 99336.0, 99337.67, 99340.0, 99341.0, 99345.0, 99346.0, 99347.88, 99348.0, 99349.0, 99350.0, 99356.0, 99356.78, 99359.56, 99360.0, 99362.0, 99365.0, 99369.0, 99370.0, 99371.0, 99372.0, 99373.0, 99375.0, 99376.0, 99383.0, 99384.0, 99385.0, 99386.0, 99387.17, 99388.0, 99389.0, 99387.0, 99391.0, 99392.0, 99395.0, 99396.0, 99400.0, 99402.79, 99403.0, 99405.0, 99406.0, 99407.76, 99408.0, 99409.0, 99410.0, 99411.0, 99413.0, 99416.0, 99417.0, 99416.64, 99420.0, 99424.0, 99425.0, 99430.0, 99432.0, 99435.0, 99436.0, 99437.4, 99438.0, 99435.83, 99440.0, 99441.0, 99442.0, 99444.0, 99450.0, 99452.0, 99453.16, 99453.0, 99455.0, 99456.0, 99457.0, 99458.59, 99457.7, 99460.0, 99465.6, 99465.0, 99468.0, 99470.0, 99472.0, 99472.25, 99474.0, 99473.0, 99480.0, 99486.4, 99486.0, 99488.0, 99489.0, 99490.0, 99487.96, 99492.0, 99495.0, 99496.0, 99498.0, 99499.0, 99500.0, 99502.2, 99504.0, 99505.0, 99507.0, 71881.0, 99512.0, 99516.8, 99518.42, 99519.36, 99519.0, 99520.0, 99524.0, 99526.0, 99528.0, 99530.0, 99531.0, 99533.34, 99536.0, 99540.0, 99541.0, 99540.6, 99544.0, 99545.0, 99548.8, 99548.0, 99550.0, 71889.48, 99552.0, 99558.0, 99560.0, 99561.0, 99563.0, 99564.0, 99567.0, 71893.0, 99569.0, 99570.0, 99570.48, 99572.0, 99574.0, 99576.0, 99578.0, 99580.0, 99581.0, 99584.0, 99586.0, 99587.0, 99588.0, 99595.0, 99596.0, 99598.56, 99599.0, 99600.0, 99601.04, 99602.0, 99607.0, 99608.0, 99609.0, 99610.0, 99611.0, 99612.0, 99614.32, 99615.0, 99617.0, 99620.0, 99624.0, 99625.0, 99628.0, 99630.0, 99631.0, 99632.0, 99633.0, 99634.0, 99635.0, 99636.0, 99638.0, 99640.0, 99641.0, 99645.0, 99646.0, 99648.0, 99650.0, 99651.0, 99656.0, 99660.0, 99670.0, 99671.0, 99672.0, 99673.0, 99676.0, 99677.0, 99678.0, 99680.0, 99683.0, 99685.0, 99686.0, 99687.0, 99688.0, 99690.67, 99691.0, 99694.0, 99696.0, 99698.0, 99700.0, 99705.0, 99707.0, 99709.0, 99710.0, 99711.0, 624000.0, 99714.0, 99720.0, 99723.0, 99725.0, 99728.0, 99732.0, 99734.0, 99735.0, 99736.0, 99737.69, 99738.0, 99740.0, 99741.0, 99744.0, 99745.0, 99746.0, 99747.0, 99750.0, 99753.0, 99760.0, 99762.0, 99765.0, 99766.0, 99768.0, 99770.0, 99771.0, 99772.0, 99775.0, 99777.0, 99778.0, 99779.0, 99780.0, 99781.0, 99782.0, 99783.0, 99784.0, 99785.0, 99786.0, 99788.0, 99789.0, 99792.0, 99793.0, 99796.0, 624086.16, 99800.0, 5604824.0, 99807.0, 99808.0, 99809.0, 99810.0, 99812.0, 99813.39, 99813.0, 99814.0, 99816.0, 99817.0, 99815.0, 99819.0, 99820.0, 99819.36, 99823.0, 99824.0, 99825.82, 99826.0, 99827.0, 99828.0, 99829.0, 99830.0, 99824.96, 99833.0, 99835.79, 99840.0, 99843.0, 99844.0, 99845.0, 99846.0, 99848.0, 99850.0, 99852.0, 362000.0, 99857.16, 99856.0, 99860.0, 99861.0, 99863.0, 99864.0, 99865.0, 99866.0, 99867.0, 99868.0, 99870.0, 99872.0, 99875.0, 99876.0, 99877.0, 99878.0, 99880.0, 99881.0, 99883.0, 99884.0, 99886.0, 99890.0, 99892.0, 99895.0, 99899.0, 99900.0, 99901.0, 99903.0, 99905.0, 99906.0, 99908.0, 99909.0, 99910.0, 99912.0, 99913.0, 99914.0, 99917.0, 99918.0, 99920.0, 99921.0, 99925.0, 99926.0, 99927.0, 99928.0, 99929.0, 99930.0, 99931.0, 99933.34, 99935.0, 99936.0, 362082.0, 99939.0, 99943.0, 99944.0, 19865.0, 99947.0, 99948.0, 99950.0, 99952.0, 99953.0, 99952.36, 99955.68, 99955.0, 99959.0, 99960.0, 99963.0, 99964.32, 99965.0, 99966.0, 99964.0, 99968.0, 99969.0, 99970.56, 99970.0, 99969.8, 99973.0, 99975.0, 99976.0, 99977.0, 99978.0, 99978.24, 99980.0, 99982.0, 99982.61, 99984.0, 99985.0, 99987.0, 99987.11, 99989.0, 99990.0, 99990.8, 99988.0, 99992.0, 99989.87, 99995.0, 99996.0, 99998.0, 99999.0, 100000.0, 100000.08, 100002.0, 100003.0, 99999.85, 100005.0, 100006.0, 100001.0, 100008.0, 100009.0, 100010.0, 100011.0, 100012.0, 100013.0, 100014.0, 100015.44, 100016.0, 100013.16, 100015.0, 4294315.0, 100020.0, 100017.0, 100022.0, 100023.33, 100024.0, 100025.0, 100026.0, 100024.2, 100023.0, 100027.2, 100030.0, 100031.0, 100033.0, 100035.0, 100036.0, 100040.0, 100044.0, 100046.0, 100047.0, 100048.0, 100049.0, 100050.0, 100052.0, 100053.0, 100054.0, 100055.0, 100056.0, 100057.0, 100058.0, 100060.0, 100064.0, 100067.0, 100068.0, 100070.0, 100072.0, 100073.64, 100074.0, 100075.0, 100076.0, 362220.0, 100079.0, 100080.0, 100081.0, 100082.0, 100084.0, 100086.0, 100087.0, 100088.0, 71996.0, 100090.0, 100093.0, 100094.0, 100095.0, 100098.0, 100099.0, 100100.0, 100101.0, 100103.0, 100104.0, 100105.0, 100105.7, 100107.0, 100106.0, 100109.36, 100110.0, 100111.0, 100114.0, 100115.0, 100116.0, 100117.0, 100118.0, 100120.0, 100121.0, 100122.0, 100123.0, 100123.56, 100125.0, 100128.0, 100129.0, 100130.0, 100131.0, 100132.0, 100133.0, 100134.0, 100133.76, 100139.0, 100140.0, 100143.0, 100150.0, 100150.08, 100152.0, 100154.0, 100155.0, 100156.0, 100157.0, 100159.0, 100160.0, 100161.0, 100164.0, 100168.0, 100170.0, 100172.0, 100178.0, 100191.0, 100192.0, 100193.0, 100197.0, 100199.0, 100200.0, 100201.0, 100203.0, 100205.0, 100207.0, 100208.0, 100209.0, 100210.0, 100212.0, 100215.0, 100220.0, 100222.0, 100223.0, 100224.0, 100225.0, 100226.0, 100227.0, 100232.0, 100234.0, 100235.4, 100237.0, 100239.96, 100240.0, 100241.0, 100244.0, 100245.0, 100246.0, 100247.0, 100250.0, 100251.0, 100253.0, 100255.0, 100256.0, 100255.07, 100260.0, 100262.0, 100263.0, 100265.0, 100269.0, 100272.0, 100273.0, 100275.0, 100276.0, 100277.0, 100280.0, 100281.0, 100283.6, 100286.0, 100287.0, 100289.0, 100290.0, 100292.0, 100295.0, 100296.0, 100297.0, 100298.0, 100299.0, 100300.0, 100308.0, 100309.0, 100310.0, 100311.0, 100314.0, 100315.0, 100316.0, 100318.0, 100320.0, 100323.0, 100330.0, 100332.0, 100333.0, 100334.49, 100334.0, 100336.0, 100338.0, 100340.0, 100344.0, 100345.0, 100346.0, 100347.0, 100349.0, 100350.0, 100351.29, 100351.0, 100353.0, 100356.0, 362500.0, 100358.0, 100360.0, 100361.0, 100366.0, 100367.0, 100368.0, 100370.0, 100371.0, 100372.0, 100375.0, 100376.0, 100377.0, 100380.0, 100384.0, 100385.0, 100390.0, 100391.0, 100392.0, 100393.0, 100394.0, 100395.0, 100394.68, 100397.0, 100398.0, 100400.0, 100401.0, 100403.0, 100404.0, 100406.4, 100407.4, 100408.0, 100410.0, 100411.0, 100411.88, 100413.0, 100416.0, 100418.0, 624706.0, 100420.0, 100421.0, 100423.0, 100425.0, 100426.0, 100428.0, 100429.0, 100430.0, 100433.0, 100435.0, 100439.0, 100440.0, 100442.0, 100443.0, 100446.0, 100447.0, 100450.0, 100451.0, 100452.0, 100456.93, 100459.0, 100460.0, 100461.0, 100464.0, 100465.0, 100467.0, 100470.0, 100471.0, 100477.0, 100479.0, 100481.0, 100484.0, 100485.8, 100487.0, 100488.0, 100491.0, 100495.0, 100496.0, 100499.98, 100500.0, 100501.0, 100499.0, 100505.0, 100511.0, 100515.0, 100516.0, 100517.0, 100519.0, 100520.0, 100524.0, 100526.0, 100528.0, 100531.0, 100533.5, 100535.0, 100536.0, 100537.0, 100538.0, 100540.0, 100541.0, 100543.0, 100544.0, 100547.0, 100550.0, 100560.0, 100563.0, 100564.0, 100566.0, 100567.0, 100568.0, 100569.0, 100570.0, 100574.0, 100576.0, 100577.06, 100577.0, 100579.0, 100580.0, 100582.0, 100583.0, 100584.0, 100585.0, 100588.0, 100590.0, 100592.0, 100596.0, 100598.0, 100600.0, 100602.0, 100604.0, 100605.47, 100607.0, 100608.0, 100609.6, 100609.0, 100610.0, 100614.0, 100615.0, 100616.0, 100618.0, 100620.0, 100622.0, 100623.0, 100624.0, 100626.0, 100628.0, 100629.0, 100630.0, 100632.0, 100633.0, 100634.0, 100636.0, 100637.0, 100638.0, 100639.0, 100640.0, 100641.0, 100643.0, 100644.0, 100645.0, 100648.0, 100650.0, 100655.0, 100656.0, 100656.39, 100660.0, 100661.0, 100662.0, 100666.0, 100667.0, 100668.0, 100670.0, 100671.39, 100672.0, 100671.0, 100674.0, 100675.0, 100678.0, 100680.0, 100681.0, 100682.0, 100683.0, 100685.0, 100686.0, 100687.0, 100688.0, 100689.0, 100690.0, 100691.0, 100693.0, 100695.0, 100696.0, 100698.0, 100700.0, 100704.0, 100705.0, 100706.0, 100708.0, 100710.0, 625000.0, 100712.0, 100713.0, 100717.0, 100719.0, 100720.0, 100722.0, 100726.0, 100727.0, 100734.0, 100736.0, 100737.0, 100738.0, 100740.0, 100742.0, 100744.0, 100750.0, 100750.42, 100752.0, 100754.0, 100755.0, 100755.2, 100764.0, 100765.0, 100767.0, 100768.0, 100770.0, 100775.0, 100776.0, 100777.0, 100778.0, 100780.0, 100781.0, 100782.36, 100784.0, 100785.0, 100786.0, 100785.6, 100788.0, 100789.0, 100790.0, 100785.39, 100794.0, 100796.0, 100798.0, 100799.0, 100800.0, 100800.56, 100801.0, 100805.0, 100807.24, 100810.0, 100812.0, 100817.0, 100819.0, 100822.0, 100824.0, 100825.0, 100826.0, 100833.0, 100836.0, 100838.0, 100839.0, 100838.4, 100840.0, 100843.0, 100846.0, 100848.0, 100850.0, 100854.6, 363000.0, 100857.0, 100858.0, 100859.0, 100860.0, 100861.0, 100862.0, 100856.9, 100864.0, 100865.7, 100868.0, 100869.0, 100874.0, 100875.0, 100876.0, 100877.0, 100878.0, 100879.92, 100880.0, 100885.0, 100890.0, 100891.0, 100896.0, 100898.0, 100900.0, 100901.0, 100902.0, 100900.08, 100904.0, 100903.0, 100906.0, 100908.0, 100912.0, 100914.0, 100917.0, 100920.0, 100923.12, 100925.0, 100928.0, 100930.81, 100936.0, 100939.0, 100940.0, 100942.6, 100944.0, 100945.0, 363090.0, 100950.0, 100951.0, 100954.0, 100959.0, 100960.0, 100961.0, 100965.0, 100966.0, 100968.0, 100970.0, 100971.62, 100978.0, 100978.46, 100980.0, 100981.0, 100981.2, 100983.0, 100984.0, 100985.0, 100987.0, 100988.0, 100990.0, 100992.5, 100992.0, 100995.0, 100998.0, 100999.0, 101000.0, 101001.0, 101004.0, 101005.0, 101006.74, 101007.0, 101008.0, 101011.0, 101012.0, 101014.0, 101015.0, 101019.0, 101020.0, 101030.0, 101034.0, 101035.0, 101036.88, 101037.0, 101036.0, 101039.0, 101039.25, 101040.56, 101040.0, 101045.0, 101046.0, 101050.0, 101051.0, 101052.0, 101053.0, 101053.44, 101057.0, 625345.18, 101059.0, 101060.0, 101061.0, 101064.0, 101065.66, 101067.0, 101069.0, 101070.56, 101070.0, 101073.0, 101075.0, 101076.0, 101077.0, 101079.0, 101080.0, 101081.0, 101082.0, 101082.41, 101082.29, 101086.0, 101088.0, 101095.0, 101100.0, 101102.0, 101108.6, 101108.0, 101110.0, 101111.0, 101109.0, 101112.0, 101108.8, 101115.0, 101116.0, 101117.0, 101118.0, 101119.0, 101120.0, 101115.24, 101122.59, 101124.0, 101125.0, 101125.61, 101127.0, 101126.0, 101129.0, 101133.0, 101135.0, 101138.0, 101141.0, 101146.19, 101146.0, 101150.0, 101153.0, 101154.0, 101155.0, 101157.0, 101160.0, 101162.0, 101170.0, 101171.0, 101176.0, 101177.0, 101180.0, 101181.0, 101183.0, 101188.0, 101193.0, 101196.0, 101200.0, 101201.0, 101204.0, 101208.0, 101211.0, 101213.99, 101214.0, 101222.0, 101223.0, 101227.0, 101228.0, 101230.0, 101233.0, 101234.0, 101235.0, 101237.0, 101238.0, 101240.0, 101242.0, 101244.0, 101245.0, 101246.0, 101247.9, 101248.0, 101250.0, 101250.24, 101252.0, 101251.0, 101254.0, 101255.0, 101256.0, 363400.0, 101258.0, 101259.0, 101260.0, 101262.0, 101263.52, 101264.0, 101265.0, 101267.0, 101268.0, 101269.0, 101272.0, 101273.0, 101275.0, 101280.0, 101281.0, 101285.0, 101288.0, 101290.0, 101291.0, 101295.0, 101296.0, 101299.0, 101300.0, 101301.0, 101303.0, 101304.0, 101305.0, 101306.0, 101308.0, 101311.0, 101313.0, 101314.0, 101313.96, 101316.0, 101319.0, 101320.0, 101322.0, 101324.0, 101325.0, 101326.0, 101325.36, 101328.0, 101329.0, 101330.0, 101333.0, 101335.0, 101340.0, 101341.0, 101343.0, 101344.0, 101345.0, 101346.0, 101347.32, 101348.0, 101349.23, 101350.0, 101350.68, 101352.0, 101347.0, 101351.0, 101355.0, 101356.0, 101357.5, 363500.0, 101359.0, 101361.0, 101362.0, 101364.0, 101366.0, 101368.0, 101369.0, 101375.0, 101376.0, 101378.0, 101383.0, 101384.0, 101386.0, 101388.0, 101389.0, 101394.0, 101395.2, 101395.0, 101394.15, 101398.0, 101399.0, 101400.0, 101405.0, 101406.0, 101407.28, 101409.0, 101410.0, 101412.0, 101415.0, 101416.0, 101417.0, 101420.0, 101421.0, 101422.0, 101424.0, 1150000.0, 101429.12, 101430.0, 101432.0, 101433.0, 101434.0, 101435.0, 101436.0, 101438.0, 101439.0, 101440.0, 101443.0, 101447.6, 101448.0, 101450.0, 101452.0, 101455.0, 101456.0, 101459.0, 101460.0, 101462.0, 101465.7, 101469.0, 101470.0, 101471.0, 101472.0, 101474.0, 101475.0, 101477.0, 101480.0, 101483.0, 101484.0, 101485.0, 101487.14, 101489.0, 101490.0, 101492.0, 101494.0, 101496.0, 101497.0, 101498.0, 101499.0, 101500.0, 101500.08, 101504.0, 101505.0, 101510.0, 101512.0, 101516.0, 101517.0, 101518.0, 101520.0, 101521.0, 101527.0, 101528.0, 101530.0, 101532.0, 101533.0, 101534.83, 101535.0, 101536.0, 101540.0, 101542.0, 101543.0, 101544.0, 101546.0, 101550.0, 101552.0, 101554.0, 101555.14, 101556.0, 101558.0, 101559.0, 101560.0, 101562.5, 101564.0, 101565.55, 101566.0, 101568.0, 101569.0, 101571.0, 101573.0, 101574.0, 101575.0, 101576.0, 101578.0, 101579.0, 101580.0, 101582.0, 101584.0, 101586.0, 101588.0, 101589.0, 101590.0, 101591.0, 101592.0, 101593.0, 101594.0, 101599.0, 101600.0, 101612.0, 101614.0, 101617.0, 101618.0, 101619.0, 101620.0, 101622.0, 101625.0, 101628.0, 101629.0, 101630.0, 101631.0, 101634.0, 101635.0, 101636.0, 101637.0, 101639.94, 101640.0, 101641.0, 101642.0, 101644.0, 101645.0, 101646.0, 101647.0, 101648.0, 101649.6, 101650.0, 363792.0, 101652.0, 101651.0, 101649.0, 101656.0, 101657.0, 101660.0, 101661.0, 101662.0, 101665.0, 101666.0, 101668.0, 101670.0, 101671.0, 101672.0, 101674.0, 101675.0, 101678.0, 101679.0, 101680.0, 101681.0, 101688.0, 101690.0, 101691.0, 101692.0, 101696.0, 101698.0, 101700.0, 101702.0, 101705.0, 101710.0, 101712.0, 626000.0, 101715.0, 101719.0, 101720.0, 101722.0, 101723.0, 101724.0, 101725.0, 101730.0, 101730.97, 101735.0, 101736.0, 101737.0, 101740.0, 101741.0, 101742.0, 101744.0, 101745.0, 101747.0, 101748.0, 101749.0, 101748.19, 101750.0, 101750.64, 101755.0, 101759.0, 101760.0, 101764.0, 101765.0, 101768.0, 101769.0, 101771.0, 101775.0, 101778.0, 101779.14, 101783.0, 101784.0, 101789.0, 101791.0, 101794.0, 101800.0, 101805.0, 101807.0, 101808.0, 101814.36, 101815.8, 101817.82, 101817.0, 101820.0, 101830.0, 101834.0, 101836.0, 101837.28, 101840.0, 101847.0, 101847.73, 101850.0, 101851.0, 364000.0, 101857.0, 101858.0, 101859.0, 101860.0, 101863.0, 101866.0, 101866.31, 101868.0, 101869.0, 101874.0, 101877.0, 101878.0, 101879.0, 101881.0, 101882.04, 101886.0, 101888.0, 101889.0, 101890.0, 101891.0, 101892.0, 101895.0, 101897.0, 101898.0, 101900.0, 101908.0, 101909.0, 101910.0, 101911.0, 101912.0, 101914.0, 101915.0, 101916.0, 101917.0, 101918.0, 101919.6, 101920.0, 101921.0, 101922.0, 101924.0, 101926.0, 101928.0, 101929.0, 101930.0, 101934.0, 101937.13, 101940.0, 101941.0, 101942.0, 101940.8, 101947.0, 101950.0, 101951.0, 101952.0, 101954.0, 101955.0, 101956.0, 101957.0, 101960.0, 101962.0, 101965.0, 101968.0, 101970.0, 101971.0, 101972.0, 101974.0, 101975.0, 101978.0, 101980.0, 101982.44, 101982.4, 101984.0, 101986.2, 101986.0, 101989.0, 101990.0, 101992.0, 101993.0, 101995.0, 101996.0, 101999.0, 102000.0, 102002.0, 102004.0, 102005.0, 102007.0, 102010.0, 102011.0, 72379.0, 102014.0, 102016.0, 102019.0, 102020.0, 102024.12, 102024.0, 102025.0, 102027.0, 102028.0, 102031.0, 102032.0, 102034.0, 102036.0, 102039.96, 102041.87, 102041.0, 102043.51, 102044.8, 102045.0, 102045.22, 102049.0, 102050.0, 102051.05, 102051.0, 102054.0, 102055.0, 102056.0, 102058.0, 102060.0, 102062.0, 102064.0, 102066.0, 102068.0, 102070.0, 102072.0, 102076.0, 102080.0, 102081.0, 102082.0, 102086.4, 102090.0, 102096.0, 102100.0, 102102.0, 102108.0, 102110.0, 102111.39, 102113.0, 102115.0, 102117.0, 102118.0, 102120.0, 102122.0, 102124.0, 102125.0, 102128.0, 102130.0, 102131.0, 102135.0, 102138.72, 102140.0, 102144.0, 102145.56, 102146.0, 102149.0, 102150.0, 102151.0, 102154.0, 102155.0, 102156.0, 102158.0, 102159.0, 102165.18, 102167.0, 102168.0, 102169.0, 102170.28, 102173.0, 102175.64, 102176.0, 102179.0, 102180.0, 102180.91, 102186.0, 102188.0, 102190.0, 102192.12, 102193.0, 102194.0, 102195.0, 102192.0, 102200.0, 102202.0, 102203.0, 102204.0, 102206.0, 102209.0, 102211.0, 102212.0, 102218.0, 102219.0, 102222.0, 102224.0, 102225.0, 102228.0, 102232.0, 102235.0, 102236.0, 102239.0, 102240.0, 102243.0, 102250.0, 102254.0, 102257.0, 102258.0, 102263.0, 102265.0, 102272.0, 102276.0, 102280.0, 102285.0, 102287.0, 102288.0, 102291.0, 102293.0, 102294.0, 102300.0, 102303.0, 102306.0, 102306.56, 102309.0, 102312.0, 102313.0, 102315.0, 102318.0, 102321.0, 102322.0, 102324.0, 102327.0, 102328.0, 102335.0, 102336.0, 102339.0, 102340.0, 102345.0, 102350.0, 102356.0, 102356.8, 102357.0, 102360.0, 102361.0, 102367.0, 102368.0, 102370.0, 102375.0, 102376.85, 102377.0, 102378.0, 102379.0, 102376.0, 102386.0, 102390.0, 102391.0, 102392.81, 102396.0, 102400.0, 102401.0, 102402.0, 102408.0, 102409.0, 102411.0, 102416.0, 102419.0, 102420.0, 102421.0, 102425.0, 102427.0, 102429.0, 102431.0, 72462.0, 102433.0, 102434.0, 72462.84, 102439.0, 102440.0, 102444.0, 102447.8, 102450.0, 102452.0, 102453.0, 102455.0, 102456.0, 102459.0, 102460.0, 102464.0, 102468.0, 102470.0, 102471.69, 102472.0, 102480.0, 102481.0, 102485.0, 102489.0, 102490.0, 102489.4, 102492.0, 102493.0, 102495.0, 102496.0, 102497.0, 102499.35, 102500.0, 102501.0, 102502.0, 102502.4, 102504.0, 102508.0, 102510.0, 102512.0, 102517.04, 102521.0, 102522.0, 102523.0, 102525.0, 102532.0, 102535.0, 102539.0, 102540.0, 102544.0, 102546.0, 102551.0, 102552.0, 102553.0, 102554.0, 102555.0, 102560.0, 102561.36, 102561.0, 102563.0, 102560.68, 102560.4, 889000.0, 102568.0, 102570.0, 102571.2, 102575.0, 102576.0, 102577.0, 102580.0, 102586.0, 102588.0, 102589.0, 102590.0, 102591.0, 102594.0, 102595.0, 102597.0, 102599.0, 102600.0, 102604.0, 102605.0, 102607.8, 102611.0, 102613.74, 102616.0, 102620.0, 102622.0, 102624.0, 102625.0, 102629.0, 102630.0, 102635.0, 102636.0, 102637.0, 102639.0, 102640.0, 102642.0, 102645.0, 102648.0, 102649.0, 102650.0, 102652.0, 102656.0, 102657.0, 102660.2, 102661.0, 102662.0, 102660.0, 102664.0, 102665.0, 102667.0, 102668.0, 102671.0, 102672.0, 102674.0, 102677.0, 102678.0, 102682.0, 102684.0, 102686.0, 102689.0, 102692.0, 102695.64, 102696.0, 102697.0, 102695.0, 102699.0, 102700.0, 102698.0, 102703.0, 102704.0, 102708.0, 102710.0, 102711.0, 102713.0, 102714.0, 102715.0, 102716.0, 102719.84, 102720.0, 102722.07, 102732.0, 102733.0, 102734.0, 102736.0, 102737.0, 102742.0, 102744.0, 102746.0, 102747.42, 102749.0, 102750.0, 102752.0, 102755.0, 102756.0, 102760.0, 102765.0, 102766.4, 102767.0, 102768.0, 102769.0, 102770.0, 102766.0, 102775.0, 102776.0, 102775.08, 102778.0, 102780.0, 102789.96, 102789.0, 102792.0, 102792.48, 102795.0, 102798.0, 102799.0, 102800.0, 102801.0, 102802.0, 102804.0, 102808.0, 102810.84, 102811.0, 102812.0, 102810.0, 102816.0, 102817.0, 102819.0, 102820.0, 102821.0, 102822.0, 102831.0, 102835.0, 102836.0, 102840.0, 102845.0, 102846.0, 102847.0, 2200000.0, 102849.0, 102850.0, 365000.0, 102858.0, 102859.0, 102860.0, 102859.09, 102863.0, 102865.92, 102867.0, 102870.0, 102871.62, 102875.0, 102882.0, 102883.0, 102892.0, 102895.0, 102897.6, 102900.0, 102901.0, 102903.0, 102904.0, 102905.04, 102906.0, 102908.0, 102910.0, 102911.0, 102912.0, 102913.0, 102925.0, 102927.0, 102928.0, 102929.0, 102931.0, 102932.0, 102936.0, 102939.0, 102940.0, 102941.0, 102943.0, 102947.0, 102948.0, 102950.0, 102951.0, 102952.0, 102953.0, 102954.0, 102955.0, 102958.0, 102960.0, 102961.32, 102964.0, 102965.0, 102967.0, 102970.0, 102972.0, 102973.8, 102975.0, 102977.28, 102978.0, 102979.0, 102982.0, 102984.0, 102986.0, 102987.0, 102990.0, 102993.0, 102995.0, 102996.0, 102998.0, 102999.0, 103000.0, 103000.1, 103001.03, 103001.0, 103000.06, 103006.0, 103008.0, 103020.0, 103020.7, 103022.0, 103023.0, 103025.0, 103028.0, 103029.0, 103032.0, 103033.44, 103034.0, 103037.0, 103040.0, 103043.42, 103046.0, 103049.0, 103050.0, 103051.0, 103052.0, 103056.0, 103060.0, 103062.0, 103064.0, 103066.0, 103071.0, 103074.0, 103083.0, 103084.0, 103089.0, 103095.0, 103100.0, 103103.0, 103104.0, 103105.0, 103106.0, 103108.0, 103110.0, 103111.0, 103112.0, 103115.0, 103116.0, 103118.0, 103122.0, 103124.0, 103125.0, 103127.0, 103128.0, 103130.13, 103135.68, 103137.0, 103140.0, 103141.0, 103143.0, 103145.16, 103147.0, 103152.0, 103154.0, 103159.0, 103161.0, 103162.5, 103164.0, 103165.0, 103166.0, 103167.0, 103168.0, 103170.0, 103173.0, 103174.0, 103176.0, 103177.0, 103178.0, 103181.0, 103182.0, 103184.94, 103190.0, 103192.41, 103196.0, 103199.2, 103200.0, 103201.0, 103204.0, 103206.0, 103208.0, 103212.0, 103217.0, 103219.0, 103220.0, 103223.34, 103224.0, 103225.0, 103226.0, 103227.0, 103232.75, 103233.0, 103234.0, 103236.0, 103239.0, 103240.0, 103241.0, 103244.0, 103244.44, 103248.0, 103250.0, 103252.0, 103253.0, 103254.0, 103256.0, 103258.0, 103259.0, 103260.0, 103260.5, 103272.0, 103274.0, 103275.0, 103278.42, 103279.0, 103280.0, 103281.0, 103278.68, 103284.0, 103285.0, 103290.0, 103291.0, 103292.0, 103294.0, 103295.0, 103300.0, 103303.39, 103306.0, 103307.0, 103308.0, 103309.0, 103310.0, 103311.0, 103312.0, 103313.0, 103315.0, 103316.0, 103320.0, 103322.0, 103330.0, 103332.0, 103333.0, 103334.0, 103336.0, 103344.0, 103346.0, 103348.0, 103349.0, 103350.0, 365500.0, 103356.0, 103358.0, 103360.0, 103363.0, 103364.0, 103365.0, 103367.0, 103368.0, 103369.2, 103370.0, 103380.7, 103380.0, 103386.0, 103387.0, 103390.0, 103400.0, 72655.32, 103404.0, 103406.0, 62724.2, 103407.0, 103409.0, 103414.0, 103416.0, 103417.0, 103420.0, 103422.0, 103423.0, 103427.0, 103428.0, 103434.0, 103435.0, 103438.0, 103440.0, 103441.6, 103441.0, 103447.0, 63377.21, 103450.0, 103451.0, 103454.0, 103455.0, 103456.0, 103458.0, 103459.91, 103460.0, 103464.0, 103466.0, 103468.0, 103470.0, 103475.0, 103480.0, 103484.36, 103485.0, 103486.0, 365633.0, 103490.0, 103489.0, 103493.0, 103494.0, 103495.0, 103500.0, 103503.0, 103503.4, 103506.0, 103507.0, 103508.0, 103510.0, 103514.84, 103516.0, 103520.0, 103523.0, 103524.0, 103525.0, 103529.0, 103530.0, 103532.0, 103535.0, 103540.0, 103541.0, 103546.0, 103548.0, 103550.0, 103552.0, 103558.7, 103560.0, 103561.0, 103562.0, 103562.56, 103564.0, 103565.0, 103566.0, 890000.0, 103571.0, 103572.0, 103577.0, 103580.0, 103581.0, 103583.0, 103584.0, 103585.0, 103586.0, 103590.0, 103596.0, 103599.0, 103600.0, 103601.4, 103603.0, 103609.0, 103615.0, 103616.0, 103618.0, 103619.0, 103620.0, 103624.0, 103625.0, 103626.0, 4560077.0, 103635.0, 103636.0, 103639.0, 103640.0, 103641.0, 103644.0, 103645.0, 103645.84, 103649.0, 103650.0, 103651.0, 103652.0, 103655.0, 103656.0, 103658.0, 103662.0, 103668.0, 103670.0, 103671.0, 103672.0, 103675.0, 103677.0, 103680.0, 103681.0, 103686.0, 103688.0, 103690.0, 103692.0, 103693.0, 365840.0, 103700.0, 103704.0, 365851.21, 103707.0, 103709.0, 103715.51, 103716.0, 103717.76, 103720.0, 103721.0, 103723.0, 103725.0, 103727.0, 103733.0, 103734.0, 103735.0, 103740.0, 103742.0, 103744.0, 103748.0, 103750.0, 103751.0, 103754.0, 103755.0, 103754.88, 103756.0, 103758.0, 103760.0, 103764.0, 103765.0, 103766.0, 103767.0, 103768.0, 103770.4, 103771.0, 103772.0, 103776.0, 103777.0, 103781.0, 103783.0, 103786.0, 103788.35, 103789.0, 103792.0, 103793.4, 103793.0, 103800.0, 103803.0, 103804.0, 103806.0, 103808.0, 103812.8, 103813.0, 103818.0, 103819.0, 103820.0, 103830.0, 103832.0, 103833.0, 103835.0, 103836.0, 72741.0, 103840.0, 103844.0, 72743.0, 103846.0, 103848.0, 103849.0, 103850.0, 103851.0, 72745.83, 366000.0, 103856.0, 103859.61, 103860.0, 103865.0, 103866.0, 103869.0, 103872.0, 103875.0, 103876.0, 103877.0, 103879.51, 103880.0, 103889.0, 103890.0, 72752.52, 72752.16, 103896.0, 103897.0, 103900.0, 103900.08, 103903.0, 366056.0, 103916.0, 103919.0, 103920.0, 103925.0, 103928.0, 103932.0, 103936.0, 103938.0, 103939.0, 103940.0, 103948.0, 103950.0, 103952.0, 103954.0, 103957.0, 103960.0, 103961.0, 103966.0, 103970.0, 103974.0, 103975.12, 103978.0, 103979.0, 103980.0, 103979.39, 103982.0, 103984.0, 103989.0, 103991.0, 103992.0, 103995.0, 103997.0, 103998.0, 103999.0, 104000.0, 104001.0, 104004.16, 104004.0, 104005.0, 104009.0, 104016.0, 104017.0, 104020.0, 104022.88, 104023.0, 104032.0, 104034.0, 104036.0, 104037.0, 104040.0, 104041.0, 104045.0, 104050.0, 104052.0, 104054.0, 104055.0, 104057.0, 104062.4, 104062.0, 104064.0, 104065.37, 104067.0, 104075.0, 104077.0, 104079.0, 104080.0, 104083.0, 104085.0, 104097.0, 104100.0, 104101.0, 104104.0, 104105.0, 104107.0, 104109.0, 104112.0, 104120.0, 104122.0, 104124.0, 104125.0, 104126.0, 104127.0, 104128.0, 104130.0, 104140.0, 104143.0, 104147.0, 104149.2, 104150.0, 104151.6, 104156.0, 104160.0, 104164.0, 104166.4, 104166.0, 104168.0, 104169.0, 104170.0, 104172.0, 104174.0, 104176.66, 104177.0, 104178.0, 104179.0, 104184.0, 104185.0, 104186.0, 104190.0, 104193.97, 104194.8, 104196.0, 104199.37, 104200.0, 104199.0, 104205.97, 104206.0, 104208.0, 104209.0, 104212.0, 104216.0, 104218.0, 104219.0, 104220.0, 104221.0, 104223.0, 104224.0, 104225.0, 104229.0, 104230.0, 104235.0, 104236.0, 104240.0, 104243.0, 104244.0, 104247.0, 104250.0, 104250.88, 104252.0, 104253.0, 104254.0, 104255.0, 104256.0, 104257.0, 104260.0, 104261.0, 104265.0, 104266.0, 104268.0, 104269.9, 104270.4, 104270.0, 104272.0, 104275.0, 104277.0, 104278.0, 104280.0, 104283.0, 104286.0, 104286.65, 104288.0, 104296.0, 104298.0, 104299.0, 104300.0, 104301.0, 104302.0, 104303.0, 104304.0, 104311.0, 104312.0, 104313.0, 72836.88, 104316.0, 104320.0, 104325.0, 104328.0, 104329.0, 104330.0, 104335.0, 104338.0, 104340.0, 104342.0, 104346.0, 104348.4, 104350.0, 104355.0, 104356.0, 104357.0, 104360.0, 104364.0, 104366.0, 104370.0, 104371.43, 104372.0, 104375.0, 104378.0, 104380.0, 104381.0, 104384.0, 104389.73, 104390.0, 104389.0, 104394.0, 104397.0, 104399.0, 104400.0, 104401.0, 104402.0, 104412.0, 104416.0, 104418.0, 104420.0, 104423.0, 104433.0, 104433.88, 104436.0, 104437.0, 104441.76, 104442.0, 104443.0, 104444.0, 104445.0, 104446.87, 104449.0, 104450.0, 104456.0, 104457.0, 366600.0, 104460.0, 104465.0, 104468.0, 104469.96, 104478.0, 104480.0, 104482.56, 104484.0, 104486.0, 104491.0, 104492.0, 104491.56, 104495.0, 104496.0, 104499.0, 104500.0, 104501.0, 104502.36, 104503.0, 104509.0, 104511.69, 104514.0, 104516.0, 104520.0, 104522.0, 104523.0, 104524.78, 104525.0, 104526.0, 104535.0, 104540.0, 104542.3, 104543.64, 104545.0, 104546.0, 104549.0, 104550.0, 104551.0, 104556.0, 104559.0, 104560.0, 104566.0, 104567.0, 104568.0, 104569.0, 104570.0, 104572.0, 104575.0, 104576.0, 104578.0, 104579.0, 104580.0, 104583.0, 104588.0, 104589.0, 104590.0, 104592.0, 104595.0, 104599.0, 104600.0, 104601.0, 104606.0, 104609.0, 104617.0, 104619.0, 104620.0, 104624.0, 104628.0, 104630.0, 104631.0, 104640.0, 104642.0, 104643.0, 104644.0, 104645.0, 104648.0, 104649.48, 104650.0, 104654.0, 104660.0, 104661.0, 104660.7, 104663.24, 104664.56, 104665.0, 104662.0, 104664.0, 104668.0, 104670.0, 104671.48, 104672.0, 104673.82, 104673.0, 104676.0, 104679.0, 104680.25, 104681.0, 366825.0, 104686.0, 104690.0, 104693.0, 104694.0, 104696.4, 104698.0, 104700.0, 629000.0, 104712.0, 104720.0, 104726.0, 104727.0, 104728.0, 104730.0, 104732.0, 104733.0, 104734.0, 104736.0, 104737.0, 104740.0, 104748.0, 104750.0, 104759.72, 104760.96, 104761.0, 104760.0, 104766.0, 104768.0, 104772.0, 104775.0, 104780.0, 104783.0, 104785.0, 104787.0, 104791.0, 104793.0, 104796.0, 104800.0, 104801.0, 104802.0, 104806.0, 104810.0, 104814.0, 104815.0, 104819.0, 104820.0, 104825.0, 104827.0, 104830.0, 104832.0, 104833.0, 104834.0, 104836.0, 104847.0, 104850.0, 104851.0, 104852.0, 367000.0, 104856.0, 104859.0, 104860.0, 104862.0, 104864.0, 104865.0, 104866.0, 104870.0, 104873.0, 104874.06, 104874.0, 104876.0, 104875.0, 104878.0, 104880.0, 104885.0, 104886.0, 104887.0, 104890.0, 104892.0, 104893.0, 104898.0, 104899.0, 104900.0, 104903.0, 104904.0, 104905.5, 104906.0, 104910.0, 104911.0, 104916.0, 104919.0, 104920.0, 104924.0, 104931.12, 104933.0, 104935.0, 104936.0, 104939.0, 104940.0, 104943.0, 104944.0, 104946.0, 104950.0, 104952.0, 104954.0, 104956.0, 104959.0, 104960.0, 104964.0, 104966.0, 104969.0, 104970.0, 104971.0, 104974.0, 104975.0, 104977.0, 104980.0, 104983.0, 104986.0, 104987.0, 104988.0, 104992.0, 104994.0, 104995.0, 891428.0, 104999.0, 105000.0, 105004.0, 105007.0, 105008.0, 105009.85, 105014.0, 105014.19, 105015.0, 105018.0, 105019.0, 105026.0, 105030.0, 105031.0, 105035.0, 105038.0, 105040.0, 105043.2, 105044.0, 105047.0, 105049.51, 105050.0, 105051.0, 105056.0, 105057.62, 105060.0, 105060.8, 105062.64, 105065.0, 105067.0, 105068.04, 105073.0, 105075.0, 105077.0, 105080.0, 367225.0, 105084.0, 105086.0, 105091.0, 105094.0, 105095.0, 105096.0, 105097.0, 105100.0, 105100.22, 105102.0, 105104.76, 105105.0, 105106.0, 367250.0, 105111.0, 105113.0, 105114.0, 105115.0, 105117.0, 105118.0, 105120.03, 105120.0, 105123.0, 105125.0, 105126.0, 105128.0, 105129.0, 105132.0, 105139.2, 105142.0, 105144.0, 105146.0, 105150.0, 105151.0, 105154.0, 105159.0, 105160.0, 105165.0, 105168.0, 105170.0, 105179.0, 105180.0, 105185.0, 105191.58, 105192.31, 105193.0, 105196.0, 105199.0, 105200.0, 105201.0, 105202.0, 105206.0, 105209.0, 105211.0, 105222.0, 105224.0, 105225.0, 105235.0, 105235.2, 105237.0, 105238.0, 105240.0, 105245.0, 105248.0, 105249.0, 105250.0, 105255.0, 105258.0, 105260.0, 105261.36, 105264.0, 105268.0, 105269.0, 105270.0, 105275.0, 105276.0, 105279.0, 1416000.0, 105285.0, 105290.0, 105294.0, 105297.53, 105300.0, 105301.0, 105302.0, 105305.0, 105315.0, 105316.0, 105317.0, 105318.0, 105321.0, 105322.0, 105323.0, 105325.0, 105328.0, 105329.0, 105330.0, 105331.0, 105332.0, 105333.0, 105337.0, 105342.0, 105343.0, 105348.0, 105348.09, 105350.0, 105354.56, 105355.0, 367500.0, 105356.15, 105360.0, 105361.0, 105364.0, 105365.0, 105368.0, 105369.0, 105372.0, 105375.0, 105378.05, 105378.0, 105381.0, 105387.81, 105388.0, 105398.0, 105399.0, 105400.0, 105402.0, 105404.0, 105405.0, 105411.0, 105414.0, 105416.16, 105420.53, 105420.0, 105423.0, 105425.0, 105430.0, 105432.0, 105433.0, 105440.0, 105443.0, 105446.0, 105449.0, 105450.0, 105456.0, 105458.0, 105461.0, 105462.0, 105468.0, 105479.74, 105480.0, 105487.0, 105488.0, 105490.0, 105493.0, 105497.0, 105498.07, 105499.0, 105500.0, 105498.52, 105502.0, 105502.28, 105504.0, 105505.0, 105507.0, 105508.0, 105510.0, 105511.0, 367655.0, 105518.0, 105519.0, 105525.0, 105528.0, 105530.0, 105533.9, 105534.0, 105536.0, 105538.0, 105540.0, 105548.0, 105550.0, 105551.0, 105552.0, 105551.52, 105555.0, 105559.0, 105560.0, 105564.0, 105565.0, 105569.0, 105570.0, 105575.0, 105576.0, 105579.23, 105580.0, 105583.0, 105586.0, 105590.0, 105591.12, 105591.0, 105594.0, 105595.27, 105596.0, 105597.0, 105599.0, 105600.0, 105600.04, 105606.0, 105607.0, 105609.0, 105610.0, 105612.0, 105615.0, 105616.0, 105618.0, 105619.0, 105621.0, 105622.0, 105625.0, 105629.0, 105633.04, 105640.0, 105644.0, 105648.0, 105650.0, 105654.0, 105655.0, 105657.0, 105658.0, 105660.0, 105662.0, 105664.0, 105666.0, 105667.0, 105675.0, 105677.07, 1154255.0, 105680.0, 105681.0, 105682.0, 105684.0, 105687.5, 105689.0, 105690.0, 105692.8, 105693.0, 105694.0, 105695.0, 4300000.0, 105697.77, 105699.0, 105700.0, 105704.0, 105705.63, 4300012.0, 105708.0, 630000.0, 105712.0, 105714.0, 105716.0, 105720.0, 105723.0, 105725.0, 105726.0, 105729.0, 105730.0, 105731.0, 73118.31, 105734.0, 367880.0, 73119.0, 105740.0, 105744.0, 105747.0, 105749.0, 105750.0, 105755.0, 105757.0, 105759.0, 105760.0, 105761.0, 105766.0, 105768.0, 105769.0, 105770.0, 105771.0, 73126.0, 105778.0, 105780.0, 105785.0, 105786.0, 105788.0, 105790.0, 105791.0, 105792.0, 105795.0, 105796.0, 105798.0, 105800.0, 105803.0, 105804.0, 105803.79, 105808.0, 105810.0, 105811.0, 105812.0, 105814.0, 105816.0, 105820.0, 105825.0, 105826.0, 105829.0, 105830.0, 105832.0, 105836.0, 105840.0, 105842.0, 105845.0, 105848.0, 105850.0, 105852.0, 105854.0, 368000.0, 105858.0, 105859.0, 105863.0, 105864.0, 105865.68, 105866.0, 105867.0, 105865.0, 105868.0, 105870.0, 105872.0, 105877.0, 105878.0, 105879.0, 105884.0, 105885.0, 105886.82, 105887.0, 105888.0, 105890.0, 105892.0, 105893.0, 105896.0, 105897.0, 105898.0, 105900.0, 105905.0, 105906.0, 105908.0, 368053.0, 105910.0, 105913.68, 105914.0, 105915.0, 105919.0, 105923.0, 105924.0, 105932.0, 105934.0, 105935.0, 105936.0, 105940.0, 105946.0, 105950.0, 105954.0, 105956.0, 105957.0, 105958.0, 105960.0, 105962.62, 105964.0, 105965.0, 105968.0, 105969.0, 105970.96, 105971.0, 105975.36, 105976.0, 105975.0, 105978.0, 105979.0, 105980.0, 105976.21, 105984.0, 105985.0, 105987.0, 105988.0, 105989.0, 105990.0, 105991.0, 105996.0, 105997.0, 105998.0, 105999.0, 106000.0, 105999.8, 106002.0, 106004.0, 106006.0, 106012.0, 106013.98, 106015.0, 106016.0, 106018.89, 106018.0, 106023.0, 106026.0, 106028.0, 106028.8, 106030.0, 106032.0, 106038.0, 106040.0, 106044.0, 106046.0, 106050.0, 106054.44, 106055.39, 106056.71, 106070.0, 106071.0, 106079.0, 106080.0, 106080.11, 106085.0, 106089.88, 106090.0, 106089.0, 106095.0, 106100.0, 106102.0, 106109.0, 106111.0, 106113.0, 106114.0, 106117.0, 106120.0, 106121.0, 106124.0, 106125.0, 106128.0, 106129.0, 106139.0, 106146.0, 106149.0, 106150.0, 106154.0, 106157.0, 106158.0, 106160.0, 106161.0, 106163.0, 106167.0, 106170.0, 106174.0, 106176.0, 106177.0, 106179.0, 106181.0, 106182.0, 106188.0, 106190.0, 106196.0, 106197.0, 106200.0, 106210.0, 106211.0, 106212.0, 106214.02, 106217.28, 106219.0, 106220.0, 106223.0, 106224.67, 106225.0, 106226.0, 106224.0, 106229.0, 106232.0, 106234.0, 106238.0, 106239.0, 106240.0, 106242.13, 106245.0, 73220.94, 106248.0, 106250.0, 73221.0, 106252.0, 106253.0, 106254.48, 106254.0, 368400.0, 106257.0, 106260.0, 368405.0, 106262.0, 106263.0, 106266.67, 106267.0, 106267.06, 106272.0, 106273.0, 106275.0, 106279.0, 106280.0, 106284.0, 106285.0, 106288.0, 106290.0, 106295.0, 106298.0, 106299.47, 106300.0, 106299.0, 106305.6, 106308.0, 106308.8, 106313.0, 106315.0, 106316.0, 106317.0, 106319.0, 106320.0, 106322.0, 106324.0, 106325.0, 106329.0, 106332.0, 106338.6, 106340.0, 106342.0, 106343.0, 106344.0, 106345.0, 106348.0, 106350.0, 106354.8, 106355.0, 106357.0, 106358.0, 106359.0, 106360.0, 7446395.0, 106365.0, 106367.0, 106368.0, 106369.0, 106370.0, 106379.0, 106380.0, 106385.0, 106387.0, 106388.0, 106390.0, 106400.0, 106404.0, 106405.0, 106404.48, 106407.0, 106410.0, 106411.0, 106414.0, 106417.0, 106420.0, 106421.0, 106425.0, 106429.0, 106433.0, 106437.92, 106440.0, 106444.0, 106450.0, 106451.0, 106452.0, 106453.0, 106454.0, 106455.0, 106458.0, 106459.0, 106460.0, 106464.0, 106473.0, 106476.0, 106477.0, 106480.0, 106482.0, 106495.0, 106496.0, 106500.0, 106504.66, 106508.0, 106513.0, 106514.0, 106516.0, 106520.0, 106521.0, 106525.0, 106528.0, 106530.0, 106537.0, 106538.0, 106539.0, 106540.0, 106547.0, 106549.0, 106550.0, 106552.0, 106553.0, 106555.0, 106557.0, 106558.0, 106560.0, 106561.0, 106564.0, 106565.0, 106571.0, 106572.0, 368718.0, 106575.0, 106577.7, 106577.0, 106581.14, 106581.0, 106583.0, 106584.0, 106585.0, 106586.0, 106583.84, 106589.91, 106590.0, 106593.0, 106599.0, 106600.0, 106604.0, 106605.0, 106606.0, 106608.0, 106610.0, 106617.0, 106636.0, 106639.0, 106640.0, 106643.0, 106646.0, 106650.0, 630938.0, 106652.0, 106653.0, 106654.0, 106656.0, 106656.48, 106659.0, 106661.0, 106662.0, 106663.0, 106661.2, 106666.0, 106667.0, 106668.0, 106669.0, 106672.47, 106675.0, 106676.0, 106680.0, 73307.0, 106684.0, 106685.0, 106686.0, 106688.0, 106689.0, 106690.0, 106692.0, 106695.0, 106696.0, 106699.0, 106700.0, 106704.0, 106710.0, 106712.0, 106718.0, 106721.0, 106728.0, 106732.0, 106733.0, 106739.0, 106740.0, 106745.0, 106748.0, 106750.0, 106752.0, 106755.0, 106756.0, 106758.0, 106772.0, 106776.0, 106777.0, 106780.0, 106782.0, 106783.32, 106784.0, 106785.0, 106787.2, 106788.0, 73328.76, 106799.0, 106800.0, 106801.0, 73331.44, 106805.0, 106809.0, 106811.0, 106813.0, 106815.0, 106818.66, 106820.0, 106821.0, 106823.0, 106825.0, 106826.0, 106828.0, 106830.0, 106838.0, 106839.0, 106840.0, 106841.0, 106845.0, 106846.0, 106847.0, 106849.0, 106850.0, 106853.0, 106854.0, 369000.0, 106856.0, 106857.0, 106861.0, 106865.0, 106869.96, 106870.0, 106869.0, 106876.0, 106877.0, 106879.0, 106880.0, 106882.0, 106883.0, 106885.0, 106886.0, 106887.0, 106889.0, 106890.0, 106895.0, 106896.0, 106898.0, 106899.0, 106900.0, 106910.0, 106911.0, 106917.6, 106917.81, 106920.0, 106921.0, 106925.0, 106927.3, 106928.0, 106929.05, 106930.0, 106931.0, 106932.0, 106932.44, 106929.0, 106935.0, 106936.0, 73358.28, 106940.0, 73358.0, 106948.0, 106950.0, 106953.0, 106955.0, 106961.0, 106962.0, 106963.0, 106965.0, 106968.0, 106970.0, 106971.04, 106971.0, 106974.0, 106977.0, 106978.0, 106977.77, 106980.0, 106981.0, 106982.0, 106983.0, 106986.0, 106988.0, 106989.0, 106991.05, 106992.0, 106993.0, 106995.0, 106997.0, 106998.0, 106999.0, 107000.0, 107000.92, 107002.0, 107004.0, 107005.0, 107008.0, 107010.0, 107012.0, 107015.0, 107016.0, 107028.0, 107030.0, 107032.0, 107041.0, 107044.0, 107045.0, 107046.0, 107049.0, 107052.0, 107056.0, 369204.0, 107062.0, 107067.3, 107069.0, 107070.0, 107073.0, 107075.0, 107076.0, 107078.0, 107081.0, 107085.0, 107087.0, 107090.0, 107100.0, 107101.0, 107102.0, 107103.0, 107106.0, 107111.0, 107114.0, 107115.0, 107117.0, 107120.0, 107124.0, 107126.0, 107129.0, 107136.0, 107140.0, 107142.0, 107144.0, 107145.0, 73399.0, 73399.68, 107151.05, 107152.0, 107155.53, 107155.0, 107157.99, 107158.0, 107159.0, 107160.0, 107161.0, 107155.92, 107167.0, 107172.0, 107175.0, 107177.0, 107183.0, 107184.0, 21304.0, 107188.0, 107193.37, 107193.0, 107199.0, 107200.0, 107202.0, 107203.0, 107204.0, 107206.0, 107207.0, 107208.0, 107215.0, 107220.0, 107221.0, 107220.57, 107223.0, 107224.0, 107225.0, 107229.0, 107230.0, 107232.0, 107234.0, 107235.0, 107237.0, 107240.0, 107244.0, 107248.0, 107250.0, 107254.0, 107255.0, 107256.0, 107258.0, 107260.0, 107262.0, 107263.0, 107266.0, 107268.0, 107270.0, 107271.0, 107272.0, 107274.2, 107275.0, 107280.0, 107282.0, 107285.0, 107292.0, 107295.0, 107296.0, 107298.0, 107300.0, 107302.0, 107303.52, 107317.0, 107320.0, 107323.0, 107325.0, 107328.0, 107330.0, 107332.0, 107333.4, 107336.0, 107340.0, 107341.0, 107343.0, 107348.0, 107349.0, 107351.0, 107352.0, 107353.0, 107355.0, 60276.3, 107360.0, 107363.0, 107364.0, 107366.0, 107368.9, 107370.0, 107375.0, 107380.0, 107385.0, 107387.14, 107388.0, 107389.0, 107390.0, 107397.0, 107398.0, 107400.0, 107402.0, 107406.0, 107407.0, 107408.0, 107411.0, 107414.0, 107415.0, 107416.0, 107417.0, 107420.0, 107421.0, 107422.0, 107425.0, 107428.0, 107432.0, 107433.0, 107434.0, 107435.0, 107438.0, 107439.0, 107440.0, 107444.0, 107446.0, 107447.0, 107448.0, 107450.0, 107455.0, 107456.0, 107460.0, 107462.0, 107463.73, 107465.0, 107469.0, 107470.0, 107473.0, 107480.0, 107485.0, 107492.0, 107494.0, 107495.0, 107496.0, 107498.0, 107499.0, 107500.0, 107503.0, 107504.0, 107505.0, 107507.76, 107508.0, 107509.0, 107510.0, 107511.0, 107514.0, 107515.0, 107520.0, 107524.0, 107532.0, 107534.0, 107535.0, 107536.0, 107540.0, 107544.0, 107544.79, 107550.0, 107551.0, 107555.0, 107560.0, 107561.69, 107563.0, 107564.0, 107565.0, 107566.0, 107567.0, 107568.0, 107570.0, 107574.0, 107576.0, 107577.0, 107580.0, 107587.0, 107592.0, 107595.0, 107600.0, 107602.0, 107604.0, 107605.0, 107606.0, 107607.25, 107608.0, 107610.0, 107611.0, 107624.0, 107625.0, 107630.0, 107635.0, 107638.0, 107640.0, 107642.0, 107645.0, 107650.0, 107651.0, 107652.0, 107652.46, 107655.0, 107656.0, 107659.0, 107667.0, 107671.0, 107675.0, 107678.0, 107680.0, 107683.0, 107684.0, 107687.0, 107688.36, 107688.0, 107692.0, 107696.0, 107697.0, 107700.0, 107708.0, 107710.0, 632000.0, 107716.0, 107720.0, 107721.0, 107724.0, 107725.0, 107728.0, 107730.0, 107731.2, 107733.0, 107735.0, 107736.0, 107737.0, 107740.0, 107742.0, 107744.0, 107745.0, 107746.0, 107750.0, 107756.0, 107758.0, 107760.0, 107762.0, 107770.0, 107772.1, 107772.0, 107775.0, 107777.0, 107778.0, 107785.0, 107790.0, 107793.0, 107796.0, 107800.0, 107801.0, 107803.32, 107804.0, 107806.4, 62899.2, 107820.0, 107823.0, 107825.0, 107828.0, 107831.0, 107833.0, 107837.0, 107840.0, 107841.0, 107849.0, 107850.0, 370000.0, 107860.0, 107862.0, 107863.0, 370008.0, 107866.0, 107868.8, 107869.0, 107870.0, 107876.0, 107877.0, 107878.0, 107879.0, 107880.0, 107888.0, 107889.0, 107893.0, 107895.0, 107895.48, 107898.84, 107899.0, 107900.0, 107901.0, 107902.08, 107909.0, 107916.0, 107918.0, 107921.0, 107926.0, 107928.0, 107929.0, 107938.0, 107940.0, 107950.0, 107953.0, 107954.0, 107960.0, 107962.0, 107964.0, 107968.0, 107971.0, 107974.0, 107975.0, 107978.0, 107979.0, 107980.0, 107983.0, 107988.0, 107991.0, 107993.0, 107995.0, 107996.0, 108000.0, 108003.0, 108005.0, 108008.58, 108009.0, 108012.0, 108019.0, 108020.0, 108021.0, 108024.0, 108027.0, 108028.0, 108034.0, 108036.0, 108039.0, 108040.0, 108041.0, 108045.0, 108060.0, 108065.0, 108066.0, 108070.0, 108071.0, 108072.0, 108076.0, 108085.0, 108090.0, 108091.0, 108096.0, 108098.0, 108100.0, 108101.0, 108102.0, 108103.0, 108104.0, 108106.0, 108112.0, 108113.29, 108116.0, 108118.0, 108120.0, 108123.0, 108129.48, 108132.0, 108133.0, 108134.0, 108138.0, 108139.0, 108144.0, 108145.0, 108150.0, 108153.96, 108156.0, 108158.0, 108159.0, 108160.0, 108162.0, 108163.7, 108169.0, 108172.0, 108173.0, 108175.0, 108180.0, 108184.0, 108187.0, 108189.96, 108191.0, 108192.0, 108194.96, 108199.0, 108200.0, 108202.0, 108204.0, 108207.0, 108211.56, 108214.3, 108215.0, 108216.0, 108223.1, 108226.0, 108227.0, 108228.0, 108229.0, 108230.0, 108235.44, 108235.0, 108240.0, 108250.0, 108252.0, 108254.9, 108254.0, 108256.0, 108262.0, 108263.0, 108265.0, 370410.0, 108270.0, 108274.0, 108275.0, 108278.0, 108280.0, 108282.0, 108284.0, 108288.0, 108294.0, 108300.0, 108302.0, 108308.0, 108309.0, 108310.0, 108313.0, 108319.0, 108320.0, 108321.0, 108322.0, 108326.0, 108327.0, 108329.0, 108330.0, 108332.0, 108333.0, 108336.0, 108340.0, 108345.0, 108346.0, 108350.0, 108353.0, 73639.0, 108355.0, 108357.0, 108360.0, 108364.0, 108365.0, 108367.0, 108368.0, 108372.0, 108378.0, 108382.0, 108395.0, 108396.0, 108400.0, 108403.0, 108404.0, 108406.0, 108407.0, 108419.0, 108420.0, 108423.0, 108424.72, 108425.0, 108427.6, 108431.0, 108433.0, 108435.0, 108436.0, 108436.35, 370588.36, 108444.0, 108450.0, 108452.0, 108455.9, 108456.0, 108458.0, 108460.0, 108462.0, 108463.0, 108464.0, 108466.0, 108468.0, 108470.0, 108471.0, 108472.0, 108473.0, 108478.0, 108480.0, 108483.0, 108488.0, 108490.0, 108492.0, 108494.0, 108495.0, 108497.0, 108500.0, 108501.0, 108503.4, 108504.0, 108507.0, 108508.0, 108510.0, 108513.46, 108521.0, 108523.0, 108526.0, 108528.0, 108533.0, 108534.0, 108535.0, 108539.0, 108540.0, 108546.0, 108547.0, 108549.0, 108550.0, 108552.0, 108553.2, 108554.0, 108555.0, 108560.0, 108563.0, 108565.0, 108566.0, 108569.0, 108570.0, 108573.0, 108576.0, 108578.0, 108580.0, 108582.0, 108584.0, 108586.0, 108588.0, 108591.0, 108597.0, 108598.0, 108599.66, 108600.0, 108602.0, 108605.0, 108610.0, 108613.7, 108619.0, 108620.0, 108622.0, 108623.0, 108624.0, 108626.0, 108627.0, 108628.0, 108629.0, 108632.0, 108633.0, 108635.0, 108639.0, 108640.0, 108641.0, 108645.0, 108646.0, 108647.0, 108648.0, 108649.0, 108650.0, 108654.0, 108657.0, 108660.0, 108662.0, 108669.0, 108670.0, 108672.0, 108675.0, 108680.0, 108684.0, 108690.0, 108693.0, 108697.0, 108699.0, 108700.0, 108703.0, 108707.0, 108711.0, 108712.0, 108717.0, 108719.0, 108720.0, 108721.0, 108722.0, 108724.0, 108725.0, 108726.0, 108730.0, 108732.0, 108737.0, 108748.0, 108750.0, 108755.0, 108756.0, 108760.0, 108762.0, 108763.0, 108765.0, 108775.0, 108777.0, 108783.0, 108790.0, 108795.0, 108796.0, 108799.0, 108800.0, 108804.0, 108805.0, 370955.0, 108813.0, 108815.0, 108817.0, 108818.0, 108819.0, 108820.0, 108821.0, 108822.0, 108824.0, 108828.0, 108832.0, 108833.0, 108840.0, 108842.0, 108846.0, 108848.0, 108850.0, 108851.0, 108854.0, 371000.0, 108857.32, 108857.0, 108866.0, 108875.0, 108878.0, 108881.0, 108884.0, 108887.0, 108888.0, 108891.0, 108892.0, 108895.0, 108900.0, 108901.0, 108904.08, 108909.0, 108910.0, 108913.0, 108924.0, 108928.0, 108928.8, 108930.0, 108932.0, 108936.0, 108938.0, 108948.0, 108957.0, 108960.0, 108963.0, 108964.0, 108967.0, 108970.0, 108971.0, 108972.0, 108973.0, 108975.0, 108976.0, 108977.0, 108979.0, 108980.0, 108981.0, 108984.0, 108987.0, 108990.0, 108996.0, 108998.0, 108999.72, 109000.0, 66854.0, 109008.0, 109009.0, 109016.0, 109027.0, 109032.0, 109034.0, 109036.0, 109038.4, 109038.0, 109040.0, 109041.0, 109044.0, 109049.0, 109050.0, 109056.0, 109059.0, 109060.0, 109063.0, 109070.0, 109074.0, 109075.0, 109080.0, 109095.0, 109096.0, 109097.0, 109100.0, 109104.0, 371250.0, 109108.0, 109113.0, 109114.0, 109116.0, 109119.0, 109120.0, 109121.0, 109122.0, 109124.0, 109130.0, 109131.0, 109134.0, 109138.0, 109140.0, 109141.0, 109142.0, 109144.0, 109150.0, 109151.0, 109152.0, 109156.0, 109158.0, 109159.0, 109160.0, 109166.0, 109170.0, 109171.0, 73802.86, 109179.98, 109180.0, 109179.2, 109182.0, 109186.0, 109187.0, 109198.32, 109200.0, 109201.0, 109207.0, 109207.5, 109208.0, 109212.0, 109215.0, 109215.5, 109220.0, 109224.0, 109226.0, 109229.0, 109230.0, 109234.0, 109242.0, 109244.0, 109250.0, 109255.08, 109256.0, 109258.0, 109260.0, 109262.4, 109262.0, 109269.0, 109272.0, 109274.0, 109275.0, 109279.0, 109285.0, 109287.0, 109288.0, 109290.0, 109291.0, 109296.0, 109300.0, 109303.0, 109308.0, 109315.0, 109317.0, 109320.0, 109324.0, 109325.0, 109327.0, 109328.0, 109330.0, 109336.0, 109340.0, 109341.0, 109348.0, 109350.0, 109354.0, 109356.0, 109359.0, 109366.0, 109368.0, 109372.0, 109373.0, 109375.0, 109380.0, 633670.0, 109387.0, 109395.84, 109400.0, 109408.0, 109409.0, 109410.0, 109411.0, 109413.0, 109415.0, 109420.0, 109428.0, 109439.1, 109440.0, 109444.0, 109445.0, 109447.0, 109450.0, 109451.0, 109452.0, 109456.0, 109460.64, 109460.0, 109463.0, 109464.0, 109468.57, 109480.0, 109480.8, 109485.0, 73864.0, 109487.0, 109488.0, 109489.2, 109493.0, 109500.0, 109504.0, 109508.13, 109512.0, 109517.0, 109520.0, 109530.0, 109535.0, 109538.0, 109539.0, 109542.0, 109543.0, 109548.0, 109550.0, 109552.0, 109553.52, 109556.0, 109563.0, 109564.0, 109565.0, 109567.0, 60364.55, 109571.0, 109572.0, 109574.0, 109580.0, 109581.0, 109584.0, 109591.0, 109592.0, 109596.0, 109600.0, 109603.0, 109614.0, 109618.0, 109620.0, 109622.0, 109626.0, 371772.0, 109630.0, 109632.0, 109633.0, 109635.0, 109636.0, 109644.0, 109645.0, 109648.0, 109650.0, 109654.0, 371800.0, 109660.0, 109668.0, 109670.0, 109675.0, 109680.0, 109681.0, 109683.0, 109687.0, 109688.0, 109689.0, 109690.0, 109691.0, 109699.0, 109700.0, 109701.0, 109704.0, 109706.0, 109707.0, 109710.3, 109711.0, 634000.0, 109720.0, 109726.0, 109736.0, 109740.0, 109742.62, 109742.0, 109745.0, 109747.0, 109748.0, 109750.0, 109753.0, 109754.0, 109758.0, 109760.0, 109761.0, 109765.0, 109768.0, 109769.0, 109776.0, 109777.0, 109781.0, 109788.0, 109789.28, 109792.0, 109798.0, 109799.0, 109800.0, 109804.0, 109806.0, 109810.0, 109816.0, 109818.0, 109823.0, 109824.0, 109831.0, 109832.0, 109832.94, 109834.0, 109835.0, 109836.0, 109842.0, 109844.0, 109846.0, 109848.0, 109850.0, 109853.0, 109854.0, 109855.0, 372000.0, 109856.0, 109860.0, 109862.0, 109864.0, 109865.0, 109870.0, 109870.44, 109872.0, 109873.0, 109875.0, 109876.0, 109882.0, 109884.0, 109885.0, 109890.0, 109895.0, 109896.0, 109899.0, 109900.0, 109907.0, 109908.0, 109912.0, 109920.0, 109922.0, 109923.0, 109924.0, 109926.0, 109934.0, 109935.0, 109936.0, 109937.0, 109940.0, 109948.0, 109950.0, 109952.0, 109957.0, 109958.0, 109960.0, 109963.0, 109965.0, 109965.6, 109968.0, 109969.0, 109970.0, 109972.0, 109974.0, 109975.0, 109978.0, 109980.0, 109984.0, 109985.0, 109986.0, 109990.0, 109992.0, 109995.0, 109998.0, 109999.0, 110000.0, 110001.0, 110000.04, 110002.0, 110004.0, 110005.0, 110006.0, 110000.9, 110010.0, 110011.0, 110016.0, 110017.0, 110020.0, 110025.0, 110029.0, 110030.0, 110031.0, 110032.0, 110039.0, 110040.0, 110044.0, 110046.0, 110049.0, 110051.0, 110052.0, 110053.44, 110053.0, 110058.0, 110064.0, 110069.0, 110071.0, 110085.0, 110090.0, 110092.0, 110095.0, 110097.96, 110099.0, 110100.0, 110099.4, 110104.0, 110110.0, 110111.0, 110112.0, 110114.0, 110118.0, 110120.0, 110121.0, 110125.0, 110136.0, 110137.0, 110139.0, 110140.0, 110144.0, 110145.0, 110150.0, 110152.0, 110156.0, 110160.0, 110166.0, 110166.67, 110167.0, 110169.0, 110168.0, 110170.0, 110182.0, 110187.0, 110188.0, 110190.0, 110193.0, 110196.0, 110197.23, 110198.0, 110200.0, 110201.0, 110202.0, 110202.52, 110207.0, 110209.0, 110210.0, 110213.0, 110216.0, 110217.48, 110218.0, 110219.0, 110225.0, 110229.0, 110235.0, 110236.0, 110240.0, 110242.08, 110244.0, 110245.0, 110247.0, 110250.0, 110251.0, 110253.0, 110256.0, 110257.0, 110258.0, 110259.0, 110261.0, 110262.0, 110263.94, 110264.0, 110263.0, 110268.0, 110270.0, 110272.0, 110277.0, 110280.0, 110286.0, 110290.0, 110292.0, 110299.0, 110300.0, 110302.0, 634592.0, 110307.0, 110313.0, 110316.0, 110319.04, 110325.0, 110326.0, 110328.0, 110330.0, 110334.0, 110338.0, 110340.0, 110344.0, 110345.0, 110349.0, 110350.0, 110351.0, 110352.0, 110354.0, 110356.84, 110357.14, 110356.0, 110365.0, 110367.0, 110369.0, 110370.0, 110372.0, 110385.0, 110388.0, 110390.0, 110391.0, 110393.0, 110394.0, 110400.0, 110401.0, 110403.0, 110405.0, 110410.0, 110411.0, 110412.0, 110410.24, 110421.0, 110422.0, 110423.0, 110424.0, 110428.0, 110432.0, 110434.0, 110440.0, 110442.0, 110446.0, 110447.0, 110448.0, 110450.0, 110452.0, 110453.0, 110460.0, 110461.0, 110463.0, 110466.0, 110470.0, 110471.0, 110472.0, 110473.0, 110476.28, 110480.0, 110485.0, 110488.0, 110490.0, 110491.0, 110492.0, 110496.0, 110497.0, 110498.0, 110500.0, 110503.0, 110508.0, 110510.0, 110511.0, 110520.0, 110522.0, 110528.0, 110530.0, 110531.0, 110533.0, 110535.0, 110540.0, 110541.0, 110545.0, 110556.0, 110558.0, 110559.0, 110560.0, 110561.0, 110565.0, 897000.0, 110569.36, 110571.0, 110572.0, 110575.0, 110578.0, 110580.0, 110581.0, 110582.16, 110583.0, 110585.0, 110588.0, 110589.0, 110592.0, 110593.0, 110594.0, 110595.0, 110596.0, 110597.0, 110598.0, 110599.0, 110600.0, 110603.0, 110604.0, 110606.0, 110614.0, 110626.0, 110630.0, 110632.0, 110637.0, 110638.0, 110639.0, 110640.0, 110644.0, 110650.0, 110653.0, 110655.0, 110656.0, 110661.0, 110668.0, 110669.0, 110675.2, 110675.0, 110676.0, 110683.0, 110686.0, 110687.08, 110688.0, 110690.0, 110695.0, 110698.0, 110700.0, 110701.0, 110707.0, 110710.0, 635000.0, 110712.0, 110717.0, 110720.0, 110723.0, 110725.0, 110727.0, 110728.0, 110729.0, 110730.0, 74111.0, 110737.0, 110740.0, 110745.0, 110746.0, 110750.0, 110752.0, 110755.0, 110758.0, 110759.0, 110760.0, 110764.0, 110768.0, 110772.0, 110774.0, 110776.0, 110777.0, 110780.0, 110785.0, 110788.0, 110790.0, 110791.0, 110792.0, 110795.0, 110800.0, 110804.0, 110805.0, 110807.0, 110808.0, 110812.0, 110817.46, 110818.0, 110817.01, 110820.0, 110823.0, 110826.94, 110828.0, 110830.2, 110832.48, 110840.0, 110843.0, 110845.0, 110846.0, 110847.76, 110850.0, 110852.0, 110853.0, 372998.0, 110855.0, 110856.0, 373000.0, 110858.0, 110860.0, 110861.0, 110864.0, 110865.0, 110866.0, 110868.0, 110875.0, 110876.0, 110880.0, 110883.0, 69534.4, 110888.0, 110892.0, 110899.2, 110900.0, 110902.0, 110903.0, 110906.0, 110907.0, 110908.0, 110914.0, 110915.0, 110925.0, 110930.0, 110935.0, 110936.0, 110944.0, 110949.0, 110950.0, 110953.0, 110961.0, 110962.0, 110963.0, 110962.28, 110968.0, 110970.0, 110975.0, 110980.0, 110982.0, 110992.0, 110994.0, 110998.0, 110999.0, 111000.0, 111004.0, 111005.0, 111006.0, 111016.0, 111018.0, 111020.0, 111023.0, 111027.0, 111030.0, 111039.0, 373183.0, 111042.0, 111045.0, 111046.0, 111048.0, 111052.0, 111056.0, 111058.0, 111065.0, 111072.0, 111076.0, 111084.0, 111096.0, 111100.0, 111101.0, 111101.11, 111104.0, 111106.08, 111110.0, 111111.0, 111113.6, 74188.68, 111116.0, 111118.12, 111120.0, 111121.0, 111122.0, 111124.0, 111134.0, 111141.0, 111142.0, 111144.0, 111147.66, 111150.0, 111153.0, 111158.0, 111165.0, 111166.0, 111167.0, 111173.54, 111176.0, 111178.0, 111179.0, 111180.0, 111182.0, 111184.0, 111190.0, 111193.0, 111196.35, 111198.0, 111200.0, 111201.0, 111211.0, 111216.0, 111217.0, 111220.0, 111221.32, 111222.0, 111225.4, 111228.0, 111230.9, 111230.0, 111232.0, 111233.4, 111233.0, 111235.0, 111239.96, 111240.0, 111250.0, 111252.0, 111262.0, 111264.0, 111265.0, 111268.0, 111273.0, 111275.0, 111276.0, 111280.0, 111286.0, 111287.0, 111288.0, 111290.0, 111291.0, 111295.0, 111299.0, 111300.0, 111302.0, 111305.0, 373450.0, 111308.0, 111311.0, 111312.0, 111314.0, 111315.0, 111316.0, 111317.0, 111318.0, 111319.92, 111320.0, 111321.0, 111322.0, 111323.0, 111325.0, 111333.0, 111336.0, 111338.0, 111339.0, 111340.0, 111341.0, 111342.0, 111345.0, 111348.0, 111350.0, 111351.0, 111356.0, 111359.0, 111360.0, 111363.2, 111366.24, 111366.0, 111368.0, 111370.0, 111371.0, 111374.0, 111375.0, 111375.6, 111384.0, 111385.0, 111390.0, 8500000.0, 111392.0, 111394.0, 111395.0, 635683.0, 111396.0, 111400.0, 111405.0, 111408.0, 111410.0, 8500021.0, 111414.0, 111418.8, 111420.0, 111421.14, 111422.1, 1160000.0, 111427.0, 111428.0, 111430.0, 111431.25, 111431.0, 111432.0, 111433.0, 111434.64, 111437.0, 111440.0, 111441.0, 111444.0, 111449.0, 111450.0, 111452.0, 111453.0, 111456.0, 111457.0, 111458.0, 111460.0, 111464.0, 111467.41, 111468.0, 111467.97, 111473.0, 111475.0, 111480.0, 111481.0, 111486.0, 111489.0, 111490.0, 111492.4, 111493.0, 111492.0, 111495.0, 111498.0, 111500.0, 111508.0, 111508.56, 111511.0, 111516.0, 111516.64, 111517.0, 111524.0, 111528.0, 111529.0, 111530.0, 111532.0, 111540.0, 111543.0, 111549.0, 111550.0, 111552.0, 111555.0, 111558.0, 111560.0, 111562.0, 111565.17, 111565.0, 111567.0, 898000.0, 111566.0, 111572.0, 111573.0, 111576.0, 111580.0, 111584.72, 111587.0, 111590.0, 111592.0, 111595.0, 111596.0, 111600.0, 111603.0, 111604.0, 111605.0, 111610.0, 111611.0, 111625.0, 111628.0, 111631.0, 111632.0, 111634.0, 111635.26, 111637.0, 111643.4, 111644.0, 111645.0, 111650.0, 111651.8, 111654.96, 111656.0, 111660.0, 111662.0, 111666.0, 111668.0, 111670.0, 111672.1, 111674.0, 111675.0, 111679.0, 111684.08, 111689.0, 111695.0, 111696.0, 111697.0, 111698.0, 111699.0, 111700.0, 111705.0, 636000.0, 111714.0, 111718.0, 111720.0, 56542.0, 111723.0, 111727.0, 111738.0, 111740.0, 111744.0, 111745.0, 111748.0, 111750.0, 111752.0, 111756.0, 111760.0, 111768.0, 111771.0, 111772.0, 111779.0, 111780.0, 111781.0, 111784.0, 111789.0, 111792.0, 111796.0, 111799.0, 111800.0, 111801.0, 111804.0, 111809.0, 111814.0, 111816.0, 111817.0, 111818.0, 111820.0, 111822.0, 111823.0, 111825.0, 111826.0, 56546.0, 111832.0, 111833.0, 111838.0, 111840.0, 111845.0, 111850.0, 111852.0, 111852.23, 111855.0, 374000.0, 111862.0, 111863.0, 111864.0, 5616890.0, 111867.37, 111871.8, 111872.88, 111873.0, 111880.0, 111884.0, 111886.0, 111887.94, 111888.0, 111890.0, 111896.0, 111900.0, 111901.0, 111904.62, 111905.0, 111906.0, 111904.0, 111910.0, 111915.0, 111920.0, 111925.0, 111926.0, 111932.0, 111933.0, 111935.0, 111940.0, 111941.0, 111942.0, 111945.0, 111950.0, 111953.0, 111956.0, 111960.0, 111961.39, 111964.0, 111969.0, 111970.0, 111971.0, 111982.0, 111983.0, 111984.0, 111988.0, 111989.0, 111990.0, 111992.0, 111994.0, 111996.0, 112000.0, 112001.0, 112003.0, 112007.0, 112008.0, 112010.0, 112012.0, 112014.0, 112020.0, 112021.0, 112029.0, 112032.0, 112035.0, 112038.0, 112041.0, 112042.0, 112050.0, 74374.1, 112060.0, 112061.0, 112064.0, 112065.0, 112074.0, 112075.0, 112076.0, 112077.0, 112079.0, 112080.0, 112083.0, 112084.0, 112085.0, 112086.0, 112092.0, 112100.0, 112103.04, 374250.0, 112108.0, 112110.0, 112111.0, 112112.0, 112113.0, 112114.0, 112115.0, 112118.0, 112120.0, 112122.0, 112123.0, 112124.0, 112125.0, 112139.0, 112140.0, 112150.0, 112152.0, 112153.0, 112152.08, 112158.0, 112159.0, 112160.0, 112164.0, 112167.0, 112174.0, 112175.0, 112180.0, 112184.0, 60468.42, 112188.0, 112192.0, 112194.0, 112197.0, 112199.0, 112200.0, 112201.0, 112200.55, 112206.36, 112207.0, 112212.0, 112213.0, 112214.0, 112215.93, 112216.0, 112217.0, 112219.0, 112224.0, 112225.0, 112226.0, 112227.0, 112232.0, 112235.0, 112236.0, 112238.0, 112240.0, 112242.29, 112243.0, 112248.0, 112252.0, 112254.0, 112256.0, 112260.0, 112266.0, 112270.0, 112272.0, 112284.0, 112285.0, 112294.0, 112296.0, 112298.0, 112300.0, 112308.0, 112317.0, 112320.0, 112330.0, 112332.0, 112337.28, 57869.64, 112340.0, 112343.0, 112345.0, 112346.0, 22330.8, 112350.0, 112354.0, 112356.0, 112360.0, 112361.0, 112362.0, 112368.0, 112375.27, 112377.0, 112390.0, 112395.0, 112397.0, 112400.0, 112404.0, 112408.0, 112410.0, 112412.0, 112415.0, 112415.52, 112417.0, 112416.0, 112420.28, 112423.0, 112426.0, 112428.0, 112430.0, 112437.0, 112440.0, 112442.0, 112444.0, 112445.0, 112446.0, 22349.64, 112450.0, 112452.0, 112452.48, 112455.0, 112457.0, 112459.0, 22352.0, 112461.0, 112462.0, 112467.0, 112469.0, 112470.0, 112473.0, 112474.0, 112475.0, 22355.0, 112477.0, 74458.0, 112480.0, 112484.0, 112486.4, 112488.0, 112497.0, 112500.0, 112502.0, 112503.0, 112508.0, 112509.0, 112509.76, 112511.52, 112512.0, 112511.0, 112514.0, 112517.0, 112519.0, 112522.0, 112524.0, 112525.0, 112527.0, 112528.0, 74469.0, 112535.0, 112536.0, 112543.0, 112544.0, 112545.0, 112547.0, 112548.0, 112550.0, 112551.0, 112555.0, 112557.0, 112558.0, 112559.0, 112560.0, 112561.0, 112565.0, 112566.22, 112568.0, 112578.0, 112580.0, 112582.0, 112584.0, 112585.0, 112584.36, 112588.0, 112590.4, 112590.0, 112590.39, 112593.0, 112597.0, 112600.0, 112604.0, 112604.12, 112609.1, 112610.0, 112610.38, 112616.0, 112619.0, 112620.0, 112624.0, 112626.0, 112627.0, 112632.0, 112632.99, 112634.0, 112636.0, 112638.0, 112647.0, 112648.0, 112650.0, 112657.0, 112665.0, 112673.0, 112678.0, 112679.0, 112680.0, 112685.0, 112688.0, 112689.0, 112694.0, 112697.0, 112700.0, 112704.0, 112705.92, 112705.0, 112711.0, 112712.0, 112714.0, 112720.0, 112728.0, 112729.0, 112732.0, 112738.0, 112740.0, 112742.0, 112744.0, 112749.0, 112750.0, 112751.04, 112752.0, 112758.0, 112760.0, 112761.0, 112764.0, 112767.0, 112768.0, 112770.0, 112772.0, 112774.0, 112776.0, 112780.0, 112781.0, 112782.0, 112786.0, 112787.12, 112788.0, 112790.0, 112791.0, 112793.0, 112794.0, 112796.0, 112798.0, 112800.0, 112801.0, 112804.0, 112808.0, 112813.0, 112814.0, 9550000.0, 112816.0, 112824.0, 112825.0, 112828.0, 112832.0, 112833.0, 112834.0, 112836.0, 112840.0, 112845.0, 112847.69, 112848.0, 112849.0, 112850.0, 112851.6, 112852.0, 112853.0, 112854.0, 375000.0, 112860.0, 112863.0, 112866.0, 112869.0, 112875.0, 112880.0, 112884.24, 112884.0, 112890.0, 112894.0, 112896.0, 112899.0, 112900.0, 112907.0, 112908.0, 112914.0, 112917.0, 112921.0, 112922.0, 112925.76, 112926.0, 74549.0, 112932.0, 112932.29, 112934.0, 112939.0, 112940.0, 74551.92, 112944.0, 112950.0, 112951.0, 112952.0, 112955.0, 112958.0, 112960.0, 112968.0, 112970.0, 112975.1, 112976.0, 112980.0, 112987.0, 112990.0, 112992.0, 112995.0, 112998.0, 112999.0, 113000.0, 113003.0, 113004.0, 113008.0, 113012.0, 113017.0, 113021.0, 113023.0, 113024.0, 113025.9, 113026.0, 113025.0, 113028.0, 113034.0, 113038.0, 375184.0, 113042.0, 113045.88, 113050.0, 113052.0, 113056.0, 113057.0, 113059.0, 113064.0, 113066.0, 113068.08, 113069.0, 113070.0, 113074.0, 113080.0, 113083.0, 113084.0, 113089.0, 113090.0, 113091.0, 113092.0, 113096.0, 113100.0, 113105.0, 113111.0, 113115.0, 113121.0, 113122.0, 113126.0, 113130.0, 113131.0, 113132.0, 113135.0, 113145.0, 113151.5, 113155.0, 113159.0, 113160.0, 113173.0, 113175.0, 113176.0, 113180.0, 113186.0, 113188.92, 113190.0, 113191.0, 113193.0, 113196.0, 113197.92, 637485.0, 113200.0, 113201.0, 113208.0, 637500.0, 113213.25, 113217.4, 113219.0, 113220.0, 113222.0, 113223.0, 375372.0, 113234.0, 113235.0, 113236.0, 113244.0, 113249.0, 113250.0, 113252.0, 113254.0, 113255.0, 113258.0, 113268.0, 113269.0, 113271.96, 113272.52, 113272.0, 113274.0, 113275.0, 113279.0, 113280.0, 113282.0, 113284.9, 113292.0, 113292.71, 113300.0, 113304.0, 113305.0, 113308.0, 113310.0, 113312.0, 113313.0, 113314.0, 113315.0, 113316.0, 113318.0, 113320.0, 113325.0, 113326.0, 113327.0, 113328.0, 113330.2, 113331.0, 113343.0, 113345.0, 113346.0, 113347.0, 113349.0, 113354.0, 113355.66, 375500.0, 113356.05, 113359.0, 113360.0, 113365.0, 113372.0, 113376.0, 113380.0, 113383.0, 113391.0, 113395.25, 113396.0, 113400.0, 113408.0, 113409.0, 113412.0, 113413.0, 113414.0, 113415.0, 113416.0, 113417.0, 113420.02, 113420.0, 113421.0, 113424.0, 113427.49, 113428.0, 113436.0, 113440.0, 113442.0, 113446.0, 113448.0, 113450.0, 113456.0, 113460.0, 113463.88, 113463.0, 113467.0, 113488.0, 113496.0, 113497.0, 113499.0, 113500.0, 113506.0, 113516.0, 113517.0, 113520.0, 113525.0, 113526.0, 113527.0, 113530.0, 113531.0, 113533.0, 113536.0, 113537.0, 113543.0, 113544.0, 113545.0, 113550.0, 113555.0, 113557.0, 113558.0, 113559.0, 113560.0, 113566.0, 113567.0, 900000.0, 113568.0, 113570.0, 113572.0, 113574.0, 113577.0, 900009.0, 113588.8, 113588.0, 113590.0, 113591.0, 113592.0, 113593.0, 113594.0, 113595.0, 113600.0, 113608.0, 113616.0, 113620.0, 74686.42, 113622.0, 113623.0, 113625.0, 113627.0, 113629.0, 113631.0, 113633.0, 113635.2, 113636.85, 113640.0, 113647.0, 113648.0, 113650.0, 113651.0, 113652.0, 113656.0, 113663.0, 113664.0, 113667.0, 113670.0, 113675.0, 113676.0, 113678.0, 113686.0, 113691.0, 375840.0, 113698.0, 113700.0, 113705.0, 113708.0, 638000.0, 113713.0, 113712.0, 113725.0, 113730.0, 113736.0, 113749.0, 113750.0, 113753.0, 113755.0, 113759.0, 113760.0, 113762.0, 113763.0, 113766.0, 113771.28, 113772.0, 113774.0, 113776.0, 113778.0, 113780.0, 113783.0, 113786.0, 113787.0, 113788.0, 113795.0, 113800.0, 113802.0, 113803.0, 113806.51, 113809.0, 113813.33, 113814.0, 113820.0, 113824.0, 113828.0, 113829.0, 113837.0, 113845.0, 113847.0, 113850.0, 113851.0, 113853.0, 113854.0, 376000.0, 113856.0, 113859.0, 113860.0, 113868.0, 113873.0, 113874.0, 113875.0, 113877.4, 113879.0, 113880.0, 113883.0, 113885.0, 113890.0, 113892.0, 113894.54, 113898.0, 113900.0, 113904.0, 113909.0, 113911.0, 113912.0, 113914.0, 113918.0, 113920.0, 113924.0, 113925.0, 113926.0, 113931.0, 113936.0, 22646.0, 113940.0, 113950.0, 113953.64, 113953.0, 113960.0, 113962.0, 113963.0, 113968.0, 113972.0, 113974.0, 113978.4, 113984.0, 113986.0, 113989.0, 113993.0, 114000.0, 114002.0, 114004.0, 114005.8, 114006.25, 114018.0, 114019.0, 114020.0, 114021.0, 114025.0, 114026.0, 114040.0, 114041.0, 114040.4, 114046.0, 114050.0, 114052.0, 114053.52, 114054.0, 114055.0, 114053.0, 376200.0, 114058.0, 114060.0, 114066.65, 114072.0, 114075.0, 114089.0, 114094.0, 114098.0, 114100.0, 114103.0, 114107.0, 114108.0, 114112.0, 114117.0, 114120.0, 114124.0, 114126.0, 114132.0, 114132.67, 114135.0, 114147.12, 114150.0, 114151.0, 114160.0, 114164.0, 114169.0, 114170.0, 114179.0, 114180.0, 114182.0, 114186.0, 114192.0, 114195.0, 114198.0, 114200.0, 114204.0, 114211.0, 114213.0, 114225.0, 114230.0, 114237.0, 114240.0, 114250.0, 114253.0, 114254.0, 114262.0, 114264.0, 114266.0, 114269.0, 114271.17, 114272.0, 114275.0, 114277.0, 114278.0, 114281.0, 114285.0, 114288.0, 114295.0, 114295.98, 114297.0, 114299.0, 114300.0, 114307.0, 114308.0, 114310.0, 114312.0, 114316.0, 114320.27, 114322.0, 114324.0, 114327.0, 114328.0, 114329.0, 114331.0, 114333.0, 114336.0, 114338.0, 114343.0, 114345.0, 114346.0, 114347.0, 114348.0, 114350.0, 114356.0, 376500.0, 114360.0, 114367.0, 114370.0, 114378.0, 114385.0, 114393.0, 114396.0, 114398.0, 65766.48, 114400.0, 114401.0, 114408.0, 114411.0, 114412.6, 114416.0, 114419.0, 114420.0, 114424.0, 114431.0, 114433.0, 114435.0, 114436.0, 114438.0, 114439.0, 114440.0, 114441.0, 114444.0, 114450.0, 114452.0, 114457.0, 114458.0, 114460.0, 114465.6, 114466.0, 114468.0, 114469.0, 114470.0, 114476.16, 114480.0, 114484.0, 114492.0, 114495.0, 114499.0, 114500.0, 114504.0, 114507.0, 376653.0, 114513.0, 114520.0, 114523.0, 114524.0, 114527.0, 114530.0, 114531.0, 114533.0, 114536.0, 114538.0, 114539.0, 114545.0, 114548.0, 114550.0, 114551.97, 114552.0, 114555.0, 114558.0, 114559.0, 114560.0, 114567.0, 65773.24, 114576.0, 114579.0, 114587.0, 114589.0, 114590.0, 114591.0, 114589.8, 114593.0, 114594.0, 114595.0, 114596.0, 114597.0, 114598.0, 114599.27, 114600.0, 114599.0, 114608.0, 114612.0, 114616.0, 114618.0, 114619.0, 114620.0, 114624.0, 114625.0, 114635.0, 114637.0, 114638.0, 114639.0, 114640.0, 114645.0, 114647.9, 114648.0, 114649.6, 114647.0, 114649.0, 114654.0, 114656.0, 114660.0, 114664.0, 114668.0, 114669.0, 114672.0, 114675.0, 114682.8, 114683.0, 114684.0, 114685.0, 114687.0, 114690.0, 114694.0, 114695.0, 114696.0, 114698.0, 114700.0, 63173.7, 114705.0, 114710.0, 114715.0, 114720.0, 114725.0, 114726.0, 114728.0, 114731.0, 114732.0, 114737.0, 114740.0, 114750.0, 114753.0, 114756.0, 114756.8, 114767.0, 114770.0, 114773.98, 114782.0, 114785.0, 114788.0, 114789.0, 114790.0, 114796.0, 114797.03, 114798.0, 114799.0, 114800.0, 114804.0, 114808.0, 114810.0, 376956.0, 114816.0, 114821.0, 114835.0, 114840.0, 114842.0, 114844.26, 114844.0, 114849.0, 114850.0, 114852.0, 377000.0, 114856.54, 114860.0, 114865.0, 114867.0, 114871.0, 114874.0, 114875.0, 114876.0, 114878.0, 114880.0, 114882.0, 114883.0, 377031.0, 114888.0, 114892.0, 114894.0, 114895.2, 114899.0, 114900.0, 114902.0, 114906.0, 114908.0, 114908.52, 114922.0, 114924.0, 114933.0, 114935.0, 114936.0, 114950.0, 114958.67, 114966.0, 114972.0, 114975.0, 114985.0, 114987.0, 114988.0, 114990.0, 114991.63, 1950000.0, 114993.0, 114991.0, 114995.0, 114996.0, 114997.0, 114998.0, 114999.0, 115000.0, 115001.0, 114999.6, 115004.0, 115005.0, 115004.48, 115008.0, 115015.0, 115016.0, 115020.0, 115024.0, 115026.0, 115032.0, 115033.0, 115036.0, 115039.7, 115040.0, 115041.0, 115043.0, 115044.0, 115048.0, 115050.0, 115056.0, 115058.0, 115059.0, 115060.0, 115072.0, 115075.0, 115076.0, 115079.0, 115082.0, 115084.0, 115086.0, 115088.0, 115088.28, 115097.0, 115100.0, 115101.0, 115103.0, 115104.0, 115117.0, 115128.0, 115129.0, 115132.0, 115137.0, 115139.0, 115140.0, 115141.0, 115144.0, 115148.0, 115150.0, 115153.0, 115154.16, 377298.0, 115157.0, 115160.0, 115161.0, 115166.0, 115167.0, 115167.89, 115169.0, 115170.0, 115172.0, 115175.0, 115180.0, 115184.0, 115185.0, 115200.0, 75000.12, 115203.0, 115210.0, 115211.0, 115216.0, 115220.0, 115224.0, 115226.0, 115233.0, 115236.0, 115237.0, 115245.0, 115248.0, 115250.0, 115252.24, 115260.0, 115265.0, 115272.0, 115275.0, 115296.0, 115297.0, 115298.88, 115300.0, 115307.0, 115315.0, 115320.0, 115321.0, 115327.0, 115330.23, 115331.0, 115332.0, 115330.0, 115336.0, 115338.0, 115340.0, 115344.0, 115345.0, 115347.0, 115348.0, 115353.0, 115358.0, 115359.0, 115360.0, 115368.92, 115373.0, 115375.0, 115380.47, 115386.0, 115387.0, 115388.0, 115392.0, 115393.0, 115400.0, 115402.0, 115407.0, 115409.0, 115410.0, 115409.78, 115413.36, 115428.0, 115430.0, 115432.0, 115433.0, 115438.0, 115439.0, 115440.0, 115441.0, 115444.0, 115450.0, 115452.0, 115456.0, 115460.0, 115461.0, 115462.0, 115464.0, 115466.0, 115467.0, 115468.3, 115470.0, 115472.0, 115473.0, 115476.0, 115484.0, 115488.0, 115491.0, 115500.0, 115501.0, 115506.0, 115511.0, 115512.0, 115516.05, 115518.0, 115523.0, 377667.0, 115524.0, 115526.66, 115530.0, 115534.92, 115535.0, 115550.0, 115555.0, 115560.0, 115561.12, 115564.0, 115569.0, 115570.0, 115572.0, 115575.2, 115575.0, 115582.0, 115594.0, 115596.0, 115598.0, 115599.0, 115600.0, 115605.0, 115606.0, 115613.0, 115619.0, 115625.0, 115626.3, 115635.31, 115636.0, 115643.0, 115645.0, 115648.0, 115650.0, 115654.0, 115656.0, 115663.0, 115664.0, 115667.0, 115668.0, 115672.0, 115678.0, 115679.0, 115680.0, 115681.0, 115682.0, 115683.0, 115684.0, 115685.0, 115688.0, 115689.0, 115692.0, 115696.0, 115697.0, 115699.0, 115700.0, 115701.0, 115703.0, 115704.0, 115708.0, 115710.0, 640000.0, 115713.0, 115714.0, 115717.0, 115720.0, 115721.0, 115730.81, 115730.0, 115731.0, 115733.0, 115735.0, 115736.0, 115740.0, 115741.0, 115742.0, 115745.0, 115746.0, 115747.0, 115748.0, 115750.0, 115752.0, 115753.0, 115755.0, 640046.0, 115759.0, 115766.0, 115769.0, 115773.0, 115776.0, 115779.45, 115780.0, 115782.0, 115784.0, 115787.0, 115788.0, 115790.0, 115791.5, 115792.0, 115793.0, 115799.0, 115800.0, 115810.0, 115812.0, 115821.0, 115825.0, 115825.51, 115828.0, 115830.0, 115831.0, 115836.0, 115837.0, 115840.0, 115842.0, 115850.0, 115852.0, 115853.0, 115855.62, 378000.0, 115856.0, 115860.0, 115862.0, 115867.0, 115868.0, 115870.0, 115872.0, 115874.0, 115875.0, 115876.0, 115875.13, 115880.0, 115884.0, 115886.0, 115888.0, 115889.0, 115890.0, 115892.0, 115897.0, 115900.0, 115902.0, 115904.0, 115906.0, 115909.0, 115920.0, 115921.0, 115926.0, 115928.0, 115930.0, 115939.0, 115942.0, 115943.0, 115945.38, 115945.0, 115950.0, 115951.0, 115952.0, 115953.0, 115956.0, 115958.0, 115960.0, 115963.0, 115964.0, 115965.0, 115964.04, 115972.0, 115976.0, 115977.0, 115978.0, 115980.0, 115984.0, 115989.0, 115992.0, 115999.0, 116000.0, 116002.0, 116004.0, 116015.68, 116016.0, 116020.0, 116023.0, 116025.0, 116027.0, 116038.0, 116040.0, 116043.0, 116052.0, 116062.92, 116067.0, 116072.0, 116076.4, 116087.0, 116088.0, 116089.0, 116090.0, 116100.0, 116102.0, 116103.0, 116109.0, 116111.0, 116112.0, 116113.0, 116115.0, 116116.0, 116120.0, 116123.0, 116125.0, 116130.0, 116133.0, 116136.0, 116137.0, 116140.0, 116142.0, 116144.0, 116145.0, 116146.0, 116147.0, 116148.0, 116150.0, 116160.0, 116162.0, 116165.0, 116168.0, 116171.0, 116176.0, 116177.0, 116179.0, 116180.03, 116181.0, 116183.0, 116184.12, 116188.0, 116196.0, 116200.0, 116201.0, 116203.0, 116209.0, 116212.0, 116216.54, 116220.0, 116225.0, 116230.0, 116232.0, 116234.6, 116236.0, 116240.0, 116244.0, 116249.0, 116250.0, 116251.0, 116252.0, 116253.0, 116257.0, 116258.0, 116259.0, 116262.0, 116270.0, 116272.0, 116280.0, 116290.0, 116292.0, 116300.0, 116302.0, 116303.0, 116305.0, 116307.0, 116309.0, 116311.0, 116314.0, 116325.0, 116328.0, 116335.0, 116340.0, 116342.57, 116343.0, 116342.86, 116350.0, 116352.0, 116354.42, 116355.0, 116361.0, 116363.52, 116364.0, 116365.0, 116374.0, 116375.0, 116390.0, 116396.0, 116399.0, 116400.0, 116402.0, 116404.0, 116405.0, 116408.0, 116409.0, 116412.0, 116415.4, 3000000.0, 116417.0, 116419.0, 116420.0, 116421.0, 116424.0, 116428.0, 116431.0, 116435.0, 116436.0, 116442.0, 116443.0, 116444.0, 116448.0, 116450.0, 116451.23, 116452.0, 116453.0, 116464.0, 116471.0, 116472.0, 116473.12, 116476.0, 116480.0, 116484.0, 116495.0, 116496.0, 116499.0, 116500.0, 116505.0, 116508.0, 116520.0, 116524.72, 116531.0, 116532.0, 116534.0, 116540.0, 116542.0, 116544.0, 116552.0, 116554.0, 116556.0, 116562.0, 116563.0, 116565.0, 116567.0, 116572.0, 116574.0, 116575.0, 116580.0, 116581.0, 116584.0, 58689.9, 116595.0, 116596.0, 116597.0, 116600.0, 116604.61, 116616.0, 116622.0, 116624.85, 116627.0, 116628.0, 116630.0, 116635.0, 116640.0, 116642.0, 116645.0, 116646.0, 116650.0, 116655.0, 116660.0, 116661.0, 116664.0, 116667.0, 116668.0, 116676.0, 116680.0, 116681.0, 116685.0, 116686.0, 116687.0, 116688.0, 23193.0, 23193.12, 116690.0, 116699.0, 116700.0, 116699.18, 116699.65, 116704.0, 23196.36, 116706.58, 116708.0, 116710.0, 116711.0, 116722.0, 116723.0, 116724.0, 116725.0, 116730.0, 116734.0, 116735.0, 116736.0, 116739.0, 116740.0, 75306.0, 116744.0, 116746.0, 116748.0, 116750.0, 116757.0, 116760.0, 116765.0, 116768.0, 116770.0, 116772.0, 116780.0, 116781.0, 116783.0, 116784.0, 116788.0, 116789.0, 116792.0, 116794.88, 116794.0, 116796.0, 116800.0, 116805.0, 116809.44, 116812.8, 116813.0, 116816.0, 116820.0, 116828.0, 116832.0, 116835.0, 116840.0, 116844.0, 378990.15, 116848.0, 116849.0, 116851.0, 116853.0, 379000.0, 116856.0, 116858.0, 116860.0, 116861.35, 116865.0, 116868.0, 116870.0, 116872.0, 116874.0, 116880.0, 116882.0, 116883.0, 116886.0, 116887.0, 116888.0, 116890.0, 116892.0, 116893.0, 116895.0, 75337.0, 116897.0, 116896.0, 116900.0, 116901.0, 116903.0, 116905.0, 116911.0, 116912.0, 116914.0, 116917.0, 116924.0, 116937.6, 116939.0, 116942.0, 116943.0, 116957.0, 116960.0, 75350.16, 116964.0, 116975.0, 116976.0, 116980.0, 116987.0, 116988.0, 116992.0, 116999.77, 117000.0, 117001.56, 117005.0, 117010.0, 117012.0, 117014.0, 117020.8, 117021.0, 117024.0, 117027.0, 75363.0, 117030.0, 117036.0, 117040.0, 117047.0, 117048.0, 117058.0, 117060.0, 117075.0, 117077.92, 117081.13, 117086.0, 117090.4, 117091.0, 117098.88, 117100.0, 117104.0, 117113.16, 117114.0, 117115.0, 117113.0, 117117.0, 117120.0, 117125.11, 117126.0, 117129.0, 117132.0, 117134.0, 117136.0, 117139.0, 117142.0, 117144.0, 117146.0, 117148.0, 117150.0, 117159.78, 117161.81, 117163.0, 117164.0, 117174.0, 117175.0, 117180.0, 117182.0, 117186.0, 117189.0, 117190.0, 117192.0, 117194.0, 117200.0, 117208.0, 117216.0, 117220.0, 117221.0, 117222.0, 117223.0, 117224.0, 117240.0, 117240.14, 117243.0, 117246.0, 117250.0, 117252.0, 117253.0, 117260.0, 117269.4, 117270.0, 117272.32, 117273.0, 1428000.0, 117281.0, 117285.0, 117287.0, 117299.0, 117300.0, 117302.0, 117306.0, 117310.1, 117312.0, 117313.0, 117328.26, 117332.0, 117339.0, 117340.0, 117347.0, 117348.0, 117350.0, 117352.0, 117353.0, 379500.0, 117360.0, 117363.0, 117366.0, 117374.0, 117375.0, 117376.0, 641667.0, 117390.0, 117391.0, 117393.0, 117395.0, 117396.0, 117397.0, 117399.0, 117400.0, 117418.0, 117420.0, 117424.0, 117430.0, 117431.0, 117432.0, 117433.0, 117439.0, 117441.0, 117444.0, 117446.0, 117450.0, 117452.0, 117457.6, 117458.0, 117460.0, 117464.0, 117469.0, 117474.74, 117474.0, 117478.4, 117480.0, 117486.0, 117488.0, 117490.0, 117495.0, 117496.0, 117500.0, 117503.0, 117510.0, 117512.0, 117515.0, 117516.0, 117519.0, 117520.0, 117521.0, 117524.94, 117527.0, 117530.0, 117538.0, 117544.0, 117547.04, 117548.0, 117551.0, 117552.0, 117554.0, 117554.28, 117557.0, 117558.91, 117558.0, 117560.0, 117565.0, 117580.0, 117586.0, 117596.0, 117599.0, 117600.0, 117603.0, 117618.0, 117629.0, 117631.02, 59382.28, 117633.0, 117636.0, 117640.0, 117645.75, 117645.0, 117647.0, 117650.0, 117651.0, 117654.0, 117661.0, 117661.92, 117663.0, 117665.0, 117670.38, 117670.0, 117676.08, 117677.0, 117678.0, 117684.0, 117685.0, 117689.0, 117693.0, 117696.0, 117700.0, 117707.0, 117712.0, 117717.0, 117719.0, 117720.0, 117725.0, 117729.0, 117736.0, 117738.0, 117744.0, 117745.0, 117750.0, 117753.0, 117754.0, 117756.0, 117757.0, 117763.0, 117764.0, 117763.96, 117768.0, 117770.0, 117771.0, 117779.4, 117780.0, 117781.0, 117784.0, 117785.0, 117786.0, 117788.0, 117795.0, 117796.0, 117800.0, 117804.96, 117811.0, 117814.0, 117819.0, 117820.0, 117821.0, 117823.0, 117825.96, 117827.28, 117832.0, 117836.0, 117837.0, 117840.0, 117848.0, 117850.0, 117852.0, 117855.17, 380000.0, 117857.0, 380001.0, 117859.0, 380004.0, 117860.0, 117864.0, 117869.0, 117873.0, 117874.3, 117875.0, 117874.0, 117894.0, 117895.0, 117896.0, 117897.0, 117898.1, 117899.0, 117900.0, 117910.0, 117911.0, 117920.0, 117920.11, 117927.0, 117936.0, 117942.6, 117945.0, 117950.0, 117952.0, 380098.0, 117955.0, 117956.0, 117960.0, 117961.0, 117963.0, 117964.0, 117975.0, 117977.76, 117984.0, 117990.0, 117992.0, 117993.0, 117995.0, 117996.0, 117999.0, 118000.0, 118002.0, 118006.68, 118008.0, 380160.0, 118018.0, 118019.0, 118027.0, 118030.0, 118035.01, 118040.0, 118044.0, 118048.0, 118050.0, 118055.47, 118057.0, 118060.0, 118063.0, 118069.0, 118075.0, 118077.95, 118079.0, 118080.0, 118088.0, 118091.0, 118096.0, 118100.0, 118102.0, 118109.0, 118110.0, 118114.0, 118120.0, 118125.0, 118137.0, 75584.0, 118146.0, 118152.0, 118159.0, 118160.0, 118163.0, 118164.0, 118167.0, 118189.0, 118196.0, 118197.0, 118200.0, 118204.0, 118210.0, 118212.0, 118213.0, 118217.0, 118224.0, 118227.0, 118230.0, 118234.0, 118236.0, 118239.0, 118240.0, 118241.0, 118244.0, 118248.04, 118250.0, 118251.0, 118257.0, 118260.0, 118263.0, 118268.0, 118269.0, 118272.0, 118273.54, 118278.0, 118280.0, 118281.0, 118289.0, 118294.0, 118299.0, 118300.0, 118301.0, 118320.0, 118325.0, 118326.0, 118329.0, 118332.0, 118340.0, 118346.0, 118350.0, 118352.0, 118353.0, 118354.61, 118360.0, 118365.0, 118374.0, 118375.0, 118377.0, 118384.0, 118388.0, 118392.45, 118392.0, 118395.67, 118400.0, 118404.0, 118407.0, 118410.0, 118412.0, 118414.4, 118416.0, 118418.0, 118420.0, 118421.0, 118423.0, 118428.0, 118431.0, 118432.0, 118435.0, 75643.88, 118441.0, 118443.0, 118444.0, 118446.0, 118450.0, 118453.0, 118455.0, 118456.0, 118461.0, 118479.0, 118480.0, 118481.0, 118484.72, 118486.0, 118488.0, 118495.0, 118496.0, 118496.4, 118497.84, 118500.0, 118507.0, 118508.0, 118515.0, 118516.0, 118519.76, 118520.0, 118527.0, 118530.0, 118532.0, 118534.0, 118536.0, 118537.0, 118540.0, 118544.0, 118550.0, 118553.0, 118555.0, 118560.0, 118565.0, 118566.0, 905000.0, 118572.0, 118575.0, 118578.0, 118579.0, 118580.0, 118581.58, 118582.0, 118583.0, 118589.69, 118591.0, 118594.0, 118594.08, 118597.0, 118600.0, 118608.0, 118609.0, 118611.0, 118612.0, 118615.0, 118617.38, 118627.0, 118636.25, 118639.0, 118640.0, 118641.0, 118650.0, 118651.0, 118664.0, 118668.0, 118670.89, 118674.0, 118676.74, 118678.0, 118680.0, 118682.0, 118686.0, 118688.0, 118700.0, 118701.0, 118705.0, 118708.0, 118710.41, 118711.0, 118716.0, 118719.0, 118720.0, 118737.0, 118739.0, 118740.0, 118741.0, 118742.0, 118745.0, 118747.75, 118750.0, 118752.0, 118765.0, 118773.0, 118774.0, 118779.91, 118779.0, 118781.0, 118790.0, 118796.0, 118797.0, 118799.98, 118800.0, 118804.0, 118806.0, 118807.55, 118812.0, 118820.0, 118824.0, 118830.0, 118836.0, 118839.0, 118841.0, 118842.0, 118843.45, 118851.0, 118855.0, 381000.0, 118856.0, 118868.0, 118872.0, 118874.0, 118878.0, 118880.0, 118884.0, 118885.56, 118900.0, 118901.0, 118905.0, 118906.29, 118909.0, 118912.0, 118920.0, 118927.0, 118934.0, 118937.0, 118938.0, 118944.0, 118950.0, 118952.0, 118954.46, 118954.0, 118956.76, 118956.0, 118961.0, 118961.62, 118964.0, 118970.0, 118971.0, 118974.0, 118975.0, 118980.0, 118986.0, 118986.25, 118989.0, 118992.0, 118995.0, 118999.0, 119000.0, 119002.0, 119004.0, 119011.0, 119012.0, 119015.0, 119016.0, 119021.0, 119025.0, 119027.0, 119038.0, 119039.0, 119040.0, 119041.0, 119045.0, 119046.57, 119050.0, 119054.0, 119059.0, 119061.0, 119072.0, 119073.0, 119076.0, 119080.0, 119082.0, 119099.0, 119100.0, 119104.0, 119108.0, 119112.0, 119113.0, 119114.0, 119116.0, 119118.0, 119120.0, 119121.0, 119124.0, 119125.0, 119130.0, 119142.0, 119147.0, 119150.0, 119153.0, 119160.0, 119163.0, 119165.0, 119168.0, 119170.0, 119177.6, 119180.0, 119182.81, 119184.0, 119190.0, 119192.0, 119196.0, 119200.0, 119205.0, 119206.0, 119210.0, 119213.0, 119215.0, 119216.0, 119228.12, 119233.0, 119234.0, 119236.0, 119238.0, 119240.0, 119248.0, 119249.0, 119250.0, 119251.0, 75805.6, 119254.0, 119255.0, 119258.0, 119260.0, 5100000.0, 119266.0, 119267.0, 119268.0, 119271.0, 119275.0, 119279.0, 119284.0, 119288.0, 119289.0, 119291.0, 119298.0, 119300.0, 119303.17, 381450.0, 119320.0, 119322.0, 119324.79, 119325.0, 119328.0, 119331.0, 119332.0, 119333.0, 119340.0, 119342.0, 119345.0, 119350.0, 119352.0, 119353.0, 381500.0, 119360.0, 119361.0, 119364.0, 119371.0, 119374.0, 119376.0, 119384.0, 119392.0, 119400.0, 119402.0, 119406.0, 119412.0, 119424.0, 119425.0, 119427.0, 119432.05, 119433.0, 119444.0, 119446.0, 119450.0, 119453.0, 119454.0, 119457.0, 119460.0, 119463.0, 119464.0, 119466.0, 119470.0, 119472.0, 119475.0, 119478.0, 119489.0, 119491.0, 119492.0, 119496.0, 119497.0, 119500.0, 119506.0, 119507.0, 75856.0, 119515.0, 119517.0, 119520.0, 119522.0, 119527.0, 60760.92, 119533.0, 119535.0, 119537.0, 119542.0, 119545.0, 119546.0, 119550.0, 119553.0, 119560.0, 119564.0, 119568.0, 119579.0, 119580.0, 119583.0, 119586.0, 119596.0, 119597.0, 119600.0, 119604.0, 75878.12, 119622.0, 119623.0, 119624.0, 119625.0, 119632.0, 119638.0, 119639.0, 119640.0, 119645.0, 119650.0, 119652.18, 119653.0, 119654.0, 119658.0, 119660.0, 119665.0, 119668.0, 119675.0, 119683.0, 119688.0, 119691.0, 119700.0, 119707.0, 119713.0, 119719.0, 119720.0, 119738.0, 119744.0, 119750.0, 119759.0, 119760.0, 119764.0, 119768.0, 119768.4, 119770.0, 119771.0, 119775.0, 119776.0, 119780.0, 63375.83, 119788.0, 119789.0, 119790.0, 119792.0, 119794.0, 119800.0, 381952.0, 119809.0, 119813.0, 119820.0, 119826.0, 119829.0, 119836.0, 119837.0, 119838.0, 119840.0, 119850.0, 382000.0, 119856.0, 119860.0, 119867.0, 119869.0, 119872.0, 119878.0, 119880.0, 119888.0, 119889.6, 119890.0, 119896.0, 119898.0, 119900.0, 119903.0, 119904.0, 119905.0, 119911.0, 119912.0, 119913.0, 119914.0, 119916.0, 119920.0, 119921.0, 119925.0, 119929.0, 119930.0, 119937.0, 119945.0, 119947.0, 119948.0, 119958.0, 119961.0, 119964.0, 119967.0, 119970.0, 119971.0, 119974.0, 119975.0, 119976.0, 119978.0, 119981.0, 119987.0, 119988.0, 119992.0, 119995.0, 119996.0, 119997.0, 119999.0, 120000.0, 120001.0, 120004.0, 120012.0, 120015.0, 120016.0, 120020.0, 120024.0, 120025.0, 120029.02, 120030.0, 120036.0, 120037.0, 120046.0, 120048.0, 120056.0, 120072.4, 120072.0, 120074.0, 120078.0, 120080.0, 120088.0, 382233.0, 120090.0, 120092.0, 120093.0, 120094.0, 120096.0, 120100.0, 120105.0, 120108.0, 120111.0, 120115.0, 120117.0, 120119.0, 120120.0, 120123.0, 120123.33, 120128.71, 120136.0, 120140.0, 120150.0, 120154.0, 120155.0, 120156.0, 120160.0, 120162.0, 120167.0, 120175.0, 120176.0, 120179.0, 120183.0, 120185.0, 120192.0, 120198.0, 120200.0, 120202.0, 120210.0, 120223.0, 120224.0, 120227.0, 120228.0, 120234.0, 120238.92, 120239.0, 120245.0, 120247.0, 120248.0, 120250.0, 120251.0, 120252.0, 120255.0, 120260.4, 120265.0, 120273.0, 120274.0, 120286.0, 120289.0, 120290.0, 120295.0, 120300.0, 120303.39, 120304.0, 120312.14, 120322.83, 120328.0, 120336.0, 120347.0, 120348.0, 120350.0, 120354.0, 120355.0, 382500.0, 120357.0, 120360.0, 120362.0, 382509.0, 120366.05, 120380.0, 120384.0, 120386.0, 120388.0, 120390.0, 120397.0, 120400.0, 120406.0, 120408.0, 120410.0, 120420.0, 120421.9, 120423.0, 120425.0, 120426.03, 120426.82, 120427.0, 120429.0, 120432.0, 120435.0, 120438.78, 120443.0, 120446.0, 120450.0, 120456.0, 120460.0, 120467.0, 120468.0, 120470.0, 120474.0, 120476.0, 120479.0, 120480.0, 120483.0, 120492.0, 120499.0, 120500.0, 120504.0, 120506.0, 120508.25, 120510.0, 120511.0, 120517.0, 120524.0, 120525.0, 120528.0, 120533.0, 120534.0, 120535.0, 120540.0, 120543.0, 120544.0, 120548.0, 120550.0, 120555.0, 120560.0, 120572.0, 120575.0, 120577.0, 120579.0, 120581.0, 120586.0, 120587.0, 120588.0, 120590.0, 120592.0, 120593.0, 120594.0, 120600.0, 120602.4, 120606.0, 120609.0, 120610.0, 120612.0, 120615.0, 120617.0, 120618.6, 120620.0, 120623.0, 120633.0, 120640.0, 120646.0, 120650.0, 120655.0, 120660.0, 120666.0, 120668.0, 120670.0, 120671.0, 120675.0, 120676.15, 120677.0, 120679.0, 120680.0, 120686.0, 120690.0, 120692.0, 120693.0, 120698.0, 120700.0, 120705.0, 645000.0, 120713.64, 120718.08, 120720.0, 120722.0, 120729.0, 120730.0, 120732.0, 120733.0, 120736.0, 120744.0, 120748.0, 120748.7, 120750.0, 120757.0, 120760.0, 120764.0, 120771.0, 120773.9, 120774.0, 120775.0, 120776.0, 120780.0, 120783.0, 382929.0, 120791.0, 120792.0, 120800.0, 120803.0, 120805.0, 120809.0, 120816.0, 120819.0, 120822.0, 382971.41, 120830.0, 120831.0, 120832.0, 120835.0, 120838.0, 120846.0, 120847.21, 120850.0, 120853.0, 383000.0, 120860.0, 120864.0, 120865.0, 120870.0, 120872.0, 120876.0, 120879.0, 120880.0, 120888.0, 120892.0, 120900.0, 120901.0, 120902.34, 120909.0, 120911.0, 120912.0, 120923.0, 120926.0, 120948.0, 120960.0, 120965.0, 120969.0, 120975.0, 120978.87, 120979.0, 120980.0, 120982.0, 120985.0, 120987.0, 120996.0, 120997.87, 120999.0, 121000.0, 120999.84, 120999.96, 121008.0, 121009.0, 121018.0, 121022.0, 121023.0, 121035.0, 121037.0, 121057.0, 121064.0, 121065.0, 121067.0, 121069.0, 121070.0, 121076.0, 121082.0, 121084.0, 121086.42, 121097.0, 121100.0, 121108.0, 121111.0, 121117.0, 121120.0, 121121.0, 121122.0, 121126.0, 121128.0, 121133.22, 121136.0, 121151.96, 121160.0, 121166.0, 121168.0, 121172.0, 121174.0, 121178.0, 121180.0, 121183.0, 121184.0, 121185.0, 121198.0, 121200.0, 383360.0, 121216.0, 121222.0, 121230.0, 121240.0, 121250.0, 121257.0, 121260.0, 121261.0, 121265.0, 121272.0, 121275.0, 121276.0, 121277.0, 121279.0, 121284.96, 121284.0, 121286.0, 121292.17, 121296.0, 121300.0, 121302.0, 121303.0, 121305.0, 121316.0, 121320.0, 121323.0, 121325.0, 121337.0, 121337.5, 121345.0, 121346.0, 121348.0, 121350.0, 121352.52, 121362.0, 121363.0, 121364.0, 121368.0, 121375.0, 121376.0, 121380.0, 121381.0, 121386.0, 121387.0, 121392.0, 121393.0, 121395.0, 121398.0, 121400.0, 121402.0, 121405.0, 121410.0, 121411.0, 121415.0, 121417.0, 121418.0, 121422.0, 121423.0, 121427.0, 121429.0, 121430.0, 121433.0, 121438.56, 121440.0, 121450.0, 121452.0, 121453.0, 121457.0, 121460.0, 1956468.0, 121469.0, 121470.0, 121471.0, 121472.0, 121476.0, 121479.0, 121480.0, 121483.0, 121488.0, 121494.0, 121500.0, 121501.0, 121505.0, 121509.0, 121516.0, 121520.0, 121524.0, 121530.0, 121533.82, 121545.0, 121555.0, 121560.0, 121562.0, 121568.0, 908000.0, 121571.0, 121584.0, 121588.0, 121591.0, 121596.0, 121596.8, 121599.0, 121600.0, 121605.0, 121612.0, 121614.0, 121615.0, 121617.0, 121620.0, 121628.0, 121629.0, 121634.0, 121635.0, 121640.0, 121644.0, 121646.0, 121648.0, 121650.0, 121652.0, 121656.0, 121657.0, 121659.2, 121659.0, 121664.0, 121665.24, 121669.0, 121677.0, 121680.0, 121683.0, 121685.0, 121687.0, 121688.0, 121689.0, 121700.0, 121703.0, 121707.0, 121709.0, 646000.0, 121713.0, 121716.0, 121722.0, 121723.0, 121726.0, 121733.0, 121734.0, 121738.0, 121740.0, 121741.0, 121746.0, 121749.0, 121750.0, 121752.0, 121752.36, 121755.0, 121760.0, 121761.0, 121769.36, 121770.0, 121776.0, 121787.0, 121792.0, 121799.0, 121800.0, 121814.0, 121815.0, 121817.0, 121826.0, 121827.0, 121829.0, 121847.0, 76321.0, 121850.0, 121851.0, 121852.0, 384000.0, 121858.0, 121858.02, 76324.68, 121869.0, 121870.0, 121875.0, 121877.0, 121878.0, 121890.0, 121890.6, 121891.0, 121893.0, 121898.0, 121900.0, 121902.0, 121903.0, 121904.0, 121908.0, 121920.0, 121922.0, 121932.0, 121939.0, 121944.0, 121947.0, 121948.0, 76341.0, 121950.0, 121952.0, 121970.0, 121975.0, 121977.0, 121980.0, 121984.0, 121985.0, 121986.84, 121987.93, 121992.0, 121993.0, 121997.0, 122000.0, 122002.0, 122003.0, 122004.0, 384149.0, 122012.0, 122018.0, 122024.0, 122027.0, 122033.0, 122038.0, 122040.0, 384186.74, 122043.0, 122048.0, 122054.0, 122069.0, 122070.0, 122072.0, 122073.0, 122078.0, 122080.0, 122082.72, 122085.0, 122089.0, 122097.0, 122100.0, 122106.0, 122110.0, 122111.0, 7200000.0, 122115.0, 122118.0, 122119.0, 122120.0, 122130.0, 122132.0, 122152.0, 122169.6, 122171.0, 122172.0, 122173.0, 122175.0, 122180.0, 122184.0, 122192.0, 122196.0, 122200.0, 122207.0, 122208.0, 122210.0, 122212.0, 122215.0, 122218.0, 122220.0, 122230.0, 122232.0, 122240.0, 122241.0, 122244.0, 122248.0, 122249.0, 122250.0, 122251.0, 646540.0, 122260.0, 122269.0, 122270.0, 122271.0, 122274.0, 122275.0, 122278.0, 122284.0, 122290.0, 122297.0, 122298.0, 122300.0, 122304.0, 122308.0, 122320.0, 122322.0, 122328.0, 122335.0, 122336.0, 122342.0, 122350.0, 122352.0, 122353.0, 122354.0, 122358.0, 122360.0, 122362.0, 122363.0, 122362.55, 122365.0, 122375.0, 122379.0, 122387.0, 122389.0, 122390.0, 122395.0, 122400.0, 122401.0, 122406.0, 122407.0, 122409.0, 122412.0, 122413.0, 122415.0, 122416.0, 122418.0, 122420.0, 122425.0, 122432.0, 122433.0, 122442.0, 122443.0, 122450.0, 122452.0, 122453.0, 122453.16, 122458.0, 122470.0, 122473.0, 122474.0, 122476.0, 122477.0, 122481.0, 122486.0, 122487.0, 122488.0, 384635.0, 122496.0, 122497.26, 122500.0, 122504.0, 122507.0, 122512.0, 122520.0, 122524.0, 122532.0, 122544.0, 76460.88, 122549.0, 122550.0, 122557.0, 122571.0, 122580.0, 122582.0, 122586.0, 122589.0, 122590.0, 122593.0, 122600.0, 122601.0, 122605.0, 122605.38, 122616.0, 122618.0, 122628.5, 122628.0, 122635.0, 122640.0, 122641.0, 122643.28, 122645.0, 122646.0, 122650.0, 122655.0, 122665.2, 122665.0, 122675.0, 76486.0, 122678.0, 122681.0, 122684.4, 122688.0, 122689.0, 122693.0, 122694.0, 122697.12, 122700.0, 122708.0, 122715.0, 122718.0, 122720.0, 122723.8, 122730.0, 122733.82, 122736.0, 122738.0, 122744.0, 122747.0, 122748.0, 122747.5, 122750.0, 122752.0, 122760.0, 122762.0, 122763.0, 122768.0, 122771.0, 122780.0, 122783.0, 122784.0, 122796.0, 122800.0, 122801.12, 122815.0, 122820.0, 122828.0, 122830.0, 122832.32, 122832.0, 122850.0, 122851.0, 385000.0, 122861.0, 122865.0, 122867.77, 122869.0, 122870.0, 122872.0, 122875.0, 122876.0, 122880.0, 122883.0, 122889.0, 122892.0, 122893.0, 122900.0, 122904.0, 122913.0, 122915.0, 122920.0, 122927.76, 122931.0, 122934.0, 122938.0, 122941.0, 122950.0, 122952.0, 122960.0, 122964.0, 122981.0, 122991.0, 122999.0, 123000.0, 123003.0, 123004.0, 123005.0, 123005.32, 123006.0, 123012.0, 123019.31, 123019.0, 123023.0, 123024.0, 123026.0, 123028.0, 123030.0, 123032.0, 123034.0, 123035.0, 123040.0, 123042.0, 123046.0, 123050.0, 123051.76, 123052.8, 123052.0, 123053.0, 123063.0, 123064.0, 909500.0, 123070.0, 123072.0, 123077.0, 123079.26, 123091.0, 123100.0, 123101.0, 123106.0, 123117.0, 123120.0, 123123.0, 123126.0, 123127.38, 123129.0, 123132.0, 123134.0, 123136.0, 123139.0, 123144.0, 123146.0, 123150.0, 385302.0, 123162.0, 123168.99, 123168.0, 123170.0, 123169.0, 123175.0, 123176.0, 123178.0, 123182.0, 123183.0, 123192.0, 123194.0, 123200.0, 123204.0, 123214.0, 123219.0, 123222.0, 123223.0, 76596.16, 123233.0, 123234.0, 123235.0, 123237.0, 123238.0, 123239.0, 123240.0, 123250.0, 123251.0, 123265.0, 123269.0, 123270.0, 123273.0, 123277.0, 123279.0, 123285.0, 123291.0, 123296.0, 123299.0, 123300.0, 123315.0, 123321.63, 123324.0, 123326.0, 123332.0, 123336.0, 123340.0, 123344.0, 123345.0, 123357.0, 123358.0, 123360.0, 123364.0, 123373.0, 123376.0, 123379.0, 123380.0, 123387.0, 123388.0, 123396.0, 123400.0, 123406.0, 123408.0, 123413.0, 123416.0, 123432.0, 123433.0, 123436.0, 123450.0, 123455.0, 123456.0, 123460.0, 123463.0, 123465.0, 123469.0, 123470.0, 123472.0, 123480.0, 123485.0, 123489.0, 123490.0, 123492.0, 123500.0, 123501.0, 123504.0, 123508.0, 123512.0, 123515.0, 123519.0, 123524.0, 123528.0, 123531.24, 8250000.0, 123540.0, 123544.59, 123547.0, 123550.0, 123551.0, 123560.0, 123562.0, 910000.0, 123568.0, 123572.0, 123573.0, 123575.16, 123578.0, 123580.0, 123587.0, 123598.0, 123599.0, 123600.0, 123605.0, 123613.0, 123616.0, 123620.0, 123625.0, 123633.0, 123641.0, 123643.0, 123645.0, 123646.0, 123650.0, 123656.0, 123658.0, 123661.0, 123662.0, 24579.0, 123667.0, 123676.0, 123677.0, 123696.0, 123699.0, 123700.0, 123702.0, 123704.0, 123709.0, 648000.0, 123717.0, 123720.0, 123721.0, 123725.0, 123727.0, 123731.0, 123735.0, 123738.0, 123745.0, 123749.0, 123750.0, 123751.0, 123754.0, 123756.0, 123760.0, 123764.0, 123778.0, 123786.0, 123787.12, 123798.0, 123800.0, 123802.0, 123804.0, 123809.0, 123820.0, 385967.0, 123824.0, 123830.0, 123832.0, 123836.0, 123840.0, 123841.0, 123847.0, 123848.0, 123850.0, 123852.0, 123853.0, 123854.0, 386000.0, 123861.0, 123866.0, 123874.0, 123875.0, 123878.0, 123880.0, 123888.0, 123895.0, 123897.0, 123900.0, 123902.0, 123911.0, 123917.64, 123920.0, 123926.0, 123932.52, 123943.0, 123948.0, 123950.0, 123959.0, 123960.0, 123962.0, 386107.0, 123964.0, 123963.0, 123970.0, 123973.0, 123974.0, 123979.92, 123987.0, 123988.0, 123989.0, 123990.16, 123990.0, 123992.0, 123996.0, 123999.0, 124000.0, 124002.0, 124003.0, 124008.0, 124011.0, 124012.0, 124031.0, 124034.0, 124036.0, 124037.0, 124044.0, 124045.0, 124053.0, 124059.0, 124061.0, 124063.0, 124067.0, 124069.0, 124072.0, 124073.0, 124075.0, 124078.0, 124080.0, 124082.0, 124090.0, 124092.0, 124100.0, 124101.0, 124102.0, 386250.0, 124111.0, 124115.0, 124116.0, 124122.0, 124123.0, 124128.0, 124129.0, 124140.0, 62897.57, 124144.0, 124152.8, 124165.0, 124167.0, 124169.0, 124172.0, 124173.0, 124175.0, 124176.0, 124178.33, 124179.0, 124189.0, 124191.0, 124195.0, 124200.0, 124213.0, 124218.0, 124222.0, 124224.0, 386370.0, 124227.3, 124231.0, 124240.0, 124243.0, 124247.0, 124248.0, 124249.68, 124250.0, 124268.0, 124270.0, 124288.0, 124299.06, 124300.0, 124302.0, 124306.2, 124312.0, 124318.0, 124319.0, 124320.0, 124322.0, 124325.0, 124329.28, 124330.0, 124332.0, 124334.0, 124335.31, 124337.0, 386489.0, 124347.38, 124354.1, 124356.0, 124360.0, 124365.78, 124368.0, 124377.0, 124380.0, 124384.0, 124386.0, 124390.0, 124399.0, 124400.0, 124410.0, 124415.0, 124420.0, 124425.0, 124428.0, 124430.0, 124432.0, 124433.0, 124434.0, 124436.0, 124439.0, 124440.0, 124443.0, 124446.0, 124447.0, 124450.0, 124452.0, 124458.0, 124460.0, 124465.0, 124469.0, 124486.15, 124488.0, 124490.0, 124499.61, 124500.0, 124504.0, 124510.0, 124516.0, 124520.0, 124523.0, 124526.0, 124528.0, 124529.34, 124535.0, 124540.0, 124542.0, 124546.0, 124548.0, 124550.4, 124560.0, 124563.0, 124569.0, 124576.0, 124580.0, 124582.0, 124585.0, 124587.0, 124596.0, 124599.0, 124600.0, 124616.0, 124621.0, 124624.0, 124627.68, 124629.0, 124632.0, 124639.0, 124640.0, 124644.0, 124645.0, 124650.0, 124658.0, 124658.81, 124660.0, 124665.0, 124676.0, 124682.0, 124683.0, 124685.0, 124688.0, 124690.12, 124695.0, 124697.0, 124700.0, 124702.0, 124704.0, 124716.0, 124720.0, 124726.0, 124730.0, 124734.0, 124737.0, 124740.0, 76897.6, 124750.0, 124753.0, 124758.4, 124764.0, 124766.0, 124775.0, 124777.0, 124784.0, 124786.0, 124789.0, 124798.0, 124799.0, 124800.0, 124802.6, 124805.0, 124815.0, 124819.0, 124820.0, 124824.0, 124833.0, 124837.0, 124840.0, 124841.0, 124845.0, 124847.81, 124848.0, 124849.0, 124850.0, 124847.0, 386996.0, 124851.0, 124854.0, 124856.96, 387000.0, 124858.0, 124870.0, 124871.45, 124872.0, 124875.0, 124883.2, 124886.0, 124889.0, 124890.0, 124891.0, 124894.0, 124896.0, 124897.0, 124900.0, 124908.0, 124920.0, 124921.0, 124923.0, 124925.0, 124936.0, 124939.0, 124941.0, 124945.0, 124946.0, 124949.0, 124950.0, 124951.0, 124955.0, 124956.0, 124960.75, 124960.0, 9300000.0, 124968.0, 124970.0, 124972.0, 124980.0, 124981.0, 124987.0, 124988.0, 124990.0, 124992.0, 124995.0, 124998.0, 124999.0, 125000.0, 125001.0, 125002.25, 125000.32, 125004.0, 125005.0, 125003.0, 125002.0, 125008.0, 125009.0, 125012.17, 125016.0, 125020.0, 125026.0, 125036.0, 125040.0, 125041.0, 9300086.0, 125053.0, 125055.0, 125059.0, 125060.0, 125061.8, 125065.0, 125067.0, 125073.0, 125076.0, 125080.0, 125081.0, 125087.0, 125099.0, 125100.0, 125120.0, 125122.0, 125123.0, 125125.0, 125127.0, 125132.0, 125133.0, 125138.0, 125146.0, 125148.0, 125149.0, 125153.6, 125154.0, 125153.0, 125162.0, 125170.0, 125173.0, 125175.0, 125177.0, 125186.0, 125188.0, 125190.0, 125194.0, 125195.2, 125200.0, 125201.0, 125208.0, 125213.0, 125216.0, 125220.0, 125221.0, 125225.0, 125228.0, 125232.0, 125234.0, 125244.0, 125250.0, 125258.0, 125268.0, 125271.0, 125279.0, 125280.0, 125282.0, 125290.0, 125293.0, 125300.0, 125311.0, 125313.0, 125320.0, 1173900.0, 125324.0, 125330.0, 125332.0, 125333.0, 125336.0, 125340.0, 125345.0, 125350.0, 125352.0, 387500.0, 125357.0, 125365.0, 125367.0, 125367.48, 125371.0, 125374.0, 125375.04, 125378.0, 125379.0, 125381.0, 125384.16, 125385.0, 125384.0, 125388.0, 125389.0, 125400.0, 125412.0, 125417.0, 125423.0, 125424.0, 125430.0, 125441.0, 125442.0, 125444.0, 125446.0, 125449.0, 125450.0, 125452.24, 125455.0, 125460.0, 125462.0, 125469.0, 125472.0, 125475.0, 125481.0, 125486.0, 125488.0, 125488.04, 125489.0, 125493.0, 125495.0, 125496.0, 125497.0, 125500.0, 125511.0, 125512.0, 125521.0, 125540.0, 125543.0, 125544.0, 125548.0, 125550.0, 125555.0, 125557.0, 387706.0, 125564.0, 125565.0, 125568.48, 912000.0, 125570.0, 125573.0, 125574.67, 125576.0, 125580.0, 125582.0, 125584.0, 125590.0, 125590.4, 125600.0, 125605.09, 125608.0, 125610.0, 125613.0, 125615.3, 125622.0, 125624.0, 125627.0, 125629.0, 125630.0, 125632.76, 125632.0, 125635.0, 125640.0, 125641.0, 125645.0, 125650.0, 125652.0, 125657.0, 125658.0, 125660.0, 125667.0, 125670.0, 125671.0, 387817.0, 125676.0, 1174260.0, 125685.0, 125691.0, 125694.91, 125694.0, 387840.0, 125697.0, 125700.0, 125703.0, 125706.0, 125710.0, 650000.0, 125714.72, 125715.0, 125715.12, 125734.0, 125740.0, 125744.4, 125750.0, 125752.0, 125771.0, 125775.0, 125782.0, 125785.0, 125788.0, 125789.0, 125793.0, 125795.0, 125797.36, 125798.0, 125797.0, 125800.0, 125808.68, 125808.0, 125810.0, 125824.0, 125825.0, 125830.0, 125831.0, 125832.0, 125835.0, 125839.56, 125840.0, 125843.0, 125846.0, 125850.0, 388000.0, 125858.0, 125860.0, 125861.0, 125862.0, 25016.0, 125866.0, 125869.0, 125870.0, 125874.0, 125875.0, 125876.0, 125877.0, 125880.0, 125888.0, 125893.0, 125899.0, 125900.0, 125908.0, 125911.0, 125917.0, 125920.0, 125925.0, 125928.0, 125933.79, 125935.0, 125937.56, 125938.0, 125940.0, 125950.0, 125954.0, 125956.0, 125957.0, 125960.0, 125964.0, 125965.0, 125968.0, 125975.0, 388121.0, 125988.0, 125990.0, 125999.0, 126000.0, 126005.0, 126006.0, 126016.0, 126019.0, 126020.0, 126026.0, 126031.0, 126032.0, 126034.0, 126035.0, 126044.0, 126049.0, 126050.0, 126053.0, 126060.0, 126062.0, 126063.0, 126073.0, 126074.84, 126075.0, 126080.0, 57113.0, 126091.0, 126094.0, 126100.0, 126101.0, 126107.28, 126109.0, 126112.0, 126115.0, 126116.0, 126120.0, 126126.0, 126132.0, 126139.0, 126150.0, 126151.0, 126152.0, 126156.8, 126159.0, 126160.0, 126168.0, 126171.0, 126180.0, 126183.0, 126188.0, 126189.0, 126192.0, 126196.0, 126200.0, 126202.0, 126210.0, 126217.0, 126218.0, 126219.0, 126225.0, 126235.2, 126245.0, 126250.0, 126250.08, 126251.0, 126252.0, 126254.0, 126267.0, 126270.0, 126282.0, 126284.0, 126288.0, 126297.0, 126298.0, 126300.0, 126310.0, 126313.0, 126314.0, 126317.0, 126318.0, 126319.2, 126320.0, 126321.0, 126324.0, 126326.0, 126350.0, 126354.0, 126358.44, 126360.0, 126373.0, 126376.0, 126377.0, 126379.83, 126384.0, 126388.0, 126391.0, 126392.0, 126400.0, 126401.0, 126402.0, 126408.0, 126420.0, 126422.0, 126432.0, 126440.0, 126447.0, 126450.0, 126453.0, 126460.0, 77239.0, 126474.0, 126476.0, 126480.0, 126490.9, 126492.0, 126493.0, 126498.0, 126500.0, 126504.0, 126508.0, 126510.0, 126512.0, 126514.0, 126515.0, 126518.0, 126522.0, 126524.0, 126535.0, 8253000.0, 126538.0, 126543.0, 126544.0, 126545.16, 126545.0, 126560.0, 126563.0, 126568.0, 126579.0, 126580.0, 126583.0, 126587.5, 126588.0, 126589.0, 126600.0, 126604.0, 126605.0, 126607.0, 126610.0, 126620.0, 126625.0, 126626.0, 126629.0, 126630.0, 388778.0, 126639.0, 126640.0, 126641.88, 126641.0, 126665.0, 126667.0, 126670.0, 126672.0, 126675.0, 126676.0, 126682.0, 126684.0, 126685.0, 126690.0, 126694.0, 126696.0, 126697.0, 126700.0, 126702.0, 126709.84, 651000.0, 126720.0, 126732.0, 126744.0, 126745.0, 126746.0, 126750.0, 126767.0, 126769.0, 126770.0, 126776.0, 126780.0, 126784.0, 126787.0, 126787.62, 126789.4, 126790.0, 126795.0, 126800.0, 126803.0, 126804.0, 126805.0, 126816.0, 126818.0, 126825.0, 126833.0, 126834.0, 126839.0, 126842.0, 126850.0, 389000.0, 126857.33, 126858.0, 126860.0, 126864.0, 126866.0, 126872.0, 126875.0, 126880.0, 126884.0, 126887.0, 126892.0, 126900.0, 126910.0, 126912.0, 126920.77, 126924.0, 126932.0, 126939.0, 126940.0, 126942.0, 126949.0, 126950.0, 126954.0, 126958.0, 126959.52, 126960.0, 126980.0, 126981.0, 126989.0, 126996.0, 126998.0, 126999.0, 127000.0, 127004.0, 127006.0, 25244.0, 127011.0, 127016.0, 127020.0, 127025.0, 127026.0, 127028.0, 127030.8, 127032.0, 127032.51, 127035.0, 127036.0, 127038.0, 127046.4, 127064.0, 127068.0, 127070.0, 127088.0, 77363.7, 127092.0, 77365.0, 127100.0, 127104.0, 127105.0, 127110.48, 127115.0, 127120.0, 127127.6, 127128.0, 1700000.0, 127140.0, 127144.0, 127151.0, 127156.0, 127158.0, 127163.0, 127164.0, 127165.0, 127168.0, 127171.0, 127174.0, 127178.0, 127183.0, 127192.0, 127200.0, 127202.0, 127204.0, 127206.0, 127207.0, 127212.0, 127213.0, 127219.0, 127220.0, 77389.0, 127222.0, 127228.0, 127229.0, 127235.0, 127239.0, 127242.0, 127245.0, 127249.0, 127250.0, 127252.0, 127255.0, 127260.0, 127264.0, 127270.0, 127272.0, 127284.0, 127296.0, 127300.0, 127301.0, 127310.0, 127326.0, 127336.5, 127337.0, 127338.0, 127342.0, 127343.0, 127348.0, 127350.0, 127352.0, 127356.0, 389500.0, 127358.0, 127361.0, 127364.0, 127377.0, 127380.0, 127384.0, 127385.52, 127389.0, 127390.0, 127400.0, 127404.0, 127416.0, 127421.0, 1176000.0, 127425.0, 127430.0, 127433.0, 127436.0, 127440.0, 127441.0, 127444.0, 127445.0, 127450.0, 127453.0, 127456.0, 127464.0, 127465.0, 127472.0, 127476.0, 127477.0, 127480.0, 127488.0, 127500.0, 127504.0, 127509.0, 127512.0, 127519.0, 127520.0, 127522.0, 127524.28, 127525.0, 127524.0, 127535.0, 127535.41, 127538.0, 127553.0, 127560.0, 127563.0, 127565.0, 127566.4, 127567.0, 127566.0, 127578.96, 127580.0, 127588.0, 127592.0, 127594.05, 127596.0, 127600.0, 127606.0, 127608.0, 127610.0, 127611.0, 127620.0, 127621.0, 127623.0, 127625.0, 127626.0, 127627.0, 127629.0, 127630.0, 127637.66, 127639.0, 127640.0, 127640.84, 127643.0, 127644.0, 127649.0, 127650.0, 127651.0, 127659.0, 127660.0, 127661.0, 127662.0, 127664.0, 127670.0, 127672.0, 127677.0, 127680.0, 127696.0, 127698.0, 127700.0, 127703.0, 127704.0, 651996.0, 652000.0, 127713.0, 127712.0, 127716.0, 127721.0, 127728.0, 127730.0, 127738.0, 127740.9, 127741.0, 127745.0, 127749.0, 127752.0, 127756.0, 127763.51, 127765.0, 127766.0, 127777.95, 127783.0, 127788.0, 127790.2, 127800.0, 127801.0, 127802.0, 127807.0, 127810.0, 127812.0, 127836.33, 127840.0, 127847.0, 127850.0, 127855.0, 390000.0, 127860.0, 127868.0, 127880.0, 127883.0, 127885.0, 127888.0, 127890.0, 127895.0, 127899.0, 127900.0, 127901.0, 127920.0, 127924.0, 127925.0, 127926.0, 127930.0, 127938.04, 127941.0, 127958.0, 127963.0, 127965.0, 127968.0, 127969.0, 127970.0, 127980.0, 127980.8, 127982.0, 127990.0, 127992.0, 127999.0, 128000.0, 128004.0, 128014.0, 128016.0, 128031.0, 128034.0, 128037.0, 128040.0, 128051.0, 128055.0, 128063.0, 128080.0, 128082.0, 128085.0, 128091.0, 128092.0, 128100.0, 128105.0, 128106.0, 128109.0, 128114.0, 128120.0, 128124.0, 128125.0, 128132.0, 128134.0, 128148.0, 128150.0, 128160.0, 128165.0, 128172.0, 128180.0, 128184.0, 25479.0, 128194.0, 128197.0, 128200.0, 25481.16, 128205.0, 128208.0, 128214.0, 128219.0, 128223.0, 128224.0, 128229.0, 128230.0, 128233.0, 128234.0, 128244.0, 128250.0, 128251.0, 390400.0, 128257.0, 128256.0, 128275.0, 128282.0, 128285.0, 128296.0, 128300.0, 128303.0, 128305.0, 128308.0, 128322.0, 128323.28, 128331.0, 128335.0, 128336.0, 128337.0, 128341.0, 128346.0, 128346.4, 128350.0, 128352.0, 128353.37, 128354.0, 128357.0, 128363.0, 128365.0, 128370.0, 128373.0, 128376.0, 128388.0, 128390.0, 128400.0, 128412.0, 128419.0, 128425.0, 128437.0, 128440.0, 128445.0, 128447.4, 128448.0, 128450.0, 128453.0, 128458.0, 128464.0, 128474.0, 128475.0, 128485.0, 128492.0, 128500.0, 128501.0, 128504.04, 128510.0, 77646.4, 128514.0, 128515.0, 128518.0, 128519.0, 128537.11, 77651.0, 128540.0, 77652.0, 128544.0, 128546.0, 128547.0, 77653.0, 128550.0, 128556.0, 128560.0, 128562.0, 128568.0, 128570.0, 128584.0, 128585.0, 128587.0, 128590.0, 128592.0, 128600.0, 128603.0, 128604.0, 128612.0, 128620.0, 128622.0, 128627.0, 128632.0, 128642.0, 128644.0, 128647.0, 128648.0, 128650.0, 128651.0, 128652.0, 128653.0, 128654.0, 128656.0, 128677.0, 128680.0, 128687.0, 128691.0, 128693.0, 128694.0, 128700.0, 128711.0, 128716.0, 128718.0, 128719.0, 128720.0, 128730.0, 128743.72, 128745.0, 128749.0, 128750.0, 128753.8, 128755.0, 128756.0, 128760.0, 128767.0, 128771.0, 128774.49, 128775.0, 128777.0, 128778.0, 128789.44, 128790.0, 128794.0, 128798.0, 128800.0, 128802.0, 128803.6, 128808.0, 128810.0, 128827.0, 128831.0, 128835.0, 128837.0, 128848.88, 128850.0, 128851.0, 391000.0, 128856.0, 128857.44, 128872.94, 128875.0, 128877.0, 128878.75, 128880.0, 128880.67, 128883.0, 128885.0, 128886.0, 128893.0, 128894.0, 128899.0, 128900.0, 128899.92, 128909.0, 128920.0, 128922.0, 128936.0, 128941.0, 128944.0, 128948.0, 128950.0, 128956.0, 128959.0, 128960.0, 128966.66, 128973.0, 128982.0, 128990.0, 128994.0, 129000.0, 129002.0, 129005.0, 129016.0, 129020.0, 129025.0, 129030.0, 129031.0, 129045.74, 129047.0, 129051.0, 129058.0, 129072.0, 129086.0, 129090.0, 129095.0, 129100.0, 129105.0, 129115.0, 129127.0, 129129.0, 129131.0, 129132.8, 129139.0, 129140.0, 129144.0, 129160.0, 129161.0, 129162.0, 129166.0, 129169.0, 129186.66, 129187.0, 129188.0, 129189.0, 129200.0, 129201.0, 129203.0, 129209.6, 129225.0, 129227.0, 129231.0, 129235.0, 129240.0, 129248.0, 129249.0, 129250.0, 129255.0, 129258.0, 129262.0, 129268.0, 129272.0, 1440000.0, 129282.0, 129295.0, 129300.0, 129307.82, 129311.0, 129312.0, 129320.0, 129321.0, 129322.75, 129323.0, 129325.0, 129329.0, 129330.0, 129332.0, 129342.0, 129343.0, 129345.24, 129350.0, 129355.0, 129360.0, 129363.96, 129368.0, 129368.55, 129375.0, 129388.0, 129390.0, 129391.0, 129393.36, 129396.0, 129399.0, 129400.0, 129404.0, 129404.9, 129408.0, 129411.0, 129411.38, 129416.0, 129420.0, 129424.0, 129426.0, 129449.0, 129450.0, 129451.0, 129454.0, 129456.0, 129459.0, 129462.0, 129474.0, 129475.0, 129476.0, 129477.0, 129480.0, 129482.0, 129485.0, 129487.0, 129489.0, 129492.0, 129496.0, 129500.0, 129504.0, 129517.0, 129518.0, 129527.0, 129530.0, 129540.0, 129545.0, 129547.0, 129550.0, 129560.0, 25751.9, 129572.0, 129580.0, 77859.2, 129585.0, 129588.0, 77860.44, 129592.0, 129600.0, 129607.8, 129613.97, 129620.0, 129623.0, 129625.0, 77868.08, 129637.0, 129642.0, 129648.21, 129650.0, 129653.0, 129656.0, 129663.0, 129672.0, 129678.0, 129680.0, 129688.0, 129693.0, 129695.0, 129696.0, 129697.0, 129699.0, 129700.0, 129710.0, 654000.0, 129714.0, 129716.0, 129717.0, 129723.0, 129726.0, 129727.49, 129730.0, 129732.0, 129734.0, 129735.0, 129740.0, 129748.0, 129750.0, 129757.0, 129758.0, 129760.0, 129770.0, 129774.72, 129775.0, 129780.0, 129789.0, 129792.0, 129800.0, 129805.0, 129808.0, 129812.0, 129818.0, 129827.0, 129834.0, 129836.0, 129840.0, 129843.0, 129850.0, 392000.0, 129861.73, 129863.0, 129868.0, 129869.0, 129870.0, 129872.0, 129886.0, 129890.0, 129896.0, 129900.0, 129912.0, 129913.0, 129935.0, 129950.0, 129958.0, 129959.0, 129965.04, 129984.0, 3800000.0, 129990.0, 129994.0, 129995.0, 129996.0, 129999.0, 130000.0, 130001.0, 130002.0, 130003.0, 130004.0, 130008.0, 130012.0, 130020.0, 130025.0, 130026.0, 130027.0, 130034.0, 130037.0, 392185.0, 130044.0, 130050.0, 130051.0, 130052.0, 392200.0, 130063.0, 130068.0, 130080.0, 130083.46, 130090.0, 130092.0, 130100.0, 130104.0, 130105.92, 130104.89, 130127.0, 130130.0, 130131.0, 392289.0, 130148.0, 130154.0, 130157.0, 130158.0, 130160.0, 130164.0, 130191.0, 130200.0, 130202.0, 130204.0, 130208.0, 130210.0, 130224.0, 130225.0, 130229.0, 130230.0, 130232.0, 130240.0, 130250.0, 130252.0, 130265.0, 130268.0, 130278.0, 130280.0, 130285.0, 130286.0, 130288.0, 130294.0, 130300.0, 130302.0, 130315.0, 130325.0, 130333.0, 130350.0, 130378.0, 130382.0, 130388.0, 130390.0, 130392.0, 130397.76, 130399.0, 130400.0, 130404.0, 130405.0, 130410.0, 130411.66, 130412.0, 130416.0, 130420.0, 130428.0, 130432.0, 130435.0, 130440.0, 130442.0, 392593.94, 130450.0, 130452.0, 130453.0, 130455.0, 130457.58, 392607.0, 130473.54, 130473.0, 130483.0, 130490.0, 130500.0, 130512.0, 130519.0, 130520.0, 130530.0, 130532.0, 130539.0, 130540.0, 130545.0, 130550.0, 130556.0, 130560.0, 130563.0, 130570.0, 130573.0, 130580.0, 130584.0, 130592.0, 130596.0, 130598.0, 130599.0, 130600.0, 130601.0, 130606.0, 130615.0, 130616.0, 130622.55, 130629.0, 130630.0, 130634.28, 130640.76, 130660.0, 130666.0, 130667.0, 130673.76, 130679.28, 130680.0, 130685.0, 130686.0, 130687.0, 130688.0, 130690.0, 130692.0, 130700.0, 130704.0, 130705.0, 130707.0, 25979.76, 130709.0, 655000.0, 130714.0, 130716.0, 130720.0, 130723.0, 130724.0, 130726.0, 130728.0, 130730.0, 130732.0, 130733.0, 130736.4, 130743.51, 130749.0, 130750.0, 130761.98, 78094.8, 130775.0, 130781.0, 130784.0, 130800.0, 130801.0, 130803.0, 130808.0, 130810.0, 130819.0, 130820.0, 130849.0, 130855.0, 393000.0, 130858.0, 130860.0, 130864.0, 130869.0, 130881.23, 130883.0, 130885.0, 130888.0, 130892.0, 130896.0, 130900.0, 130901.0, 130909.0, 130920.0, 130922.0, 130935.0, 130945.0, 130948.0, 130950.0, 130953.0, 130956.0, 130958.0, 130963.9, 130976.0, 130977.0, 130980.0, 130992.0, 130996.0, 131000.0, 131002.0, 131004.7, 131004.0, 131006.36, 131008.72, 131010.0, 131016.44, 131019.0, 131020.0, 131027.0, 131028.0, 131031.0, 131040.0, 131046.0, 131048.0, 131050.0, 131053.0, 131058.0, 131064.0, 131065.0, 131069.0, 131070.0, 131081.0, 131085.0, 393230.0, 131087.0, 131096.0, 131097.0, 131100.0, 131112.0, 131121.0, 131123.2, 131125.0, 131136.0, 131140.0, 131143.0, 131144.0, 131150.0, 131158.0, 131161.0, 131165.0, 131174.0, 131179.0, 131188.0, 131192.0, 131200.0, 131209.0, 131211.0, 131220.0, 131229.0, 131234.0, 131240.0, 131244.0, 131248.0, 131250.0, 131251.0, 131257.0, 131267.0, 131271.0, 393428.0, 131286.0, 131300.0, 131304.0, 131309.0, 131315.0, 131320.0, 131324.0, 131325.0, 131328.0, 131331.0, 131338.0, 131349.0, 131352.0, 131356.3, 131364.0, 131366.0, 131367.0, 131384.0, 131389.0, 131393.0, 131400.0, 4850000.0, 131409.0, 131408.0, 131421.94, 131424.0, 1180000.0, 131450.0, 131456.0, 131461.0, 131462.0, 131464.0, 131469.0, 131472.0, 131475.0, 131482.0, 131487.0, 131492.0, 131500.0, 131502.0, 131506.0, 131510.0, 131518.0, 131529.0, 131536.0, 131539.0, 131541.0, 131560.0, 131567.0, 131569.0, 131570.0, 131576.0, 131580.0, 131587.0, 131595.0, 131600.0, 131605.0, 131607.0, 131609.0, 131615.0, 131619.0, 131620.0, 131626.0, 131635.0, 131635.38, 131651.0, 131660.0, 131667.0, 131674.0, 131677.11, 131678.0, 131680.0, 131689.0, 131699.58, 131700.0, 131712.0, 131713.0, 656000.0, 131715.0, 131723.0, 131741.0, 131746.0, 131750.0, 131760.0, 131767.0, 131769.0, 131770.0, 131771.0, 131772.0, 131774.0, 131777.0, 131788.0, 131799.0, 131800.0, 131801.0, 131805.36, 131805.0, 131808.0, 131810.0, 131811.0, 131827.0, 131833.0, 131840.0, 131849.0, 131850.0, 394000.0, 131859.0, 131877.0, 131882.0, 131890.0, 131892.0, 131900.0, 131904.0, 131912.0, 131919.0, 131934.0, 131938.0, 131939.0, 131943.0, 131950.0, 131960.0, 131966.0, 131975.0, 131977.0, 131982.0, 131984.0, 131990.0, 131996.0, 132000.0, 132005.0, 394150.0, 132009.0, 132016.0, 132035.0, 132036.0, 132045.0, 132050.0, 132060.0, 132080.0, 132087.0, 132089.0, 132090.0, 132093.52, 132100.0, 132101.0, 132104.0, 132110.0, 132120.0, 132127.0, 132132.0, 132134.0, 132135.0, 132142.82, 132146.0, 132154.0, 132157.0, 132160.0, 132165.0, 132177.0, 132182.4, 132183.0, 132191.0, 132200.0, 132207.0, 132209.48, 132209.0, 132217.0, 132220.0, 132223.0, 132225.0, 132227.57, 132230.0, 132232.0, 132250.0, 132253.0, 132255.0, 132260.0, 132261.51, 132263.0, 132270.0, 132280.0, 132281.0, 132287.0, 132288.0, 132293.0, 132298.0, 132300.0, 132302.0, 132304.0, 132314.0, 132320.0, 132323.0, 132324.0, 67781.0, 132327.0, 132330.0, 132340.0, 132350.0, 132352.0, 132353.0, 132359.4, 132360.0, 132366.0, 132368.0, 132369.0, 132374.0, 132375.0, 132384.0, 132388.08, 132395.0, 132396.0, 132400.0, 132402.0, 132419.0, 132432.0, 132442.0, 132444.0, 132450.0, 132454.0, 132456.0, 132480.0, 132485.0, 132487.0, 132492.0, 132495.0, 132498.0, 132499.0, 132500.0, 132502.0, 132504.0, 132512.0, 132520.0, 132533.0, 132540.0, 132548.0, 132552.0, 132555.0, 132557.0, 132562.0, 132564.0, 132565.37, 132566.0, 132571.0, 132575.0, 132592.0, 132593.95, 132600.0, 132600.84, 132608.0, 132612.0, 132615.0, 919048.7, 132616.0, 132620.8, 132620.0, 132626.0, 132630.0, 132632.0, 132640.0, 132644.0, 132646.0, 132648.0, 132650.0, 132651.0, 132652.8, 132656.0, 132669.0, 132670.0, 656962.0, 132679.0, 132680.0, 132683.0, 132700.0, 132704.0, 132706.0, 132710.0, 657000.0, 132720.0, 132725.0, 132725.38, 132731.0, 132732.0, 132736.0, 132740.0, 132744.0, 132749.0, 132750.0, 132753.0, 132754.0, 132764.0, 132765.0, 132768.0, 132776.0, 132780.0, 132785.0, 132790.0, 132795.0, 132797.0, 132800.0, 132804.0, 132808.0, 132816.0, 132825.0, 132828.0, 132840.0, 132843.8, 132843.0, 132850.0, 132851.0, 132852.0, 395000.0, 132860.0, 132870.0, 132872.0, 132875.0, 132884.0, 132891.0, 132892.9, 132892.0, 132900.0, 132906.0, 132918.0, 132923.0, 132924.0, 132925.0, 132940.0, 132946.0, 132954.24, 132958.0, 132962.0, 132964.0, 132966.0, 132974.0, 132975.0, 132980.0, 132988.0, 132990.0, 132995.0, 132996.0, 133000.0, 133004.0, 133020.0, 133024.74, 133024.0, 133032.0, 133038.0, 133050.0, 133052.0, 133056.0, 133065.0, 133066.0, 133067.0, 133068.12, 133075.0, 133089.0, 133093.0, 133100.0, 133106.0, 133116.0, 133118.0, 133119.0, 133120.0, 133122.0, 133128.0, 1181708.0, 133134.0, 133140.0, 395285.0, 133150.0, 133157.0, 133161.0, 133168.0, 133171.0, 133183.0, 133197.0, 133200.0, 133210.0, 133213.0, 133217.0, 133224.0, 133228.0, 395375.0, 133232.0, 133234.0, 133248.0, 133250.0, 133255.0, 133256.0, 133260.0, 133269.76, 133272.0, 133280.0, 133296.0, 133300.0, 133302.0, 133308.0, 133318.0, 133323.0, 133325.0, 133331.0, 133333.0, 133350.0, 133358.0, 133368.0, 133383.12, 133390.0, 133392.0, 78616.0, 133400.0, 133401.48, 133401.0, 133404.0, 133408.0, 133409.0, 133415.0, 133420.0, 133430.12, 133432.0, 133437.0, 133440.0, 133452.0, 133456.0, 133457.0, 133458.8, 133459.0, 133461.0, 133468.0, 133473.6, 133480.0, 133484.0, 133488.0, 133494.0, 133495.34, 133499.04, 133500.0, 133504.39, 133508.0, 133512.0, 133515.0, 133517.0, 133522.0, 133523.0, 133532.0, 133533.0, 78644.0, 133535.0, 133543.0, 133546.0, 133548.64, 133550.0, 133552.2, 657848.0, 133560.0, 133567.0, 920000.0, 133575.0, 133584.0, 133590.0, 133592.0, 133595.0, 133599.0, 133600.0, 133604.64, 133611.2, 133614.63, 133618.0, 133619.0, 133620.0, 133628.0, 133635.0, 133637.0, 133640.0, 133642.0, 133643.0, 133650.0, 133656.0, 133669.0, 133674.08, 133680.0, 133681.0, 133684.0, 133689.0, 133690.0, 133692.0, 133694.0, 133700.0, 133710.98, 133711.0, 133724.0, 133725.0, 133732.0, 133736.0, 133740.0, 133744.0, 133748.0, 133750.0, 133752.0, 133760.0, 133764.0, 133770.0, 133773.0, 133774.32, 133780.0, 133782.0, 133785.0, 133791.0, 133794.0, 133797.0, 133798.0, 133800.0, 133805.0, 133815.0, 133836.0, 133843.0, 396000.0, 133869.0, 133870.0, 133871.0, 133889.0, 133890.0, 133891.0, 133896.0, 133900.0, 133904.0, 133911.0, 133912.0, 133952.0, 133974.0, 133980.0, 133986.35, 133987.0, 133990.0, 133992.0, 133995.0, 134000.0, 134004.0, 134009.0, 134016.0, 134029.0, 134038.0, 134057.0, 134063.0, 134068.0, 134070.0, 134096.0, 134100.0, 134110.0, 134111.0, 134118.0, 134132.0, 134133.0, 134136.0, 134140.0, 134147.0, 134148.0, 134151.0, 134156.0, 134160.0, 134162.0, 134167.0, 134173.0, 134175.0, 134176.0, 134177.0, 134179.0, 134180.0, 134181.36, 134200.0, 134203.0, 134207.0, 134208.0, 134215.0, 134232.0, 134234.0, 134238.0, 134239.89, 134244.0, 134246.0, 134250.0, 134256.0, 134260.0, 134262.0, 134263.0, 134271.0, 134275.0, 134281.0, 134284.0, 134285.0, 134290.0, 134294.0, 134296.0, 134299.0, 134300.0, 134310.0, 134315.0, 134319.0, 134323.0, 134328.0, 134328.7, 134340.0, 134363.0, 134364.36, 134368.0, 134375.0, 134380.0, 134381.0, 134383.0, 134388.0, 134395.0, 134399.0, 134400.0, 134404.7, 134406.0, 134407.0, 134410.0, 134419.0, 134426.0, 134428.0, 134432.0, 134435.0, 134437.0, 134447.0, 134452.0, 134461.0, 134472.0, 134478.0, 134479.8, 134480.0, 134485.0, 134498.0, 134500.0, 134501.0, 134503.0, 134508.0, 134512.0, 134520.0, 134521.0, 134529.0, 134535.0, 134545.0, 134547.0, 134550.0, 134552.0, 134553.0, 134556.0, 134564.0, 134570.0, 134575.0, 134576.0, 134583.0, 134591.39, 134595.0, 134600.0, 134604.0, 134608.0, 134610.0, 134611.0, 134635.28, 134640.0, 134641.49, 134654.0, 134657.0, 134661.0, 134662.0, 134666.2, 134676.0, 134677.0, 134680.0, 134685.0, 134690.0, 134700.0, 396850.0, 134707.0, 659000.0, 134737.0, 134740.0, 134748.0, 134749.0, 134750.0, 134751.71, 134751.0, 134770.0, 134777.0, 134779.0, 134781.06, 134784.0, 134789.0, 134800.0, 134802.0, 134803.0, 134820.0, 134840.0, 134849.0, 134850.0, 134852.0, 397000.0, 134859.0, 134866.0, 397019.41, 134875.0, 134886.0, 134895.0, 134900.0, 134904.0, 134905.0, 134909.0, 134923.0, 134925.0, 134933.0, 134945.0, 134950.0, 134966.0, 134981.0, 134986.0, 135000.0, 135006.0, 135012.0, 135013.0, 135024.0, 135031.13, 135063.0, 135065.64, 135072.0, 135075.0, 135086.0, 135095.0, 135100.0, 135106.38, 135110.0, 135113.0, 135118.0, 135120.0, 135122.0, 135123.0, 397270.0, 135128.0, 135141.0, 135143.0, 135145.0, 135155.0, 135163.0, 135164.0, 135167.0, 135168.0, 135176.0, 135179.0, 135180.0, 135182.0, 135185.0, 135192.0, 135200.0, 135208.0, 135213.1, 135217.0, 135218.0, 135225.0, 135225.63, 135228.0, 135230.0, 26879.0, 135244.0, 135250.0, 135251.0, 135252.0, 135253.0, 135256.0, 135258.0, 135273.0, 135278.0, 135283.2, 135287.0, 135299.0, 135300.0, 135306.0, 135320.0, 135323.0, 135337.0, 135350.0, 397500.0, 135356.0, 135371.67, 135371.0, 135372.43, 135372.0, 135376.0, 135391.81, 135392.0, 135396.0, 135400.0, 135404.0, 135408.0, 135410.0, 135411.96, 135416.0, 397562.0, 135419.0, 135420.0, 135418.0, 135424.0, 135432.0, 135446.0, 135450.0, 135456.0, 135465.0, 135466.0, 135469.0, 135470.0, 135472.0, 135474.16, 135480.0, 135486.0, 135492.0, 135495.0, 135497.0, 135500.0, 135506.0, 135513.06, 135518.0, 135522.0, 135527.04, 135537.0, 135540.0, 135548.0, 135550.0, 135550.14, 135558.0, 135562.0, 135563.0, 135566.0, 135574.0, 135576.0, 135580.0, 135585.0, 135588.0, 135594.0, 135600.0, 135602.0, 135619.95, 135621.0, 135629.0, 135634.0, 135641.56, 135647.0, 135660.0, 135666.0, 135668.0, 135671.0, 135675.0, 135676.0, 135680.0, 8000000.0, 135686.0, 135696.0, 135699.0, 135700.0, 135700.78, 660000.0, 135713.0, 135718.0, 135720.0, 135725.0, 135731.0, 135736.0, 135738.41, 135740.0, 135752.0, 135756.0, 135759.0, 135763.0, 135765.0, 135767.0, 135770.0, 135771.0, 135775.0, 135780.0, 135795.0, 135798.0, 135800.0, 135804.0, 135812.0, 135820.0, 135826.0, 135827.0, 135847.0, 135850.0, 135852.0, 398000.0, 135860.0, 135865.0, 135867.64, 135876.0, 135880.0, 135886.0, 9573072.0, 135896.0, 135899.0, 135900.0, 135915.0, 135919.0, 135925.0, 135928.0, 135938.0, 135944.0, 135945.0, 135959.0, 135960.0, 135994.88, 135996.0, 135999.0, 136000.0, 136000.02, 136004.0, 136008.0, 136009.0, 136014.0, 136032.0, 136040.0, 136041.0, 136060.0, 136067.0, 136071.0, 136073.0, 136075.0, 136076.0, 136082.0, 136092.0, 136095.0, 136100.0, 136102.0, 136113.0, 136114.0, 136115.0, 136116.0, 136118.0, 136125.0, 136127.0, 136134.0, 136147.0, 136160.0, 136164.0, 136171.0, 136173.0, 136174.0, 136185.77, 136186.0, 136185.0, 136200.0, 136208.0, 136212.0, 136217.0, 136224.0, 136230.0, 136233.0, 136237.0, 136240.0, 136245.0, 136247.0, 136248.0, 136250.16, 136250.0, 136251.0, 136259.0, 136260.0, 136266.0, 136269.72, 136272.0, 136281.6, 136284.0, 136289.0, 136290.0, 136292.4, 136300.0, 136320.0, 136322.16, 136322.0, 136332.0, 136333.0, 136340.0, 136344.0, 136346.0, 136349.26, 136350.0, 136355.0, 136356.0, 136357.0, 136367.0, 136374.0, 136378.0, 136380.0, 136400.0, 136404.0, 136406.0, 136407.74, 136412.66, 136414.0, 136419.0, 136423.0, 136428.0, 136430.0, 136432.0, 136435.0, 136450.0, 136453.0, 136463.0, 136468.0, 136470.0, 136475.0, 136476.0, 136478.0, 136480.0, 136495.0, 136500.0, 136501.0, 136512.0, 136514.58, 136521.0, 136524.0, 136525.0, 136526.0, 136530.0, 136535.0, 136536.0, 136540.0, 136543.08, 136548.0, 136551.0, 136564.0, 136568.0, 136572.8, 136574.0, 136575.0, 136580.0, 136584.0, 398736.0, 136600.0, 136614.0, 136622.0, 136625.0, 136632.0, 136634.0, 136635.0, 136640.0, 136664.0, 136671.0, 136675.0, 136676.0, 136678.0, 136684.0, 136692.0, 136695.0, 136696.0, 136700.0, 136704.0, 136709.0, 136715.0, 136716.0, 136717.0, 136718.0, 136721.0, 136725.0, 136728.0, 136735.0, 136740.0, 136750.0, 136757.0, 136760.0, 136770.0, 136771.0, 136774.0, 136781.0, 136782.0, 136799.0, 136800.0, 136802.0, 136808.0, 136809.0, 136815.0, 136826.0, 136827.0, 136836.0, 136843.0, 136846.0, 136849.0, 136850.0, 136851.0, 399000.0, 136858.0, 136860.0, 136862.0, 136863.0, 136865.0, 136871.0, 136879.0, 136886.25, 136886.0, 136890.0, 136896.0, 136899.0, 136900.0, 136901.0, 136920.0, 136922.0, 136927.0, 136937.0, 136941.0, 136946.3, 661235.0, 136950.0, 136956.0, 136958.0, 136970.26, 136973.28, 136974.0, 136988.0, 136990.0, 136992.0, 136999.0, 137000.0, 137004.0, 137005.0, 137005.56, 137006.0, 137013.0, 137020.0, 137040.0, 137047.0, 137049.0, 137050.0, 137057.0, 137069.0, 137072.0, 137080.0, 137085.0, 137091.0, 137100.0, 137122.0, 137123.0, 137131.0, 137136.0, 137142.0, 137150.0, 137156.0, 137163.0, 137170.0, 137172.0, 137176.0, 7215069.0, 137193.0, 137200.0, 137203.0, 137218.0, 137220.0, 137222.0, 137224.0, 137227.0, 137230.0, 137238.0, 137240.0, 137242.0, 137249.0, 137250.0, 137254.0, 137256.0, 137258.0, 137260.0, 137265.94, 137275.0, 137280.0, 137287.0, 137300.0, 137304.0, 137309.0, 137313.0, 137315.0, 137318.46, 137319.0, 137322.0, 137337.0, 137338.0, 137340.0, 137359.0, 137366.0, 137367.0, 137369.0, 137374.0, 137383.0, 137388.0, 137398.0, 137400.0, 137408.0, 137419.0, 137424.0, 137433.0, 137437.0, 137440.0, 137441.0, 137450.0, 137452.0, 137458.0, 137460.0, 137470.0, 137473.7, 137479.0, 137480.0, 137481.0, 137487.0, 137489.0, 137490.0, 137494.0, 137496.0, 137498.0, 137499.0, 137500.0, 137501.0, 137504.0, 137508.0, 137509.0, 137520.0, 137529.0, 137531.0, 137540.0, 137543.0, 137545.0, 137555.0, 137558.0, 137560.0, 137562.0, 137565.0, 137566.0, 924000.0, 137570.0, 137572.0, 137576.0, 137580.0, 137592.0, 137596.0, 137600.0, 137612.0, 137620.0, 137628.0, 137629.0, 137638.08, 137640.0, 137643.0, 137650.0, 137653.0, 137654.44, 137664.0, 137668.0, 399821.0, 137694.0, 137700.0, 137709.0, 137711.0, 662000.0, 137720.0, 137722.0, 137730.0, 137740.0, 137749.0, 137754.0, 137761.0, 137767.0, 137773.0, 137779.0, 137780.0, 137784.0, 137790.0, 137796.97, 137796.0, 137800.0, 137809.0, 137812.0, 137822.0, 137824.0, 137825.0, 137826.0, 137828.0, 137831.0, 137842.0, 137848.0, 137850.0, 399996.0, 137855.0, 400000.0, 137856.0, 400005.0, 137865.0, 137868.6, 137869.0, 137875.0, 137888.0, 137900.0, 137901.0, 137904.0, 137907.0, 137916.0, 137917.0, 58886.64, 137936.0, 137939.0, 137947.0, 137949.0, 137950.0, 137952.0, 137955.0, 137974.0, 137976.0, 137977.0, 137985.0, 137987.0, 137990.0, 138000.0, 138005.0, 138008.0, 138010.0, 138017.0, 138020.0, 400166.0, 138032.0, 138037.0, 138050.0, 138055.0, 138059.0, 138060.0, 138061.0, 138072.0, 138080.0, 138082.0, 138096.0, 138099.0, 138100.0, 138104.0, 138112.0, 138120.0, 138125.0, 138136.0, 138144.0, 138148.0, 138150.0, 138159.93, 138160.0, 138161.0, 138164.0, 138200.0, 138221.68, 138236.37, 138240.0, 138248.0, 138250.0, 138252.0, 138257.0, 138260.0, 138262.0, 138268.0, 138270.0, 138272.0, 138279.7, 138280.0, 138282.0, 138286.0, 138288.0, 138290.0, 138295.88, 5119032.0, 138300.0, 138303.0, 138315.0, 138320.0, 138321.0, 138333.0, 138340.0, 138342.0, 138345.0, 138346.0, 138347.0, 138350.0, 138355.0, 400500.0, 138357.0, 138361.6, 138363.0, 138370.0, 138372.0, 138375.0, 138388.26, 138400.0, 138402.0, 138408.0, 138419.0, 138436.0, 138440.0, 138444.0, 138450.0, 138456.0, 138463.0, 138476.0, 138477.0, 138482.0, 138484.0, 138500.0, 138515.0, 138517.0, 138519.0, 138525.0, 138527.0, 138539.2, 138542.0, 138548.0, 138550.0, 138555.0, 138560.0, 138562.0, 138564.0, 925000.0, 138570.0, 138571.0, 138583.0, 138586.0, 138598.0, 138600.0, 138611.0, 138612.0, 138625.0, 138628.0, 138631.0, 138637.0, 138640.0, 138645.0, 138650.0, 138652.0, 138665.0, 138667.0, 138673.0, 138678.45, 138679.0, 138694.0, 138696.0, 138699.0, 138700.0, 138708.0, 663000.0, 138713.68, 138717.0, 138720.0, 138728.0, 138732.0, 138735.0, 138744.0, 138748.0, 138750.0, 138776.0, 138782.0, 138785.0, 138790.0, 138792.0, 138793.0, 138798.0, 138800.0, 138809.0, 138815.0, 138824.0, 138829.0, 138831.0, 138840.0, 138843.0, 138844.0, 138846.0, 138850.0, 138854.0, 401000.0, 138871.0, 138880.0, 138887.0, 138890.16, 138891.0, 138900.0, 138902.0, 138906.0, 138910.0, 138927.99, 138929.08, 138943.0, 138947.0, 138950.0, 138960.0, 138962.0, 138964.0, 138968.0, 138970.0, 138980.0, 138984.0, 138985.0, 401136.0, 138996.0, 139000.0, 139005.0, 139008.0, 925441.0, 139019.35, 139028.0, 139030.0, 401189.0, 139048.0, 139049.0, 139050.0, 139052.0, 139061.0, 139075.0, 139089.0, 139092.0, 139095.0, 139099.0, 139100.0, 401253.0, 139110.0, 139111.0, 139112.5, 139121.0, 139123.0, 139124.0, 139128.0, 139133.96, 139136.0, 139137.0, 139140.0, 139141.04, 139164.0, 139168.0, 139176.0, 139178.0, 139187.0, 139200.0, 139208.0, 139211.0, 139224.0, 139224.7, 79775.0, 139236.0, 139244.0, 139249.0, 139250.0, 139252.0, 139256.0, 139266.0, 139270.0, 139272.0, 139274.0, 139277.0, 1450000.0, 139282.0, 139283.0, 139284.0, 139294.0, 139296.0, 139300.0, 139301.0, 139304.0, 139320.0, 139327.26, 139329.0, 139333.0, 139333.9, 139344.0, 139351.0, 139360.0, 139368.0, 139370.0, 139375.0, 139380.0, 27706.0, 139400.0, 139404.0, 139416.0, 6693021.0, 139422.0, 139444.0, 139453.0, 139464.0, 139467.0, 139477.0, 139483.0, 139486.0, 139488.0, 139489.0, 139500.0, 139509.0, 139514.0, 139517.0, 139519.08, 139520.0, 139521.0, 57647.0, 139523.0, 139525.03, 139529.5, 139536.0, 139551.59, 139552.0, 139560.0, 139570.0, 139576.0, 139577.0, 139578.0, 139580.0, 139584.0, 139593.0, 139600.0, 401749.0, 139607.0, 139608.0, 139611.0, 139619.0, 139623.0, 139634.0, 139636.0, 139650.0, 139660.0, 139671.0, 401818.0, 139675.0, 139676.0, 139686.0, 139700.0, 139708.0, 139720.0, 139723.0, 139730.0, 139738.0, 139740.0, 139741.0, 139746.0, 139748.0, 139750.0, 139753.0, 139754.0, 139755.0, 139756.0, 139765.0, 139771.0, 139773.0, 139776.0, 139790.0, 139800.0, 139806.0, 139816.0, 139820.0, 139821.0, 139832.0, 139835.0, 139838.0, 139839.0, 139840.0, 139850.0, 139852.0, 402000.0, 139860.0, 139872.0, 139874.0, 139875.0, 139876.0, 139880.0, 139884.0, 139900.0, 139906.0, 139907.0, 139911.0, 139913.0, 139920.0, 139924.0, 139925.0, 139932.0, 139939.0, 139940.0, 139942.0, 139946.0, 139947.0, 139950.0, 139962.0, 139966.0, 139970.0, 139984.0, 139992.0, 139996.0, 139997.0, 139998.0, 139999.0, 140000.0, 140001.0, 140004.0, 140005.0, 140008.0, 140009.0, 140011.0, 140020.0, 664311.0, 27831.0, 140028.0, 140034.0, 140051.0, 140055.76, 140056.0, 140070.0, 140071.0, 140080.0, 140085.0, 140088.0, 140089.0, 140093.0, 140099.0, 140100.0, 140114.0, 140119.0, 27850.44, 140124.0, 140125.72, 140129.0, 140144.0, 140149.0, 140152.0, 140160.0, 140166.0, 140179.0, 140182.0, 140189.0, 140199.96, 140200.0, 140201.0, 140207.0, 140219.0, 140220.0, 140222.0, 140225.0, 140230.0, 140238.0, 140247.0, 140249.0, 140250.0, 140272.0, 140275.0, 140285.0, 140292.0, 140295.0, 140300.0, 27886.62, 140304.0, 140320.0, 140322.0, 27893.0, 140338.0, 140341.44, 140345.0, 140350.0, 140351.0, 140355.0, 80000.04, 140358.0, 140360.0, 140370.0, 140372.0, 140384.0, 140400.0, 140409.0, 140414.0, 140420.74, 140436.0, 140440.0, 140440.5, 140444.0, 140446.0, 140450.0, 140454.0, 140462.0, 140468.0, 140472.0, 140495.0, 140496.0, 140498.0, 140500.0, 140501.0, 140512.0, 140523.0, 140525.0, 140547.0, 140550.0, 140552.0, 140560.0, 140575.0, 140581.0, 140595.0, 140600.0, 140625.0, 140642.88, 140647.3, 140650.0, 140651.0, 140662.0, 140671.0, 140680.0, 140698.0, 140700.0, 2500000.0, 665000.0, 140712.0, 140723.0, 140730.0, 140732.0, 140740.0, 140753.0, 402900.0, 140764.0, 140776.0, 140785.97, 140785.0, 140793.0, 140800.0, 140806.69, 140820.0, 140834.0, 140837.0, 140840.0, 403000.0, 140864.0, 140875.0, 140876.0, 140880.0, 140881.0, 140892.0, 140893.0, 140893.48, 140894.0, 140899.0, 140900.0, 140901.0, 140909.0, 140910.0, 140919.38, 140927.0, 140932.0, 140936.0, 140943.0, 140962.0, 140967.0, 140974.56, 140987.0, 140999.0, 141000.0, 141005.0, 141024.0, 141025.0, 141028.0, 141038.0, 141040.0, 141041.0, 141049.0, 141050.0, 141052.0, 403200.0, 141056.0, 141057.8, 141098.0, 141100.0, 141101.0, 141110.0, 141111.0, 141120.0, 141126.0, 141129.0, 141143.0, 141145.0, 141150.0, 141182.0, 141185.0, 141200.0, 141208.0, 141223.0, 141225.0, 141230.0, 141232.0, 141233.0, 141235.0, 141242.0, 141250.0, 141252.0, 141254.0, 141264.0, 141275.0, 141277.0, 141280.0, 141284.0, 141293.0, 141299.0, 141300.0, 141309.0, 141310.0, 141326.0, 141327.0, 141328.0, 141333.0, 141349.0, 141366.0, 141372.0, 141375.0, 141378.0, 141381.0, 141389.0, 141390.0, 141393.0, 141397.73, 141398.0, 141399.0, 141400.0, 141411.0, 141416.2, 4860012.0, 141420.0, 141428.0, 141429.0, 141430.0, 141432.0, 141438.0, 141439.0, 141440.0, 141441.0, 141444.0, 28113.0, 141446.0, 141447.0, 28114.0, 141450.0, 141454.0, 141455.0, 141455.08, 141463.0, 141466.0, 141470.0, 141481.0, 141493.0, 141498.0, 141500.0, 141504.0, 141506.0, 141510.0, 141515.0, 141523.0, 141534.0, 141540.0, 141548.0, 141550.0, 141555.0, 141558.0, 141559.0, 141579.0, 141580.92, 141581.0, 141585.0, 141600.0, 141610.0, 141612.0, 141620.0, 141621.0, 141627.0, 141660.0, 141671.28, 141675.0, 141676.0, 141688.0, 141690.0, 141693.0, 141696.0, 141700.0, 141712.0, 141725.0, 141732.0, 141740.0, 141744.74, 141748.0, 141750.0, 141756.0, 141774.0, 141776.0, 141778.0, 141800.0, 141804.0, 141810.0, 141815.28, 141816.0, 141819.0, 141825.0, 141835.0, 141840.0, 141842.0, 141849.0, 141853.0, 404000.0, 141864.0, 141865.0, 141878.0, 141894.0, 141899.0, 141900.0, 141904.0, 141916.0, 141920.0, 141934.0, 141945.0, 141945.04, 141949.0, 141965.0, 141970.0, 141973.0, 141977.0, 141978.0, 141980.0, 141986.0, 141990.0, 141992.0, 141996.0, 141996.8, 141999.0, 142000.0, 142021.0, 142025.0, 142035.0, 142041.0, 404200.0, 142065.0, 142070.0, 142079.0, 142090.0, 142100.0, 142103.0, 142107.96, 142108.0, 142119.0, 142128.0, 142129.0, 142132.0, 142138.0, 142140.0, 142144.0, 142145.0, 142146.12, 142150.0, 142156.0, 142160.0, 142162.0, 142164.0, 142167.0, 142170.0, 142176.0, 142183.56, 142197.0, 142200.0, 142209.36, 142214.0, 142227.0, 142230.0, 142249.98, 142250.0, 142263.0, 142264.0, 142269.0, 142270.0, 142277.32, 142278.0, 142284.0, 142285.0, 142294.5, 142300.0, 142313.0, 142318.0, 142325.0, 142334.4, 142335.0, 142340.0, 142350.0, 142356.0, 404500.0, 142369.0, 63653.0, 142389.0, 142400.0, 142414.0, 142416.24, 142417.0, 142420.0, 142438.0, 142439.0, 142440.0, 142450.0, 142451.0, 142454.0, 142466.0, 142474.79, 142480.0, 142491.0, 142495.0, 142500.0, 142505.0, 28326.6, 142524.0, 142540.0, 142560.0, 142561.0, 142575.0, 142587.0, 142597.0, 142600.0, 142612.0, 142617.0, 142622.0, 142624.0, 142628.0, 142635.0, 142636.0, 142642.0, 142648.0, 142650.0, 142653.0, 142654.66, 142660.0, 142679.0, 142688.0, 142691.0, 142700.0, 667000.0, 142722.6, 142728.0, 142730.0, 142750.0, 142752.0, 142763.0, 142766.0, 142767.0, 142771.0, 142776.0, 142777.0, 142781.21, 142785.65, 667074.0, 142790.0, 142798.0, 142800.0, 142825.0, 142834.0, 142850.0, 142852.0, 405000.0, 142860.0, 142865.0, 142872.0, 142876.66, 142886.0, 142890.0, 142891.0, 142897.0, 142900.0, 142905.0, 142909.0, 142917.0, 142920.0, 142944.0, 142950.0, 142951.0, 142955.0, 142960.0, 142962.0, 142966.77, 142971.0, 142980.0, 142992.0, 142999.0, 143000.0, 143004.0, 143015.0, 143046.0, 143060.0, 143073.36, 143079.0, 143095.0, 143100.0, 143110.0, 143112.0, 143115.0, 143120.0, 143123.0, 143124.8, 143124.0, 143134.0, 143138.0, 143146.0, 143150.0, 143200.0, 143207.0, 143212.0, 143216.0, 143218.0, 143220.0, 143232.0, 143247.0, 143249.0, 143250.0, 143251.0, 143252.0, 143258.0, 143270.0, 143280.0, 143293.0, 143299.0, 143300.0, 143303.0, 143312.0, 143340.0, 143345.0, 143362.0, 143367.0, 143375.0, 143376.0, 143382.0, 143387.0, 667680.0, 143398.0, 143400.0, 143415.0, 143416.0, 143429.0, 143431.1, 143436.0, 143442.0, 143444.0, 143448.0, 143452.0, 143465.0, 143485.0, 143486.0, 143487.0, 143489.0, 143496.0, 143500.0, 143509.0, 143518.0, 143520.0, 143532.0, 143535.0, 143542.0, 143543.0, 143548.0, 143550.0, 143555.0, 143559.0, 143561.0, 930000.0, 143600.0, 143600.47, 143603.0, 143604.0, 143607.0, 143611.0, 143612.0, 143613.0, 143616.0, 143618.0, 143620.0, 143630.0, 143633.0, 143638.0, 143640.0, 143641.0, 143650.0, 143658.0, 143672.0, 143678.0, 143683.0, 143687.5, 143689.0, 143700.0, 143711.0, 143718.12, 143720.0, 143728.0, 143730.0, 143734.1, 143740.0, 143743.0, 143750.0, 143751.0, 143757.0, 143760.0, 143762.0, 143763.36, 143774.0, 143776.0, 143780.26, 143781.84, 143782.91, 143784.0, 143788.0, 143790.0, 143795.0, 143796.0, 143800.0, 143801.0, 143801.65, 143804.9, 143832.0, 143844.0, 143854.0, 406000.0, 143858.0, 143860.0, 143863.0, 143865.0, 143872.92, 143876.0, 143880.0, 143881.0, 143900.0, 143904.0, 143906.0, 143908.0, 143917.0, 143918.0, 143927.0, 143928.0, 143930.01, 143936.0, 143950.0, 143954.0, 143956.0, 143960.0, 143967.0, 143976.08, 143980.0, 143981.0, 144000.0, 144001.0, 144009.0, 144020.0, 144026.4, 144030.0, 406181.0, 144051.0, 144070.0, 144081.0, 144083.4, 144092.46, 144099.0, 144100.0, 144105.0, 144121.0, 144134.0, 144144.0, 144145.0, 144150.0, 144155.0, 144159.0, 144179.0, 144185.0, 144192.0, 144195.0, 144196.0, 144200.0, 144215.0, 144216.0, 144219.0, 144224.0, 144230.0, 144232.0, 144239.0, 144240.0, 144248.0, 144250.0, 144254.0, 144255.0, 144276.0, 144280.0, 406434.0, 406440.0, 144300.0, 144304.65, 144304.16, 144305.0, 406450.0, 144310.0, 144328.0, 144332.0, 144333.0, 144337.0, 144340.0, 144344.0, 144348.0, 144357.0, 144362.0, 668655.0, 144379.0, 144380.0, 144385.0, 144392.0, 144396.0, 144400.0, 144415.97, 144417.0, 144420.0, 144423.0, 144425.0, 144426.53, 144440.0, 144444.0, 144453.0, 144462.0, 144476.0, 144480.0, 144484.0, 144493.0, 144500.0, 144520.0, 144522.47, 144525.0, 144529.92, 668820.0, 144539.0, 144542.0, 144549.79, 144550.37, 144560.0, 144562.0, 144568.0, 144580.0, 144583.0, 144594.0, 144594.52, 144598.0, 144600.0, 144607.0, 144616.0, 144621.0, 144631.0, 144644.0, 144652.0, 144655.0, 144663.0, 144675.0, 144678.0, 144680.0, 144682.0, 144690.0, 144693.0, 144696.0, 144700.0, 144708.0, 28762.0, 144719.0, 144720.0, 144721.0, 144731.0, 144738.0, 144739.0, 144744.0, 144750.0, 144752.0, 144781.0, 144785.3, 144785.0, 144792.6, 144794.0, 144799.0, 144800.0, 144803.0, 144820.0, 144831.0, 144832.0, 144839.0, 144840.0, 144842.0, 144852.0, 144854.0, 407000.0, 144860.0, 144866.0, 144870.0, 144871.0, 144879.0, 144898.0, 144900.0, 144915.0, 144919.0, 144928.0, 144930.0, 144934.4, 144945.0, 144946.72, 144950.0, 144955.0, 144960.0, 144976.0, 144978.0, 1980000.0, 144993.0, 144996.0, 144996.8, 144997.0, 144999.0, 145000.0, 144998.0, 145004.0, 145007.0, 145008.0, 145014.6, 57866.0, 145056.0, 145066.0, 145069.0, 145072.0, 145080.0, 145088.0, 145093.0, 145100.0, 145109.0, 145116.0, 145128.0, 145135.0, 145148.0, 145152.0, 145162.0, 145164.0, 145181.0, 145184.03, 145195.0, 145198.0, 145200.0, 145204.0, 145206.22, 145213.0, 669510.0, 145236.0, 145241.0, 145244.0, 145250.0, 28870.4, 145262.0, 145263.0, 145278.0, 145280.0, 145282.0, 145286.0, 145287.0, 145292.0, 145299.0, 145300.0, 145307.0, 57877.0, 145320.0, 145321.0, 145325.0, 145329.0, 145344.0, 145346.0, 28889.73, 145350.0, 145362.0, 145364.0, 145368.0, 145370.0, 145386.0, 145391.0, 145392.0, 145400.0, 145412.8, 145416.0, 145422.0, 145423.0, 145425.0, 145432.0, 145435.56, 145436.0, 145469.78, 145471.0, 669763.0, 145496.0, 145499.0, 145500.0, 145511.0, 145520.0, 145525.0, 145536.0, 145540.0, 145556.0, 145557.0, 932000.0, 145577.0, 145579.0, 145596.0, 145600.0, 145612.0, 145615.0, 145620.0, 145629.0, 145631.0, 145635.0, 145641.0, 145642.0, 145650.0, 407816.0, 145678.0, 145680.0, 145692.0, 145700.16, 145701.0, 145700.0, 670000.0, 145716.0, 145718.0, 145724.0, 145732.0, 145739.0, 145741.0, 145751.0, 145758.0, 145774.0, 145780.0, 145784.0, 145785.0, 145786.0, 145787.0, 145788.0, 145790.0, 145792.0, 145795.09, 145800.0, 145808.0, 145812.0, 145822.0, 145827.0, 145832.0, 408000.0, 145857.0, 145856.0, 145860.0, 145900.0, 145907.0, 145920.0, 145929.0, 145938.0, 145952.0, 145972.0, 145980.0, 145985.0, 145986.62, 145992.0, 145995.0, 145999.0, 146000.0, 146002.0, 146002.56, 146004.0, 146008.0, 146016.0, 146031.0, 146034.0, 146064.0, 146079.0, 146085.24, 146086.0, 146089.0, 146100.0, 146104.0, 146113.0, 146134.0, 146136.0, 146143.0, 146151.0, 146155.0, 146195.0, 146200.0, 146203.0, 146215.0, 146220.0, 146221.0, 146234.0, 408384.0, 146244.0, 146249.0, 146250.0, 146256.0, 146264.0, 146271.0, 146272.0, 146279.0, 146280.0, 146282.0, 146300.0, 146304.9, 146311.0, 146312.0, 146321.0, 146332.0, 146333.0, 146335.0, 146340.0, 146342.38, 146344.0, 146345.76, 146347.0, 146352.0, 408500.0, 146364.0, 146368.13, 146370.0, 146395.0, 146400.0, 146403.0, 146420.0, 146430.06, 146431.0, 146432.0, 146447.0, 146454.0, 146460.0, 146472.0, 146473.0, 146475.0, 146478.0, 146488.0, 146500.0, 146501.0, 146512.0, 670800.0, 146520.0, 146530.0, 146547.0, 146556.0, 146560.0, 408709.0, 146575.0, 146587.0, 146600.0, 408750.0, 146609.0, 146626.0, 146628.0, 146632.0, 146640.0, 146667.0, 146669.0, 146672.0, 146677.0, 146681.0, 146700.0, 146707.0, 146710.0, 146720.0, 146728.0, 146729.0, 146733.0, 146734.0, 146740.0, 146744.0, 146745.0, 146750.0, 146751.0, 146758.0, 146760.0, 146764.0, 146775.0, 146780.0, 146789.0, 146793.6, 146796.0, 146797.0, 146800.0, 146811.0, 146813.0, 146814.0, 146816.0, 146817.42, 409000.0, 146864.0, 146875.0, 146882.0, 146885.0, 409030.0, 146896.0, 146898.0, 146900.0, 146916.0, 146926.0, 146928.0, 146932.0, 146936.37, 146936.0, 146948.0, 146950.0, 146953.0, 146963.0, 29212.8, 146989.0, 146990.04, 146995.0, 147000.0, 147014.0, 147020.0, 147024.0, 147030.0, 147034.0, 147041.0, 147049.0, 147050.0, 147054.0, 147080.0, 147080.37, 147084.0, 147085.0, 147084.09, 147091.0, 147100.0, 147120.0, 147123.0, 147137.0, 147144.0, 59904.9, 147172.0, 147180.0, 147192.0, 147200.0, 147201.0, 147203.68, 147211.0, 147214.0, 147224.0, 147230.0, 147236.0, 147246.0, 147250.0, 147251.0, 147257.0, 147263.0, 147265.0, 147277.0, 147286.0, 147288.0, 147290.0, 147300.0, 147310.0, 147313.0, 147314.0, 147316.0, 147320.0, 147337.0, 147339.0, 147346.0, 147360.0, 147363.96, 147374.0, 147376.0, 147380.0, 147381.0, 147384.0, 147388.8, 147391.0, 147400.0, 147420.0, 147440.0, 147442.0, 147450.0, 147460.0, 147462.0, 147472.0, 147478.0, 147487.0, 147488.0, 147498.0, 147500.0, 147510.0, 147513.0, 147529.0, 147530.0, 147532.0, 147533.0, 29324.0, 147544.0, 147548.0, 147550.0, 147555.0, 147564.0, 147568.0, 147575.0, 147580.0, 147586.0, 147592.0, 147593.0, 147595.0, 147600.0, 147615.0, 147621.0, 147621.67, 147631.0, 147636.0, 147641.0, 147650.0, 147652.0, 147657.0, 147660.0, 147665.0, 147668.0, 147676.0, 147678.0, 147680.0, 147700.0, 147704.0, 672000.0, 147717.0, 147720.0, 147728.0, 147750.0, 147755.0, 147774.0, 147777.43, 147781.0, 147791.0, 147800.0, 147804.0, 147811.0, 147815.0, 147824.0, 147831.0, 147840.0, 147841.0, 29386.0, 147850.0, 147855.0, 410000.0, 147856.0, 29388.6, 147864.0, 147866.0, 147877.0, 147884.76, 147888.0, 147900.0, 147903.0, 147912.0, 147918.0, 147918.88, 147922.0, 147947.0, 147948.0, 147950.0, 147953.0, 147956.0, 147968.0, 81515.04, 147981.0, 147982.0, 147984.0, 147994.0, 147996.0, 148000.0, 148019.0, 148028.0, 148033.0, 148050.0, 148054.0, 148065.0, 148066.0, 148075.0, 148078.8, 148080.0, 148085.0, 148090.0, 148100.0, 148106.0, 148112.0, 148120.0, 148126.0, 148135.0, 148146.0, 148147.0, 148152.0, 148158.0, 148186.0, 148200.0, 148204.0, 148207.0, 148222.11, 148229.0, 148231.0, 148249.0, 148250.0, 410396.0, 148255.0, 148259.0, 148262.0, 148267.0, 148272.0, 148275.0, 148300.0, 148305.0, 410450.0, 148320.0, 148332.0, 148350.0, 148350.86, 672658.0, 410529.6, 148400.0, 148404.0, 148408.46, 148408.0, 148419.0, 148436.0, 148451.0, 148459.0, 148460.0, 148470.0, 148500.0, 148510.0, 148512.0, 148516.0, 148521.0, 148527.0, 148531.5, 148540.0, 148546.0, 148548.0, 148550.0, 148551.0, 6702150.0, 148556.07, 148563.0, 148569.0, 148578.0, 148580.0, 148585.0, 148588.0, 148593.0, 148594.0, 148595.0, 148596.0, 148600.0, 148613.0, 148616.0, 148625.0, 148626.0, 148630.0, 148644.0, 148650.0, 148652.0, 148656.0, 148659.0, 148663.0, 148675.0, 148676.0, 148685.0, 148692.0, 148700.0, 148711.0, 148716.0, 148720.0, 148730.2, 148740.0, 148742.0, 148748.0, 148750.0, 148760.4, 148770.0, 148772.0, 148775.0, 148788.0, 148800.0, 148808.76, 148838.0, 411000.0, 148880.0, 148883.0, 148885.0, 148893.0, 148896.0, 148900.0, 148904.0, 411058.0, 148936.0, 148959.0, 148967.0, 148968.0, 148970.0, 148984.0, 148992.0, 149000.0, 149010.0, 149020.0, 149026.0, 149041.0, 149044.0, 149047.0, 149050.0, 149052.0, 149056.0, 149073.0, 149082.0, 149089.0, 149100.0, 149112.0, 149120.0, 149124.0, 149138.0, 149149.0, 149154.0, 149160.0, 149182.0, 149199.0, 149200.0, 149212.0, 149213.0, 149218.0, 149240.0, 149250.0, 149254.0, 149256.0, 149268.0, 1460000.0, 149300.0, 149311.0, 149320.0, 149322.0, 149330.0, 149337.0, 149350.0, 149364.0, 149377.0, 149379.0, 149385.0, 149387.0, 149388.0, 149399.0, 149400.0, 149403.0, 149404.0, 149428.0, 149430.0, 149432.0, 149434.0, 149438.0, 411593.0, 149450.0, 149452.0, 149456.0, 149467.0, 149480.0, 149495.0, 149500.0, 149501.0, 149512.0, 149516.43, 149520.0, 411667.0, 149532.0, 149533.0, 149547.0, 149555.0, 936000.0, 149583.0, 149595.0, 149599.0, 149600.0, 149602.0, 149612.0, 149616.0, 411781.0, 149648.0, 149653.0, 149655.0, 149657.0, 149660.0, 149663.0, 149670.0, 149676.0, 149679.0, 149694.0, 149696.0, 149697.0, 149700.0, 149712.0, 149720.0, 149723.0, 149728.0, 149732.0, 149733.72, 149740.0, 149742.0, 149749.0, 149760.0, 29768.0, 149775.0, 149800.0, 149805.0, 149814.0, 149815.0, 149820.0, 149839.0, 149847.0, 411996.0, 412000.0, 149857.0, 149870.0, 149874.0, 149875.0, 149900.0, 149905.0, 149911.0, 149920.0, 149931.0, 149940.0, 149950.0, 149956.0, 412102.0, 149960.0, 29806.4, 149974.08, 149975.0, 149980.0, 149981.0, 149986.0, 149990.0, 149993.0, 149998.0, 149999.0, 150000.0, 150001.0, 150005.0, 412151.0, 150008.0, 150009.0, 150011.0, 150020.0, 150025.0, 150028.0, 150032.0, 150040.0, 150041.0, 150050.0, 150051.0, 150060.0, 150061.0, 150075.0, 150083.0, 150086.0, 150100.0, 150113.0, 150120.0, 150122.8, 150142.0, 150150.0, 150151.0, 150154.0, 150166.0, 150176.0, 150179.0, 150200.0, 150201.0, 150203.0, 150221.0, 150223.0, 150233.0, 150250.0, 150291.0, 150295.0, 150297.0, 150300.0, 150307.0, 150319.0, 150333.0, 150336.0, 150340.0, 29882.0, 150345.0, 150348.0, 412500.0, 150358.0, 150363.2, 150375.0, 412523.0, 150380.0, 29889.6, 150385.0, 150388.0, 150389.0, 150390.0, 150391.0, 150393.0, 150400.0, 150412.0, 150415.0, 150433.0, 150450.0, 150451.0, 150457.2, 150472.0, 150473.0, 150480.0, 150484.0, 150489.0, 150492.0, 150500.0, 150504.0, 150520.0, 150525.0, 150533.0, 150540.0, 150549.0, 150550.0, 412698.0, 150555.0, 150562.0, 150585.0, 150600.0, 150633.0, 150634.02, 150637.5, 150638.0, 150640.0, 150641.0, 150650.0, 150657.0, 150661.01, 150668.0, 150672.0, 150676.0, 150690.0, 150692.0, 150696.0, 150700.0, 150704.0, 675000.0, 150712.0, 150741.0, 150750.0, 150754.0, 675048.0, 150763.79, 150768.0, 150780.0, 150792.0, 150800.0, 150806.0, 150818.75, 150823.0, 150824.1, 150835.0, 150850.0, 413000.0, 150857.0, 150860.0, 150862.0, 150866.0, 150880.0, 150881.0, 150887.0, 150893.0, 150900.0, 150917.0, 150932.02, 150936.0, 150938.0, 150945.0, 150948.0, 150960.0, 150966.0, 150972.0, 150984.0, 150986.0, 150996.0, 151000.0, 151004.04, 151005.0, 151016.08, 151020.0, 151029.0, 151050.0, 151058.0, 151100.0, 151110.0, 151112.0, 151114.49, 151120.0, 151125.0, 151129.2, 151145.0, 151148.0, 151149.0, 151153.0, 151155.0, 151174.0, 151176.0, 151180.0, 151186.0, 151190.0, 151195.0, 151200.0, 151201.07, 151202.0, 151207.0, 151212.0, 151230.0, 151237.0, 151250.0, 413399.0, 151255.0, 151262.51, 151283.0, 151297.0, 151300.0, 151307.0, 151356.0, 290600.0, 151368.0, 151380.0, 151395.0, 151400.0, 151406.0, 151411.0, 1200000.0, 151435.0, 413580.0, 151440.0, 151450.0, 151456.0, 151480.0, 151486.68, 151488.0, 151493.0, 151496.0, 151500.0, 151515.0, 151540.0, 151544.0, 151548.6, 151552.0, 151561.0, 151575.0, 151578.0, 151581.0, 151583.0, 151600.0, 151602.0, 151608.0, 151620.0, 151626.0, 151656.0, 151658.0, 151660.0, 151662.0, 151664.0, 151677.0, 151680.0, 151686.0, 151687.0, 151688.9, 151692.0, 151695.0, 151700.0, 151704.0, 151715.64, 151716.0, 151731.0, 151732.81, 151735.0, 151741.45, 151744.0, 151758.0, 151785.0, 151791.0, 151800.0, 151812.0, 151821.96, 151836.0, 151840.0, 151843.75, 151852.0, 414000.0, 151874.0, 151875.0, 151891.0, 151894.0, 151900.0, 30193.44, 151925.0, 151940.0, 151944.0, 30201.6, 151964.0, 151975.0, 151986.0, 151988.16, 151992.0, 152000.0, 152004.0, 152005.0, 152007.0, 152028.0, 152038.0, 152040.0, 152063.04, 152064.0, 152092.0, 152100.0, 152131.0, 152135.0, 152137.0, 152139.0, 152140.0, 152150.0, 152156.0, 152160.0, 152162.0, 152163.6, 152172.0, 152178.0, 152193.6, 152194.0, 152200.0, 152224.0, 152229.36, 152244.6, 152250.0, 152256.0, 152257.0, 152276.0, 152281.0, 152288.0, 152300.0, 152307.0, 152320.0, 152321.0, 152325.0, 152335.0, 152337.0, 152339.23, 152340.0, 152343.0, 152357.0, 152358.0, 152364.0, 152367.33, 30284.52, 152375.0, 152384.0, 152387.0, 152389.0, 152397.0, 152400.0, 152430.0, 152440.0, 152449.0, 152450.0, 152451.0, 414600.0, 152479.0, 152481.0, 152490.0, 152493.43, 152496.0, 152500.0, 152515.0, 152520.0, 152539.0, 152542.0, 152548.0, 152566.0, 152575.0, 152580.0, 152586.0, 152593.0, 152600.0, 152604.0, 152606.0, 152613.0, 152624.0, 152637.0, 152640.0, 152641.0, 152644.8, 152654.0, 152660.0, 152680.0, 152681.0, 152685.0, 152687.0, 152688.0, 152700.0, 414849.0, 152709.0, 152712.0, 677000.0, 414864.0, 152724.0, 152736.0, 152745.0, 152750.0, 152754.0, 152755.0, 152756.0, 152758.0, 152760.0, 152785.0, 152788.0, 152791.0, 152800.0, 152808.0, 152832.0, 152838.0, 152840.0, 152844.0, 2250000.0, 414996.0, 152852.0, 415000.0, 677160.0, 152874.0, 152880.0, 152894.35, 152900.0, 152903.0, 152929.0, 152950.0, 152954.0, 152955.0, 152960.0, 152964.0, 152967.36, 152976.0, 152982.55, 677276.0, 152989.0, 153000.0, 153010.0, 153019.0, 153020.0, 153028.0, 153030.0, 153033.49, 30417.0, 153046.0, 153050.0, 153060.0, 153096.0, 153097.0, 153100.0, 153105.0, 153114.0, 153115.0, 153126.0, 153129.0, 153133.0, 153135.0, 153140.0, 153144.0, 153151.0, 153200.0, 415359.0, 153223.0, 153239.0, 153243.0, 153260.0, 153261.0, 153264.0, 153274.0, 153300.0, 153333.0, 153346.0, 153350.0, 153400.0, 153407.0, 153413.0, 153450.0, 153453.0, 153456.0, 153462.0, 153475.0, 153480.0, 153482.0, 153488.0, 153489.0, 153498.0, 153500.0, 153504.0, 153510.0, 153523.0, 153525.0, 153537.0, 153539.0, 153541.0, 153559.0, 153562.32, 940000.0, 153588.0, 153597.0, 153600.0, 153605.0, 153607.0, 153609.0, 153619.0, 153621.0, 153624.0, 153625.0, 153626.0, 153635.0, 153660.0, 153673.0, 153675.0, 153678.0, 153700.0, 153702.0, 153720.0, 153729.0, 153730.0, 153731.0, 153732.0, 153736.44, 153750.0, 153770.0, 153778.0, 153788.0, 153800.0, 153808.0, 153816.0, 153820.16, 153830.0, 153831.0, 153842.0, 153850.0, 416000.0, 153880.0, 153888.0, 153900.0, 153916.0, 153920.0, 153936.0, 153937.0, 153952.0, 153960.0, 153996.0, 154000.0, 154003.0, 154008.0, 154010.0, 154014.0, 154020.0, 154035.0, 154043.0, 154061.0, 154081.0, 154098.0, 154100.0, 154104.0, 416250.0, 154153.0, 154160.0, 154164.0, 154166.56, 154172.0, 154182.0, 154190.0, 154199.0, 154200.0, 154215.69, 154224.0, 154233.0, 154234.0, 154244.0, 154250.0, 154258.0, 154260.0, 3300000.0, 416422.0, 154280.0, 154285.0, 154290.0, 154300.0, 154310.0, 154324.0, 154326.0, 154331.0, 154345.0, 154350.0, 154370.3, 154380.0, 154381.0, 154400.0, 154416.0, 154428.0, 154438.0, 154440.52, 154441.0, 154448.0, 154450.0, 154452.0, 154456.0, 154481.0, 154492.0, 154499.0, 154500.0, 154509.0, 154516.0, 154542.0, 154543.0, 154544.0, 154546.0, 416700.0, 941000.0, 154600.0, 154611.0, 154612.0, 154618.48, 154619.0, 154620.0, 154625.0, 154631.6, 154640.0, 154661.0, 154674.0, 154685.0, 154693.0, 154700.0, 154726.0, 154760.0, 154788.0, 154789.0, 154800.0, 154822.0, 154839.0, 154840.0, 417000.0, 154856.0, 154858.0, 154860.0, 154880.0, 154889.0, 154898.0, 154899.0, 154900.0, 154908.0, 154911.0, 154920.0, 154922.0, 154928.0, 154950.0, 154960.0, 154962.0, 154980.6, 154992.0, 155000.0, 155004.0, 30811.0, 155030.0, 417176.0, 155036.16, 155052.0, 155064.0, 155080.0, 155083.0, 155093.0, 155096.0, 155099.0, 155100.0, 1728000.0, 155138.0, 155142.0, 155150.0, 155160.0, 155169.0, 155173.0, 30842.0, 155188.0, 155189.97, 155191.72, 155200.0, 155215.0, 155220.0, 155221.0, 155239.0, 155244.0, 155250.0, 155259.0, 155280.0, 155288.0, 155289.0, 155297.0, 155300.0, 417451.0, 155313.0, 155316.0, 30872.0, 155350.0, 155358.0, 155384.0, 155388.0, 155400.0, 155424.0, 155425.0, 155432.0, 155433.0, 155438.16, 155438.0, 155460.0, 155478.0, 155480.0, 155486.0, 155496.0, 155500.0, 155501.0, 155520.0, 155526.24, 155527.0, 155546.0, 155555.0, 155568.0, 155573.0, 155579.0, 155592.1, 155592.0, 155599.0, 155600.0, 155610.0, 155640.0, 155646.0, 155658.0, 155661.0, 155664.0, 155665.0, 30941.0, 155680.0, 155692.0, 155700.0, 155703.0, 155705.0, 155709.0, 155710.0, 680000.0, 155714.0, 155720.0, 155725.0, 155741.0, 155764.0, 155769.45, 155769.0, 155772.0, 155785.0, 155796.0, 155800.0, 155850.0, 155855.0, 418000.0, 155858.0, 155860.0, 155863.0, 155867.0, 155880.0, 155887.0, 155892.0, 155896.0, 155900.0, 155904.0, 155924.0, 155926.0, 155935.0, 155938.0, 155940.0, 155945.08, 155950.0, 155963.0, 155999.0, 156000.0, 156001.0, 156005.0, 156024.0, 156029.0, 156046.0, 156073.0, 156077.0, 156083.0, 156085.0, 156100.0, 156111.0, 156130.0, 156143.0, 156148.0, 156160.0, 156179.16, 156186.0, 156186.51, 156192.0, 156200.0, 156210.0, 156226.0, 156240.0, 156242.0, 156244.0, 156250.0, 156268.0, 156270.0, 156280.0, 156293.0, 156300.0, 156307.0, 156315.0, 156320.0, 156325.0, 156326.0, 156332.0, 156333.0, 156336.0, 156340.0, 156353.0, 156362.0, 156399.88, 156400.0, 156405.0, 156411.04, 156443.0, 83201.0, 156464.0, 156479.0, 156499.0, 156500.0, 156522.0, 156540.0, 31114.8, 156545.0, 156550.14, 8020871.0, 31117.0, 156560.0, 156576.0, 156579.34, 156592.0, 31124.0, 156600.0, 156605.0, 156620.0, 156649.0, 418800.0, 156661.0, 156671.0, 156680.0, 156690.0, 156700.0, 156711.0, 156720.0, 156732.0, 156744.0, 156750.0, 156751.0, 156760.0, 156776.0, 156783.0, 156784.0, 156800.0, 156812.0, 156825.0, 156842.0, 156850.0, 419000.0, 156858.0, 156863.0, 156871.0, 156880.0, 156888.0, 156891.0, 156900.0, 156901.0, 156906.0, 156912.0, 156917.0, 156942.0, 156980.0, 156982.0, 156996.0, 157000.0, 157007.0, 157024.0, 157028.0, 157033.0, 157040.0, 157046.0, 157047.0, 157055.0, 157100.0, 58346.0, 5400000.0, 157124.0, 157137.0, 157199.0, 157200.0, 157202.0, 157203.0, 157206.29, 157212.0, 157248.0, 157250.0, 157259.0, 157263.0, 157272.0, 157300.0, 157319.0, 157320.0, 157333.0, 157339.0, 157350.0, 157353.0, 157368.0, 157372.0, 157381.0, 157386.0, 157400.0, 157401.0, 157410.0, 157416.0, 157419.0, 157429.0, 157431.0, 157458.0, 157460.0, 157465.0, 157467.7, 157469.0, 157470.0, 157471.0, 157500.0, 157525.0, 157528.0, 157537.0, 157540.0, 157541.0, 157544.0, 157555.0, 157560.0, 157561.0, 157576.0, 157584.0, 157592.0, 157595.0, 157597.0, 157600.0, 157604.0, 157608.0, 157610.0, 157630.0, 157640.0, 157645.0, 157647.0, 157650.0, 157656.0, 157678.0, 157680.0, 157685.0, 157700.0, 157717.0, 157744.0, 157746.36, 157750.0, 157769.0, 157770.0, 157780.0, 157800.0, 157806.0, 157832.07, 157833.0, 157842.0, 157850.0, 420000.0, 157877.0, 157900.0, 157914.0, 157920.0, 157922.0, 157938.0, 157950.0, 157971.0, 157990.0, 157992.0, 158000.0, 158004.0, 158016.0, 158025.0, 158038.0, 158041.0, 158062.0, 158080.0, 31423.44, 158100.0, 158116.0, 158123.0, 158124.0, 158125.0, 158133.0, 158142.0, 158143.0, 158152.0, 158163.0, 158166.05, 158182.0, 158200.0, 158218.0, 31449.0, 158234.0, 158242.0, 158246.0, 158251.0, 158255.0, 158256.0, 158257.0, 158264.0, 158276.0, 158290.0, 31462.8, 158300.0, 420458.0, 158319.0, 158335.0, 158339.0, 158343.0, 158353.0, 420509.0, 158367.0, 158379.0, 158400.0, 58398.0, 158422.0, 158450.0, 158461.0, 158464.0, 158469.0, 158472.0, 158488.0, 158491.0, 158494.0, 158500.0, 158503.0, 158528.0, 158535.0, 158544.0, 158551.0, 158584.0, 158600.0, 158609.0, 158635.0, 158649.0, 158657.88, 158669.0, 158700.0, 158709.0, 158712.0, 158715.0, 158720.0, 158734.0, 158750.0, 158758.0, 158760.0, 158774.0, 158790.0, 158800.0, 31564.36, 158820.0, 158850.0, 420998.0, 158855.0, 421000.0, 158866.0, 158875.0, 158884.0, 158899.0, 158900.0, 158909.0, 31587.36, 158933.0, 158943.0, 158950.0, 158970.0, 158993.0, 159000.0, 159039.24, 159047.0, 159068.0, 159070.0, 159083.06, 159097.0, 159100.0, 159107.0, 159120.0, 159135.0, 159136.0, 159174.0, 159181.0, 159200.0, 159212.0, 159216.0, 159228.0, 159229.0, 159270.0, 159300.0, 31665.0, 159350.0, 159356.0, 159400.0, 159432.0, 159458.11, 159472.0, 159500.0, 159507.0, 159516.0, 159519.34, 159525.0, 159556.0, 159580.0, 159583.0, 159600.0, 31723.0, 159610.0, 159632.0, 159650.0, 31737.6, 159684.0, 159700.0, 684000.0, 159716.0, 159723.0, 159737.0, 159744.0, 159750.0, 159751.0, 159760.38, 159760.0, 159770.0, 159775.0, 159800.0, 159825.0, 159838.0, 159842.28, 422000.0, 159860.0, 159862.0, 159883.0, 159888.0, 159900.0, 159921.0, 159925.0, 159930.0, 159931.0, 159943.0, 159950.0, 159953.0, 159959.0, 159960.0, 159965.0, 7500000.0, 159971.0, 159978.0, 159981.0, 159989.0, 159996.0, 159999.0, 160000.0, 160000.2, 160004.0, 160006.0, 160024.0, 160042.0, 160060.0, 160074.0, 160093.34, 160100.0, 160118.0, 160132.0, 160160.0, 160164.0, 160168.0, 160180.8, 160190.0, 160200.0, 160220.0, 160250.0, 160253.0, 160278.0, 160285.0, 160290.0, 160300.0, 160302.0, 160326.0, 160331.0, 160332.0, 160347.0, 422500.0, 160360.0, 160370.0, 160371.0, 31876.39, 160380.0, 160390.0, 160395.0, 160396.0, 160400.0, 422550.0, 160408.0, 160420.0, 160421.0, 160465.0, 160482.0, 160485.0, 160489.0, 160500.0, 160524.0, 160529.0, 160536.0, 160549.0, 160556.0, 160558.0, 160575.0, 160590.91, 160600.0, 160610.0, 160611.0, 160622.69, 160623.0, 160630.0, 160635.0, 160636.0, 160647.0, 160650.0, 160669.75, 160675.0, 160680.0, 160700.0, 160711.0, 685000.0, 160712.0, 160716.96, 160723.0, 160728.0, 160743.0, 160754.32, 160773.99, 160774.0, 160800.0, 160814.0, 160830.0, 160840.0, 160841.0, 160842.0, 160854.0, 423000.0, 160859.0, 160875.0, 160881.0, 160893.0, 423042.0, 160900.0, 160912.0, 160923.86, 160935.0, 160942.0, 160968.0, 160980.0, 161000.0, 161018.0, 161027.63, 161058.0, 161060.0, 161089.0, 161094.48, 161100.0, 161105.0, 161123.0, 161127.0, 161128.0, 161160.0, 161163.5, 161195.0, 161200.0, 161221.0, 161236.0, 161250.0, 161262.0, 161268.0, 161288.0, 161300.0, 161304.0, 161327.46, 161331.0, 161338.0, 161345.0, 161346.0, 161347.0, 161350.0, 161351.0, 161387.0, 161388.6, 161400.0, 161409.0, 161420.0, 161423.0, 161434.0, 161436.0, 161448.0, 161450.0, 161456.0, 161465.0, 161468.0, 161470.0, 161480.0, 161482.0, 161487.0, 161500.0, 161517.0, 161520.0, 161524.0, 161539.0, 161548.0, 161550.0, 161567.0, 948000.0, 161573.0, 161576.0, 161594.0, 161595.0, 161597.43, 161600.0, 161605.0, 161630.44, 161634.0, 161650.59, 110000000.0, 161664.0, 84235.38, 161689.0, 161697.0, 161700.0, 161710.0, 161718.0, 161746.0, 161750.0, 161765.0, 161789.0, 161800.0, 161801.0, 423956.0, 161817.9, 161825.0, 161838.0, 161840.0, 161841.0, 424000.0, 161865.0, 424029.0, 161888.0, 161900.0, 161923.0, 161927.0, 161965.0, 161985.0, 161989.0, 161990.4, 162000.0, 1210612.0, 32207.76, 162048.0, 162050.0, 424204.0, 162061.0, 162065.0, 162080.0, 162082.0, 162100.0, 162102.0, 162105.0, 162115.0, 162135.0, 162154.46, 162160.0, 162200.0, 162203.0, 162216.0, 162225.0, 162233.0, 162240.0, 162249.0, 162250.0, 162265.0, 1473000.0, 162297.0, 162300.0, 162314.0, 162320.0, 162325.0, 162329.0, 32269.0, 162358.0, 162363.0, 162380.0, 162400.0, 162410.0, 162420.33, 32283.21, 162425.0, 32284.28, 162432.4, 162437.0, 162452.0, 162453.65, 162463.0, 32291.0, 162477.0, 162480.0, 162484.0, 162500.0, 32298.0, 162506.0, 162517.0, 162520.0, 162528.0, 162529.0, 162537.0, 162540.0, 162550.0, 162560.0, 162564.0, 162583.0, 162584.0, 162586.0, 162600.0, 162615.0, 162648.0, 32328.28, 162652.0, 162677.0, 32336.49, 162698.0, 162700.0, 162708.0, 162712.0, 162720.0, 162740.0, 162749.5, 162750.0, 162767.0, 162768.0, 58571.0, 162785.0, 162800.0, 162807.0, 32360.16, 162816.0, 162833.0, 162838.0, 162854.0, 425000.0, 162862.0, 162864.0, 162872.0, 162880.0, 162889.0, 162900.0, 32378.4, 162913.0, 162947.0, 162948.0, 162950.0, 162956.0, 162985.0, 163000.0, 163008.0, 163012.0, 163016.0, 163018.0, 163030.0, 163072.0, 163074.07, 163080.0, 163092.0, 163100.0, 163129.53, 163134.0, 163150.0, 163172.0, 163177.37, 163182.0, 163200.0, 163211.0, 163216.5, 163228.0, 163232.0, 163233.0, 163283.0, 163293.0, 163300.0, 163333.0, 163358.0, 163365.0, 163392.0, 32476.68, 163400.0, 163424.0, 163440.0, 163444.0, 163457.0, 163472.0, 163500.0, 163516.0, 4620000.0, 163552.0, 163556.32, 163556.22, 163568.0, 950000.0, 163570.0, 163587.0, 687877.0, 163590.0, 163600.0, 163620.0, 163627.0, 163642.0, 163650.0, 163654.0, 163668.0, 163675.0, 163683.74, 163696.0, 163700.0, 163705.0, 163712.0, 163715.0, 163717.0, 32540.1, 163720.0, 163733.0, 32546.8, 163750.0, 163765.0, 163776.0, 32554.0, 163791.0, 163795.0, 163800.0, 32558.97, 163823.0, 32561.0, 163825.0, 163829.0, 163836.0, 163845.0, 163848.0, 426000.0, 163860.0, 163868.0, 163875.0, 32572.8, 163882.0, 163883.0, 163900.0, 32579.0, 163947.6, 163952.0, 163976.0, 163984.0, 164000.0, 164004.0, 164048.0, 164068.0, 164069.0, 164100.0, 164120.0, 164126.0, 164153.0, 164160.0, 164185.0, 164200.0, 164201.0, 164208.0, 164214.0, 164234.0, 164238.0, 164250.0, 32647.6, 164268.0, 32650.75, 164275.0, 164280.0, 164289.0, 164292.0, 164300.0, 164320.0, 164351.0, 164355.0, 164400.0, 164405.0, 164410.0, 164425.0, 164440.0, 164450.0, 164458.0, 164460.0, 164480.0, 164484.0, 164484.84, 164500.0, 164502.0, 164508.0, 164518.0, 164519.0, 164542.0, 164550.0, 164555.0, 164566.0, 951000.0, 164580.0, 164585.0, 164607.51, 164639.0, 164646.0, 164652.0, 164656.0, 164679.0, 164700.0, 164732.0, 164733.0, 164750.0, 164759.0, 164768.0, 164780.0, 164788.0, 164793.0, 164800.0, 164818.0, 164831.0, 164835.0, 164850.0, 427000.0, 164875.16, 164900.0, 164901.0, 164911.0, 164973.0, 32790.16, 164985.0, 164990.0, 2000000.0, 165000.0, 165001.0, 165025.0, 165030.0, 165050.0, 165074.0, 32810.04, 165088.0, 165100.0, 165120.0, 5408000.0, 165130.0, 165152.0, 165160.0, 165172.0, 165200.0, 165211.0, 165218.0, 165240.0, 32843.2, 165246.0, 165252.0, 165268.0, 165275.0, 165283.0, 165295.0, 165300.0, 165301.0, 165317.0, 165321.0, 165336.0, 165384.0, 165394.0, 165396.0, 165398.0, 165399.0, 165400.0, 165429.0, 165430.0, 165432.0, 165450.0, 165466.0, 165484.0, 165500.0, 84998.4, 165504.0, 85000.08, 165515.0, 85004.0, 165540.0, 165545.0, 165562.0, 165564.0, 165568.0, 32909.0, 165580.0, 165584.54, 165589.0, 165598.0, 165600.0, 165603.0, 165648.0, 165650.0, 165655.0, 165666.0, 165672.0, 165700.0, 165710.0, 690000.0, 165732.0, 165733.0, 165749.0, 165750.0, 165756.0, 165762.0, 165765.0, 427927.0, 165790.0, 165800.0, 165805.0, 165808.0, 165811.0, 165830.0, 165833.0, 165840.0, 165850.0, 165854.0, 165855.0, 428000.0, 165870.0, 165887.0, 165890.62, 32972.88, 165900.0, 165921.0, 165925.0, 165938.0, 165944.0, 32987.0, 165971.0, 165976.0, 165987.0, 165990.0, 165996.0, 165999.0, 166000.0, 166011.0, 166013.0, 166026.0, 166058.0, 166071.0, 166074.0, 166083.15, 166088.0, 166099.0, 166100.0, 428258.0, 166124.0, 166130.0, 166150.0, 166151.0, 166161.0, 166171.0, 166188.0, 166196.0, 166200.0, 166201.0, 166208.0, 166211.0, 166233.77, 166244.0, 166250.0, 166254.0, 166260.0, 166272.0, 166295.0, 166300.0, 166314.0, 166327.0, 166353.0, 166358.0, 166361.0, 166366.0, 166369.0, 428521.0, 166378.0, 166380.0, 166383.0, 166390.0, 166397.51, 166399.0, 166400.0, 166410.0, 166417.0, 1215000.0, 166434.0, 166457.0, 166460.0, 166470.0, 166500.0, 166515.0, 166527.0, 166545.0, 166547.0, 166548.0, 166550.0, 166556.0, 166558.86, 953000.0, 166600.0, 33113.6, 166604.0, 166607.0, 166624.0, 166632.75, 166650.0, 166678.0, 166681.0, 166700.0, 166705.0, 166707.0, 33136.0, 166737.0, 166745.0, 166747.45, 166750.0, 33145.75, 166772.0, 166800.0, 166829.0, 166842.0, 166846.0, 166850.0, 429000.0, 166860.0, 33169.24, 429035.0, 166900.0, 166915.0, 166950.0, 166960.0, 166962.0, 166992.0, 167000.0, 33195.2, 167040.0, 167044.0, 167050.0, 167086.0, 167100.0, 167105.0, 167120.0, 167124.0, 167151.0, 167162.4, 167182.0, 167192.0, 167200.0, 167215.0, 167250.0, 167275.0, 167276.0, 167280.0, 167288.0, 167292.0, 167300.0, 167302.0, 167321.0, 167337.0, 167338.0, 167343.0, 167350.0, 167358.0, 167359.0, 167375.0, 167400.0, 167407.0, 167424.0, 167432.0, 167440.0, 167450.0, 167489.46, 85393.0, 167500.0, 85395.0, 167506.0, 167509.0, 691803.0, 167521.0, 167558.0, 167564.0, 954000.0, 167584.0, 167604.0, 167605.0, 167625.0, 167640.0, 167673.0, 33328.0, 33330.44, 167696.0, 33331.0, 167697.0, 167700.0, 33332.0, 167724.0, 33340.0, 167750.0, 167760.0, 167774.0, 167800.0, 167820.0, 167826.0, 167829.0, 167838.0, 4100000.0, 167840.0, 954285.0, 430000.0, 167861.0, 33364.0, 33366.0, 167890.0, 167900.0, 167935.0, 167961.0, 167990.0, 168000.0, 168002.0, 293908.0, 168018.0, 168021.0, 168041.0, 168061.0, 168064.0, 168075.0, 168080.0, 168087.0, 168100.0, 168120.0, 168122.0, 168168.0, 33425.78, 168175.0, 168200.0, 168221.0, 168240.0, 168250.0, 168265.0, 168270.0, 168288.0, 168296.0, 168300.0, 168332.0, 168340.0, 168375.0, 58794.0, 168384.0, 168400.0, 168420.0, 168421.0, 168425.0, 168438.0, 168473.0, 168480.0, 168500.0, 430644.0, 168515.0, 168520.0, 168528.0, 168541.0, 168548.0, 168548.16, 168550.0, 168555.0, 168586.0, 168587.0, 168600.0, 168642.0, 168663.81, 168673.63, 168694.0, 168700.0, 168701.0, 33532.0, 168710.0, 693000.0, 168723.0, 168725.0, 33536.0, 168730.0, 168732.0, 168734.0, 33538.0, 168750.0, 168753.0, 168755.0, 168760.0, 168762.0, 168764.0, 33545.0, 168782.0, 168800.0, 168810.0, 168833.0, 168844.92, 168845.0, 168846.12, 168847.0, 33560.87, 168850.0, 33561.6, 431000.0, 168860.0, 168868.0, 168878.0, 33566.0, 168880.0, 168888.0, 33568.0, 33569.0, 168900.0, 33571.2, 168912.0, 33573.0, 33575.88, 168950.0, 168953.0, 168968.0, 168972.0, 168975.0, 33586.0, 168980.0, 168996.0, 169000.0, 169006.0, 169019.0, 169026.0, 169041.0, 169044.0, 169062.0, 169067.0, 169069.0, 169098.0, 169100.0, 169109.0, 169125.0, 33621.1, 169200.0, 169211.0, 169223.16, 169243.6, 169248.0, 169250.0, 169260.0, 169270.0, 169276.0, 169285.54, 169300.0, 169308.0, 169322.0, 169371.0, 169378.0, 33668.0, 169400.0, 169404.0, 169415.38, 169420.0, 169422.36, 33675.2, 33676.64, 33676.18, 169442.0, 169450.0, 169452.0, 169458.0, 169498.0, 169500.0, 169520.0, 33694.0, 169533.07, 169548.0, 431700.0, 169596.0, 169600.0, 169604.0, 169614.0, 169620.0, 33714.0, 33717.6, 169646.0, 33720.04, 169655.0, 169671.0, 169672.0, 169680.0, 169681.0, 58846.0, 169687.0, 169700.0, 169704.0, 169730.0, 169735.0, 33736.32, 169750.0, 169755.37, 33741.12, 169769.0, 33743.99, 85847.42, 33744.0, 33747.0, 169799.0, 169800.0, 169802.0, 169822.0, 169825.0, 169829.0, 169832.0, 169845.0, 169850.0, 169855.0, 432000.0, 169864.0, 169870.0, 169875.0, 169880.0, 169890.0, 169900.0, 169906.0, 169917.0, 169928.0, 169939.47, 169950.0, 169961.05, 169982.0, 169983.0, 169990.0, 169992.0, 169995.0, 170000.0, 170001.0, 170004.0, 170017.0, 170026.0, 170040.0, 170042.0, 170055.0, 170080.0, 170100.0, 170131.0, 170146.0, 170160.0, 170164.0, 170186.12, 170193.0, 170198.0, 170200.0, 170210.0, 694500.0, 170213.0, 170216.0, 170250.0, 170254.0, 170260.0, 170272.0, 170280.0, 33848.0, 170300.0, 170301.0, 170322.0, 170352.0, 170360.0, 33861.0, 170381.0, 170392.0, 170395.0, 170400.0, 170425.0, 33874.08, 170433.0, 170448.0, 170449.0, 432600.0, 170484.0, 432632.0, 170500.0, 170518.0, 170520.0, 170523.0, 170529.16, 170543.0, 170555.0, 170560.0, 957000.0, 170569.0, 170595.0, 170625.0, 170660.0, 33921.0, 33921.6, 33922.2, 170673.0, 432824.0, 6200000.0, 33926.0, 170692.0, 33926.8, 170700.0, 695000.0, 170714.0, 170720.0, 170744.0, 33946.32, 170794.0, 170796.0, 170800.0, 170802.0, 170817.0, 170819.0, 170824.0, 170829.0, 433000.0, 170875.0, 170884.0, 170888.0, 170896.0, 170899.5, 170900.0, 170937.6, 170961.0, 170968.0, 171000.0, 171034.0, 171060.0, 171082.0, 171090.0, 171100.0, 171111.0, 171138.0, 171151.0, 171156.0, 171175.0, 171187.0, 171194.0, 171200.0, 171205.0, 171213.0, 171220.0, 433367.0, 171224.0, 171250.0, 171258.0, 171272.0, 171275.0, 171283.0, 171296.0, 34048.37, 171312.0, 171321.0, 171323.0, 171347.0, 433500.0, 171360.0, 171396.0, 171412.0, 1220000.0, 171452.0, 171456.0, 171458.0, 171474.0, 171476.0, 171500.0, 34088.0, 171521.0, 171526.0, 86202.96, 171565.0, 171574.0, 86208.48, 171600.0, 171609.0, 171624.0, 86215.08, 171648.0, 171653.0, 171658.0, 171661.8, 171700.0, 171717.0, 171741.0, 171750.0, 171756.0, 171778.7, 171787.0, 171794.0, 171800.0, 171812.0, 171826.92, 171834.0, 171837.0, 171838.0, 171840.0, 434000.0, 171875.0, 171880.0, 294683.0, 171908.0, 171912.0, 171931.0, 171950.0, 171951.0, 171960.0, 171995.0, 171996.0, 172000.0, 172008.0, 172021.0, 34191.26, 172028.64, 172029.0, 172050.0, 172053.0, 172070.0, 172086.77, 172099.99, 172100.0, 172099.0, 172120.0, 172142.0, 172150.0, 172152.0, 172163.0, 172200.0, 172222.0, 172231.0, 172238.0, 172249.0, 172250.0, 172252.0, 172278.0, 172283.0, 172300.0, 172321.0, 172323.0, 172329.0, 172340.0, 172350.0, 172353.0, 172354.0, 172356.0, 172370.0, 172377.0, 172390.0, 172400.0, 434600.0, 34279.19, 172492.0, 172498.0, 172500.0, 172515.0, 172516.0, 172521.0, 34293.0, 172550.0, 172552.0, 172557.36, 172564.0, 172565.0, 172585.0, 172589.0, 172600.0, 172625.0, 172640.0, 172666.1, 172684.0, 172700.0, 172705.0, 172730.0, 172736.0, 172768.0, 172789.0, 172800.0, 172825.44, 172830.0, 172851.59, 172852.0, 435000.0, 172872.0, 34361.96, 172900.0, 172908.0, 172915.7, 172927.0, 34376.0, 172992.0, 173000.0, 173004.0, 173047.0, 173072.0, 34401.0, 34402.0, 173088.0, 34403.88, 173094.0, 4367398.0, 173102.0, 173118.0, 173125.0, 34410.0, 173154.0, 173156.0, 173180.0, 173200.0, 34430.0, 435391.98, 173250.0, 34439.6, 34439.0, 173276.0, 34440.56, 173280.0, 34441.0, 173300.0, 173321.0, 34450.89, 173329.0, 173334.0, 34455.0, 173356.0, 173370.0, 173380.0, 173393.0, 173400.0, 173433.0, 173455.0, 173467.0, 173472.0, 173500.0, 173516.0, 295004.0, 173520.0, 173521.0, 8300000.0, 173543.0, 173556.0, 960000.0, 173581.0, 173595.0, 173600.0, 173604.0, 173624.0, 173626.0, 173658.0, 173659.0, 173668.04, 435828.0, 173687.0, 173696.0, 173700.0, 698000.0, 173730.51, 173749.0, 173750.0, 173757.0, 173760.0, 173776.79, 173800.0, 173822.0, 34553.4, 34554.0, 436000.0, 173865.0, 173867.0, 173873.0, 173880.0, 173884.0, 34562.0, 173900.0, 173909.75, 173918.0, 173920.0, 34569.0, 173929.6, 173971.0, 173976.0, 174000.0, 174020.0, 174062.0, 174072.0, 174079.0, 174165.0, 174180.0, 174188.0, 174196.0, 34623.24, 174200.0, 174204.0, 174250.0, 174254.0, 436400.0, 174263.0, 3320000.0, 174280.0, 174289.0, 174300.0, 34647.0, 34650.2, 174334.0, 174341.0, 34653.0, 174350.0, 174362.0, 174370.0, 174380.0, 34662.0, 174400.0, 34665.0, 174426.0, 34677.0, 174477.0, 174480.0, 174487.0, 174500.0, 34684.32, 174527.0, 174532.0, 174537.0, 174566.0, 174584.0, 174600.0, 174638.0, 174660.0, 174682.0, 174692.0, 174694.0, 174697.0, 174700.0, 174706.0, 174708.0, 174720.0, 174740.0, 34732.0, 34732.12, 174756.0, 34735.0, 174792.0, 174800.0, 34744.0, 174807.0, 174808.0, 174812.34, 174827.0, 174849.0, 437000.0, 174888.0, 174895.0, 174900.0, 34773.0, 174973.81, 174994.0, 174995.0, 174996.0, 174999.84, 175000.0, 174999.0, 175020.0, 175042.0, 175075.0, 175100.0, 175101.0, 175115.0, 175125.0, 175138.0, 175146.0, 175154.0, 175163.0, 175176.0, 175180.0, 175187.0, 175192.0, 175200.0, 175220.0, 175240.0, 175247.0, 175250.0, 175278.0, 175283.0, 699587.0, 175300.0, 175307.0, 175322.0, 175340.0, 175350.0, 175353.0, 437500.0, 175375.0, 175400.0, 175420.0, 34868.4, 175436.0, 175440.0, 175443.0, 175448.0, 175452.0, 175467.0, 175499.8, 175500.0, 175513.0, 175522.0, 175549.0, 175550.0, 175557.0, 175560.0, 175573.0, 175576.0, 175600.0, 175603.0, 175605.0, 175606.0, 175615.0, 4255262.0, 175650.0, 175656.0, 175675.0, 175695.0, 175700.0, 700000.0, 175730.74, 175732.0, 175740.0, 175760.0, 700053.85, 175775.0, 175784.0, 175800.0, 175805.0, 175812.0, 175823.0, 175828.0, 34947.0, 34948.0, 34948.8, 175850.0, 438000.0, 34953.16, 34955.15, 175875.0, 34958.0, 175893.0, 175897.0, 175900.0, 34964.8, 175922.0, 175933.0, 175966.0, 175983.0, 175987.2, 175995.0, 175999.0, 176000.0, 176004.0, 176012.0, 1224600.0, 34987.22, 176040.0, 176052.0, 176074.0, 176078.0, 176124.0, 176127.0, 176131.0, 176162.0, 35014.0, 176188.0, 176196.0, 176200.0, 176205.0, 176220.0, 176237.0, 35029.0, 35030.0, 176250.0, 35034.0, 176275.0, 176280.0, 176300.0, 176311.0, 176330.0, 176343.0, 176349.0, 176352.0, 176371.0, 176385.0, 176400.0, 35061.12, 176404.0, 176411.0, 176416.35, 176442.0, 176450.0, 176490.0, 176498.0, 176500.0, 176501.0, 176522.0, 176539.0, 176561.0, 176600.0, 176606.0, 176607.0, 176608.0, 176611.0, 176613.0, 176664.0, 176676.0, 176688.0, 176700.0, 176705.0, 176726.0, 176736.45, 35128.32, 176750.0, 176756.0, 176759.0, 176771.0, 176797.0, 176800.0, 35143.92, 176820.0, 176841.0, 176844.0, 439000.0, 176860.0, 176885.0, 176899.0, 176900.0, 176918.0, 176950.0, 176966.0, 176975.79, 176976.56, 176975.0, 176979.0, 176982.0, 177000.0, 177037.0, 177048.0, 177085.0, 177096.0, 177100.0, 439250.0, 177108.0, 177128.0, 1750000.0, 177136.0, 35209.8, 177153.0, 177160.0, 177200.0, 177222.0, 439376.49, 177240.0, 177250.0, 177257.0, 177268.0, 35234.0, 177277.9, 1488000.0, 35237.28, 35237.0, 177298.5, 177300.0, 177302.0, 177310.0, 177350.0, 177353.0, 177400.0, 35264.2, 177424.47, 177466.0, 177480.0, 177500.0, 177509.0, 177510.0, 177539.0, 177540.0, 177574.0, 177589.0, 177596.0, 177599.0, 177600.0, 177604.0, 177625.0, 177632.0, 439840.0, 177700.0, 177709.0, 177711.0, 702000.0, 702009.0, 439868.0, 177762.0, 177800.0, 177804.0, 177816.0, 177822.0, 177837.0, 35350.2, 440000.0, 177889.0, 177900.0, 177905.0, 177912.0, 35368.0, 177984.0, 177992.0, 177996.0, 178000.0, 964434.0, 178007.0, 178008.0, 178025.0, 178064.0, 35395.72, 178113.0, 35402.0, 58530.37, 178134.0, 178136.0, 35416.56, 178192.0, 35417.83, 178200.0, 35423.0, 178226.0, 178230.0, 35426.0, 35427.0, 178250.0, 178285.0, 178287.36, 178293.0, 178299.0, 178300.0, 178303.0, 702598.0, 178312.0, 178323.45, 35446.08, 178350.0, 178360.0, 178364.0, 964800.0, 178380.0, 178385.0, 178400.0, 35466.0, 178448.0, 178460.0, 178462.0, 178463.0, 178464.0, 178464.72, 178476.0, 178480.0, 35477.52, 178495.0, 35478.0, 178500.0, 35481.57, 35481.0, 35482.2, 178523.0, 178547.0, 178548.0, 178593.0, 178600.0, 35502.8, 178631.0, 178647.0, 178650.0, 178656.0, 178663.0, 178700.0, 35527.0, 178750.0, 35530.0, 178775.0, 178776.0, 35533.0, 178780.0, 35534.0, 703069.0, 178800.0, 178809.0, 178818.0, 35542.0, 178833.0, 178846.0, 178850.0, 35548.76, 441000.0, 441009.0, 178870.0, 178880.0, 178900.0, 178922.48, 178930.0, 178950.0, 178961.0, 178970.0, 178971.0, 178985.0, 35576.32, 179000.0, 179004.0, 179040.0, 179055.68, 35591.0, 179100.0, 179141.96, 179163.0, 179191.0, 179192.0, 179200.0, 179204.0, 179223.0, 179225.0, 179291.0, 179300.0, 179320.0, 179330.0, 179350.0, 179353.0, 179356.0, 35651.2, 179375.0, 179376.0, 179380.0, 179383.0, 35656.98, 179397.0, 179400.0, 35657.88, 35661.0, 179434.0, 179443.0, 179453.0, 179467.0, 179492.0, 179500.0, 179510.0, 179520.0, 179532.0, 179550.0, 179570.0, 179571.0, 179596.0, 179602.0, 179623.0, 179631.0, 179688.0, 179700.0, 704000.0, 179732.0, 179746.0, 179750.0, 179800.0, 179816.0, 179818.0, 179822.0, 442000.0, 179856.0, 179865.0, 179900.0, 179905.0, 179930.0, 966398.0, 179980.0, 179994.0, 180000.0, 180001.0, 180018.0, 35788.0, 180062.0, 180064.0, 180082.0, 180100.0, 442250.0, 35799.87, 180120.0, 35801.0, 180126.0, 180139.0, 180148.0, 35806.0, 180176.0, 180192.0, 180200.0, 180210.0, 180211.2, 35819.0, 180220.0, 35821.0, 180240.0, 180242.0, 180250.0, 35831.18, 35831.0, 180299.0, 180300.0, 180360.0, 180370.44, 180378.0, 180396.0, 180400.0, 180405.0, 180415.44, 180416.0, 35866.0, 180467.0, 180468.0, 180480.0, 180488.08, 35874.0, 180500.0, 180504.0, 180550.0, 180580.0, 180582.0, 180583.0, 180600.0, 180624.0, 180660.0, 180661.0, 180667.0, 180677.0, 180699.0, 180700.0, 180701.0, 180702.0, 180710.02, 705000.0, 442864.0, 442892.0, 180750.0, 180792.0, 180800.0, 35942.4, 443000.0, 180876.0, 180880.0, 180900.0, 180947.0, 180950.0, 180952.0, 180960.0, 181000.0, 181022.0, 181025.0, 181066.0, 35989.0, 181078.0, 443224.0, 181100.0, 181106.0, 181108.0, 181115.38, 181150.0, 181197.0, 181200.0, 443345.0, 36022.08, 36023.0, 181250.0, 181251.0, 181252.0, 181265.0, 181280.0, 181300.0, 181303.0, 181336.0, 181344.0, 181350.0, 36046.4, 181359.0, 181389.14, 181389.46, 181392.0, 181396.0, 181400.0, 4900000.0, 181408.5, 181425.0, 181448.0, 181472.0, 36069.0, 181474.0, 36070.2, 181480.0, 181500.0, 443646.0, 181518.0, 181545.0, 181555.0, 443700.0, 181560.0, 181575.0, 181585.0, 181586.0, 181588.0, 181596.0, 181600.0, 181603.0, 181640.0, 36104.6, 36104.0, 181653.0, 36106.08, 36106.0, 36107.0, 181720.0, 181729.6, 181742.0, 181750.0, 181752.0, 36127.0, 181772.0, 36132.53, 181800.0, 181820.0, 444000.0, 181858.0, 181889.0, 181898.0, 181900.0, 36165.0, 181980.0, 181998.0, 182000.0, 182004.0, 182016.0, 36179.0, 182040.0, 182045.45, 182070.0, 182099.29, 36195.36, 182112.0, 182118.0, 182120.0, 182152.0, 182172.0, 182173.2, 36208.13, 182200.0, 182212.0, 182224.4, 36219.32, 182237.0, 182265.0, 36227.07, 182292.0, 182295.0, 182301.0, 36234.0, 182345.0, 182357.0, 182366.0, 36248.0, 36249.6, 182390.54, 182400.0, 36255.0, 182424.0, 444569.0, 36264.76, 182466.0, 182496.0, 182500.0, 182505.0, 182515.0, 444689.0, 182547.0, 182551.0, 182560.0, 182579.0, 61000000.0, 182596.0, 182600.0, 182608.52, 182652.0, 182700.0, 182712.0, 182739.0, 182750.0, 182771.0, 36332.0, 182800.0, 182803.0, 182850.0, 445000.0, 182859.0, 182864.0, 182941.0, 182944.0, 36362.0, 182952.0, 182971.0, 6998721.0, 182993.0, 183000.0, 183040.0, 183041.0, 183050.0, 183088.0, 183100.0, 183136.0, 183147.0, 36403.08, 36403.2, 183157.0, 36404.0, 36405.29, 183171.0, 183180.0, 183199.0, 183236.0, 59385.0, 183248.0, 36423.36, 183258.0, 183275.0, 183290.0, 183297.0, 183300.0, 36438.0, 36439.0, 183340.0, 183360.0, 183374.0, 183400.0, 183449.0, 183456.0, 183466.0, 183467.0, 183472.0, 183488.0, 183500.0, 183520.6, 36483.72, 36484.92, 970000.0, 36489.84, 183600.0, 36493.78, 36498.51, 183655.0, 183664.0, 183683.0, 708000.0, 183750.0, 183798.84, 183840.0, 183855.0, 446000.0, 183870.0, 183900.0, 183905.0, 183920.0, 36557.0, 183937.0, 183960.0, 183962.0, 183966.0, 183993.46, 183993.0, 184000.0, 970454.0, 184030.0, 184044.0, 184050.0, 184079.0, 184093.0, 184100.0, 184102.0, 184137.0, 184158.0, 36606.32, 36606.58, 184176.0, 36609.41, 184200.0, 184212.0, 184217.0, 184234.0, 184247.0, 7000000.0, 184256.96, 184260.0, 1495000.0, 184284.0, 184300.0, 36632.0, 7000055.0, 36640.18, 36642.32, 184400.0, 184404.0, 1233000.0, 184429.0, 184433.0, 184437.0, 184440.0, 184444.0, 36666.2, 184478.0, 36667.47, 36668.22, 184488.0, 184489.0, 557700.0, 36670.4, 184500.0, 36674.16, 184520.0, 184525.0, 184527.0, 184549.0, 184560.0, 184572.0, 184600.0, 184601.0, 184660.0, 184666.0, 184678.0, 3330432.0, 184714.0, 36714.0, 184736.0, 184739.0, 184750.0, 184756.0, 184760.0, 184777.0, 184785.0, 184793.0, 446940.0, 184800.0, 184809.0, 184843.0, 447000.0, 184859.0, 184900.0, 36752.45, 184938.0, 36759.0, 447095.0, 184970.0, 184978.0, 184984.0, 184990.0, 184992.0, 185000.0, 185004.0, 185014.0, 185015.0, 185016.0, 185027.0, 36778.0, 36778.32, 36780.96, 185052.0, 36781.3, 185100.0, 185106.0, 185120.0, 185132.0, 185140.0, 36798.0, 185196.0, 185200.0, 185210.0, 185220.0, 185232.0, 185250.0, 185266.0, 185275.0, 185286.0, 185300.0, 185320.0, 185325.0, 185332.0, 185333.0, 185340.0, 447500.0, 185400.0, 447555.0, 185451.0, 185455.0, 185500.0, 185516.41, 185521.0, 185530.0, 36879.59, 185555.0, 36886.0, 36889.0, 185600.0, 36890.0, 447750.0, 185616.0, 36895.02, 185640.0, 36899.2, 185658.0, 36903.65, 185672.0, 36907.56, 185692.0, 185700.0, 185707.0, 710000.0, 185738.22, 185750.0, 185796.0, 185800.0, 185811.0, 185844.0, 185846.0, 447996.0, 448000.0, 185893.0, 185900.0, 185915.0, 185934.0, 185963.0, 185981.0, 185995.0, 185999.0, 186000.0, 186029.0, 186053.0, 710350.0, 186063.0, 320003.0, 36987.62, 36988.0, 186098.0, 186100.0, 186112.5, 36992.64, 186192.0, 186200.0, 186202.0, 186205.0, 186219.0, 186245.0, 186250.0, 186268.21, 186271.0, 186278.9, 186285.0, 186295.0, 186300.0, 448447.0, 186335.0, 186350.0, 186352.0, 186356.0, 37041.36, 37044.0, 186394.0, 186400.0, 186411.0, 186432.0, 186435.12, 186440.0, 186455.0, 186462.0, 186472.0, 186480.0, 186500.0, 186599.0, 186600.0, 186657.0, 186668.0, 186684.0, 37105.68, 711000.0, 186721.6, 186728.0, 37115.83, 186743.0, 37117.67, 186750.0, 37123.0, 37124.52, 186800.0, 186813.0, 37133.0, 186830.0, 449000.0, 186900.0, 37148.17, 37162.0, 186983.0, 186996.0, 187000.0, 187019.96, 187076.0, 187093.0, 187116.0, 187118.0, 187144.0, 187147.0, 187169.0, 187200.0, 187217.0, 187220.0, 187250.0, 37224.36, 449436.0, 187296.0, 187300.0, 187321.0, 187325.0, 449500.0, 187359.0, 187400.0, 63458.17, 187440.0, 187450.0, 63460.8, 187476.0, 187485.0, 187496.0, 187500.0, 187509.0, 187523.0, 37274.0, 187541.0, 187542.0, 187550.0, 187560.0, 187568.0, 37284.85, 187595.0, 187600.0, 37288.0, 187609.72, 187616.0, 187650.0, 37301.63, 187680.0, 187695.0, 187700.0, 187706.0, 712000.0, 187726.0, 37315.2, 37318.0, 187768.0, 187777.0, 187787.0, 187800.0, 187834.0, 187845.0, 450000.0, 450001.0, 187858.0, 450002.0, 450004.0, 37342.08, 187900.0, 187915.0, 187920.0, 187937.0, 187950.0, 187960.0, 187980.0, 187986.0, 450130.0, 188000.0, 188004.0, 37375.97, 188053.0, 450200.0, 450213.0, 712400.0, 188134.0, 188160.0, 37401.0, 37403.0, 188200.0, 188252.0, 188295.0, 188296.0, 188300.0, 37433.53, 188339.0, 37434.31, 188348.0, 188358.0, 37438.8, 188372.0, 188386.0, 188400.0, 188408.0, 37455.94, 37455.0, 188488.0, 188500.0, 188508.0, 188517.0, 188532.0, 188543.0, 188555.0, 975000.0, 188571.0, 188592.0, 188600.0, 37491.0, 188640.0, 188665.0, 188692.0, 37504.84, 188700.0, 2548000.0, 188709.0, 188717.0, 37509.32, 188722.0, 188727.0, 37511.0, 37513.0, 37514.44, 188750.0, 37519.0, 188777.0, 37522.0, 188800.0, 188809.0, 188836.0, 37534.0, 451000.0, 37538.0, 188888.0, 188892.4, 188900.0, 37547.0, 37547.4, 37554.36, 188961.0, 37559.12, 188987.0, 188994.0, 188996.0, 189000.0, 37569.0, 189023.0, 189040.0, 189080.0, 189094.0, 189145.0, 189200.0, 37617.0, 37619.04, 1500000.0, 189280.0, 1500006.0, 189300.0, 189318.0, 189323.33, 189329.0, 189350.0, 189352.0, 189354.0, 189400.0, 189404.0, 1500150.0, 189460.0, 37658.28, 189475.0, 37664.68, 189500.0, 189503.0, 189506.0, 189523.0, 37670.96, 37673.0, 189566.0, 189573.0, 189579.0, 189600.0, 37686.44, 37686.55, 189626.0, 189632.0, 189635.0, 189644.0, 189648.0, 189657.0, 189659.0, 189674.0, 189700.0, 189704.0, 714000.0, 189750.0, 189764.0, 189800.0, 189812.0, 451956.0, 189856.25, 452000.0, 37736.52, 189875.0, 189876.0, 189895.0, 189900.0, 189915.0, 452060.0, 189919.0, 189950.0, 189951.0, 452100.0, 37757.2, 189982.0, 189995.0, 189996.0, 189997.0, 189999.0, 190000.0, 190008.0, 190032.0, 190040.0, 190060.25, 190075.0, 190091.0, 190138.0, 190140.0, 190160.0, 190165.0, 190176.0, 190179.0, 190191.0, 37803.83, 190200.0, 37806.0, 190225.0, 37814.4, 452400.0, 190257.0, 190276.0, 37820.51, 190291.0, 190300.0, 190329.0, 190400.0, 452555.0, 190413.0, 190434.0, 190459.0, 190465.0, 190486.0, 190499.0, 190500.0, 37863.0, 37864.0, 190512.0, 37867.72, 190550.0, 190566.89, 298400.0, 190606.0, 190610.0, 190611.0, 190617.0, 190634.0, 37891.68, 190649.0, 37898.4, 37902.0, 715000.0, 190742.16, 4647200.0, 37915.0, 37917.0, 37918.08, 190800.0, 190849.0, 190850.0, 453000.0, 190900.0, 190904.0, 190912.0, 190960.0, 190973.0, 37961.18, 191000.0, 1239600.0, 191068.0, 37979.85, 191100.0, 37983.4, 298500.0, 37985.0, 37986.5, 191136.0, 37991.0, 37994.4, 37995.0, 37995.3, 37996.25, 191179.0, 191182.0, 38000.04, 38001.36, 191200.0, 191277.0, 191284.0, 191300.0, 191387.0, 191400.0, 191405.0, 191430.0, 191442.16, 453600.0, 191500.0, 191508.0, 191520.0, 191556.0, 191608.0, 191630.0, 191640.0, 453800.0, 191700.0, 191727.0, 191734.0, 191744.0, 191745.0, 191776.0, 191780.0, 191781.0, 191800.0, 191812.0, 191818.0, 38126.42, 38126.4, 454000.0, 191870.0, 191871.8, 191880.0, 191887.0, 191900.0, 191938.0, 38152.94, 191962.0, 191976.0, 60384.88, 192000.0, 454189.0, 192057.0, 192100.0, 192118.0, 38185.0, 3600000.0, 38187.0, 192135.0, 38189.0, 192150.0, 192168.0, 38195.0, 192195.5, 192200.0, 192224.0, 192242.0, 192247.0, 192250.0, 38212.0, 38214.0, 192284.0, 192297.0, 192334.0, 192352.0, 192400.0, 454551.0, 192409.68, 38246.0, 192440.0, 38249.0, 38253.36, 192500.0, 192547.0, 192562.0, 454744.0, 192600.0, 454745.0, 192647.0, 192651.0, 192678.0, 192692.0, 192700.0, 717000.0, 192729.0, 192750.0, 192800.0, 192818.0, 192821.0, 192840.0, 455000.0, 192856.0, 192868.0, 192892.0, 192937.0, 192972.0, 38357.04, 38358.4, 193000.0, 193025.0, 38366.0, 193050.0, 193064.0, 193076.96, 193086.0, 38378.35, 193122.0, 193131.2, 193155.0, 38396.8, 193187.0, 38398.0, 193200.0, 193201.0, 38401.0, 193212.0, 193215.0, 455400.0, 38418.24, 455438.0, 193300.0, 193308.0, 38423.87, 38423.0, 193368.0, 193400.0, 38443.0, 193440.0, 455593.0, 193472.46, 193500.0, 38470.36, 38472.24, 980000.0, 38474.56, 38475.0, 193596.0, 193600.0, 193673.0, 193693.0, 193737.0, 193748.0, 193783.0, 193800.0, 193838.0, 193840.0, 456000.0, 193900.0, 193920.0, 193968.0, 456128.0, 193992.0, 194000.0, 194040.0, 194050.0, 194080.0, 38575.8, 194082.0, 194100.0, 194106.0, 194160.0, 38593.65, 194180.0, 194184.0, 194199.0, 194208.0, 38601.72, 38604.96, 194231.0, 194280.0, 194292.0, 38618.0, 38626.57, 194369.0, 194371.0, 194377.0, 194400.0, 38643.0, 194447.31, 38653.0, 194500.0, 194512.0, 194515.0, 194558.0, 194560.0, 194562.0, 194600.0, 38683.61, 194650.0, 38691.88, 194670.0, 38697.84, 38701.0, 38708.8, 194751.0, 38711.0, 194800.0, 194808.0, 194821.0, 38726.0, 38728.0, 7010600.0, 457000.0, 194900.0, 194938.0, 38751.96, 5700000.0, 194978.0, 195000.0, 38767.6, 195054.78, 38769.0, 38774.0, 38776.0, 38778.83, 38778.84, 195111.37, 195147.0, 195150.0, 195155.0, 38793.57, 195200.0, 195250.0, 38812.8, 38813.75, 195300.0, 195312.0, 195322.0, 195324.0, 38830.0, 195372.0, 38833.6, 195400.0, 1244000.0, 38842.0, 195431.0, 195443.0, 457600.0, 195478.45, 195480.0, 195500.0, 195520.0, 195529.0, 195543.0, 195545.0, 38867.0, 195550.0, 982000.0, 195579.0, 195598.0, 195599.0, 195600.0, 38887.0, 38888.4, 38893.92, 195700.0, 720000.0, 195750.0, 195787.0, 195800.0, 38918.0, 195808.0, 38922.0, 195850.0, 458000.0, 195857.0, 720153.0, 195900.0, 195960.0, 195974.28, 38952.84, 38955.0, 195996.0, 196000.0, 38957.0, 38957.67, 38962.8, 196030.5, 196036.0, 196041.0, 38971.0, 38973.0, 196112.0, 38979.2, 38982.0, 196140.0, 196153.0, 38996.64, 196200.0, 196202.0, 196210.0, 196247.0, 39006.0, 196264.0, 720565.0, 196281.0, 196287.0, 196288.0, 196295.0, 196300.0, 196335.0, 196343.0, 196351.0, 458514.0, 196400.0, 196417.2, 196433.0, 196483.0, 39054.7, 196500.0, 39056.0, 39058.0, 39059.0, 196520.0, 39062.6, 196540.0, 39065.76, 39065.0, 39066.0, 196560.0, 196580.0, 196600.0, 196623.0, 196640.0, 196649.0, 196700.0, 721000.0, 39109.8, 196769.0, 196777.59, 196790.0, 39115.96, 196800.0, 39119.0, 39123.0, 39124.8, 39124.48, 196852.0, 459000.0, 39127.0, 196875.0, 196886.0, 196893.0, 196936.0, 196942.0, 196959.0, 196974.0, 196983.0, 39152.4, 39154.8, 196996.0, 197000.0, 197004.0, 39158.39, 39165.0, 197058.72, 197085.0, 39174.0, 39175.0, 197100.0, 39177.6, 197132.0, 39184.0, 197153.0, 39186.0, 39187.53, 39187.33, 197200.0, 459375.0, 197250.0, 197264.0, 197279.0, 197280.0, 1508000.0, 197300.0, 197302.0, 197335.0, 197347.6, 197383.0, 197404.0, 197416.0, 197450.0, 197500.0, 197520.0, 197540.0, 197550.0, 197564.0, 984000.0, 197600.0, 197616.0, 39282.0, 197647.0, 197648.0, 197660.0, 197662.0, 39292.0, 197690.0, 197724.0, 197741.26, 197795.0, 460000.0, 197886.0, 197908.0, 197999.0, 198000.0, 198022.0, 198050.0, 198085.0, 198100.0, 198117.0, 198171.96, 39389.0, 198180.0, 39393.0, 198200.0, 198204.0, 722511.0, 198225.0, 198248.0, 198250.0, 39405.48, 198281.0, 198285.3, 460443.0, 198300.0, 198314.0, 39418.0, 198328.0, 39423.0, 39428.0, 198400.0, 39434.94, 39436.8, 198425.64, 198454.0, 198459.0, 198475.0, 198500.0, 198520.0, 460700.0, 198560.0, 39466.8, 985000.0, 198583.0, 198600.0, 198639.0, 198667.0, 198679.0, 198700.0, 198701.0, 198720.0, 198791.0, 60003.0, 198800.0, 57399.72, 39522.0, 461000.0, 198875.0, 461023.0, 461029.0, 198888.0, 198900.0, 198925.0, 198928.93, 39544.5, 198959.0, 198999.0, 199000.0, 199009.0, 199074.0, 199103.0, 199132.0, 39580.8, 39581.31, 300100.0, 199192.0, 300107.0, 199200.0, 199205.0, 39602.04, 199250.0, 199260.0, 199276.0, 1510000.0, 199376.0, 199400.0, 199410.0, 1248000.0, 199492.08, 199500.0, 199512.0, 199520.0, 4260016.0, 199560.0, 199578.0, 199597.0, 199600.0, 199647.0, 199650.0, 199680.0, 199683.0, 199690.0, 199717.0, 199730.0, 39705.6, 39705.0, 199772.0, 39707.2, 199780.0, 199790.0, 199800.0, 39712.0, 39714.78, 199825.0, 39721.56, 39721.0, 462000.0, 199866.0, 199900.0, 199940.0, 39740.0, 199946.0, 199950.0, 199956.0, 199980.0, 199990.0, 199992.0, 39750.96, 39751.0, 199999.0, 200000.0, 200001.0, 200003.0, 200004.0, 200005.0, 200010.0, 200013.0, 200052.0, 200066.0, 200100.0, 200145.0, 724458.0, 200174.0, 200200.0, 724494.0, 462370.8, 200250.0, 200300.0, 200310.0, 200323.0, 200354.0, 462500.0, 39823.36, 200400.0, 200410.0, 200416.0, 39839.28, 200450.0, 200500.0, 200508.96, 200518.0, 200529.0, 200550.0, 200556.0, 987000.0, 39865.0, 200592.0, 200600.0, 39871.0, 200625.0, 200640.0, 39889.0, 39890.64, 200700.0, 39892.0, 725000.0, 3870776.0, 200800.0, 39911.23, 200810.0, 200838.0, 200842.0, 463000.0, 200865.0, 200900.0, 200946.0, 200974.0, 200986.0, 201000.0, 201015.0, 201029.0, 201037.0, 201067.2, 39967.0, 201096.0, 201120.0, 39974.0, 39979.0, 300500.0, 39998.5, 39999.6, 39999.82, 201272.0, 201287.0, 201300.0, 201311.0, 201341.71, 201350.0, 201360.0, 201384.0, 201400.0, 201422.24, 201423.0, 1250000.0, 1250007.0, 201432.0, 201440.0, 201471.0, 201500.0, 201519.0, 40057.0, 40058.0, 40061.0, 201563.0, 988000.0, 201576.0, 201595.86, 201600.0, 201654.0, 201655.0, 201658.0, 201667.0, 201673.0, 201700.0, 726000.0, 201724.0, 201753.0, 201760.0, 40102.08, 201770.0, 40105.0, 40113.0, 40116.12, 40116.97, 40117.0, 40119.0, 464000.0, 40121.76, 40122.52, 201922.0, 201960.0, 201964.0, 40143.0, 201975.0, 201996.0, 202000.0, 40151.0, 202040.0, 40164.84, 40164.8, 40172.0, 202155.0, 202200.0, 40189.0, 40191.0, 60139.0, 202235.0, 40202.64, 40206.8, 202290.0, 202295.0, 202300.0, 40209.0, 202364.0, 202389.0, 40227.2, 202405.0, 40231.36, 202492.0, 40247.0, 202500.0, 40252.8, 40252.0, 40254.0, 40254.77, 40255.0, 202540.0, 202542.0, 464691.0, 40260.61, 202560.0, 40264.0, 202580.0, 40266.0, 40267.5, 202600.0, 202614.0, 202624.0, 40273.0, 202626.0, 40275.24, 202650.0, 202656.0, 202680.0, 40287.0, 202700.0, 1251300.0, 202781.0, 202800.0, 202803.0, 202809.0, 202823.0, 202840.0, 2300000.0, 202852.0, 465000.0, 202878.0, 202888.0, 202900.0, 202910.0, 202916.0, 40334.0, 203000.0, 203004.0, 465154.0, 203040.0, 203129.0, 203168.0, 203179.0, 203200.0, 203210.0, 203241.0, 465397.0, 203311.42, 203320.0, 203329.0, 203336.0, 203400.0, 40428.48, 203426.0, 40433.0, 465580.0, 203441.0, 40436.0, 203449.67, 40438.18, 203455.0, 203492.0, 203496.0, 203500.0, 203508.0, 203532.59, 40457.0, 203561.0, 990000.0, 203570.0, 203572.0, 40462.0, 203580.0, 203585.0, 203600.0, 203603.0, 203609.5, 40471.0, 40473.0, 203670.0, 203700.0, 40487.72, 203712.0, 728000.0, 203718.0, 40494.48, 40494.0, 203739.0, 203749.0, 203750.0, 203786.0, 203800.0, 466000.0, 40519.0, 203872.0, 203880.0, 203890.0, 203908.0, 203950.0, 203976.0, 203997.0, 204000.0, 204018.0, 204035.0, 204053.0, 40557.0, 204060.0, 40564.88, 561600.0, 204152.0, 301099.0, 204200.0, 204215.0, 204218.0, 40600.46, 40601.6, 40603.0, 204308.0, 204395.0, 40638.6, 40638.0, 40639.0, 40639.13, 204468.0, 204480.0, 40643.2, 204500.0, 991000.0, 204600.0, 204685.0, 40683.0, 204725.0, 204750.0, 2039784.0, 40703.0, 204800.0, 40707.0, 40708.63, 40709.0, 204821.0, 204827.0, 204842.0, 204855.0, 467000.0, 204864.0, 40723.2, 204900.0, 204920.0, 40732.0, 40733.94, 204939.32, 204950.0, 204951.0, 204962.0, 204978.0, 204996.0, 205000.0, 205027.0, 205071.5, 729368.0, 40767.0, 40768.12, 205136.0, 205176.0, 205200.0, 205215.0, 205219.0, 205250.0, 40802.0, 205293.0, 205300.0, 40806.0, 40809.0, 40810.8, 40812.0, 205340.0, 467500.0, 205400.0, 1254000.0, 205450.0, 205481.0, 205486.0, 205500.0, 205515.0, 40855.0, 205566.0, 205572.0, 205600.0, 205639.0, 205647.52, 205650.0, 60277.0, 205691.0, 4400000.0, 729991.0, 730000.0, 205752.0, 205768.0, 205800.0, 205833.0, 468000.0, 205878.0, 205885.0, 205900.0, 205925.0, 60938.88, 206000.0, 614083.0, 206040.0, 40954.0, 206100.0, 206200.0, 206250.0, 40999.92, 206280.0, 41000.54, 206338.0, 206378.0, 206400.0, 41031.0, 41033.0, 206467.0, 206494.0, 206500.0, 206510.0, 206538.0, 206539.0, 206563.0, 206585.0, 206600.0, 206615.0, 41069.76, 206640.0, 206667.0, 206671.2, 206690.0, 206691.0, 469000.0, 206889.0, 41123.0, 41125.5, 206971.0, 207000.0, 41146.0, 41154.24, 207100.0, 41164.8, 207158.0, 207182.0, 207200.0, 469428.89, 207300.0, 207330.0, 207332.0, 60345.0, 207400.0, 207489.0, 41242.33, 207500.0, 207505.0, 41252.0, 41254.98, 207569.0, 41257.0, 207582.0, 207585.0, 207593.0, 207600.0, 207640.0, 207700.0, 732000.0, 41295.16, 41298.0, 41299.8, 60361.0, 207800.0, 470000.0, 207890.0, 207900.0, 41324.8, 41326.0, 41327.0, 41328.72, 207932.0, 41329.03, 41330.0, 41331.0, 41331.6, 41334.0, 207996.0, 208000.0, 208031.16, 41356.8, 41358.96, 41359.0, 208100.0, 208119.0, 470275.0, 41373.0, 41381.0, 208200.0, 208207.0, 41386.8, 208228.0, 208263.0, 208300.0, 41418.0, 41419.0, 208392.0, 208400.0, 208416.52, 208421.0, 208431.0, 41431.0, 208457.0, 208488.0, 208500.0, 208521.0, 6500000.0, 995000.0, 6500031.0, 208575.0, 41458.32, 41464.28, 208627.5, 2568000.0, 208707.0, 208725.0, 41490.72, 41493.0, 208765.0, 470924.94, 208788.0, 208800.0, 208808.0, 41503.0, 208849.0, 208851.0, 471000.0, 41514.0, 41515.0, 208888.0, 208900.0, 41524.08, 41527.39, 471087.0, 41529.6, 208960.0, 41537.6, 208987.0, 208989.0, 209000.0, 209004.0, 209041.0, 209048.6, 209100.0, 41564.0, 41565.0, 1782000.0, 41569.0, 41571.0, 209205.0, 41582.0, 209232.0, 209250.0, 209300.0, 209329.1, 209340.0, 209393.0, 209398.0, 41623.0, 209431.0, 209491.0, 209500.91, 209500.0, 209545.0, 41657.0, 209600.0, 41671.0, 209700.0, 734000.0, 209761.65, 209765.0, 209788.0, 209828.0, 472000.0, 209862.0, 41715.5, 209893.0, 41718.0, 209940.0, 58679.92, 210000.0, 210100.0, 210120.0, 210124.0, 210131.0, 41766.4, 41769.24, 41769.36, 41771.0, 41778.0, 210200.0, 210204.0, 41781.0, 41782.0, 41783.0, 210250.0, 210260.0, 210266.0, 210292.0, 210311.0, 210323.0, 210352.0, 210400.0, 210432.0, 5715456.0, 41828.8, 210473.0, 41833.92, 210500.0, 210538.0, 210558.0, 210559.6, 41858.75, 41866.0, 41866.56, 41870.52, 41871.0, 41872.22, 6240000.0, 735000.0, 210716.0, 302400.0, 210800.0, 41899.26, 210820.0, 473000.0, 210906.54, 41926.0, 41928.42, 41933.0, 210984.0, 210990.0, 211000.0, 211100.0, 211155.0, 473335.0, 211200.0, 41978.3, 473349.0, 211223.0, 211300.0, 211331.0, 211382.0, 211400.0, 1784265.71, 211404.0, 211408.0, 1260000.0, 211440.0, 211498.0, 211500.0, 998000.0, 211575.0, 42055.0, 42056.0, 211600.0, 60513.0, 211646.0, 211680.0, 211713.0, 42080.9, 211724.0, 211765.0, 211781.0, 42094.0, 42095.64, 211794.0, 42099.2, 474000.0, 211870.0, 211900.0, 211978.67, 212000.0, 59877.24, 212098.0, 212100.0, 42166.0, 42168.82, 42169.0, 42171.6, 212189.0, 42176.0, 42178.5, 42183.0, 212267.0, 212307.0, 212353.28, 212362.0, 212400.0, 42220.68, 212422.0, 212455.0, 212472.0, 212500.0, 474697.0, 212560.0, 42253.0, 42255.0, 42261.0, 42263.0, 212658.0, 212660.0, 42269.0, 42271.0, 212695.0, 212709.0, 212712.0, 212738.0, 212750.0, 474910.0, 212785.0, 212800.0, 42296.76, 42299.0, 475000.0, 42307.2, 212860.0, 212869.0, 42311.44, 475055.0, 212916.0, 212920.0, 212935.0, 212950.0, 212952.0, 42326.18, 42327.84, 42332.0, 213000.0, 42339.0, 42342.0, 42343.77, 213040.0, 42344.0, 42344.07, 42345.96, 213100.0, 42362.0, 42366.01, 42367.0, 213195.0, 42376.0, 42388.0, 42388.41, 42389.39, 42391.0, 42394.0, 213300.0, 42407.28, 42408.36, 42410.4, 42413.0, 213397.0, 213400.0, 213409.0, 42421.6, 42424.68, 42426.99, 213468.0, 213500.0, 213540.0, 999999.0, 1000000.0, 1000001.0, 213600.0, 213650.0, 42473.0, 213721.0, 42481.0, 42492.0, 213800.0, 42499.0, 213831.56, 213840.0, 476000.0, 213856.0, 42507.0, 213865.0, 1000303.0, 213888.0, 213899.0, 213900.0, 213902.0, 42515.2, 213907.0, 213908.0, 42524.4, 42527.0, 213990.0, 213996.0, 214000.0, 214070.0, 42556.8, 42562.0, 214200.0, 42578.0, 214255.0, 42604.0, 42605.72, 42610.08, 214400.0, 42614.16, 42619.2, 42621.0, 42624.12, 214456.0, 214458.0, 42625.0, 42626.65, 214500.0, 214583.0, 214590.36, 214596.0, 42655.8, 42655.65, 214621.0, 214644.0, 42664.0, 42665.88, 214704.0, 739000.0, 214718.0, 214750.0, 214793.0, 214800.0, 214813.0, 214825.68, 214835.0, 214850.0, 477000.0, 42708.0, 42708.59, 58689.48, 42713.76, 42716.0, 3098500.0, 214927.0, 214944.0, 42730.38, 42731.0, 214992.0, 42732.77, 214997.0, 215000.0, 215004.0, 215033.0, 215040.0, 42744.78, 42751.0, 215099.0, 215100.0, 215115.0, 215149.0, 215200.0, 42773.0, 477372.0, 215300.0, 215400.0, 42817.0, 42823.0, 215471.0, 215484.0, 215500.0, 215623.0, 215625.0, 215650.0, 4147811.0, 215662.0, 42866.78, 42869.6, 740000.0, 215719.0, 215740.0, 215750.0, 215800.0, 478000.0, 215900.0, 60683.0, 42918.0, 42924.0, 215964.0, 216000.0, 42936.84, 1085000.0, 216096.0, 216120.0, 216126.0, 216144.0, 42962.77, 42963.0, 42973.56, 42977.0, 478400.0, 303500.0, 216287.0, 216300.0, 216380.0, 216400.0, 3100000.0, 216495.0, 216500.0, 216554.0, 43044.98, 216575.0, 303562.0, 216600.0, 216640.0, 478800.0, 216754.0, 216770.0, 2838240.0, 216850.0, 479000.0, 216880.0, 216960.0, 216996.0, 217000.0, 217068.0, 217100.0, 43151.18, 43153.0, 217124.0, 217142.0, 217160.0, 741600.0, 479484.0, 479500.0, 217364.0, 43205.0, 217406.0, 43215.56, 43217.67, 43220.6, 217454.0, 43221.0, 43224.75, 43225.08, 217487.0, 217500.0, 217547.0, 217600.0, 217650.0, 217818.71, 480000.0, 480003.0, 43305.0, 43305.85, 43307.42, 43308.88, 217900.0, 217914.0, 43315.32, 43317.93, 43323.0, 43325.0, 43326.66, 43328.64, 218000.0, 480159.0, 43336.8, 43337.0, 43342.8, 43349.98, 218108.0, 43351.2, 218123.0, 43354.0, 218144.0, 43358.0, 218250.0, 43387.92, 218308.0, 480480.0, 43396.0, 43402.0, 43404.24, 218390.0, 218400.0, 43409.52, 43409.0, 43418.0, 43420.78, 480610.0, 43422.0, 43426.08, 218500.0, 742800.0, 43432.1, 218554.0, 218646.0, 218647.37, 43466.43, 218693.0, 742984.09, 43473.0, 218750.0, 218760.0, 43482.0, 43484.0, 218800.0, 43490.0, 43493.0, 43494.0, 218833.0, 43496.0, 43497.36, 743160.0, 218900.0, 218902.0, 43510.0, 218930.53, 43515.0, 218958.0, 218960.0, 219000.0, 43530.44, 219024.0, 219056.0, 219076.0, 43552.48, 219152.0, 219185.0, 219200.0, 219250.0, 5200000.0, 219268.0, 219289.0, 219294.0, 43590.0, 43592.64, 43594.0, 43595.0, 43604.72, 43607.08, 43607.0, 43610.0, 219420.0, 219463.0, 43620.58, 219500.0, 43628.88, 219596.0, 219600.0, 219605.0, 743935.0, 43663.0, 219684.0, 219686.0, 744000.0, 43671.81, 219750.0, 219778.0, 219797.0, 219800.0, 219818.4, 219820.0, 43691.16, 219825.12, 219858.0, 43699.2, 219873.0, 219896.0, 219900.0, 219906.0, 219923.0, 43713.0, 219948.0, 43718.0, 219996.0, 220000.0, 220001.0, 43727.0, 43733.96, 43735.0, 43743.18, 220083.0, 1006556.0, 220161.0, 43761.0, 43762.0, 43763.2, 43766.0, 62808.77, 220258.0, 220264.42, 220271.0, 220286.0, 220300.0, 220320.0, 220387.0, 220404.0, 220412.0, 220434.0, 220456.0, 43818.0, 43822.0, 220500.0, 220563.0, 43843.7, 220587.0, 220590.0, 220640.0, 43856.0, 220681.0, 43862.0, 2580000.0, 745000.0, 43873.0, 220738.0, 43874.0, 43878.9, 482916.0, 43882.14, 43885.0, 220800.0, 43886.0, 43887.9, 220828.0, 220829.0, 220836.0, 220840.0, 220842.0, 483000.0, 220900.0, 220920.0, 220991.0, 220992.0, 221000.0, 221014.0, 221038.44, 221110.0, 221150.0, 43956.0, 221158.0, 221192.0, 221204.0, 43967.0, 221291.0, 43984.23, 43984.0, 4750000.0, 1007787.0, 483505.0, 221370.0, 221450.0, 221451.0, 221500.0, 221544.0, 221554.0, 221557.0, 1008000.0, 221580.0, 221663.0, 221680.0, 221696.9, 221747.0, 221750.0, 44076.96, 221788.0, 221796.0, 221829.0, 221846.0, 484000.0, 221887.0, 221951.0, 222000.0, 44128.0, 222027.0, 222033.0, 44131.0, 222046.77, 222048.0, 44135.0, 222069.75, 44139.84, 44139.47, 44141.4, 44144.0, 7300000.0, 44151.0, 44152.2, 222145.85, 222150.0, 44157.48, 44157.0, 222200.0, 44164.0, 222211.0, 222222.0, 222271.0, 222300.0, 222355.0, 222395.0, 222400.0, 222424.0, 222435.0, 222440.0, 222465.8, 44219.33, 222500.0, 222507.0, 222560.0, 222587.8, 222588.0, 484790.0, 747000.0, 222716.0, 44269.0, 44276.0, 44277.0, 44278.41, 44282.74, 44283.2, 44283.0, 222800.0, 304800.0, 44286.0, 44287.0, 222853.0, 485000.0, 222857.0, 44298.73, 44304.07, 5990095.0, 222956.0, 222960.0, 223000.0, 223018.0, 223050.0, 44337.86, 223117.0, 223145.0, 44354.44, 44366.0, 223243.0, 44383.0, 223308.0, 44384.88, 44389.0, 44396.0, 223375.0, 223400.0, 223445.0, 223478.0, 223500.0, 223515.0, 223525.82, 44433.12, 1010000.0, 223600.0, 44447.08, 44449.6, 223699.0, 748000.0, 44465.52, 44471.0, 223765.0, 44481.0, 223800.0, 44483.0, 223808.0, 223819.0, 223871.0, 223922.0, 223924.0, 44507.0, 61003.0, 224000.0, 486169.0, 44536.0, 44539.0, 44542.0, 224110.0, 44546.26, 44546.0, 44547.0, 44548.49, 44551.0, 224221.0, 44566.4, 224258.0, 224260.0, 44576.0, 224294.61, 486441.0, 44583.71, 44588.0, 4680804.0, 224370.0, 224400.0, 224481.0, 44620.8, 44635.0, 224640.0, 44650.04, 44651.0, 224673.0, 749000.0, 224750.0, 224775.0, 224800.0, 224810.0, 44683.0, 44686.0, 224845.0, 224851.0, 486996.0, 44691.0, 487000.0, 44692.0, 224900.0, 224914.0, 224963.0, 44714.1, 44719.92, 225000.0, 225021.0, 225022.0, 44732.0, 44743.0, 225160.0, 225179.0, 225244.0, 225274.0, 44779.12, 44779.0, 225300.0, 44782.92, 44783.42, 225321.0, 225369.0, 225400.0, 225480.0, 225500.0, 44824.08, 749810.0, 44828.28, 225556.0, 225600.0, 225612.0, 44844.56, 305362.0, 44859.0, 225700.0, 750000.0, 750001.0, 750002.0, 750003.0, 225750.0, 225764.0, 225800.0, 225840.0, 488000.0, 44898.72, 44908.03, 44918.0, 44919.0, 226000.0, 226008.0, 226045.0, 226063.0, 305448.0, 305452.0, 226098.0, 44955.87, 226200.0, 44971.0, 226300.0, 226305.0, 226320.0, 226323.0, 305500.0, 226340.0, 44988.02, 44991.48, 44993.0, 226379.0, 44997.0, 45000.02, 45000.18, 1275000.0, 226427.0, 45005.04, 45009.0, 45014.0, 226500.0, 226514.0, 226572.0, 226580.0, 226600.0, 45051.96, 45052.1, 45054.0, 751000.0, 488888.0, 226775.0, 226800.0, 61117.0, 226841.0, 489000.0, 226883.0, 61121.0, 226925.0, 45112.0, 489133.0, 226992.0, 226995.12, 227000.0, 45119.0, 45120.4, 227011.0, 45124.0, 45124.8, 751329.0, 1800000.0, 227165.0, 227166.0, 227200.0, 227226.0, 227236.0, 227240.0, 227243.0, 227278.0, 227344.0, 227388.0, 45196.0, 227400.0, 45199.0, 227415.0, 45201.36, 227421.0, 227446.0, 45208.0, 227469.0, 227500.0, 45249.91, 227698.0, 227700.0, 227718.0, 227733.0, 45267.23, 45271.0, 4160000.0, 45286.8, 490000.0, 45288.72, 227859.0, 45311.16, 45314.4, 227995.0, 228000.0, 45330.0, 228068.0, 228081.0, 228090.0, 45338.0, 45342.0, 45347.76, 228199.0, 45358.0, 228230.0, 228250.0, 45377.0, 45379.53, 45380.4, 228400.0, 61181.0, 228490.0, 228500.0, 228510.0, 228515.0, 228538.0, 228552.0, 228572.78, 228600.0, 228604.0, 752910.0, 305958.0, 45449.0, 228672.0, 228690.0, 45455.0, 228708.0, 228750.0, 228755.0, 45468.8, 45470.16, 228800.0, 228852.0, 491000.0, 45496.0, 228996.0, 229000.0, 45516.84, 45517.32, 229076.0, 229112.0, 45542.0, 45543.23, 61211.0, 229200.0, 229209.0, 566600.0, 229278.0, 45576.36, 229308.0, 45593.16, 45596.28, 306118.0, 45607.66, 229500.0, 45615.96, 45616.0, 229565.95, 229571.67, 45633.25, 229603.0, 45644.41, 229650.0, 45647.0, 45650.8, 45651.24, 491834.0, 45655.0, 229705.0, 45657.0, 45658.0, 45665.74, 45665.87, 45668.0, 45669.42, 45669.0, 45671.0, 45672.75, 45673.92, 45676.8, 45678.48, 45678.84, 229850.0, 45685.0, 492000.0, 229857.0, 63192.52, 45688.0, 45691.7, 229900.0, 229933.0, 229956.0, 3900000.0, 229992.0, 230000.0, 230004.0, 230009.0, 45718.0, 63199.77, 45727.2, 45731.04, 45734.52, 230100.0, 45736.0, 45737.4, 45742.0, 45744.82, 230153.75, 45746.0, 230168.0, 230192.0, 230200.0, 45768.0, 230316.0, 230318.0, 45793.0, 45794.0, 45798.0, 230500.0, 492666.0, 45819.0, 230600.0, 45834.0, 45837.59, 230625.0, 0.36, 230669.0, 755000.0, 230748.0, 45863.0, 230750.0, 45867.0, 230800.0, 492960.0, 230855.0, 493000.0, 45885.0, 230863.44, 230868.0, 230891.0, 45894.0, 230957.0, 755281.0, 231000.0, 231045.0, 231084.0, 493236.0, 231100.0, 755425.86, 9930475.0, 231240.0, 231250.0, 231252.0, 231280.0, 493433.0, 45976.0, 231319.0, 231356.17, 1280000.0, 46001.0, 231463.0, 231487.0, 231500.0, 231504.0, 231553.0, 231669.0, 231677.0, 46049.86, 46052.0, 46052.39, 231700.0, 46053.5, 756000.0, 231800.0, 494000.0, 231875.0, 231910.0, 46095.0, 46098.31, 46109.0, 232000.0, 46113.6, 232032.0, 46123.0, 46125.65, 46125.04, 46133.0, 46136.04, 232174.0, 232200.0, 232212.0, 232224.0, 232234.0, 46161.0, 232250.0, 494400.0, 46165.0, 232271.0, 232325.0, 46179.0, 232344.0, 46180.0, 46184.18, 46189.03, 46192.0, 232424.0, 232455.0, 232462.0, 232500.0, 232560.0, 46226.16, 46227.0, 232597.0, 232651.0, 232652.0, 46247.0, 46254.72, 46255.6, 757032.0, 232785.0, 232800.0, 6000000.0, 232839.0, 232845.0, 495000.0, 232872.0, 232883.0, 232900.0, 232963.0, 232992.0, 233000.0, 233004.0, 46323.0, 46324.8, 233100.0, 46341.0, 46342.46, 46345.0, 46349.0, 233200.0, 46352.0, 233265.0, 46364.0, 46371.52, 495500.0, 2854800.0, 46384.56, 46385.64, 61378.0, 46388.0, 46390.0, 233407.0, 233410.0, 233450.0, 46402.0, 46403.0, 46404.0, 233482.0, 233500.0, 46410.01, 46410.4, 46413.07, 46417.32, 46417.0, 1020000.0, 46425.6, 46429.32, 46444.0, 233672.0, 828000.0, 233750.0, 233777.0, 233800.0, 233837.0, 233850.0, 496000.0, 233860.54, 233900.0, 233932.0, 233988.0, 234000.0, 234000.35, 46511.0, 5214756.0, 46524.96, 46528.0, 234100.0, 496289.0, 46538.0, 46539.0, 46540.92, 46541.0, 46544.0, 46550.4, 98654.0, 46551.0, 98659.0, 46564.0, 234302.0, 3642200.0, 46589.0, 234430.0, 234500.0, 234528.0, 234558.0, 234575.0, 234591.5, 234600.0, 234620.0, 234635.0, 234643.0, 234650.0, 234700.0, 234733.0, 234772.0, 46667.0, 234800.0, 497000.0, 46695.0, 46698.34, 234976.98, 234996.0, 235000.0, 46716.72, 46718.2, 46721.22, 46739.4, 235166.0, 235168.0, 46743.0, 46744.0, 46744.32, 46747.0, 235200.0, 46762.08, 46762.0, 235300.0, 235332.0, 235349.0, 759650.0, 235369.0, 46789.2, 1284000.0, 46797.71, 235455.0, 235500.0, 46810.56, 46812.64, 46813.0, 235548.32, 46819.54, 46823.0, 46833.0, 235648.69, 46839.0, 235674.0, 760000.0, 235750.0, 46858.08, 235800.0, 46873.12, 46873.0, 498000.0, 46881.8, 235888.0, 235901.0, 46887.36, 46887.0, 235947.0, 46902.6, 236000.0, 236004.0, 46909.0, 236014.0, 46911.82, 46911.0, 236050.0, 236144.23, 236300.0, 46976.0, 236400.0, 3120000.0, 498565.0, 236450.0, 46999.16, 236500.0, 236540.0, 47014.61, 236544.0, 47019.0, 47021.0, 47026.44, 47040.08, 761000.0, 47050.54, 47051.0, 47056.8, 236800.0, 47066.0, 236819.0, 499000.0, 236900.0, 237000.0, 47109.0, 237073.0, 47122.4, 47123.0, 47142.31, 47142.0, 47144.4, 47144.0, 47145.0, 237200.0, 237216.0, 47151.0, 237254.0, 47173.0, 237378.0, 47182.0, 47183.0, 47185.0, 47188.0, 47201.14, 47202.0, 237500.0, 237510.0, 61542.0, 237518.0, 237561.0, 237600.0, 47229.67, 47235.0, 47235.04, 47236.8, 237667.24, 237667.72, 237687.0, 237700.0, 762000.0, 47249.8, 237742.0, 237754.0, 307780.0, 499992.0, 500000.0, 500003.0, 500009.0, 237875.0, 237903.0, 237911.0, 500090.0, 237950.0, 237963.0, 47298.0, 237982.0, 47302.0, 238000.0, 47306.0, 238164.0, 238178.0, 47341.0, 238191.0, 238201.0, 47347.44, 47348.4, 47349.0, 238230.0, 238250.0, 47355.0, 307872.0, 307877.0, 238300.0, 238309.0, 238389.0, 47386.75, 238425.0, 238451.0, 238494.0, 238500.0, 1025000.0, 238588.0, 238600.0, 47428.99, 238650.0, 238671.0, 47438.0, 47438.25, 238680.0, 47440.95, 238697.0, 238700.0, 8365188.0, 238726.0, 47455.0, 47457.72, 47458.0, 47459.0, 47461.0, 47462.04, 47463.36, 238800.0, 238806.0, 47471.0, 47471.97, 501000.0, 238860.0, 47477.0, 47478.0, 60945.24, 47489.0, 47499.4, 47501.0, 238992.0, 239000.0, 4695472.0, 239174.0, 239188.0, 239200.0, 239303.0, 47565.36, 47566.09, 47582.0, 239400.0, 47593.8, 239520.0, 47611.92, 47615.2, 47618.0, 47619.36, 239590.0, 47627.52, 239655.0, 239730.0, 239740.0, 47652.8, 239772.0, 239800.0, 47664.52, 239811.0, 47668.0, 239850.0, 502000.0, 239900.0, 47689.56, 47691.84, 240000.0, 47705.58, 47708.07, 240051.0, 47713.38, 240088.0, 240100.0, 240121.0, 3648000.0, 47740.16, 240200.0, 47753.0, 47754.0, 240276.0, 240384.0, 240472.0, 240500.0, 47805.0, 47818.68, 240600.0, 47821.04, 47839.92, 765000.0, 240720.0, 240734.0, 47852.0, 47858.0, 47859.0, 47861.0, 503000.0, 240909.0, 47884.65, 47885.15, 240941.0, 240947.0, 61679.0, 99999.84, 99999.9, 100000.16, 241000.0, 100018.0, 241083.0, 47949.0, 47953.13, 241280.0, 241300.0, 241400.0, 241411.12, 241440.0, 241442.0, 241449.0, 47998.0, 241500.0, 241520.0, 241770.0, 241800.0, 504000.0, 48072.11, 766157.0, 241875.0, 48079.68, 241973.0, 48097.8, 48097.0, 242000.0, 242004.0, 48125.53, 3650010.0, 48129.0, 48130.0, 48131.0, 242194.0, 48139.0, 48140.0, 308656.0, 48141.0, 242290.0, 242330.0, 48169.0, 504500.0, 48170.52, 242368.0, 242400.0, 48180.03, 242488.0, 48198.36, 242499.0, 242500.0, 48201.0, 7582566.0, 48209.0, 242596.0, 48218.0, 242600.0, 242617.0, 48223.5, 242630.83, 242636.0, 48227.0, 242700.0, 48239.0, 48239.28, 48243.61, 48249.84, 48251.0, 48253.81, 242787.7, 242787.0, 242800.0, 48261.48, 48261.0, 48263.0, 505000.0, 48273.0, 242876.0, 242887.31, 48276.8, 242890.0, 48278.0, 242900.0, 48279.6, 243000.0, 243100.0, 7845315.0, 243150.0, 243200.0, 48369.0, 243360.0, 243375.0, 48375.51, 48375.47, 308900.0, 505591.06, 243464.0, 48395.0, 243500.0, 243522.0, 1030000.0, 243600.0, 243617.0, 48423.0, 48429.97, 48439.0, 243709.0, 768000.0, 243750.0, 48453.42, 243810.0, 48462.44, 48463.74, 506000.0, 48475.0, 48476.04, 48479.46, 243920.0, 48491.0, 243989.0, 244000.0, 48498.89, 48499.0, 506183.0, 48505.6, 48506.2, 48510.84, 48511.0, 48513.14, 48525.0, 48526.8, 244204.0, 506350.0, 48542.0, 244285.0, 48556.0, 48560.56, 48562.0, 48566.0, 244375.0, 244400.0, 244444.0, 48591.8, 48592.0, 244500.0, 48602.0, 244531.0, 48605.0, 244549.0, 244560.0, 48613.0, 244660.0, 244700.0, 244738.0, 244740.0, 244785.0, 48654.23, 244800.0, 507000.0, 244864.0, 48679.0, 48684.6, 48688.68, 244992.0, 48695.65, 245000.0, 245004.0, 245017.0, 48709.1, 48710.0, 48712.44, 245085.0, 48714.6, 245134.0, 48724.0, 48725.0, 48728.0, 245200.0, 48754.8, 48757.61, 48764.15, 507500.0, 48769.5, 245400.0, 245465.0, 245500.0, 48796.68, 245518.6, 245524.0, 48802.0, 48802.03, 48805.0, 48806.0, 1032000.0, 245600.0, 48817.6, 48829.3, 245688.0, 769992.0, 770000.0, 245760.0, 245800.0, 48856.0, 48859.2, 48861.0, 48863.2, 508000.0, 246000.0, 48898.9, 246108.0, 48921.0, 48924.2, 246161.0, 246172.0, 48933.96, 246200.0, 246204.0, 246225.0, 246315.78, 48965.0, 246420.0, 508600.0, 246460.0, 246500.0, 48998.22, 48998.4, 508700.0, 246600.0, 246636.0, 49021.0, 246680.0, 246700.0, 246720.0, 246733.0, 49057.89, 49060.0, 49062.48, 508992.0, 246850.0, 509000.0, 246892.0, 67126.44, 49081.6, 49084.6, 246969.97, 246979.0, 246980.0, 246996.0, 247000.0, 247010.0, 247200.0, 247224.93, 247276.8, 247277.15, 771697.0, 247500.0, 247545.0, 247600.0, 247700.0, 247705.0, 49234.36, 49243.2, 49246.97, 247779.0, 247800.0, 247815.0, 247851.0, 61953.0, 510000.0, 49268.8, 49268.48, 49278.0, 49281.0, 49281.44, 248000.0, 248004.0, 49300.08, 248146.0, 510344.0, 49338.0, 49341.0, 49342.32, 49343.75, 248265.77, 248300.0, 49354.0, 49355.85, 49355.8, 248320.0, 49357.0, 49358.39, 49364.0, 248366.0, 248400.0, 49381.36, 248450.0, 49394.4, 248576.0, 49413.0, 248661.0, 49429.0, 248724.0, 248824.0, 511000.0, 248892.0, 248900.0, 49490.0, 65255.04, 249000.0, 49491.0, 249011.0, 511161.0, 49496.0, 249055.0, 62003.0, 49520.42, 8900000.0, 249250.0, 249272.0, 1560000.0, 49547.0, 249300.0, 8900060.0, 249349.0, 49581.0, 249470.0, 49585.0, 249500.0, 249510.0, 249560.0, 1036000.0, 249600.0, 49610.0, 49614.0, 49619.37, 774000.0, 49633.0, 49635.0, 49637.76, 49641.6, 249802.0, 512000.0, 249900.0, 49676.64, 49678.8, 249955.0, 49682.0, 249985.0, 49687.91, 49688.0, 249996.0, 49689.0, 249999.0, 250000.0, 250001.0, 249999.96, 250008.0, 49691.2, 49703.0, 49708.0, 49709.5, 49713.04, 49717.0, 250142.0, 49724.64, 250200.0, 49748.84, 49748.0, 49751.0, 49752.5, 250327.0, 4182504.0, 250395.0, 250400.0, 49772.64, 49777.0, 49777.4, 250455.0, 250500.0, 250552.0, 250568.0, 49808.0, 512788.0, 250658.64, 250700.0, 775000.0, 49835.0, 250800.0, 250812.0, 250842.0, 49857.6, 49858.0, 513000.0, 49860.21, 49865.0, 49868.0, 250900.0, 250918.0, 49873.0, 49874.0, 49876.0, 250940.0, 49881.6, 251000.0, 49897.98, 49902.0, 251125.0, 49922.08, 251252.0, 10999200.0, 775600.0, 49957.0, 251374.0, 49965.0, 49969.0, 1300000.0, 1300001.0, 49988.28, 49993.0, 49994.0, 49997.16, 49997.28, 49999.92, 50000.99, 50000.04, 50000.16, 50001.56, 50002.8, 50002.56, 50003.02, 50003.28, 50004.56, 251600.0, 50017.0, 251719.0, 50036.0, 251794.0, 514000.0, 50061.6, 50062.74, 251880.0, 50064.85, 50067.0, 50071.91, 50071.0, 50072.0, 50083.0, 252000.0, 50087.4, 252011.0, 50091.0, 252088.0, 252155.0, 50121.39, 50123.0, 50123.22, 50131.92, 50142.0, 252361.0, 50159.88, 50159.52, 50161.0, 252400.0, 50172.84, 50173.17, 50181.36, 514621.0, 50183.0, 252500.0, 50200.92, 50208.0, 50210.0, 50211.2, 50212.75, 50212.8, 252637.0, 50224.46, 50228.62, 252720.0, 50231.0, 50233.12, 252745.0, 50237.0, 50242.4, 252800.0, 50247.0, 252809.0, 252824.0, 50251.0, 252832.0, 50254.0, 515000.0, 50257.08, 252884.0, 50264.8, 50265.0, 252937.0, 252946.87, 50276.4, 50277.0, 50279.0, 50281.69, 253000.0, 253111.0, 253122.0, 310831.0, 50329.55, 50333.93, 50334.0, 50339.74, 50343.0, 253300.0, 50353.0, 253370.0, 50369.88, 50371.14, 50376.48, 50377.56, 50384.0, 253500.0, 50386.0, 50389.0, 50391.0, 253545.0, 50396.0, 50402.03, 50402.43, 253600.0, 50407.0, 253631.0, 253675.0, 253685.3, 515830.0, 253700.0, 253800.0, 50452.0, 50454.0, 516000.0, 50456.06, 50465.62, 50465.0, 50467.0, 50467.29, 50468.0, 50470.55, 253956.0, 253996.0, 254000.0, 254059.0, 50499.0, 50535.26, 50537.0, 50539.58, 3400015.0, 50545.08, 60257.6, 254346.42, 50558.52, 254402.75, 254403.0, 516600.0, 516631.0, 254500.0, 254506.0, 50586.6, 50586.26, 50595.72, 254616.0, 50616.96, 254667.0, 50618.91, 50624.0, 779000.0, 50626.8, 50627.0, 50643.0, 254800.0, 50645.0, 254842.0, 517000.0, 50658.0, 254892.0, 255000.0, 311200.0, 50686.8, 50686.02, 50689.6, 311208.0, 50696.0, 255070.0, 50705.0, 50707.67, 50707.53, 50708.0, 50711.0, 255142.63, 255160.0, 255192.0, 50724.0, 50729.68, 255291.0, 255360.0, 255376.0, 50776.0, 255500.0, 255520.0, 50791.2, 255576.0, 255600.0, 255658.0, 255700.0, 780000.0, 50837.0, 255802.0, 518000.0, 50858.04, 255900.0, 255955.0, 256000.0, 50886.0, 256027.0, 311416.0, 50909.0, 50910.0, 50911.0, 50911.2, 50917.0, 256200.0, 50922.96, 256226.0, 256250.0, 50937.0, 50939.2, 50943.22, 50946.0, 256335.0, 256400.0, 50978.09, 50979.0, 50979.53, 50981.0, 256500.0, 50992.0, 518700.0, 50995.91, 50996.0, 50999.78, 50999.69, 51001.0, 5499500.0, 311523.0, 51011.0, 51013.0, 2616000.0, 781000.0, 51045.6, 519000.0, 8121180.0, 51054.0, 51058.0, 256961.0, 256974.82, 781275.0, 257000.0, 257036.0, 257100.0, 257143.0, 257154.0, 59067.96, 51123.0, 257240.0, 51130.04, 51153.72, 257400.0, 257424.0, 257475.0, 51178.4, 51178.8, 51179.0, 257500.0, 257504.0, 51187.0, 51193.0, 51196.0, 51197.0, 51205.44, 257631.0, 64571.0, 257846.0, 520000.0, 51253.77, 51258.65, 51260.04, 51261.24, 257908.0, 51268.5, 257944.0, 51271.0, 258000.0, 51280.0, 258122.0, 258124.0, 51316.0, 51316.44, 51319.0, 258240.0, 51340.57, 51353.0, 258385.0, 51358.0, 258396.0, 51368.0, 258500.0, 258504.0, 51381.0, 51386.0, 1045000.0, 258600.0, 782925.0, 51425.0, 258750.0, 51429.0, 258852.0, 521010.0, 258876.0, 258940.0, 62396.0, 259000.0, 259029.0, 259200.0, 259211.0, 259234.0, 51533.52, 51535.0, 51536.0, 259300.0, 51541.36, 51547.23, 259379.0, 259397.0, 51562.0, 51563.99, 259500.0, 259600.0, 51604.8, 259653.0, 259700.0, 259707.0, 784000.0, 51624.69, 51637.0, 259800.0, 312153.0, 51640.15, 522000.0, 259900.0, 7600000.0, 259992.0, 260000.0, 7600033.0, 51684.36, 51687.24, 51688.5, 51690.48, 260156.0, 522301.0, 51711.22, 51712.0, 51719.76, 260227.0, 51724.0, 51729.6, 51729.0, 51731.99, 260400.0, 260425.0, 260500.0, 51791.0, 51811.0, 51816.38, 785000.0, 51820.54, 260735.0, 51823.68, 51823.2, 51833.6, 51833.82, 51836.28, 51838.0, 260812.0, 522969.0, 260829.0, 51844.56, 51862.0, 51868.28, 261000.0, 261048.0, 261072.0, 261250.0, 5766366.0, 573000.0, 261476.0, 261500.0, 261510.0, 51979.2, 312500.0, 51999.69, 261755.0, 62507.0, 52037.6, 261813.0, 261814.0, 261821.0, 52042.0, 524000.0, 261857.0, 52047.0, 52060.51, 52060.8, 62513.0, 52062.0, 52062.14, 52071.0, 262000.0, 312600.0, 262080.0, 262090.99, 262110.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "N0778sf7--Lp",
        "outputId": "1bace565-afe1-4d38-8d33-59df0263709b"
      },
      "source": [
        "sc = 'loan_amnt, int_rate, inq_last_6mths, revol_util, delinq_2yrs, pub_rec, open_acc, total_acc, annual_inc, dti'\n",
        "sc = sc.split(', ')\n",
        "print(type(data.annual_inc.iloc[0]))\n",
        "data[sc] = data[sc].astype(np.float32)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "Aj3bkLmq4pZP",
        "outputId": "b90e4a6c-9999-4910-bc48-34860f5082d5"
      },
      "source": [
        "display(data.term.value_counts())\n",
        "data.groupby('term').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              " 36 months    1609754\n",
              " 60 months     650913\n",
              "Name: term, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>int_rate</th>\n",
              "      <th>fico_range_low</th>\n",
              "      <th>fico_range_high</th>\n",
              "      <th>inq_last_6mths</th>\n",
              "      <th>revol_util</th>\n",
              "      <th>revol_bal</th>\n",
              "      <th>delinq_2yrs</th>\n",
              "      <th>pub_rec</th>\n",
              "      <th>open_acc</th>\n",
              "      <th>total_acc</th>\n",
              "      <th>annual_inc</th>\n",
              "      <th>dti</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>term</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36 months</th>\n",
              "      <td>1.148048e+06</td>\n",
              "      <td>12745.526879</td>\n",
              "      <td>11.947973</td>\n",
              "      <td>698.346400</td>\n",
              "      <td>702.346593</td>\n",
              "      <td>0.577691</td>\n",
              "      <td>49.281468</td>\n",
              "      <td>15446.261241</td>\n",
              "      <td>0.311921</td>\n",
              "      <td>0.203896</td>\n",
              "      <td>11.344336</td>\n",
              "      <td>23.432710</td>\n",
              "      <td>75414.009671</td>\n",
              "      <td>18.299525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60 months</th>\n",
              "      <td>1.086569e+06</td>\n",
              "      <td>20738.468966</td>\n",
              "      <td>15.924137</td>\n",
              "      <td>699.186266</td>\n",
              "      <td>703.186464</td>\n",
              "      <td>0.574718</td>\n",
              "      <td>52.949310</td>\n",
              "      <td>19656.320623</td>\n",
              "      <td>0.294411</td>\n",
              "      <td>0.181779</td>\n",
              "      <td>12.275342</td>\n",
              "      <td>25.967475</td>\n",
              "      <td>84368.999756</td>\n",
              "      <td>20.122336</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Unnamed: 0     loan_amnt  ...    annual_inc        dti\n",
              "term                                    ...                         \n",
              " 36 months  1.148048e+06  12745.526879  ...  75414.009671  18.299525\n",
              " 60 months  1.086569e+06  20738.468966  ...  84368.999756  20.122336\n",
              "\n",
              "[2 rows x 14 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "fUSH2ly03F8o",
        "outputId": "a233f7f4-6de8-4430-9de7-df7baf402326"
      },
      "source": [
        "data.delinq_2yrs.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "39.0"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "Kb8i9HNu4aAP",
        "outputId": "2f9a2a75-3fb8-4319-eda8-468ffe003bf9"
      },
      "source": [
        "for i in data:\n",
        "  print(i+': ', len(data) - len(data.dropna(subset=[i])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unnamed: 0:  0\n",
            "loan_amnt:  0\n",
            "term:  0\n",
            "int_rate:  0\n",
            "purpose:  0\n",
            "fico_range_low:  0\n",
            "fico_range_high:  0\n",
            "grade:  0\n",
            "sub_grade:  0\n",
            "inq_last_6mths:  30\n",
            "revol_util:  1802\n",
            "revol_bal:  0\n",
            "delinq_2yrs:  29\n",
            "pub_rec:  29\n",
            "open_acc:  29\n",
            "total_acc:  29\n",
            "earliest_cr_line:  29\n",
            "annual_inc:  4\n",
            "emp_length:  146907\n",
            "home_ownership:  0\n",
            "verification_status:  0\n",
            "dti:  1711\n",
            "desc:  2134600\n",
            "loan_status:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkhWOmRGxqbk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "NT96O-D35WTP",
        "outputId": "fc8f5463-fe2f-4f99-c088-f1d40382b892"
      },
      "source": [
        "ll = list(set(data.loan_status))\n",
        "for i in ll:\n",
        "  print(i, len(data[data.loan_status == i ]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default 40\n",
            "Current 878316\n",
            "Charged Off 268559\n",
            "Does not meet the credit policy. Status:Charged Off 761\n",
            "In Grace Period 8436\n",
            "Late (16-30 days) 4349\n",
            "Late (31-120 days) 21467\n",
            "Does not meet the credit policy. Status:Fully Paid 1988\n",
            "Fully Paid 1076751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "24Iw2h6lfACb",
        "outputId": "9e650dbe-c420-42d7-beea-7ee4d8dfa807"
      },
      "source": [
        "#dataset.emp_length = dataset.emp_length.replace(to_replace=np.float32(None), value='None')\n",
        "print(set(dataset.revol_util))\n",
        "print(dataset.revol_util[0:100])\n",
        "len(dataset.revol_util[dataset.revol_util == np.float32(None)])\n",
        "print(len(dataset.dropna()))\n",
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0.0, 0.5, 2.5, 3.0, 4.5, 4.0, 5.0, 5.5, 8.2, 6.0, 10.5, 10.3, 11.7, 13.5, 12.8, 15.5, 13.8, 17.2, 18.6, 19.2, 20.1, 21.3, 21.2, 23.8, 24.6, 24.1, 26.6, 24.2, 26.7, 26.8, 29.6, 29.7, 31.1, 32.7, 27.4, 34.5, 34.4, 35.2, 36.6, 39.1, 40.7, 39.9, 41.7, 42.0, 44.8, 45.0, 46.0, 46.7, 47.6, 40.9, 50.3, 50.9, 52.7, 51.9, 54.3, 51.2, 56.2, 56.3, 57.1, 59.4, 52.5, 57.8, 62.8, 63.1, 64.5, 65.3, 64.9, 67.2, 68.4, 69.0, 63.7, 71.8, 72.0, 72.1, 74.9, 73.9, 76.6, 75.3, 78.0, 76.0, 80.2, 77.4, 79.3, 83.0, 84.5, 84.8, 83.1, 87.6, 87.4, 88.5, 83.6, 91.3, 83.9, 85.0, 94.9, 90.9, 96.8, 95.2, 95.7, 97.2, 100.6, 101.5, 101.6, 102.4, 102.0, 96.9, 98.7, 100.3, 100.8, 20.0, 21.5, 22.5, 21.0, 22.0, 112.7, 106.1, 23.0, 23.5, 118.4, 110.0, 111.1, 24.0, 24.5, 123.9, 122.8, 121.2, 25.5, 25.0, 128.6, 121.0, 130.5, 26.5, 26.0, 133.1, 134.3, 126.0, 27.0, 27.5, 137.2, 139.7, 132.7, 28.0, 28.5, 143.6, 141.1, 145.8, 29.5, 29.0, 140.1, 145.6, 144.3, 30.0, 30.5, 152.5, 153.7, 155.3, 31.0, 31.5, nan, 6.5, nan, nan, 32.5, 32.0, 1.0, 161.5, 166.9, 33.5, 33.0, 162.1, 165.8, nan, 34.0, 172.0, nan, 175.0, nan, 35.5, 35.0, 177.7, 180.3, 7.0, 36.5, 36.0, 7.5, 182.8, 184.6, 37.5, 37.0, 183.8, nan, nan, 38.5, 38.0, 193.0, nan, nan, 39.5, 39.0, nan, 191.0, 40.5, 40.0, 8.5, 41.0, 41.5, 8.0, 42.5, 43.0, 43.5, 44.0, 44.5, 45.5, 9.5, 46.5, 9.0, 47.5, 47.0, 48.5, 48.0, 49.5, 49.0, 50.0, 50.5, 10.0, 51.0, 51.5, 52.0, 53.5, 53.0, 54.0, 54.5, 55.0, 55.5, 11.0, 56.5, 56.0, 11.5, 2.0, 57.5, 57.0, 58.5, 58.0, 59.5, 59.0, 60.5, 60.0, 12.5, 61.5, 61.0, 12.0, 62.0, 62.5, 63.5, 63.0, nan, nan, nan, nan, 64.0, nan, nan, nan, 65.0, 65.5, 13.0, 66.0, 66.5, 67.5, 67.0, 68.0, 68.5, 69.5, 70.5, 70.0, 14.5, 14.0, 71.0, 71.5, 72.5, 366.6, nan, 73.5, 73.0, nan, nan, nan, 74.5, 74.0, nan, nan, nan, 75.0, 75.5, nan, 15.0, nan, 76.5, nan, nan, nan, 77.0, 77.5, 77.63, 78.5, 79.0, 79.5, 80.0, 80.5, 16.0, 3.5, 81.0, 16.5, 81.5, 82.5, 82.0, 83.5, 84.0, 85.5, 17.5, 17.0, 86.5, 86.0, 87.5, 87.0, 88.0, 89.0, 89.5, 90.5, 90.0, nan, 18.5, 18.0, 91.0, 91.5, nan, nan, nan, 92.0, 92.5, 93.5, 93.0, 94.5, 94.0, 95.0, 95.5, 19.5, 19.0, 96.5, 96.0, nan, nan, 97.5, 97.0, nan, nan, nan, 98.0, 98.5, nan, 99.5, 99.0, 100.0, 100.5, 20.5, 101.0, 102.5, 103.5, 103.0, 104.0, 104.5, 105.0, 105.5, 106.0, 106.5, nan, nan, 107.0, 107.5, nan, nan, nan, 108.5, 108.0, nan, 109.0, 109.5, 110.5, 111.0, 111.5, 112.5, 112.0, 113.0, 113.5, 114.5, 114.0, 115.0, 115.5, 116.0, 116.5, 117.5, 117.0, 118.5, 118.0, nan, nan, 119.5, 119.0, nan, nan, nan, 120.5, nan, nan, 120.0, 121.5, nan, nan, nan, nan, nan, nan, 123.0, nan, nan, nan, 123.5, 124.5, nan, nan, nan, nan, 125.0, nan, nan, nan, nan, nan, 127.0, 127.5, 128.0, 129.5, nan, nan, nan, nan, 130.0, nan, nan, nan, nan, 131.0, 132.5, 134.0, 134.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 136.0, 36.88, 138.5, 37.63, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 142.5, 43.38, 145.0, 148.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 49.63, 153.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 56.26, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 131.8, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 892.3, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 3.18, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 131.3, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 24.89, 5.33, 33.14, 33.39, 8.58, 34.89, 10.08, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.86, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 16.02, nan, 69.14, 70.26, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 29.77, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.7, 1.7, 1.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 16.9, 16.4, nan, nan, 17.4, 17.9, nan, nan, nan, 18.9, 18.4, nan, nan, nan, 19.4, 19.9, 20.4, 20.9, 21.4, 21.9, 22.9, 22.4, 23.9, 23.4, 24.4, 24.9, 24.65, 25.9, 25.4, 0.2, 26.9, 26.4, 27.9, nan, nan, nan, nan, 28.9, 28.4, nan, nan, nan, 29.4, 29.9, 30.9, 30.4, 31.9, 31.4, 32.4, 32.9, 33.4, 33.9, 34.9, 35.4, 35.9, 36.9, 36.4, 37.9, 37.4, 0.75, 38.4, 38.9, nan, nan, nan, 39.4, nan, nan, nan, nan, 40.4, 41.4, 41.9, 42.9, 42.4, 43.9, 43.4, 44.4, 44.9, 45.9, 45.4, 46.9, 46.4, 47.9, 47.4, nan, nan, nan, 48.4, 48.9, nan, nan, nan, 49.9, 49.4, 50.4, 51.4, 52.9, 52.4, 53.9, 53.4, 54.4, 54.9, nan, nan, 55.9, 55.4, 56.4, 56.9, 57.9, 57.4, 58.9, 58.4, nan, nan, nan, 59.9, nan, nan, nan, nan, 60.4, 60.9, nan, nan, nan, 61.9, 61.4, nan, 62.9, 62.4, 63.4, 63.9, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.04, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.54, 1.88, 2.2, 2.7, 4.2, 5.7, 3.2, 3.7, 4.7, 5.2, 6.2, 6.7, 7.7, 7.2, 5.79, 17.78, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 29.53, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 36.78, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 64.4, 65.4, 65.9, 66.4, 66.9, 67.4, 67.9, 68.9, 69.9, 69.4, 70.4, 70.9, 71.9, 71.4, nan, 72.9, 72.4, nan, nan, nan, 73.4, nan, nan, nan, 74.4, 24.66, 75.4, 75.9, 76.4, 76.9, 77.9, nan, nan, nan, nan, 78.9, 78.4, nan, nan, nan, 79.9, 79.4, 28.41, 80.4, 80.9, 4.6, 4.1, 4.85, 81.4, 81.9, 5.6, 82.4, 82.9, 6.6, 83.4, 7.6, 84.4, 84.9, 85.9, 85.4, 9.1, 86.9, 86.4, 87.9, 88.4, 88.9, 12.6, 89.4, 89.9, 90.4, 91.9, 91.4, 92.4, 92.9, 93.4, 93.9, 94.4, 95.4, 95.9, 96.4, 97.4, 97.9, 98.4, 98.9, 99.4, 99.9, 100.4, 100.9, 101.9, 101.4, 102.9, 103.9, 103.4, 104.9, 104.4, 105.9, 105.4, 106.4, 106.9, 107.4, 107.9, 108.4, 108.9, nan, nan, nan, 109.9, 109.4, nan, nan, nan, 110.9, 110.4, 111.4, 111.9, 112.4, 112.9, 113.9, 113.4, 114.4, 114.9, 115.4, 115.9, 116.4, 117.9, 117.4, 118.9, 119.4, 119.9, 120.9, 121.4, 121.9, 122.4, 123.4, 124.4, 126.4, 126.9, nan, 127.4, nan, nan, nan, nan, nan, 128.4, nan, nan, 128.9, 129.4, 130.4, 131.4, 132.4, 132.9, 32.04, 134.4, 33.29, 136.4, 137.9, 138.9, 139.9, 140.4, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 154.9, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.9, 2.4, 3.9, 1.4, 3.4, 2.9, 122.5, nan, nan, nan, nan, nan, nan, 124.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 17.67, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 21.92, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 32.8, 32.3, 33.8, 33.3, 34.3, 34.8, 35.3, 35.8, 129.0, 36.3, 36.8, nan, nan, nan, 37.8, 37.3, nan, nan, nan, 38.3, 38.8, nan, nan, 39.8, 39.3, nan, nan, nan, 40.8, 40.3, nan, 41.3, 41.8, 42.3, 42.8, 43.3, 43.8, 44.3, 45.8, 45.3, 46.8, 46.3, 47.8, 47.3, 48.8, 48.3, 49.3, 49.8, 50.8, 0.9, 51.8, 51.3, 0.4, 52.3, 52.8, 53.3, 53.8, nan, nan, nan, 54.8, nan, nan, nan, nan, 55.3, 55.8, nan, nan, nan, 56.8, nan, nan, nan, nan, 57.3, nan, nan, 58.8, 58.3, 8.3, 59.8, 8.8, 59.3, 9.3, 60.8, 9.8, 60.3, nan, 10.8, 61.8, 61.3, nan, nan, 62.3, 11.8, 11.3, nan, nan, 12.3, 63.8, 63.3, nan, nan, 13.3, 64.8, 64.3, nan, nan, 14.3, 65.8, 14.8, nan, nan, 15.8, 66.3, 66.8, 15.3, 16.3, 67.8, 16.8, 67.3, 17.3, 17.8, 68.3, 68.8, 18.8, 69.8, 69.3, 18.3, nan, 19.3, 70.8, 19.8, 70.3, nan, 20.8, 71.3, 20.3, 21.8, 72.8, 72.3, 22.8, 73.3, 22.3, 73.8, 23.3, 74.8, 74.3, 24.3, 75.8, 24.8, 25.8, 76.8, 25.3, 76.3, nan, 26.3, 77.3, 77.8, nan, nan, 27.8, 78.8, 78.3, 27.3, 28.3, 79.8, 28.8, 29.8, 80.8, 80.3, 29.3, 30.3, 81.8, 81.3, 30.8, 31.3, 82.3, 31.8, 82.8, 83.3, 83.8, 84.3, 8.49, 85.8, 85.3, 86.3, 86.8, 87.8, 87.3, 88.3, 88.8, 89.3, 89.8, 90.8, 90.3, 91.8, 92.3, 92.8, 93.3, 93.8, nan, nan, nan, 94.3, 94.8, nan, nan, nan, 95.3, 95.8, 96.3, 97.8, 97.3, nan, nan, nan, 98.3, 98.8, nan, nan, nan, 99.8, 99.3, 101.8, 101.3, nan, 0.49, nan, 102.3, 102.8, nan, nan, nan, 103.8, 103.3, 104.8, 104.3, 105.8, 105.3, 106.3, 106.8, 107.8, 107.3, 108.8, 108.3, 7.43, 109.8, 109.3, 110.3, 110.8, 111.8, 111.3, 112.3, 112.8, 113.8, 113.3, 114.8, 115.8, 115.3, 116.3, 116.8, 117.8, 118.8, 118.3, 119.3, 119.8, 120.8, 120.3, 121.3, 123.3, 123.8, 125.8, 125.3, 126.3, nan, nan, nan, 127.3, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 11.62, 0.83, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 27.81, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 4.4, 5.1, 6.1, 7.1, 8.1, 9.6, 10.1, 11.6, 10.6, 13.4, 8.6, 11.1, 12.1, 13.1, 13.6, 15.1, 15.6, 14.1, 14.6, 14.4, 15.9, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 36.94, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 49.69, nan, nan, nan, nan, nan, nan, 125.6, nan, nan, nan, nan, 57.56, nan, nan, nan, nan, 58.19, nan, nan, nan, 60.69, 62.31, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 70.94, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 30.19, 81.31, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 11.63, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.01, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 18.82, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 26.32, 128.8, 128.7, nan, nan, nan, nan, nan, 130.8, 130.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 132.2, 8.01, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 136.7, 137.8, 137.3, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 141.8, 146.3, 150.7, 152.7, 0.6, 154.3, 156.3, 158.7, nan, nan, nan, nan, nan, nan, nan, nan, nan, 8.7, 9.7, 9.2, 10.7, 10.2, 11.2, 12.2, 12.7, 0.1, 13.2, 13.7, 14.7, 14.2, 15.7, 15.2, nan, nan, nan, 16.2, 16.7, nan, nan, nan, 17.7, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.5, 18.7, 18.2, 19.7, 20.7, 20.2, 173.2, 21.7, 22.7, 22.2, 23.7, 23.2, 24.7, 25.2, 25.7, 26.2, 27.7, 27.2, 28.7, 28.2, 29.2, 30.7, 30.2, 31.7, 31.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.1, 2.1, 3.1, 1.6, 2.6, 3.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 25.33, 26.33, 32.2, 33.2, 33.7, 34.7, 34.2, 35.7, 36.2, 36.7, 37.2, 37.7, 38.7, 38.2, nan, 39.7, 39.2, 39.95, nan, nan, 40.2, nan, nan, nan, nan, 41.2, nan, nan, nan, nan, 42.7, 42.2, 43.2, 43.7, 44.7, 44.2, 45.7, 45.2, 46.2, 47.2, 47.7, 48.2, 48.7, 49.7, 49.2, 50.2, 50.7, 51.7, nan, nan, nan, nan, 52.2, nan, nan, nan, nan, 53.7, 53.2, 52.58, 54.7, 54.2, 55.7, 55.2, nan, nan, nan, 56.7, nan, nan, nan, nan, 57.2, 57.7, 58.7, 58.2, 59.7, 59.2, 60.7, 60.2, 61.7, 61.2, 62.2, 62.7, 63.2, 64.7, 64.2, 65.7, 65.2, 66.7, 66.2, 67.7, 68.2, 68.7, 69.7, 69.2, 70.7, 70.2, 71.2, 71.7, 72.7, 72.2, 73.7, 73.2, 74.7, 74.2, 75.2, 75.7, 76.2, 76.7, 77.7, 77.2, 78.2, 78.7, nan, nan, 79.2, nan, nan, nan, nan, 80.7, nan, nan, 79.7, nan, 81.2, 81.7, nan, nan, nan, 82.2, 82.7, nan, nan, 83.2, 83.7, nan, nan, nan, 84.7, 84.2, nan, nan, nan, 85.7, 85.2, 86.2, 86.7, 87.2, 87.7, 88.2, 88.7, 89.7, 89.2, 0.03, 90.2, 90.7, 91.2, 91.7, 92.7, 92.2, nan, nan, nan, 93.2, 93.7, nan, nan, nan, 94.7, 94.2, 94.46, 96.2, 96.7, 97.7, 98.2, 99.2, 99.7, nan, 100.7, 100.2, nan, nan, nan, 101.2, 101.7, nan, nan, 102.7, 102.2, 103.7, 103.2, 104.2, 104.7, 105.7, 105.2, 106.2, 106.7, 107.2, 107.7, 108.7, 108.2, 109.2, 109.7, 8.46, 110.2, 110.7, 9.71, 111.2, 111.7, 112.2, 113.7, 113.2, 114.2, 114.7, 115.7, 115.2, nan, nan, nan, nan, nan, nan, nan, nan, 117.2, 117.7, 116.2, 118.2, 17.71, 119.7, 119.2, 120.2, 121.7, 122.7, 122.2, 123.2, 124.7, 125.2, 125.7, nan, nan, nan, 126.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 4.9, 5.9, 5.4, 6.9, 6.4, 32.71, 7.4, 7.9, 8.9, 8.4, 9.4, 9.9, 10.9, 10.4, 11.4, 11.9, 12.4, 12.9, nan, nan, nan, 13.9, nan, nan, nan, nan, 14.9, nan, 15.4, 0.12, 5.34, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 9.34, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 21.59, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 2.8, 2.3, 4.3, 5.8, 6.8, 6.3, 5.3, 7.8, 7.3, 3.8, 3.3, 4.8, 7.64, 7.28, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 21.72, 0.46, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 6.75, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 54.22, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 64.1, 64.6, 65.1, 65.6, 66.1, 66.6, 67.6, 67.1, 68.6, 68.1, 69.6, 69.1, 70.6, 70.1, 71.1, 71.6, 72.6, 73.1, 73.6, 74.1, 74.6, 75.1, 75.6, 76.1, 0.3, 77.6, 77.1, 78.1, 78.6, 79.1, 79.6, 80.6, 80.1, 81.1, 81.6, 82.6, 82.1, 0.05, 84.1, 84.6, 85.6, 85.1, 86.1, 86.6, 87.1, nan, 88.6, 88.1, nan, nan, nan, 89.6, 89.1, nan, nan, 90.1, 90.6, 91.6, 91.1, 92.1, 92.6, 93.1, 93.6, 94.1, 94.6, 95.1, 95.6, 96.6, 96.1, 97.1, 97.6, 98.1, 98.6, 99.1, 99.6, 100.1, 101.1, 0.8, 1.8, 1.3, 102.6, 102.1, 103.6, 103.1, 104.6, 104.1, 105.6, 105.1, nan, nan, 106.6, nan, nan, nan, nan, 107.1, 107.6, nan, 108.1, 108.6, 109.1, 109.6, 110.6, 110.1, 111.6, nan, nan, nan, nan, 112.1, 112.6, nan, nan, nan, 113.1, 113.6, nan, nan, 114.6, 114.1, nan, nan, nan, 115.1, 115.6, nan, nan, 116.1, nan, nan, nan, nan, 117.6, 117.1, nan, nan, 16.1, 16.6, 118.1, 118.6, nan, 17.1, 17.6, 119.6, 119.1, nan, 18.1, nan, nan, 19.6, 19.1, 121.1, nan, nan, 20.6, 122.1, nan, nan, 21.1, 21.6, 123.1, nan, 22.6, 22.1, nan, nan, nan, 23.1, 23.6, 125.1, nan, nan, nan, 126.1, nan, nan, nan, 25.1, 25.6, 127.6, 127.1, 126.6, 26.1, 128.1, 27.1, 27.6, 129.6, 28.6, 28.1, 130.1, 29.1, 30.6, 30.1, 132.1, 132.6, 31.6, 133.6, 32.6, 32.1, 134.1, nan, 33.6, 33.1, 135.6, nan, nan, 34.6, 34.1, 136.1, nan, nan, 35.6, 35.1, nan, nan, nan, 36.1, nan, nan, nan, nan, 37.1, 37.6, 37.73, 38.1, 38.6, nan, nan, nan, 39.6, nan, nan, nan, nan, 40.6, 40.1, 41.1, 41.6, 41.85, 42.6, 42.1, 43.1, 43.6, 44.1, 44.6, 146.1, nan, nan, 45.1, 45.6, nan, nan, 162.0, 46.6, 46.1, nan, nan, nan, 47.1, nan, nan, nan, nan, 48.6, 48.1, 49.1, 49.6, 50.6, 50.1, 152.6, 51.1, 51.6, nan, nan, nan, 52.1, 52.6, nan, nan, nan, 53.6, 53.1, nan, nan, nan, 54.6, 54.1, 55.1, 55.6, 56.1, 56.6, 57.6, 58.6, 58.1, nan, 59.1, 59.6, nan, nan, nan, 60.1, 60.6, nan, nan, 61.1, 61.6, 62.6, 62.1, 63.6, 33.26, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 10.17, 12.42, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 2.64, 10.61, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 23.11, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 43.61, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 47.36, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 69.98, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 25.74, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 88.48, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 38.77, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 24.63, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.16, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 46.74, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 13.56, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 58.77}\n",
            "0     29.7\n",
            "1     19.2\n",
            "2     56.2\n",
            "3     11.6\n",
            "4     64.5\n",
            "      ... \n",
            "95    35.5\n",
            "96    41.4\n",
            "97    33.2\n",
            "98    28.7\n",
            "99    32.6\n",
            "Name: revol_util, Length: 100, dtype: float64\n",
            "2257159\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2260668"
            ]
          },
          "execution_count": 205,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb20nzzRbRGS"
      },
      "source": [
        "print(dataset.iloc[0:3])\n",
        "#trem = list(set(dataset.term))\n",
        "purpose = list(set(dataset.purpose))\n",
        "grade = list(set(dataset.grade))\n",
        "sub_grade = list(set(dataset.sub_grade))\n",
        "inq = list(set(dataset.inq_last_6mths))\n",
        "earl = list(set(dataset.earliest_cr_line))\n",
        "emp = list(set(dataset.emp_length))\n",
        "home = list(set(dataset.home_ownership))\n",
        "ver = list(set(dataset.verification_status))\n",
        "\n",
        "print(purpose, grade, inq, earl, emp, home, ver)\n",
        "#y = to_categorical(y, num_classes=2, dtype='float32')\n",
        "#to_categorical(dataset.term, num_classes=len(set(dataset.term)))\n",
        "\n",
        "# earliest_cr_line need to convert to credit ages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "gAi076YzTPT_",
        "outputId": "d2a8ef29-5537-4cf3-ebb8-2065b6ad2de4"
      },
      "source": [
        "print(len(dataset.annual_inc[dataset.annual_inc == 0]))\n",
        "len(acp_loans.annual_inc[np.log(acp_loans.annual_inc) == 0])\n",
        "print(type(dataset.annual_inc[2258293]))\n",
        "\n",
        "np.log(acp_loans.annual_inc[acp_loans.annual_inc != 0]).iloc[2258293]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42\n",
            "<class 'numpy.float64'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "11.082142548877775"
            ]
          },
          "execution_count": 131,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ1Kv_qMJUNg"
      },
      "source": [
        "print(len(dataset.dropna(how='all')))\n",
        "print(len(dataset.dropna(subset=['fico_range_low'])))\n",
        "print(len(dataset.dropna(subset=['fico_range_high'])))\n",
        "dataset = dataset.dropna(how='all')\n",
        "print((dataset['fico_range_low'].iloc[0]+dataset['fico_range_high'].iloc[0])/2)\n",
        "print(len(dataset.dropna(subset=['fico_range_low'])))\n",
        "print(len(dataset.dropna(subset=['fico_range_high'])))\n",
        "#print(dataset[['fico_range_low']].add(dataset[['fico_range_high']], axis=1))\n",
        "fico['fico_range_high'] = dataset[['fico_range_high']]\n",
        "fico['fico_range_low'] = dataset[['fico_range_low']]\n",
        "fico['ficos'] = fico.sum(axis=1)\n",
        "#print(fico['fico'].iloc[0])\n",
        "#fico['fico'] = fico.div(2, axis=1)\n",
        "fico['fico'] = fico['ficos']/2\n",
        "fico\n",
        "#print(fico['fico'].iloc[0])\n",
        "#fico['fico']/2\n",
        "#dataset[['fico']] = (dataset[['fico_range_low']]+dataset[['fico_range_high']])/2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XahiiuM_Dpt-",
        "outputId": "7830fa40-9d9e-4ddf-8233-02ad45216459"
      },
      "source": [
        "dictionary_df = pd.read_excel(\"https://resources.lendingclub.com/LCDataDictionary.xlsx\")\n",
        "\n",
        "# Drop blank rows, strip white space, convert to Python dictionary, fix one key name\n",
        "dictionary_df.dropna(axis=\"index\", inplace=True)\n",
        "dictionary_df = dictionary_df.applymap(lambda x: x.strip())\n",
        "dictionary_df.set_index(\"LoanStatNew\", inplace=True)\n",
        "dictionary = dictionary_df[\"Description\"].to_dict()\n",
        "dictionary[\"verification_status_joint\"] = dictionary.pop(\"verified_status_joint\")\n",
        "\n",
        "dictionary\n",
        "# Print in order of dataset columns (which makes more sense than dictionary's order)\n",
        "\n",
        "for col in dataset.columns:\n",
        "    print(f\"•{col}: {dictionary[col]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "•id: A unique LC assigned ID for the loan listing.\n",
            "•member_id: A unique LC assigned Id for the borrower member.\n",
            "•loan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n",
            "•funded_amnt: The total amount committed to that loan at that point in time.\n",
            "•funded_amnt_inv: The total amount committed by investors for that loan at that point in time.\n",
            "•term: The number of payments on the loan. Values are in months and can be either 36 or 60.\n",
            "•int_rate: Interest Rate on the loan\n",
            "•installment: The monthly payment owed by the borrower if the loan originates.\n",
            "•grade: LC assigned loan grade\n",
            "•sub_grade: LC assigned loan subgrade\n",
            "•emp_title: The job title supplied by the Borrower when applying for the loan.*\n",
            "•emp_length: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n",
            "•home_ownership: The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n",
            "•annual_inc: The self-reported annual income provided by the borrower during registration.\n",
            "•verification_status: Indicates if income was verified by LC, not verified, or if the income source was verified\n",
            "•issue_d: The month which the loan was funded\n",
            "•loan_status: Current status of the loan\n",
            "•pymnt_plan: Indicates if a payment plan has been put in place for the loan\n",
            "•url: URL for the LC page with listing data.\n",
            "•desc: Loan description provided by the borrower\n",
            "•purpose: A category provided by the borrower for the loan request.\n",
            "•title: The loan title provided by the borrower\n",
            "•zip_code: The first 3 numbers of the zip code provided by the borrower in the loan application.\n",
            "•addr_state: The state provided by the borrower in the loan application\n",
            "•dti: A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n",
            "•delinq_2yrs: The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years\n",
            "•earliest_cr_line: The month the borrower's earliest reported credit line was opened\n",
            "•fico_range_low: The lower boundary range the borrower’s FICO at loan origination belongs to.\n",
            "•fico_range_high: The upper boundary range the borrower’s FICO at loan origination belongs to.\n",
            "•inq_last_6mths: The number of inquiries in past 6 months (excluding auto and mortgage inquiries)\n",
            "•mths_since_last_delinq: The number of months since the borrower's last delinquency.\n",
            "•mths_since_last_record: The number of months since the last public record.\n",
            "•open_acc: The number of open credit lines in the borrower's credit file.\n",
            "•pub_rec: Number of derogatory public records\n",
            "•revol_bal: Total credit revolving balance\n",
            "•revol_util: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n",
            "•total_acc: The total number of credit lines currently in the borrower's credit file\n",
            "•initial_list_status: The initial listing status of the loan. Possible values are – W, F\n",
            "•out_prncp: Remaining outstanding principal for total amount funded\n",
            "•out_prncp_inv: Remaining outstanding principal for portion of total amount funded by investors\n",
            "•total_pymnt: Payments received to date for total amount funded\n",
            "•total_pymnt_inv: Payments received to date for portion of total amount funded by investors\n",
            "•total_rec_prncp: Principal received to date\n",
            "•total_rec_int: Interest received to date\n",
            "•total_rec_late_fee: Late fees received to date\n",
            "•recoveries: post charge off gross recovery\n",
            "•collection_recovery_fee: post charge off collection fee\n",
            "•last_pymnt_d: Last month payment was received\n",
            "•last_pymnt_amnt: Last total payment amount received\n",
            "•next_pymnt_d: Next scheduled payment date\n",
            "•last_credit_pull_d: The most recent month LC pulled credit for this loan\n",
            "•last_fico_range_high: The upper boundary range the borrower’s last FICO pulled belongs to.\n",
            "•last_fico_range_low: The lower boundary range the borrower’s last FICO pulled belongs to.\n",
            "•collections_12_mths_ex_med: Number of collections in 12 months excluding medical collections\n",
            "•mths_since_last_major_derog: Months since most recent 90-day or worse rating\n",
            "•policy_code: publicly available policy_code=1\n",
            "new products not publicly available policy_code=2\n",
            "•application_type: Indicates whether the loan is an individual application or a joint application with two co-borrowers\n",
            "•annual_inc_joint: The combined self-reported annual income provided by the co-borrowers during registration\n",
            "•dti_joint: A ratio calculated using the co-borrowers' total monthly payments on the total debt obligations, excluding mortgages and the requested LC loan, divided by the co-borrowers' combined self-reported monthly income\n",
            "•verification_status_joint: Indicates if the co-borrowers' joint income was verified by LC, not verified, or if the income source was verified\n",
            "•acc_now_delinq: The number of accounts on which the borrower is now delinquent.\n",
            "•tot_coll_amt: Total collection amounts ever owed\n",
            "•tot_cur_bal: Total current balance of all accounts\n",
            "•open_acc_6m: Number of open trades in last 6 months\n",
            "•open_act_il: Number of currently active installment trades\n",
            "•open_il_12m: Number of installment accounts opened in past 12 months\n",
            "•open_il_24m: Number of installment accounts opened in past 24 months\n",
            "•mths_since_rcnt_il: Months since most recent installment accounts opened\n",
            "•total_bal_il: Total current balance of all installment accounts\n",
            "•il_util: Ratio of total current balance to high credit/credit limit on all install acct\n",
            "•open_rv_12m: Number of revolving trades opened in past 12 months\n",
            "•open_rv_24m: Number of revolving trades opened in past 24 months\n",
            "•max_bal_bc: Maximum current balance owed on all revolving accounts\n",
            "•all_util: Balance to credit limit on all trades\n",
            "•total_rev_hi_lim: Total revolving high credit/credit limit\n",
            "•inq_fi: Number of personal finance inquiries\n",
            "•total_cu_tl: Number of finance trades\n",
            "•inq_last_12m: Number of credit inquiries in past 12 months\n",
            "•acc_open_past_24mths: Number of trades opened in past 24 months.\n",
            "•avg_cur_bal: Average current balance of all accounts\n",
            "•bc_open_to_buy: Total open to buy on revolving bankcards.\n",
            "•bc_util: Ratio of total current balance to high credit/credit limit for all bankcard accounts.\n",
            "•chargeoff_within_12_mths: Number of charge-offs within 12 months\n",
            "•delinq_amnt: The past-due amount owed for the accounts on which the borrower is now delinquent.\n",
            "•mo_sin_old_il_acct: Months since oldest bank installment account opened\n",
            "•mo_sin_old_rev_tl_op: Months since oldest revolving account opened\n",
            "•mo_sin_rcnt_rev_tl_op: Months since most recent revolving account opened\n",
            "•mo_sin_rcnt_tl: Months since most recent account opened\n",
            "•mort_acc: Number of mortgage accounts.\n",
            "•mths_since_recent_bc: Months since most recent bankcard account opened.\n",
            "•mths_since_recent_bc_dlq: Months since most recent bankcard delinquency\n",
            "•mths_since_recent_inq: Months since most recent inquiry.\n",
            "•mths_since_recent_revol_delinq: Months since most recent revolving delinquency.\n",
            "•num_accts_ever_120_pd: Number of accounts ever 120 or more days past due\n",
            "•num_actv_bc_tl: Number of currently active bankcard accounts\n",
            "•num_actv_rev_tl: Number of currently active revolving trades\n",
            "•num_bc_sats: Number of satisfactory bankcard accounts\n",
            "•num_bc_tl: Number of bankcard accounts\n",
            "•num_il_tl: Number of installment accounts\n",
            "•num_op_rev_tl: Number of open revolving accounts\n",
            "•num_rev_accts: Number of revolving accounts\n",
            "•num_rev_tl_bal_gt_0: Number of revolving trades with balance >0\n",
            "•num_sats: Number of satisfactory accounts\n",
            "•num_tl_120dpd_2m: Number of accounts currently 120 days past due (updated in past 2 months)\n",
            "•num_tl_30dpd: Number of accounts currently 30 days past due (updated in past 2 months)\n",
            "•num_tl_90g_dpd_24m: Number of accounts 90 or more days past due in last 24 months\n",
            "•num_tl_op_past_12m: Number of accounts opened in past 12 months\n",
            "•pct_tl_nvr_dlq: Percent of trades never delinquent\n",
            "•percent_bc_gt_75: Percentage of all bankcard accounts > 75% of limit.\n",
            "•pub_rec_bankruptcies: Number of public record bankruptcies\n",
            "•tax_liens: Number of tax liens\n",
            "•tot_hi_cred_lim: Total high credit/credit limit\n",
            "•total_bal_ex_mort: Total credit balance excluding mortgage\n",
            "•total_bc_limit: Total bankcard high credit/credit limit\n",
            "•total_il_high_credit_limit: Total installment high credit/credit limit\n",
            "•revol_bal_joint: Sum of revolving credit balance of the co-borrowers, net of duplicate balances\n",
            "•sec_app_fico_range_low: FICO range (high) for the secondary applicant\n",
            "•sec_app_fico_range_high: FICO range (low) for the secondary applicant\n",
            "•sec_app_earliest_cr_line: Earliest credit line at time of application for the secondary applicant\n",
            "•sec_app_inq_last_6mths: Credit inquiries in the last 6 months at time of application for the secondary applicant\n",
            "•sec_app_mort_acc: Number of mortgage accounts at time of application for the secondary applicant\n",
            "•sec_app_open_acc: Number of open trades at time of application for the secondary applicant\n",
            "•sec_app_revol_util: Ratio of total current balance to high credit/credit limit for all revolving accounts\n",
            "•sec_app_open_act_il: Number of currently active installment trades at time of application for the secondary applicant\n",
            "•sec_app_num_rev_accts: Number of revolving accounts at time of application for the secondary applicant\n",
            "•sec_app_chargeoff_within_12_mths: Number of charge-offs within last 12 months at time of application for the secondary applicant\n",
            "•sec_app_collections_12_mths_ex_med: Number of collections within last 12 months excluding medical collections at time of application for the secondary applicant\n",
            "•sec_app_mths_since_last_major_derog: Months since most recent 90-day or worse rating at time of application for the secondary applicant\n",
            "•hardship_flag: Flags whether or not the borrower is on a hardship plan\n",
            "•hardship_type: Describes the hardship plan offering\n",
            "•hardship_reason: Describes the reason the hardship plan was offered\n",
            "•hardship_status: Describes if the hardship plan is active, pending, canceled, completed, or broken\n",
            "•deferral_term: Amount of months that the borrower is expected to pay less than the contractual monthly payment amount due to a hardship plan\n",
            "•hardship_amount: The interest payment that the borrower has committed to make each month while they are on a hardship plan\n",
            "•hardship_start_date: The start date of the hardship plan period\n",
            "•hardship_end_date: The end date of the hardship plan period\n",
            "•payment_plan_start_date: The day the first hardship plan payment is due. For example, if a borrower has a hardship plan period of 3 months, the start date is the start of the three-month period in which the borrower is allowed to make interest-only payments.\n",
            "•hardship_length: The number of months the borrower will make smaller payments than normally obligated due to a hardship plan\n",
            "•hardship_dpd: Account days past due as of the hardship plan start date\n",
            "•hardship_loan_status: Loan Status as of the hardship plan start date\n",
            "•orig_projected_additional_accrued_interest: The original projected additional interest amount that will accrue for the given hardship payment plan as of the Hardship Start Date. This field will be null if the borrower has broken their hardship payment plan.\n",
            "•hardship_payoff_balance_amount: The payoff balance amount as of the hardship plan start date\n",
            "•hardship_last_payment_amount: The last payment amount as of the hardship plan start date\n",
            "•disbursement_method: The method by which the borrower receives their loan. Possible values are: CASH, DIRECT_PAY\n",
            "•debt_settlement_flag: Flags whether or not the borrower, who has charged-off, is working with a debt-settlement company.\n",
            "•debt_settlement_flag_date: The most recent date that the Debt_Settlement_Flag has been set\n",
            "•settlement_status: The status of the borrower’s settlement plan. Possible values are: COMPLETE, ACTIVE, BROKEN, CANCELLED, DENIED, DRAFT\n",
            "•settlement_date: The date that the borrower agrees to the settlement plan\n",
            "•settlement_amount: The loan amount that the borrower has agreed to settle for\n",
            "•settlement_percentage: The settlement amount as a percentage of the payoff balance amount on the loan\n",
            "•settlement_term: The number of months that the borrower will be on the settlement plan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCiaxSRsPRQy"
      },
      "source": [
        "•loan_status: Current status of the loan\n",
        "\n",
        "1.   •grade: LC assigned loan grade\n",
        "2.   •sub_grade: LC assigned loan subgrade\n",
        "3.   •emp_title: The job title supplied by the Borrower when applying for the loan.*\n",
        "4.   •emp_length: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n",
        "5.   •home_ownership: The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n",
        "6.   •annual_inc: The self-reported annual income provided by the borrower during registration.\n",
        "7.   •verification_status: Indicates if income was verified by LC, not verified, or if the income source was verified\n",
        "8.   •pymnt_plan: Indicates if a payment plan has been put in place for the loan\n",
        "9. •purpose: A category provided by the borrower for the loan request.\n",
        "\n",
        "\n",
        "11. •zip_code: The first 3 numbers of the zip code provided by the borrower in the loan application.\n",
        "\n",
        "12. •addr_state: The state provided by the borrower in the loan application\n",
        "\n",
        "\n",
        "13. •dti: A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n",
        "14. •delinq_2yrs: The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years\n",
        "15. •earliest_cr_line: The month the borrower's earliest reported credit line was opened\n",
        "16. •fico_range_low: The lower boundary range the borrower’s FICO at loan origination belongs to.\n",
        "17. •fico_range_high: The upper boundary range the borrower’s FICO at loan origination belongs to.\n",
        "18. •inq_last_6mths: The number of inquiries in past 6 months (excluding auto and mortgage inquiries)\n",
        "\n",
        "\n",
        "19. ？ •mths_since_last_delinq: The number of months since the borrower's last delinquency. ? latent model has used this variable but \n",
        "\n",
        "\n",
        "20. •mths_since_last_record: The number of months since the last public record.\n",
        "21. issued_d\n",
        "\n",
        "22. •open_acc: The number of open credit lines in the borrower's credit file.\n",
        "23. •pub_rec: Number of derogatory public records\n",
        "24. •revol_bal: Total credit revolving balance\n",
        "25. •revol_util: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n",
        "26. •total_acc: The total number of credit lines currently in the borrower's credit file\n",
        "27. •initial_list_status: The initial listing status of the loan. Possible values are – W, F\n",
        "28. •application_type: Indicates whether the loan is an individual application or a joint application with two co-borrowers\n",
        "29. •annual_inc_joint: The combined self-reported annual income provided by the co-borrowers during registration\n",
        "30. •dti_joint: A ratio calculated using the co-borrowers' total monthly payments on the total debt obligations, excluding mortgages and the requested LC loan, divided by the co-borrowers' combined self-reported monthly income\n",
        "31. •verification_status_joint: Indicates if the co-borrowers' joint income was verified by LC, not verified, or if the income source was verified\n",
        "\n",
        "32. •tot_coll_amt: Total collection amounts ever owed\n",
        "\n",
        "36. •open_il_12m: Number of installment accounts opened in past 12 months\n",
        "37. •open_il_24m: Number of installment accounts opened in past 24 months\n",
        "\n",
        "\n",
        "39. •open_rv_12m: Number of revolving trades opened in past 12 months\n",
        "40. •open_rv_24m: Number of revolving trades opened in past 24 months\n",
        "41. •max_bal_bc: Maximum current balance owed on all revolving accounts\n",
        "42. •all_util: Balance to credit limit on all trades\n",
        "43. •total_rev_hi_lim: Total revolving high credit/credit limit\n",
        "44. •inq_fi: Number of personal finance inquiries\n",
        "45. •total_cu_tl: Number of finance trades\n",
        "46. •inq_last_12m: Number of credit inquiries in past 12 months\n",
        "47. •acc_open_past_24mths: Number of trades opened in past 24 months.\n",
        "48. •bc_open_to_buy: Total open to buy on revolving bankcards.\n",
        "49. •bc_util: Ratio of total current balance to high credit/credit limit for all bankcard accounts.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "51. •mo_sin_old_il_acct: Months since oldest bank installment account opened\n",
        "52. •mo_sin_old_rev_tl_op: Months since oldest revolving account opened\n",
        "\n",
        "55. •mort_acc: Number of mortgage accounts.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "56. •num_accts_ever_120_pd: Number of accounts ever 120 or more days past due\n",
        "\n",
        "57. •num_actv_bc_tl: Number of currently active bankcard accounts\n",
        "58. •num_actv_rev_tl: Number of currently active revolving trades\n",
        "\n",
        "\n",
        "59. •num_bc_sats: Number of satisfactory bankcard accounts\n",
        "60. •num_bc_tl: Number of bankcard accounts\n",
        "61. •num_il_tl: Number of installment accounts\n",
        "62. •num_op_rev_tl: Number of open revolving accounts\n",
        "63. •num_rev_accts: Number of revolving accounts\n",
        "64. •num_rev_tl_bal_gt_0: Number of revolving trades with balance >0\n",
        "65. •num_sats: Number of satisfactory accounts\n",
        "66. •num_tl_op_past_12m: Number of accounts opened in past 12 months\n",
        "67. •pct_tl_nvr_dlq: Percent of trades never delinquent\n",
        "68. •percent_bc_gt_75: Percentage of all bankcard accounts > 75% of limit.\n",
        "69. •pub_rec_bankruptcies: Number of public record bankruptcies\n",
        "70. •tax_liens: Number of tax liens\n",
        "71. •tot_hi_cred_lim: Total high credit/credit limit\n",
        "72. •total_bal_ex_mort: Total credit balance excluding mortgage\n",
        "73. •total_bc_limit: Total bankcard high credit/credit limit\n",
        "\n",
        "\n",
        "74. •total_il_high_credit_limit: Total installment high credit/credit limit\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "82. •revol_bal_joint: Sum of revolving credit balance of the co-borrowers, net of duplicate balances\n",
        "83. •sec_app_fico_range_low: FICO range (high) for the secondary applicant\n",
        "84. •sec_app_fico_range_high: FICO range (low) for the secondary applicant\n",
        "85. •sec_app_earliest_cr_line: Earliest credit line at time of application for the secondary applicant\n",
        "86. •sec_app_inq_last_6mths: Credit inquiries in the last 6 months at time of application for the secondary applicant\n",
        "87. •sec_app_mort_acc: Number of mortgage accounts at time of application for the secondary applicant\n",
        "88. •sec_app_open_acc: Number of open trades at time of application for the secondary applicant\n",
        "89. •sec_app_revol_util: Ratio of total current balance to high credit/credit limit for all revolving accounts\n",
        "90. •sec_app_open_act_il: Number of currently active installment trades at time of application for the secondary applicant\n",
        "91. •sec_app_num_rev_accts: Number of revolving accounts at time of application for the secondary applicant\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "yWA3NXg-vCom",
        "outputId": "a8806ebd-ff0e-4078-d94e-9d0b79fcbece"
      },
      "source": [
        "print(tf.random.uniform([1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([0.49902463], shape=(1,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "W_oxNafL3pTZ",
        "outputId": "452d98c4-62f4-47be-e384-bf11c687a621"
      },
      "source": [
        "len(train_dataset.skip(9))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33660"
            ]
          },
          "execution_count": 48,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "MFzbg04SkhlI",
        "outputId": "ef2e5574-d99e-41a9-a1b4-fbc21312c4cc"
      },
      "source": [
        "dictionary_df = pd.read_excel(\"https://resources.lendingclub.com/LCDataDictionary.xlsx\")\n",
        "\n",
        "# Drop blank rows, strip white space, convert to Python dictionary, fix one key name\n",
        "dictionary_df.dropna(axis=\"index\", inplace=True)\n",
        "dictionary_df = dictionary_df.applymap(lambda x: x.strip())\n",
        "dictionary_df.set_index(\"LoanStatNew\", inplace=True)\n",
        "dictionary = dictionary_df[\"Description\"].to_dict()\n",
        "dictionary[\"verification_status_joint\"] = dictionary.pop(\"verified_status_joint\")\n",
        "\n",
        "dictionary\n",
        "# Print in order of dataset columns (which makes more sense than dictionary's order)\n",
        "print(len(acp_loans.columns))\n",
        "for col in acp_loans.columns:\n",
        "    print(f\"•{col}: {dictionary[col]}\")\n",
        "\n",
        "\n",
        "# Hiding the output because it's quite a few lines, but feel free to take a peek by\n",
        "# clicking the \"Output\" button\n",
        "\n",
        "# loan_status\n",
        "# loan_amnt term int_rate purpose\n",
        "# fico_range_low fico_range_high grade sub_grade inq_last_6mths revol_util\n",
        "# delinq_2yrs pub_rec open_acc 　total_acc earliest_cr_line?\n",
        "# annual_inc emp_length home_ownership verification_status dti\n",
        "# len(desc)\n",
        "# revol_bal \n",
        "# Revolving to Income Ratio: Ratio of revolving credit balance to\n",
        "# the borrower’s monthly income. Data manipulation: the natural\n",
        "# logarithm function. This is another non-standard financial feature that we introduce in this study.\n",
        "\n",
        "# fico"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e1e984da26c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Print in order of dataset columns (which makes more sense than dictionary's order)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macp_loans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0macp_loans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"•{col}: {dictionary[col]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'acp_loans' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx-2dryR4kQa"
      },
      "source": [
        "# Model blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1dSsUrU4o1P"
      },
      "source": [
        "## MHSA layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39muiGLHpfb4"
      },
      "source": [
        "class MultiHeadSelfAttention(Layer):\n",
        "    \"\"\"\n",
        "    Base class for Multi-head Self-Attention layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads: int, use_masking: bool,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        :param num_heads: number of attention heads\n",
        "        :param use_masking: when True, forbids the attention to see the further\n",
        "          elements in the sequence.\n",
        "        :param kwargs: any extra arguments typical for a Keras layer,\n",
        "          such as name, etc.\n",
        "        \"\"\"\n",
        "        self.num_heads = num_heads\n",
        "        self.use_masking = use_masking\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['num_heads'] = self.num_heads\n",
        "        config['use_masking'] = self.use_masking\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_shape2 = input_shape\n",
        "        #if not isinstance(input_shape, tuple):\n",
        "        #    raise ValueError('Invalid input')\n",
        "        d_model = input_shape[-1]\n",
        "        self.d_model2 = d_model\n",
        "        \n",
        "        self.validate_model_dimensionality(d_model)\n",
        "        self.qkv_weights = self.add_weight(\n",
        "            name='qkv_weights',\n",
        "            shape=(d_model, d_model * 3),  # * 3 for q, k and v\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True)\n",
        "        \n",
        "        return super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        \"\"\"\n",
        "        if not K.is_keras_tensor(inputs):\n",
        "            raise ValueError(\n",
        "                'The layer can be called only with one tensor as an argument')\n",
        "        \"\"\"\n",
        "        _, seq_len, d_model = K.int_shape(inputs)\n",
        "        \n",
        "        # Perform affine transformations to get the Queries, the Keys and the Values.\n",
        "        qkv = K.dot(inputs, self.qkv_weights) # (-1,seq_len,d_model*3)\n",
        "        qkv = K.reshape(qkv,[-1,d_model*3])\n",
        "\n",
        "        # splitting the keys, the values and the queries.\n",
        "        pre_q, pre_k, pre_v = [qkv[:, i * d_model:(i + 1) * d_model] for i in range(3)]\n",
        "        pre_q = K.reshape(pre_q,(-1, seq_len, self.num_heads, d_model // self.num_heads))\n",
        "        pre_k = K.reshape(pre_k,(-1, seq_len, self.num_heads, d_model // self.num_heads))\n",
        "        pre_v = K.reshape(pre_v,(-1, seq_len, self.num_heads, d_model // self.num_heads))\n",
        "        \"\"\"\n",
        "        pre_q, pre_k, pre_v = [\n",
        "            K.reshape(\n",
        "                qkv[:, i * d_model:(i + 1) * d_model],\n",
        "                (-1, seq_len, self.num_heads, d_model // self.num_heads))\n",
        "            for i in range(3)]\n",
        "        \"\"\"\n",
        "        \n",
        "        attention_out = self.attention(pre_q, pre_v, pre_k, seq_len, d_model, training=kwargs.get('training'))\n",
        "        # of shape (-1, seq_len, d_model)\n",
        "        return attention_out\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        shape_a, seq_len, d_model = input_shape\n",
        "        return (shape_a, seq_len, d_model)\n",
        "    \n",
        "    def validate_model_dimensionality(self, d_model: int):\n",
        "        if d_model % self.num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f'The size of the last dimension of the input '\n",
        "                f'({d_model}) must be evenly divisible by the number'\n",
        "                f'of the attention heads {self.num_heads}')\n",
        "    \n",
        "    def attention(self, pre_q, pre_v, pre_k, seq_len: int, d_model: int,\n",
        "                  training=None):\n",
        "        \"\"\"\n",
        "        Calculates the output of the attention once the affine transformations\n",
        "        of the inputs are done. Here's the shapes of the arguments:\n",
        "        :param pre_q: (batch_size, q_seq_len, num_heads, d_model // num_heads)\n",
        "        :param pre_v: (batch_size, v_seq_len, num_heads, d_model // num_heads)\n",
        "        :param pre_k: (batch_size, k_seq_len, num_heads, d_model // num_heads)\n",
        "        :param seq_len: the length of the output sequence\n",
        "        :param d_model: dimensionality of the model (by the paper)\n",
        "        :param training: Passed by Keras. Should not be defined manually.\n",
        "          Optional scalar tensor indicating if we're in training\n",
        "          or inference phase.\n",
        "        \"\"\"\n",
        "        d_submodel = d_model // self.num_heads\n",
        "        \n",
        "        # shaping Q and V into (batch_size, num_heads, seq_len, d_model//heads)\n",
        "        q = K.permute_dimensions(pre_q, [0, 2, 1, 3])\n",
        "        v = K.permute_dimensions(pre_v, [0, 2, 1, 3])\n",
        "        k = K.permute_dimensions(pre_k, [0, 2, 3, 1])\n",
        "        \n",
        "        q = K.reshape(q, (-1,seq_len,d_submodel))\n",
        "        k = K.reshape(k, (-1,seq_len,d_submodel))\n",
        "        v = K.reshape(v, (-1,seq_len,d_submodel))\n",
        "        \n",
        "        qk = tf.einsum('aib,ajb->aij', q, k)\n",
        "        sqrt_d = K.constant(np.sqrt(d_model // self.num_heads),\n",
        "                            dtype=K.floatx())\n",
        "        a = qk/sqrt_d\n",
        "        a = self.mask_attention(a)\n",
        "        a = K.softmax(a)\n",
        "        attention_heads = tf.einsum('aij,ajb->aib', a, v)\n",
        "        attention_heads = K.reshape(attention_heads, (-1, self.num_heads, seq_len, d_submodel))\n",
        "        #attention_heads = K.reshape(attention_heads, (-1, self.num_heads, seq_len, d_model))\n",
        "        attention_heads = K.permute_dimensions(attention_heads, [0, 2, 1, 3])\n",
        "        attention_heads = K.reshape(attention_heads,(-1, seq_len, d_model))\n",
        "\n",
        "        return attention_heads\n",
        "\n",
        "    def mask_attention(self, dot_product):\n",
        "        \"\"\"\n",
        "        Makes sure that (when enabled) each position\n",
        "        (of a decoder's self-attention) cannot attend to subsequent positions.\n",
        "        :param dot_product: scaled dot-product of Q and K after reshaping them\n",
        "        to 3D tensors (batch * num_heads, rows, cols)\n",
        "        \"\"\"\n",
        "        if not self.use_masking:\n",
        "            return dot_product\n",
        "        last_dims = K.int_shape(dot_product)[-2:]\n",
        "        low_triangle_ones = (\n",
        "            np.tril(np.ones(last_dims))\n",
        "            # to ensure proper broadcasting\n",
        "            .reshape((1,) + last_dims))\n",
        "        inverse_low_triangle = 1 - low_triangle_ones\n",
        "        close_to_negative_inf = -1e9\n",
        "        result = (\n",
        "            K.constant(low_triangle_ones, dtype=K.floatx()) * dot_product +\n",
        "            K.constant(close_to_negative_inf * inverse_low_triangle))\n",
        "        return result\n",
        "\n",
        "\n",
        "get_custom_objects().update({\n",
        "    'MultiHeadSelfAttention': MultiHeadSelfAttention,\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzKX1xwm4uJY"
      },
      "source": [
        "## layernorm transition layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhiqGxjg5nnL"
      },
      "source": [
        "keras.utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDw0ixzkbgQS"
      },
      "source": [
        "import math\n",
        "from typing import Union, Callable, Optional\n",
        "\n",
        "from keras.layers import Layer, Add, Dropout\n",
        "from tensorflow.keras.layers import InputSpec\n",
        "from keras import initializers, constraints, activations\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "#from tLobAttention import MultiHeadSelfAttention\n",
        "\n",
        "\n",
        "class LayerNormalization(Layer):\n",
        "    \"\"\"\n",
        "    Implementation of Layer Normalization (https://arxiv.org/abs/1607.06450).\n",
        "    \"\"\"\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        self.axis = axis\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['axis'] = self.axis\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        dim = input_shape[-1]\n",
        "        self.gain = self.add_weight(\n",
        "            name='gain',\n",
        "            shape=(dim,),\n",
        "            initializer='ones',\n",
        "            trainable=True)\n",
        "        self.bias = self.add_weight(\n",
        "            name='bias',\n",
        "            shape=(dim,),\n",
        "            initializer='zeros',\n",
        "            trainable=True)\n",
        "        return super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        mean = K.mean(inputs, axis=self.axis, keepdims=True)\n",
        "        variance = K.mean(\n",
        "            K.square(inputs - mean), axis=self.axis, keepdims=True)\n",
        "        epsilon = K.constant(1e-5, dtype=K.floatx())\n",
        "        normalized_inputs = (inputs - mean) / K.sqrt(variance + epsilon)\n",
        "        result = self.gain * normalized_inputs + self.bias\n",
        "        return result\n",
        "\n",
        "\n",
        "class TransformerTransition(Layer):\n",
        "    \"\"\"\n",
        "    Transformer transition function. The same function is used both\n",
        "    in classical in Universal Transformers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, activation: Union[str, Callable],\n",
        "                 size_multiplier: int = 4, **kwargs):\n",
        "        \"\"\"\n",
        "        :param activation: activation function. Must be a string or a callable.\n",
        "        :param size_multiplier: How big the hidden dimension should be.\n",
        "          Most of the implementation use transition functions having 4 times\n",
        "          more hidden units than the model itself.\n",
        "        :param kwargs: Keras-specific layer arguments.\n",
        "        \"\"\"\n",
        "        self.activation = activations.get(activation)\n",
        "        self.size_multiplier = size_multiplier\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['activation'] = activations.serialize(self.activation)\n",
        "        config['size_multiplier'] = self.size_multiplier\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        d_model = input_shape[-1]\n",
        "        self.weights1 = self.add_weight(\n",
        "            name='weights1',\n",
        "            shape=(d_model, self.size_multiplier * d_model),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True)\n",
        "        self.biases1 = self.add_weight(\n",
        "            name='biases1',\n",
        "            shape=(self.size_multiplier * d_model,),\n",
        "            initializer='zeros',\n",
        "            trainable=True)\n",
        "        self.weights2 = self.add_weight(\n",
        "            name='weights2',\n",
        "            shape=(self.size_multiplier * d_model, d_model),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True)\n",
        "        self.biases2 = self.add_weight(\n",
        "            name='biases2',\n",
        "            shape=(d_model,),\n",
        "            initializer='zeros',\n",
        "            trainable=True)\n",
        "        return super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        input_shape = K.int_shape(inputs)\n",
        "        d_model = input_shape[-1]\n",
        "        step1 = self.activation(\n",
        "            K.bias_add(\n",
        "                K.dot(K.reshape(inputs, (-1, d_model)),\n",
        "                      self.weights1),\n",
        "                self.biases1,\n",
        "                data_format='channels_last'))\n",
        "        step2 = K.bias_add(\n",
        "            K.dot(step1, self.weights2),\n",
        "            self.biases2,\n",
        "            data_format='channels_last')\n",
        "        result = K.reshape(step2, (-1,) + input_shape[-2:])\n",
        "        return result\n",
        "    \n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])\n",
        "\n",
        "\n",
        "class TransformerBlock:\n",
        "    \"\"\"\n",
        "    A pseudo-layer combining together all nuts and bolts to assemble\n",
        "    a complete section of both the Transformer and the Universal Transformer\n",
        "    models, following description from the \"Universal Transformers\" paper.\n",
        "    Each such block is, essentially:\n",
        "    - Multi-head self-attention (masked or unmasked)\n",
        "    - Residual connection,\n",
        "    - Layer normalization\n",
        "    - Transition function\n",
        "    - Residual connection\n",
        "    - Layer normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, dff, name: str, num_heads: int,\n",
        "                 use_masking: bool = True):\n",
        "        self.attention_layer = MultiHeadSelfAttention(\n",
        "            num_heads, use_masking=use_masking, \n",
        "            name=f'{name}_self_attention')\n",
        "        self.norm1_layer = LayerNormalization(name=f'{name}_normalization1')\n",
        "        self.norm2_layer = LayerNormalization(name=f'{name}_normalization2')\n",
        "        self.transition_layer = TransformerTransition(\n",
        "            name=f'{name}_transition', activation='relu')\n",
        "        self.addition_layer = Add(name=f'{name}_add')\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    def __call__(self, _input):\n",
        "        output = self.attention_layer(_input)\n",
        "        post_residual1 = (\n",
        "            self.addition_layer([_input, output]))\n",
        "        norm1_output = self.norm1_layer(post_residual1)\n",
        "        #output = self.transition_layer(norm1_output)\n",
        "        post_residual2 = (\n",
        "            self.addition_layer([norm1_output, output]))\n",
        "        output = self.norm2_layer(post_residual2)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjX6HiSP42hr"
      },
      "source": [
        "## MLP layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBFYyc5WcaEI"
      },
      "source": [
        "# MLP block\n",
        "\"\"\"\n",
        "class mlpBLock(Layer):\n",
        "  def __init__(layers:list):\n",
        "    self.layers = layers\n",
        "\"\"\"\n",
        "def mlpBlock(layers, input):\n",
        "  for i in layers:\n",
        "      output = Dense(i, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.l2(1e-4),\n",
        "        activity_regularizer=regularizers.l2(1e-5))(input)\n",
        "      output = Dropout(0.1)(output)\n",
        "  return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT3BIToW46l1"
      },
      "source": [
        "## MHEm layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OftbuvJ0H14L"
      },
      "source": [
        "class inputW_Em(Layer):\n",
        "  def __init__(self, units: int, num_heads,**kwargs):\n",
        "    self.units = units\n",
        "    self.num_heads = num_heads\n",
        "    #self.num_w = num_w\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    #output_shape = self.compute_output_shape(input_shape)\n",
        "    self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.num_heads, self.units//self.num_heads),\n",
        "            #shape=( input_shape[-1], self.units),\n",
        "            #initializer=\"random_normal\",\n",
        "            initializer=\"Ones\",\n",
        "            trainable=True,\n",
        "        )\n",
        "    self.w_2 = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units, self.units),\n",
        "            #shape=( input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "    self.b = self.add_weight(shape=(input_shape[-1],), initializer=\"random_normal\", trainable=True)\n",
        "    self.b_2 = self.add_weight(shape=(input_shape[-1],self.units), initializer=\"random_normal\", trainable=True)\n",
        "\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    if input_shape.rank == 2:\n",
        "      shape_a, d_model = input_shape\n",
        "      output_shape = (shape_a, d_model, self.units)\n",
        "      return output_shape\n",
        "    elif input_shape.rank == 1:\n",
        "      d_model = input_shape\n",
        "      output_shape = (d_model, self.units)\n",
        "      return output_shape\n",
        "    else:\n",
        "      return\n",
        "\n",
        "  def call(self, input, **kwargs):\n",
        "\n",
        "    input = tf.math.add(input, self.b)\n",
        "\n",
        "    w1 = tf.einsum('ai,ijb->aijb', input, self.w)\n",
        "    result1 = tf.reshape(w1, [input.shape[0], input.shape[1], self.units])\n",
        "    #result1 = tf.concat([w1,w2, w3,w4], 2)\n",
        "    result = tf.math.add(result1, self.b_2)\n",
        "    result = tf.einsum('aji,jib->ajb', result, self.w_2)\n",
        "    result = tf.math.add(result1, result)\n",
        "    #print('finished inputw', result.shape)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXHmqqmJgPR2"
      },
      "source": [
        "class inputW(Layer):\n",
        "  def __init__(self, units: int, **kwargs):\n",
        "    self.units = units\n",
        "    #self.num_w = num_w\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    #output_shape = self.compute_output_shape(input_shape)\n",
        "    self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            #shape=( input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "    self.w_2 = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units, self.units),\n",
        "            #shape=( input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "    self.b = self.add_weight(shape=(input_shape[-1],), initializer=\"random_normal\", trainable=True)\n",
        "\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    if input_shape.rank == 2:\n",
        "      shape_a, d_model = input_shape\n",
        "      output_shape = (shape_a, d_model, self.units)\n",
        "      return output_shape\n",
        "    elif input_shape.rank == 1:\n",
        "      d_model = input_shape\n",
        "      output_shape = (d_model, self.units)\n",
        "      return output_shape\n",
        "    else:\n",
        "      return\n",
        "\n",
        "  def call(self, input, **kwargs):\n",
        "    #print(input.shape)\n",
        "    #inputs = input\n",
        "    #inputs = tf.expand_dims(inputs, axis=-1)\n",
        "    #repeated_w = tf.repeat(self.w, repeats=input.shape[0])\n",
        "    #result = layers.multiply(inputs=[inputs, self.w])\n",
        "    #print(inputs.shape, self.w)\n",
        "    #result = tf.math.multiply(inputs, self.w)\n",
        "    input = tf.math.add(input, self.b)\n",
        "\n",
        "    result = tf.einsum('ai,ib->aib', input, self.w)\n",
        "    result = tf.einsum('aji,jib->ajb', result, self.w_2)\n",
        "    #print('finished inputw', result.shape)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "rjq8tbwSdEqy",
        "outputId": "06937a96-63f8-4614-86f0-af35f6cca3d2"
      },
      "source": [
        "class CS_p2p(Model):\n",
        "  \n",
        "  def __init__(self, intermdediate_dim:init, output_dim:init, **kwargs):\n",
        "    self.intermdediate_dim = intermdediate_dim\n",
        "    self.oupput_dim = output_dim\n",
        "    self.inputW() = inputW()\n",
        "    self.TransformerBlock = TransformerBlock(name='cs_p2p', num_heads=3, use_masking=False)\n",
        "    self.mlpBlock = mlpBlock(intermediate_dim=self.intermediate_dim, output_dim=self.output_dim)\n",
        "\n",
        "  def call(self, input, **kwargs):\n",
        "    x = self.inputW(input)\n",
        "    x = self.TransformerBlock(x)\n",
        "    x = self.mlpBlock(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-186-b0da6ab8809c>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    self.inputW() = inputW()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to function call\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xnXyyZ4oMrx"
      },
      "source": [
        "data = pd.read_csv('/content/test (1).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJWCQ1OaKRJk"
      },
      "source": [
        "## MHEmME layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4EV2ruvKbQq"
      },
      "source": [
        "class inputW_EmME(Layer):\n",
        "  def __init__(self, units: int, num_heads,**kwargs):\n",
        "    self.units = units\n",
        "    self.num_heads = num_heads\n",
        "    #self.num_w = num_w\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    #output_shape = self.compute_output_shape(input_shape)\n",
        "    self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.num_heads, self.units//self.num_heads),\n",
        "            #shape=( input_shape[-1], self.units),\n",
        "            #initializer=\"random_normal\",\n",
        "            initializer=\"Ones\",\n",
        "            trainable=True,\n",
        "        )\n",
        "    self.w_2 = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units, self.units),\n",
        "            #shape=( input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "    self.b = self.add_weight(shape=(input_shape[-1],), initializer=\"random_normal\", trainable=True)\n",
        "    self.b_2 = self.add_weight(shape=(input_shape[-1],self.units), initializer=\"random_normal\", trainable=True)\n",
        "\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    if input_shape.rank == 2:\n",
        "      shape_a, d_model = input_shape\n",
        "      output_shape = (shape_a, d_model, self.units)\n",
        "      return output_shape\n",
        "    elif input_shape.rank == 1:\n",
        "      d_model = input_shape\n",
        "      output_shape = (d_model, self.units)\n",
        "      return output_shape\n",
        "    else:\n",
        "      return\n",
        "\n",
        "  def call(self, input, **kwargs):\n",
        "\n",
        "    input = tf.math.add(input, self.b)\n",
        "\n",
        "    w1 = tf.einsum('ai,ijb->aijb', input, self.w)\n",
        "    result1 = tf.reshape(w1, [input.shape[0], input.shape[1], self.units])\n",
        "    #result1 = tf.concat([w1,w2, w3,w4], 2)\n",
        "    result = tf.math.add(result1, self.b_2)\n",
        "    result = tf.einsum('aji,jib->ajb', result, self.w_2)\n",
        "    result = tf.math.add(result1, result)\n",
        "    #print('finished inputw', result.shape)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-MmISPKjrlY"
      },
      "source": [
        "#@title MLP\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC0e3B4_9wmb"
      },
      "source": [
        "#@title positional encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV55ByFu-D4_"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model+1))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # 配列中の偶数インデックスにはsinを適用; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # 配列中の奇数インデックスにはcosを適用; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVxM-gMgQXOP",
        "outputId": "a070fd5d-a2a9-4017-f868-43a967c4e21c"
      },
      "source": [
        "positional_encoding(3,4)[:,:3, :]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=\n",
              "array([[[ 0.        ,  1.        ,  0.        ,  1.        ],\n",
              "        [ 0.84147096,  0.5403023 ,  0.00999983,  0.99995   ],\n",
              "        [ 0.9092974 , -0.41614684,  0.01999867,  0.9998    ]]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 74,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNL8Fmo5NlNM"
      },
      "source": [
        "class PositionalEncoder(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def get_angles(self, pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # 配列中の偶数インデックスにはsinを適用; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # 配列中の奇数インデックスにはcosを適用; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "    \n",
        "\n",
        "  def call(self, position, d_model):\n",
        "    return self.positional_encoding(position, d_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfRSE_1_0FzY"
      },
      "source": [
        "#@title FFN layers\n",
        "class FFN(Layer):\n",
        "  def __init__(self, D, drop_rate, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.dense = layers.Dense(D, activation='relu', \n",
        "                              #name=f'LN_dense',\n",
        "                              #kernel_initializer=tf.keras.initializers.he_normal(),\n",
        "                              kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "                              #kernel_regularizer=tf.keras.regularizers.l2()\n",
        "                              )\n",
        "    self.LN = layers.LayerNormalization(\n",
        "        #name=f'LN'\n",
        "        )\n",
        "    self.dropout = layers.Dropout(rate=drop_rate, \n",
        "                                  #name=f'dropout_ffn'\n",
        "                                  )\n",
        "    #self.add = layers.Add()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.dense2 = layers.Dense(input_shape[-1], #name=f'LN_dense2',\n",
        "                               #kernel_regularizer=tf.keras.regularizers.l2(),\n",
        "                               #kernel_initializer=tf.keras.initializers.he_normal())\n",
        "                               kernel_initializer=tf.keras.initializers.HeUniform())\n",
        "\n",
        "  #@tf.function\n",
        "  def call(self, inputs):\n",
        "    res = inputs\n",
        "    inputs = self.LN(inputs)\n",
        "    inputs = self.dense(inputs)\n",
        "    inputs = self.dropout(inputs)\n",
        "    inputs = self.dense2(inputs)\n",
        "    #inputs = self.add([inputs, res])\n",
        "    inputs += res\n",
        "    inputs = self.LN(inputs)\n",
        "    return inputs\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W0d3cNZ51aw"
      },
      "source": [
        "## AutoInt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLxZ6LUsZit_"
      },
      "source": [
        "#@title AutoInt_EM \n",
        "class AutoInt_EM(Layer): #multi-valued feature field \n",
        "  def __init__(self, emb_d, feat_size, num_emb, drop_rate=0.0, em_em='linear', \n",
        "               name=None, PE=False, regularizers=True, string_emb=True):\n",
        "    super().__init__(name=name)\n",
        "    '''\n",
        "    self.feature_embedding = self.add_weight(\n",
        "        shape=(feat_size, emb_d), initializer = tf.keras.initializers.GlorotNormal(), \n",
        "        #regularizer=tf.keras.regularizers.l1_l2(),\n",
        "        #regularizer=tf.keras.regularizers.l1(),\n",
        "        regularizer=tf.keras.regularizers.l2() if regularizers else None,\n",
        "        trainable=True, name='embedding_dic', dtype=np.float32)\n",
        "    '''\n",
        "    self.feature_embedding = layers.Embedding(feat_size, emb_d, \n",
        "                                              #embeddings_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "                                              embeddings_initializer=tf.keras.initializers.HeUniform(), \n",
        "                                              #embeddings_regularizer=tf.keras.regularizers.l2() if regularizers else None,\n",
        "                                              name='embedding_dic', dtype=np.float32)\n",
        "    self.drop = layers.Dropout(rate=drop_rate, name='em_dropout')\n",
        "    self.emb_d = emb_d\n",
        "    self.num_emb = num_emb\n",
        "    self.string_emb = string_emb\n",
        "    self.em_em = em_em\n",
        "    self.pe= PE\n",
        "    if self.em_em == 'linear':\n",
        "      pass\n",
        "    elif self.em_em == 'dense':\n",
        "      self.em_dense = layers.Dense(emb_d, kernel_initializer=tf.keras.initializers.HeUniform(), name='em_dnese')\n",
        "    elif self.em_em == 'matrix':\n",
        "      self.em_nn = self.add_weight(shape=(num_emb,128,emb_d ),\n",
        "                                   initializer = tf.keras.initializers.HeUniform(), \n",
        "                                   #regularizer=tf.keras.regularizers.l1_l2(),\n",
        "                                   #regularizer=tf.keras.regularizers.l2(),\n",
        "                                   trainable=True, name='em_matrix') \n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.field_size = input_shape[-1]\n",
        "    self.batch_size = input_shape[0]\n",
        "    if self.pe:\n",
        "      self.positional_emb = positional_encoding\n",
        "\n",
        "  #@tf.function\n",
        "  def call(self, value, index, value_emb, index_emb):\n",
        "    #out = tf.nn.embedding_lookup(self.feature_embedding, index)\n",
        "    # print(f\"value {value.shape}, value_emb {value_emb.shape}\" )\n",
        "    out = self.feature_embedding(index)\n",
        "\n",
        "    feat_value = tf.reshape(value, shape=[-1, self.field_size, 1])\n",
        "    out = tf.multiply(out, feat_value)\n",
        "\n",
        "    #emb_out =  tf.nn.embedding_lookup(self.feature_embedding, index_emb)\n",
        "    emb_out = self.feature_embedding(index_emb)\n",
        "    # print(f\"emb out:{out.shape}, vec emb:{emb_out.shape}\")\n",
        "\n",
        "    if self.string_emb :\n",
        "\n",
        "      if self.em_em == 'linear':\n",
        "        # print(\"autoint l\", emb_out.shape, value_emb)\n",
        "        emb_out = tf.multiply(emb_out, value_emb)\n",
        "\n",
        "      elif self.em_em == 'matrix':\n",
        "        value_emb = tf.einsum('abj, bjk-> abk', value_emb, self.em_nn)\n",
        "        # print(\"autoint m\", emb_out.shape, value_emb)\n",
        "        emb_out = tf.multiply(emb_out, value_emb)\n",
        "      elif self.em_em == 'dense':\n",
        "\n",
        "        value_emb = self.em_dense(value_emb)\n",
        "\n",
        "        # print(\"autoint d\", emb_out.shape, value_emb)\n",
        "        emb_out = tf.multiply(emb_out, value_emb)\n",
        "\n",
        "      out= tf.concat([out, emb_out], axis=1)\n",
        "    if self.pe:\n",
        "      out = tf.nn.bias_add(tf.reshape(out, [self.batch_size, -1])*math.sqrt(self.emb_d), \n",
        "                           tf.reshape(self.positional_emb(self.field_size+self.num_emb, self.emb_d), [-1]))\n",
        "      out = tf.reshape(out, shape=[out.shape[0], self.field_size+self.num_emb, self.emb_d])\n",
        "\n",
        "    out = self.drop(out)\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NbtlQ7ktKrh"
      },
      "source": [
        "#@title AutoInt_MHS\n",
        "class AutoInt_MHS(Layer):\n",
        "  def __init__(self, num_heads=1, has_residual=True,  dropout=0.0, **kwds):\n",
        "    self.num_heads = num_heads\n",
        "    self.has_residual = has_residual\n",
        "    self.dropout= layers.Dropout(rate=dropout)\n",
        "    self.LN = layers.LayerNormalization()\n",
        "    super().__init__(**kwds)\n",
        "\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.num_d = input_shape[-1]\n",
        "    self.query = layers.Dense(self.num_d, \n",
        "                              activation=('relu'), \n",
        "                              #name='MHS_query'\n",
        "                              #kernel_regularizer=tf.keras.regularizers.l2()\n",
        "                              kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "                              )\n",
        "    self.key = layers.Dense(self.num_d, \n",
        "                            activation=('relu'), \n",
        "                            #name='MHS_key'\n",
        "                            #kernel_regularizer=tf.keras.regularizers.l2()\n",
        "                            kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "                            )\n",
        "    self.value = layers.Dense(self.num_d, \n",
        "                              activation=('relu'), \n",
        "                              trainable=True, \n",
        "                              #name='MHS_value'\n",
        "                              #kernel_regularizer=tf.keras.regularizers.l2()\n",
        "                              kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "                              )\n",
        "    if self.has_residual:\n",
        "      self.value_r = layers.Dense(self.num_d,  \n",
        "                                  #name='residual'\n",
        "                                  #kernel_regularizer=tf.keras.regularizers.l2()\n",
        "                                  kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "                                  )\n",
        "\n",
        "  #@tf.function\n",
        "  def call(self, inputs):\n",
        "    # Linear projections\n",
        "    Q = self.query(inputs)\n",
        "    K = self.key(inputs)\n",
        "    V = self.value(inputs)\n",
        "    if self.has_residual:\n",
        "        V_res = self.value_r(inputs)\n",
        "\n",
        "    # Split and concat\n",
        "    Q_ = tf.concat(tf.split(Q, self.num_heads, axis=2), axis=0)\n",
        "    K_ = tf.concat(tf.split(K, self.num_heads, axis=2), axis=0)\n",
        "    V_ = tf.concat(tf.split(V, self.num_heads, axis=2), axis=0)\n",
        "\n",
        "    # Multiplication\n",
        "    weights = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
        "\n",
        "    # Scale\n",
        "    weights = weights / (K_.get_shape().as_list()[-1] ** 0.5)\n",
        "\n",
        "    # Activation\n",
        "    weights = tf.nn.softmax(weights)\n",
        "\n",
        "\n",
        "    # Dropouts\n",
        "    weights = self.dropout(weights)\n",
        "\n",
        "    # Weighted sum\n",
        "    outputs = tf.matmul(weights, V_)\n",
        "\n",
        "    # Restore shape\n",
        "    outputs = tf.concat(tf.split(outputs, self.num_heads, axis=0), axis=2)\n",
        "\n",
        "    # Residual connection\n",
        "    if self.has_residual:\n",
        "        outputs += V_res\n",
        "        outputs = tf.nn.relu(outputs)\n",
        "    # Normalize\n",
        "    outputs = self.LN(outputs)\n",
        "        \n",
        "    return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gntj2OszFGP"
      },
      "source": [
        "class AutoInt(Model):\n",
        "  def __init__(self, num_layers, num_h, num_d, feature_size, **kwds):\n",
        "    super().__init__(**kwds)\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding = AutoInt_EM(num_d, feature_size)\n",
        "    self.mhsa = AutoInt_MHS(num_h)\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.prediction = layers.Dense(1, kernel_initializer=\"glorot_normal\", bias_initializer='random_normal')\n",
        "\n",
        "\n",
        "  def call(self, value, index, value_emb, index_emb):\n",
        "    x = self.embedding(value, index, value_emb, index_emb)\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.mhsa(x, name=f'{name}_self_attention_{i}')\n",
        "    x = self.flatten(x)\n",
        "    x = self.prediction(x)\n",
        "    return x\n",
        "\n",
        "    \n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc1Sf3reLEFC"
      },
      "source": [
        "#@title Autoint_builder_maker \n",
        "import random\n",
        "\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "def Autoint_builder_maker(feat_size, num_input, num_emb, emb_d, batch_size, \n",
        "                          num_y=2, autodis=False):\n",
        "  def Autoint_builder(hp):\n",
        "    \n",
        "    #hp_emb_d = hp.Int('num_of_d', min_value=32, max_value=128, step=32, default=128)\n",
        "    hp_emb_d = hp.Fixed('num_of_d', value=128)\n",
        "    with hp.conditional_scope('num_of_d', [128]):\n",
        "      hp_emb_layer = hp.Choice( 'emb_layer_choice', values=['linear', 'dense', 'matrix'])\n",
        "      #hp_emb_layer = hp.Choice( 'emb_layer_choice', values=['dense', 'matrix'])\n",
        "    #with hp.conditional_scope('num_of_d', [32,  64,  96]):\n",
        "      #hp_emb_layer = hp.Choice('emb_layer_choice', values=['matrix', 'dense'], default='matrix')\n",
        "    print(hp_emb_layer, hp_emb_d)\n",
        "    #hp_emb_d = 128\n",
        "    #hp_emb_layer = 'linear'\n",
        "\n",
        "    #batch_size needs to implement later \n",
        "    value = Input(shape=(num_input,), batch_size=batch_size, name='value', dtype=np.float32)\n",
        "    index = Input(shape=(num_input,), batch_size=batch_size, name='index', dtype=np.int32)\n",
        "    emb_value = Input(shape=(num_emb, emb_d, ), batch_size=batch_size, name='emb_value')\n",
        "    emb_index = Input(shape=(num_emb,), batch_size=batch_size, name='emb_index', dtype=np.int32)\n",
        "    if autodis:\n",
        "      meta_index =  Input(shape=(num_input,), batch_size=batch_size, name='meta_index', dtype=np.float32)\n",
        "      reverse_meta_index = Input(shape=(num_input,), batch_size=batch_size, \n",
        "                                 name='reverse_meta_index', dtype=np.float32)\n",
        "    hp_em_dropout_rate = hp.Float('EM_dropout_rate', min_value=0.0, max_value=0.2, step=0.1, default=0.0)\n",
        "    #hp_em_dropout_rate = hp.Fixed('EM_dropout_rate', value=0.0)\n",
        "    if autodis:\n",
        "      hp_meta_size = hp.Int('meta_size', min_value=4, max_value=20, step=4)\n",
        "      hp_t = hp.Float('temperatue_rate', 1e-6, 1e-2, sampling='log', default=1e-3)\n",
        "      out = AutoDis_EM(emb_d=hp_emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "              drop_rate=hp_em_dropout_rate, em_em= hp_emb_layer, meta_size=hp_meta_size,\n",
        "              t=hp_t\n",
        "              #name=f'autoint em {hp_emb_d}, {hp_emb_layer} {hp_em_dropout_rate}'\n",
        "              )(value, index, emb_value, emb_index, meta_index, reverse_meta_index) \n",
        "    else:\n",
        "      out = AutoInt_EM(emb_d=hp_emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "              drop_rate=hp_em_dropout_rate, em_em= hp_emb_layer, \n",
        "              #name=f'autoint em {hp_emb_d}, {hp_emb_layer} {hp_em_dropout_rate}'\n",
        "              )(value, index, emb_value, emb_index)\n",
        "      print(\"autoEM\", out.shape)\n",
        "    \n",
        "\n",
        "    #mha_head = hp.Choice('num_head', values=[1,2,4,8], default=1)\n",
        "    mha_head = hp.Fixed('num_head', value=4)\n",
        "\n",
        "    #mha_residual = hp.Boolean('mha_residual', default=True)\n",
        "    mha_residual =  hp.Fixed(name='mha_residual', value=True)\n",
        "    mha_dropout_rate = hp.Float('mha_dropout_rate', min_value=0.0, max_value=0.4, step=0.1, default=0.0)\n",
        "\n",
        "    \n",
        "    ffn_d = hp.Int('hp_ffn_d_size', min_value=128, max_value=512, step=128)\n",
        "    ffn_drop = hp.Float('hp_ffn_drop', min_value=0.0, max_value=0.9, step=0.3)\n",
        "\n",
        "    hp_ffn_boolean = hp.Boolean('ffn_layer')\n",
        "    uid = random.randint(1,9999999)\n",
        "    #for i in range(hp.Int('num_MHA_layer', min_value=1, max_value=4, step=1, default=1)):\n",
        "    for i in range(hp.Fixed('num_MHA_layer', value=4)):\n",
        "      out = AutoInt_MHS(num_heads=mha_head, has_residual=mha_residual, dropout=mha_dropout_rate, \n",
        "                        name=f'MHA_{i}_{mha_residual}_{mha_dropout_rate}_{mha_head}')(out)\n",
        "      if hp_ffn_boolean:\n",
        "        out = FFN(ffn_d, drop_rate=ffn_drop, name=f'ffn_{i}_{uid}_{ffn_drop}_{ffn_d}')(out)\n",
        "    hp_ffn = hp.Boolean('ffn')\n",
        "\n",
        "    out = layers.Flatten(name='flatten')(out)\n",
        "    \n",
        "\n",
        "    pred_drop = hp.Float('hp_pred_ffn_drop', min_value=0.0, max_value=0.90, step=0.3)\n",
        "    out = layers.LayerNormalization()(out)\n",
        "    out = layers.Dropout(rate=pred_drop)(out)\n",
        "    \n",
        "    pred_ffn_d = hp.Int('hp_pred_ffn_d_size', min_value=64, max_value=256, step=64)\n",
        "    out = Dense(pred_ffn_d, activation='relu', kernel_initializer=tf.keras.initializers.he_normal(), bias_initializer='random_normal')(out)\n",
        "      \n",
        "    out = layers.LayerNormalization()(out)\n",
        "    out = layers.Dropout(rate=pred_drop)(out)\n",
        "\n",
        "    if num_y == 2:\n",
        "      out = Dense(num_y, activation='relu', kernel_initializer='glorot_normal', bias_initializer='random_normal')(out)\n",
        "      out = layers.Softmax()(out)\n",
        "    else:\n",
        "      out = Dense(num_y, activation='relu', kernel_initializer='glorot_normal', bias_initializer='random_normal')(out)\n",
        "      out = layers.Softmax()(out)\n",
        "    optimizer = 'adam'\n",
        "    print(out.shape)\n",
        "\n",
        "\n",
        "    if autodis:\n",
        "      model = Model(inputs=[value, index, emb_value, emb_index, meta_index, reverse_meta_index], outputs=out, \n",
        "                  name=f'autoint_{hp_emb_d}_{hp_emb_layer}_{mha_head}{hp_em_dropout_rate}_{mha_residual}{uid}')\n",
        "    else:\n",
        "      model = Model(inputs=[value, index, emb_value, emb_index], outputs=out, \n",
        "                  name=f'autoint_{hp_emb_d}_{hp_emb_layer}_{mha_head}{hp_em_dropout_rate}_{mha_residual}{uid}')\n",
        "\n",
        "    initial_learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
        "    decay_steps=hp.Int('decay_steps', 50000,  90000, step=20000)\n",
        "    decay_rate=hp.Float('decay_rate', 0.0, 1.0, step=0.2)\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        decay_steps=decay_steps,\n",
        "        #decay_steps=steps_each,\n",
        "        decay_rate=decay_rate)\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "    #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "    #optimizer = RAdamOptimizer(total_steps=total_steps, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "    \n",
        "    label_smoothing = hp.Float('label_smoothing', min_value=0.0, max_value=0.60, step=0.3)\n",
        "    #checkpoint = tf.train.Checkpoint(model=model)\n",
        "    #local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost/replica:0/task:0/device:CPU:0\")\n",
        "    #checkpoint.write(\"/content\", options=local_device_option)\n",
        "\n",
        "    model.compile(optimizer, \n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing) if num_y == 2 else tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
        "            metrics=['categorical_accuracy',\n",
        "            #tf.keras.metrics.Precision(),\n",
        "            #tf.keras.metrics.Recall(),\n",
        "            #tf.keras.metrics.AUC(multi_label=True, num_labels=num_y),\n",
        "            #tf.keras.metrics.AUC(multi_label=True, num_labels=num_y, curve='PR'),\n",
        "            tf.keras.metrics.AUC(curve='PR'),\n",
        "            #tf.keras.metrics.F1\n",
        "            MCM(num_classes=num_y)\n",
        "            ])\n",
        "    print('model compiled')\n",
        "    return model\n",
        "  return Autoint_builder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "iQ21iCnt54NG"
      },
      "source": [
        "#@title AutoInt Original implements\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "The following two functions are adapted from kyubyong park's implementation of transformer\n",
        "We slightly modify the code to make it suitable for our work.(add relu, delete key masking and causality mask)\n",
        "June 2017 by kyubyong park. \n",
        "kbpark.linguist@gmail.com.\n",
        "https://www.github.com/kyubyong/transformer\n",
        "'''\n",
        "\n",
        "\n",
        "def normalize(inputs, epsilon=1e-8):\n",
        "    '''\n",
        "    Applies layer normalization\n",
        "    Args:\n",
        "        inputs: A tensor with 2 or more dimensions\n",
        "        epsilon: A floating number to prevent Zero Division\n",
        "    Returns:\n",
        "        A tensor with the same shape and data dtype\n",
        "    '''\n",
        "    inputs_shape = inputs.get_shape()\n",
        "    params_shape = inputs_shape[-1:]\n",
        "\n",
        "    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
        "    beta = tf.Variable(tf.zeros(params_shape))\n",
        "    gamma = tf.Variable(tf.ones(params_shape))\n",
        "    normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n",
        "    outputs = gamma * normalized + beta\n",
        "\n",
        "    return outputs\n",
        "\n",
        "        \n",
        "def multihead_attention(queries,\n",
        "                        keys,\n",
        "                        values,\n",
        "                        num_units=None,\n",
        "                        num_heads=1,\n",
        "                        dropout_keep_prob=1,\n",
        "                        is_training=True,\n",
        "                        has_residual=True):\n",
        "\t\n",
        "    if num_units is None:\n",
        "        num_units = queries.get_shape().as_list[-1]\n",
        "\n",
        "    # Linear projections\n",
        "    Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu)\n",
        "    K = tf.layers.dense(keys, num_units, activation=tf.nn.relu)\n",
        "    V = tf.layers.dense(values, num_units, activation=tf.nn.relu)\n",
        "    if has_residual:\n",
        "        V_res = tf.layers.dense(values, num_units, activation=tf.nn.relu)\n",
        "\n",
        "    # Split and concat\n",
        "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)\n",
        "    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)\n",
        "    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)\n",
        "\n",
        "    # Multiplication\n",
        "    weights = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
        "\n",
        "    # Scale\n",
        "    weights = weights / (K_.get_shape().as_list()[-1] ** 0.5)\n",
        "\n",
        "    # Activation\n",
        "    weights = tf.nn.softmax(weights)\n",
        "\n",
        "\n",
        "    # Dropouts\n",
        "    weights = tf.layers.dropout(weights, rate=1-dropout_keep_prob,\n",
        "                                        training=tf.convert_to_tensor(is_training))\n",
        "\n",
        "    # Weighted sum\n",
        "    outputs = tf.matmul(weights, V_)\n",
        "\n",
        "    # Restore shape\n",
        "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)\n",
        "\n",
        "    # Residual connection\n",
        "    if has_residual:\n",
        "        outputs += V_res\n",
        "\n",
        "    outputs = tf.nn.relu(outputs)\n",
        "    # Normalize\n",
        "    outputs = normalize(outputs)\n",
        "        \n",
        "    return outputs\n",
        "\n",
        "\n",
        "class AutoInt():\n",
        "    def __init__(self, args, feature_size, run_cnt):\n",
        "        #print(args.block_shape)\n",
        "        #print(type(args.block_shape))\n",
        "\n",
        "        self.feature_size = feature_size        # denote as n, dimension of concatenated features\n",
        "        self.field_size = args.field_size            # denote as M, number of total feature fields 入力の数\n",
        "        self.embedding_size = args.embedding_size    # denote as d, size of the feature embedding　dimention\n",
        "        self.blocks = args.blocks                    # number of the blocks\n",
        "        self.heads = args.heads                      # number of the heads\n",
        "        self.block_shape = args.block_shape\n",
        "        self.output_size = args.block_shape[-1] \n",
        "        self.has_residual = args.has_residual\n",
        "        self.has_wide = args.has_wide                # whether to add wide part\n",
        "        self.deep_layers = args.deep_layers          # whether to joint train with deep networks as described in paper\n",
        "\n",
        "\n",
        "        self.batch_norm = args.batch_norm\n",
        "        self.batch_norm_decay = args.batch_norm_decay\n",
        "        self.drop_keep_prob = args.dropout_keep_prob\n",
        "        self.l2_reg = args.l2_reg\n",
        "        self.epoch = args.epoch\n",
        "        self.batch_size = args.batch_size\n",
        "        self.learning_rate = args.learning_rate\n",
        "        self.learning_rate_wide = args.learning_rate_wide\n",
        "        self.optimizer_type = args.optimizer_type\n",
        "\n",
        "        self.save_path = args.save_path + str(run_cnt) + '/'\n",
        "        self.is_save = args.is_save\n",
        "        if (args.is_save == True and os.path.exists(self.save_path) == False):\n",
        "            os.makedirs(self.save_path)\t\n",
        "\n",
        "        self.verbose = args.verbose\n",
        "        self.random_seed = args.random_seed\n",
        "        self.loss_type = args.loss_type\n",
        "        self.eval_metric = roc_auc_score\n",
        "        self.best_loss = 1.0\n",
        "        self.greater_is_better = args.greater_is_better\n",
        "        self.train_result, self.valid_result = [], []\n",
        "        self.train_loss, self.valid_loss = [], []\n",
        "        \n",
        "        self._init_graph()\n",
        "\n",
        "        \n",
        "    def _init_graph(self):\n",
        "        self.graph = tf.Graph()\n",
        "        with self.graph.as_default():\n",
        "\n",
        "            tf.set_random_seed(self.random_seed)\n",
        "\n",
        "            self.feat_index = tf.placeholder(tf.int32, shape=[None, None],\n",
        "                                                 name=\"feat_index\")  # None * M\n",
        "            self.feat_value = tf.placeholder(tf.float32, shape=[None, None],\n",
        "                                                 name=\"feat_value\")  # None * M\n",
        "            self.label = tf.placeholder(tf.float32, shape=[None, 1], name=\"label\")  # None * 1\n",
        "            # In our implementation, the shape of dropout_keep_prob is [3], used in 3 different parts.\n",
        "            self.dropout_keep_prob = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_prob\")\n",
        "            self.train_phase = tf.placeholder(tf.bool, name=\"train_phase\")\n",
        "\n",
        "            self.weights = self._initialize_weights()\n",
        "\n",
        "            # model\n",
        "            self.embeddings = tf.nn.embedding_lookup(self.weights[\"feature_embeddings\"],\n",
        "                                                             self.feat_index)  # None * M * d\n",
        "            feat_value = tf.reshape(self.feat_value, shape=[-1, self.field_size, 1])\n",
        "            self.embeddings = tf.multiply(self.embeddings, feat_value)      # None * M * d\n",
        "            self.embeddings = tf.nn.dropout(self.embeddings, self.dropout_keep_prob[1]) # None * M * d\n",
        "            if self.has_wide: \n",
        "                self.y_first_order = tf.nn.embedding_lookup(self.weights[\"feature_bias\"], self.feat_index) # None * M * 1\n",
        "                self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order, feat_value), 1)  # None * 1\n",
        "\n",
        "            # joint training with feedforward nn\n",
        "            if self.deep_layers != None:\n",
        "                self.y_dense = tf.reshape(self.embeddings, shape=[-1, self.field_size * self.embedding_size])\n",
        "                for i in range(0, len(self.deep_layers)):\n",
        "                    self.y_dense = tf.add(tf.matmul(self.y_dense, self.weights[\"layer_%d\" %i]), self.weights[\"bias_%d\"%i]) # None * layer[i]\n",
        "                    if self.batch_norm:\n",
        "                        self.y_dense = self.batch_norm_layer(self.y_dense, train_phase=self.train_phase, scope_bn=\"bn_%d\" %i)\n",
        "                    self.y_dense = tf.nn.relu(self.y_dense)\n",
        "                    self.y_dense = tf.nn.dropout(self.y_dense, self.dropout_keep_prob[2])\n",
        "                self.y_dense = tf.add(tf.matmul(self.y_dense, self.weights[\"prediction_dense\"]),\n",
        "                                      self.weights[\"prediction_bias_dense\"], name='logits_dense')  # None * 1\n",
        "            \n",
        "            \n",
        "            # ---------- main part of AutoInt-------------------\n",
        "            self.y_deep = self.embeddings # None * M * d\n",
        "            for i in range(self.blocks):   \n",
        "                self.y_deep = multihead_attention(queries=self.y_deep,\n",
        "                                                  keys=self.y_deep,\n",
        "                                                  values=self.y_deep,\n",
        "                                                  num_units=self.block_shape[i],\n",
        "                                                  num_heads=self.heads,\n",
        "                                                  dropout_keep_prob=self.dropout_keep_prob[0],\n",
        "                                                  is_training=self.train_phase,\n",
        "                                                  has_residual=self.has_residual)\n",
        "\n",
        "            self.flat = tf.reshape(self.y_deep, \n",
        "                                   shape=[-1, self.output_size * self.field_size]) \n",
        "            #if self.has_wide:\n",
        "            #    self.flat = tf.concat([self.flat, self.y_first_order], axis=1)\n",
        "            #if self.deep_layers != None:\n",
        "            #    self.flat = tf.concat([self.flat, self.y_dense], axis=1)\n",
        "            self.out = tf.add(tf.matmul(self.flat, self.weights[\"prediction\"]), \n",
        "                              self.weights[\"prediction_bias\"], name='logits')  # None * 1\n",
        "            \n",
        "            if self.has_wide:\n",
        "                self.out += self.y_first_order\n",
        "\n",
        "            if self.deep_layers != None:\n",
        "                self.out += self.y_dense\n",
        "        \n",
        "            # ---------- Compute the loss ----------\n",
        "            # loss\n",
        "            if self.loss_type == \"logloss\":\n",
        "                self.out = tf.nn.sigmoid(self.out, name='pred')\n",
        "                self.loss = tf.losses.log_loss(self.label, self.out)\n",
        "            elif self.loss_type == \"mse\":\n",
        "                self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))\n",
        "\n",
        "            # l2 regularization on weights\n",
        "            if self.l2_reg > 0:\n",
        "                if self.deep_layers != None:\n",
        "                    for i in range(len(self.deep_layers)):\n",
        "                        self.loss += tf.contrib.layers.l2_regularizer(\n",
        "                                                    self.l2_reg)(self.weights[\"layer_%d\"%i])\n",
        "                #self.loss += tf.contrib.layers.l2_regularizer(self.l2_reg)(self.embeddings)\n",
        "                #all_vars = tf.trainable_variables()\n",
        "                #lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in all_vars\n",
        "                #            if 'bias' not in v.name and 'embeddings' not in v.name]) * self.l2_reg\n",
        "                #self.loss += lossL2            \n",
        "           \n",
        "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "            self.var1 = [v for v in tf.trainable_variables() if v.name != 'feature_bias:0']\n",
        "            self.var2 = [tf.trainable_variables()[1]]    # self.var2 = [feature_bias]\n",
        "            # optimizer\n",
        "            # here we should use two different optimizer for wide and deep model(if we add wide part).\n",
        "            if self.optimizer_type == \"adam\":\n",
        "                if self.has_wide:\n",
        "                    optimizer1 = tf.train.AdamOptimizer(learning_rate=self.learning_rate, \n",
        "                                                        beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
        "                    optimizer2 = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate_wide)\n",
        "                                                        #minimize(self.loss, global_step=self.global_step)\n",
        "                    var_list1 = self.var1\n",
        "                    var_list2 = self.var2\n",
        "                    grads = tf.gradients(self.loss, var_list1 + var_list2)\n",
        "                    grads1 = grads[:len(var_list1)]\n",
        "                    grads2 = grads[len(var_list1):]\n",
        "                    train_op1 = optimizer1.apply_gradients(zip(grads1, var_list1), global_step=self.global_step)\n",
        "                    train_op2 = optimizer2.apply_gradients(zip(grads2, var_list2))\n",
        "                    self.optimizer = tf.group(train_op1, train_op2)\n",
        "                else:\n",
        "                    self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, \n",
        "                                                        beta1=0.9, beta2=0.999, epsilon=1e-8).\\\n",
        "                                                        minimize(self.loss, global_step=self.global_step)\n",
        "            elif self.optimizer_type == \"adagrad\":\n",
        "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n",
        "                                                           initial_accumulator_value=1e-8).\\\n",
        "                                                           minimize(self.loss)\n",
        "            elif self.optimizer_type == \"gd\":\n",
        "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).\\\n",
        "                                                                   minimize(self.loss)\n",
        "            elif self.optimizer_type == \"momentum\":\n",
        "                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).\\\n",
        "                                                            minimize(self.loss)\n",
        "\n",
        "            # init\n",
        "            self.saver = tf.train.Saver(max_to_keep=5)\n",
        "            init = tf.global_variables_initializer()\n",
        "            self.sess = self._init_session()\n",
        "            self.sess.run(init)\n",
        "            self.count_param()\n",
        "\n",
        "\n",
        "    def count_param(self):\n",
        "        k = (np.sum([np.prod(v.get_shape().as_list()) \n",
        "                                                    for v in tf.trainable_variables()]))\n",
        "\n",
        "        #print(tf.trainable_variables())\n",
        "        print(\"total parameters :%d\" % k) \n",
        "        print(\"extra parameters : %d\" % (k - self.feature_size * self.embedding_size))\n",
        "        \n",
        "\n",
        "    def _init_session(self):\n",
        "        config = tf.ConfigProto(allow_soft_placement=True)\n",
        "        config.gpu_options.allow_growth = True\n",
        "        return tf.Session(config=config)\n",
        "\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        weights = dict()\n",
        "\n",
        "        # embeddings\n",
        "        weights[\"feature_embeddings\"] = tf.Variable(\n",
        "            tf.random_normal([self.feature_size, self.embedding_size], 0.0, 0.01),\n",
        "            name=\"feature_embeddings\")  # feature_size(n) * d\n",
        "  \n",
        "        if self.has_wide:\n",
        "            weights[\"feature_bias\"] = tf.Variable(\n",
        "                    tf.random_normal([self.feature_size, 1], 0.0, 0.001),\n",
        "                    name=\"feature_bias\")    # feature_size(n) * 1\n",
        "        input_size = self.output_size * self.field_size\n",
        "        #if self.deep_layers != None:\n",
        "        #    input_size += self.deep_layers[-1]\n",
        "        #if self.has_wide:\n",
        "        #    input_size += self.field_size\n",
        "\n",
        "        # dense layers\n",
        "        if self.deep_layers != None:\n",
        "            num_layer = len(self.deep_layers)\n",
        "            layer0_size = self.field_size * self.embedding_size\n",
        "            glorot = np.sqrt(2.0 / (layer0_size + self.deep_layers[0]))\n",
        "            weights[\"layer_0\"] = tf.Variable(\n",
        "                np.random.normal(loc=0, scale=glorot, size=(layer0_size, self.deep_layers[0])), dtype=np.float32)\n",
        "            weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[0])),\n",
        "                                                            dtype=np.float32)  # 1 * layers[0]\n",
        "            for i in range(1, num_layer):\n",
        "                glorot = np.sqrt(2.0 / (self.deep_layers[i-1] + self.deep_layers[i]))\n",
        "                weights[\"layer_%d\" % i] = tf.Variable(\n",
        "                    np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i-1], self.deep_layers[i])),\n",
        "                    dtype=np.float32)  # layers[i-1] * layers[i]\n",
        "                weights[\"bias_%d\" % i] = tf.Variable(\n",
        "                    np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),\n",
        "                    dtype=np.float32)  # 1 * layer[i]\n",
        "            glorot = np.sqrt(2.0 / (self.deep_layers[-1] + 1))\n",
        "            weights[\"prediction_dense\"] = tf.Variable(\n",
        "                                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[-1], 1)),\n",
        "                                dtype=np.float32, name=\"prediction_dense\")\n",
        "            weights[\"prediction_bias_dense\"] = tf.Variable(\n",
        "                                np.random.normal(), dtype=np.float32, name=\"prediction_bias_dense\")\n",
        "\n",
        "\n",
        "        #---------- prediciton weight ------------------#                                \n",
        "        glorot = np.sqrt(2.0 / (input_size + 1))\n",
        "        weights[\"prediction\"] = tf.Variable(\n",
        "                            np.random.normal(loc=0, scale=glorot, size=(input_size, 1)),\n",
        "                            dtype=np.float32, name=\"prediction\")\n",
        "        weights[\"prediction_bias\"] = tf.Variable(\n",
        "                            np.random.normal(), dtype=np.float32, name=\"prediction_bias\")\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def x(self, x, train_phase, scope_bn):\n",
        "        bn_train = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
        "                is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
        "        bn_inference = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
        "                is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
        "        z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n",
        "        return z\n",
        "\n",
        "    \n",
        "    def get_batch(self, Xi, Xv, y, batch_size, index):\n",
        "        start = index * batch_size\n",
        "        end = (index+1) * batch_size\n",
        "        end = end if end < len(y) else len(y)\n",
        "        return Xi[start:end], Xv[start:end], [[y_] for y_ in y[start:end]]\n",
        "\n",
        "\n",
        "    # shuffle three lists simutaneously\n",
        "    def shuffle_in_unison_scary(self, a, b, c):\n",
        "        rng_state = np.random.get_state()\n",
        "        np.random.shuffle(a)\n",
        "        np.random.set_state(rng_state)\n",
        "        np.random.shuffle(b)\n",
        "        np.random.set_state(rng_state)\n",
        "        np.random.shuffle(c)\n",
        "\n",
        "\n",
        "    def fit_on_batch(self, Xi, Xv, y):\n",
        "        feed_dict = {self.feat_index: Xi,\n",
        "                     self.feat_value: Xv,\n",
        "                     self.label: y,\n",
        "                     self.dropout_keep_prob: self.drop_keep_prob,\n",
        "                     self.train_phase: True}\n",
        "        step, loss, opt = self.sess.run((self.global_step, self.loss, self.optimizer), feed_dict=feed_dict)\n",
        "        return step, loss\n",
        "\n",
        "    # Since the train data is very large, they can not be fit into the memory at the same time.\n",
        "    # We separate the whole train data into several files and call \"fit_once\" for each file.\n",
        "    def fit_once(self, Xi_train, Xv_train, y_train,\n",
        "                 epoch, file_count, Xi_valid=None, \n",
        "\t             Xv_valid=None, y_valid=None,\n",
        "                 early_stopping=False):\n",
        "        \n",
        "        has_valid = Xv_valid is not None\n",
        "        last_step = 0\n",
        "        t1 = time()\n",
        "        self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
        "        total_batch = int(len(y_train) / self.batch_size)\n",
        "        for i in range(total_batch):\n",
        "            Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train, self.batch_size, i)\n",
        "            step, loss = self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
        "            last_step = step\n",
        "\n",
        "        # evaluate training and validation datasets\n",
        "        train_result, train_loss = self.evaluate(Xi_train, Xv_train, y_train)\n",
        "        self.train_result.append(train_result)\n",
        "        self.train_loss.append(train_loss)\n",
        "        if has_valid:\n",
        "            valid_result, valid_loss = self.evaluate(Xi_valid, Xv_valid, y_valid)\n",
        "            self.valid_result.append(valid_result)\n",
        "            self.valid_loss.append(valid_loss)\n",
        "            if valid_loss < self.best_loss and self.is_save == True:\n",
        "                old_loss = self.best_loss\n",
        "                self.best_loss = valid_loss\n",
        "                self.saver.save(self.sess, self.save_path + 'model.ckpt',global_step=last_step)\n",
        "                print(\"[%d-%d] model saved!. Valid loss is improved from %.4f to %.4f\" \n",
        "                      % (epoch, file_count, old_loss, self.best_loss))\n",
        "\n",
        "        if self.verbose > 0 and ((epoch-1)*9 + file_count) % self.verbose == 0:\n",
        "            if has_valid:\n",
        "                print(\"[%d-%d] train-result=%.4f, train-logloss=%.4f, valid-result=%.4f, valid-logloss=%.4f [%.1f s]\" % (epoch, file_count, train_result, train_loss, valid_result, valid_loss, time() - t1))\n",
        "            else:\n",
        "                print(\"[%d-%d] train-result=%.4f [%.1f s]\" \\\n",
        "                    % (epoch, file_count, train_result, time() - t1))\n",
        "        if has_valid and early_stopping and self.training_termination(self.valid_loss):\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "\n",
        "\n",
        "    def training_termination(self, valid_result):\n",
        "        if len(valid_result) > 5:\n",
        "            if self.greater_is_better:\n",
        "                if valid_result[-1] < valid_result[-2] and \\\n",
        "                    valid_result[-2] < valid_result[-3] and \\\n",
        "                    valid_result[-3] < valid_result[-4] and \\\n",
        "                    valid_result[-4] < valid_result[-5]:\n",
        "                    return True\n",
        "            else:\n",
        "                if valid_result[-1] > valid_result[-2] and \\\n",
        "                    valid_result[-2] > valid_result[-3] and \\\n",
        "                    valid_result[-3] > valid_result[-4] and \\\n",
        "                    valid_result[-4] > valid_result[-5]:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def predict(self, Xi, Xv):\n",
        "        \"\"\"\n",
        "        :param Xi: list of list of feature indices of each sample in the dataset\n",
        "        :param Xv: list of list of feature values of each sample in the dataset\n",
        "        :return: predicted probability of each sample\n",
        "        \"\"\"\n",
        "        # dummy y\n",
        "        dummy_y = [1] * len(Xi)\n",
        "        batch_index = 0\n",
        "        Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
        "        y_pred = None\n",
        "        #y_loss = None\n",
        "        while len(Xi_batch) > 0:\n",
        "            num_batch = len(y_batch)\n",
        "            feed_dict = {self.feat_index: Xi_batch,\n",
        "                         self.feat_value: Xv_batch,\n",
        "                         self.label: y_batch,\n",
        "                         self.dropout_keep_prob: [1.0] * len(self.drop_keep_prob),\n",
        "                         self.train_phase: False}\n",
        "            batch_out = self.sess.run(self.out, feed_dict=feed_dict)\n",
        "\n",
        "            if batch_index == 0:\n",
        "                y_pred = np.reshape(batch_out, (num_batch,))\n",
        "\t        #y_loss = np.reshape(batch_loss, (num_batch,))\n",
        "            else:\n",
        "                y_pred = np.concatenate((y_pred, np.reshape(batch_out, (num_batch,))))\n",
        "                #y_loss = np.concatenate((y_loss, np.reshape(batch_loss, (num_batch,))))\n",
        "\n",
        "            batch_index += 1\n",
        "            Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "    def evaluate(self, Xi, Xv, y):\n",
        "        \"\"\"\n",
        "        :param Xi: list of list of feature indices of each sample in the dataset\n",
        "        :param Xv: list of list of feature values of each sample in the dataset\n",
        "        :param y: label of each sample in the dataset\n",
        "        :return: metric of the evaluation\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(Xi, Xv)\n",
        "        y_pred = np.clip(y_pred,1e-6,1-1e-6)\n",
        "        return self.eval_metric(y, y_pred), log_loss(y, y_pred)\n",
        "\n",
        "    def restore(self, save_path=None):\n",
        "        if (save_path == None):\n",
        "            save_path = self.save_path\n",
        "        ckpt = tf.train.get_checkpoint_state(save_path)  \n",
        "        if ckpt and ckpt.model_checkpoint_path:  \n",
        "            self.saver.restore(self.sess, ckpt.model_checkpoint_path) \n",
        "            if self.verbose > 0:\n",
        "                print (\"restored from %s\" % (save_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDATi662Am-H"
      },
      "source": [
        "## Dynamic Conv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iyK1YGvbTSK"
      },
      "source": [
        "'''\n",
        "input_dim = int(input_shape[channel_axis])  num of inputs\n",
        "depthwise_kernel_shape = (self.kernel_size[0],\n",
        "                          input_dim,\n",
        "                          self.depth_multiplier)\n",
        "\n",
        "The role of the width multiplier α is to thin a network uniformly at each layer. \n",
        "for a given layer and width multiplier α, the number of input channels M becomes αM and the number of output channels N becomes αN.\n",
        "\n",
        "The resolution multiplier ρ is applied to the input image and the internal representation of every layer is subsequently reduced by the same multiplier. \n",
        "In practice we implicitly set ρ by setting the input resolution.\n",
        "\n",
        "In the code: The depth_multiplier is used to reduce the number of channels at each layer. So the depth_multiplier corresponds the width multiplier α.\n",
        "\n",
        "\n",
        "self.depthwise_kernel = self.add_weight(\n",
        "       shape=depthwise_kernel_shape,\n",
        "       initializer=self.depthwise_initializer,\n",
        "       name='depthwise_kernel',\n",
        "       regularizer=self.depthwise_regularizer,\n",
        "       constraint=self.depthwise_constraint)\n",
        "depthwise_kernel = array_ops.expand_dims(self.depthwise_kernel, 0)\n",
        "\n",
        "outputs = backend.depthwise_conv2d(\n",
        "        inputs,\n",
        "        depthwise_kernel,\n",
        "        strides=strides,\n",
        "        padding=self.padding,\n",
        "        dilation_rate=dilation_rate,\n",
        "        data_format=self.data_format)\n",
        "\n",
        "'''\n",
        "depthwise_kernel = tf.constant(value=np.ones(shape=(1,3,5, 3))) # height, width, input channel(num_d), output channell\n",
        "\n",
        "\n",
        "#input_embedding = Input(shape=[67,])\n",
        "input_embedding = np.ones(shape=(3, 1,10, 15)) # batch, height, feature_size, num_d\n",
        "tf.nn.depthwise_conv2d(input_embedding, depthwise_kernel, [1,1,1,1], padding='VALID') #,data_format='NCHW')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI29egbAShQX"
      },
      "source": [
        "kernel = np.array([[1,1],[2,2,]], dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B34rRaQTGbA"
      },
      "source": [
        "kernel = kernel.reshape((2,1,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPu5Fh2uez2v"
      },
      "source": [
        "input_embedding = np.ones(shape=(2, 5, 6), dtype=np.float32)\n",
        "input_embedding =tf.multiply(input_embedding, np.array([1,1,2,2,3,3,]))\n",
        "# transpose to B C T\n",
        "input_embedding = tf.transpose(input_embedding, perm=[0, 2, 1])\n",
        "# reshape to B*C/H, H, T\n",
        "input_embedding = tf.reshape(input_embedding, [-1, 2, 1,  5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC2f5YIf_Za2",
        "outputId": "13b51201-17cc-4678-a461-1d3ac0d8ab39"
      },
      "source": [
        "input_embedding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6, 2, 1, 5), dtype=float32, numpy=\n",
              "array([[[[1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[2., 2., 2., 2., 2.]],\n",
              "\n",
              "        [[2., 2., 2., 2., 2.]]],\n",
              "\n",
              "\n",
              "       [[[3., 3., 3., 3., 3.]],\n",
              "\n",
              "        [[3., 3., 3., 3., 3.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[2., 2., 2., 2., 2.]],\n",
              "\n",
              "        [[2., 2., 2., 2., 2.]]],\n",
              "\n",
              "\n",
              "       [[[3., 3., 3., 3., 3.]],\n",
              "\n",
              "        [[3., 3., 3., 3., 3.]]]], dtype=float32)>"
            ]
          },
          "execution_count": 199,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_rkoBAV_UwY"
      },
      "source": [
        "input_embedding = tf.transpose(input_embedding, perm=[1, 0, 2, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goZdLr5D8T3e",
        "outputId": "9c8fb74d-561f-42ab-df6b-50f782782ed7"
      },
      "source": [
        "input_embedding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 6, 1, 5), dtype=float32, numpy=\n",
              "array([[[[1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[2., 2., 2., 2., 2.]],\n",
              "\n",
              "        [[3., 3., 3., 3., 3.]],\n",
              "\n",
              "        [[1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[2., 2., 2., 2., 2.]],\n",
              "\n",
              "        [[3., 3., 3., 3., 3.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[2., 2., 2., 2., 2.]],\n",
              "\n",
              "        [[3., 3., 3., 3., 3.]],\n",
              "\n",
              "        [[1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[2., 2., 2., 2., 2.]],\n",
              "\n",
              "        [[3., 3., 3., 3., 3.]]]], dtype=float32)>"
            ]
          },
          "execution_count": 197,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFovDkj02Mt2"
      },
      "source": [
        "input_embedding = tf.transpose(input_embedding, perm=[1, 0, 2, 3])\n",
        "input_embedding = tf.reshape(input_embedding, [-1, 6,  5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-oihuJqCspo"
      },
      "source": [
        "kernel = tf.constant(np.ones(shape=[32,10, 32]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa3sGdoawO5w",
        "outputId": "4da54d3d-a615-4a35-a992-72dd4b2d092e"
      },
      "source": [
        "tf.math.reduce_sum(kernel ,1 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32, 32), dtype=float64, numpy=\n",
              "array([[10., 10., 10., ..., 10., 10., 10.],\n",
              "       [10., 10., 10., ..., 10., 10., 10.],\n",
              "       [10., 10., 10., ..., 10., 10., 10.],\n",
              "       ...,\n",
              "       [10., 10., 10., ..., 10., 10., 10.],\n",
              "       [10., 10., 10., ..., 10., 10., 10.],\n",
              "       [10., 10., 10., ..., 10., 10., 10.]])>"
            ]
          },
          "execution_count": 14,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXv7J1rt6IvA"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  B = 2\n",
        "  T = 5\n",
        "  C = 6\n",
        "  H = 3\n",
        "  K = 2\n",
        "  R = C//H\n",
        "  # B T C\n",
        "  input_embedding = np.ones(shape=(B, T, C), dtype=np.float32)\n",
        "  input_embedding =tf.multiply(input_embedding, np.array([1,1,2,2,3,3,]))\n",
        "  # transpose to B C T\n",
        "  input_embedding = tf.transpose(input_embedding, perm=[0, 2, 1])\n",
        "  input_embedding = tf.reshape(input_embedding, [-1, R, 1,  T])\n",
        "  input_embedding = tf.transpose(input_embedding, perm=[1, 0, 3, 2])\n",
        "  # return to BCT\n",
        "  input_embedding = tf.reshape(input_embedding, [-1, C,  T])\n",
        "  # reshape to B*C/H, H, T\n",
        "  input_embedding = tf.reshape(input_embedding, [-1, H, T])\n",
        "\n",
        "  # kernel H 1 K\n",
        "  # torch ver: out in k : H 1 K\n",
        "  # tf: ver f in out : K 1 H\n",
        "  kernel = np.array([[1,2,3],[1,2,3] ], dtype=np.float32)\n",
        "  kernel = kernel.reshape((K,1,H))\n",
        "  #kernel = np.array([[1,1],[2,2,], [3,3] ], dtype=np.float32)\n",
        "  #kernel = kernel.reshape((3,1,2))\n",
        "  #kernel = tf.transpose(kernel, perm=[2,1,0])\n",
        "  kernel = tf.constant(value=kernel)\n",
        "\n",
        "  result = tf.nn.conv1d(input_embedding, kernel, 1, padding='SAME', data_format='NCW')\n",
        "  result = tf.reshape(result, [-1, 2, 3,  5])\n",
        "  result = tf.transpose(result, perm=[0, 2, 1, 3])\n",
        "  result = tf.reshape(result, [-1, 6,  5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHF7zfX6BT1V"
      },
      "source": [
        "B = 2\n",
        "T = 5\n",
        "C = 6\n",
        "H = 3\n",
        "K = 2\n",
        "R = C//H\n",
        "input_embedding = np.ones(shape=(B, T, C), dtype=np.float32)\n",
        "input_embedding =tf.multiply(input_embedding, np.array([1,1,2,2,3,3,]))\n",
        "#kernel = np.array([[1,2,3],[1,2,3] ], dtype=np.float32)\n",
        "kernel = np.ones(shape=(C,H*K ), dtype=np.float32)\n",
        "#tf.einsum\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "o-rTum1zl_mk",
        "outputId": "67fe9fec-f0f2-40fe-e48c-0e619cd09b85"
      },
      "source": [
        "input_embedding = np.ones(shape=(B, T, C), dtype=np.float32)\n",
        "input_embedding =tf.multiply(input_embedding, np.array([1,2,3,4,5,6,]))\n",
        "input_embedding = tf.reshape(input_embedding, shape=(B,1,T, C))\n",
        "tf.image.extract_patches(input_embedding, sizes=(1,1,2,1), strides=(1,1,1,1), rates=(1,1,1,1), padding=\"SAME\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-7039b21130f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_embedding\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#input_embedding = tf.reshape(input_embedding, shape=(B,1,T, C))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minput_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mextract_image_patches_v2\u001b[0;34m(images, sizes, strides, rates, padding, name)\u001b[0m\n\u001b[1;32m   5984\u001b[0m   \"\"\"\n\u001b[1;32m   5985\u001b[0m   return gen_array_ops.extract_image_patches(images, sizes, strides, rates,\n\u001b[0;32m-> 5986\u001b[0;31m                                              padding, name)\n\u001b[0m\u001b[1;32m   5987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mextract_image_patches\u001b[0;34m(images, ksizes, strides, rates, padding, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2342\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: input must be 4-dimensional[2,5,6] [Op:ExtractImagePatches]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15iGkCJ0pUr8",
        "outputId": "503acea5-b6da-45d8-ebf9-77a44e8c2903"
      },
      "source": [
        "input_embedding = np.ones(shape=(B, T, C), dtype=np.float32)\n",
        "input_embedding =tf.multiply(input_embedding, np.array([1,2,3,4,5,6,]))\n",
        "#input_embedding = layers.Dense(C, activation='sigmoid')(input_embedding)\n",
        "tf.keras.activations.sigmoid(input_embedding)\n",
        "input_embedding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 5, 6), dtype=float32, numpy=\n",
              "array([[[1., 2., 3., 4., 5., 6.],\n",
              "        [1., 2., 3., 4., 5., 6.],\n",
              "        [1., 2., 3., 4., 5., 6.],\n",
              "        [1., 2., 3., 4., 5., 6.],\n",
              "        [1., 2., 3., 4., 5., 6.]],\n",
              "\n",
              "       [[1., 2., 3., 4., 5., 6.],\n",
              "        [1., 2., 3., 4., 5., 6.],\n",
              "        [1., 2., 3., 4., 5., 6.],\n",
              "        [1., 2., 3., 4., 5., 6.],\n",
              "        [1., 2., 3., 4., 5., 6.]]], dtype=float32)>"
            ]
          },
          "execution_count": 27,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSWZm-Vmn6RO",
        "outputId": "c006f6b4-ebad-49b7-ac89-5ae39fb5253a"
      },
      "source": [
        "input_embedding = np.ones(shape=(B, T, C), dtype=np.float32)\n",
        "ll = [i for i in range(6)]\n",
        "input_embedding =tf.multiply(input_embedding, np.array([[1]*6,[2]*6, [3]*6, [4]*6, [5]*6]))\n",
        "input_embedding = tf.constant(input_embedding)\n",
        "#input_embedding = tf.transpose(input_embedding, [1,0, 2])\n",
        "input_embedding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 5, 6), dtype=float32, numpy=\n",
              "array([[[1., 1., 1., 1., 1., 1.],\n",
              "        [2., 2., 2., 2., 2., 2.],\n",
              "        [3., 3., 3., 3., 3., 3.],\n",
              "        [4., 4., 4., 4., 4., 4.],\n",
              "        [5., 5., 5., 5., 5., 5.]],\n",
              "\n",
              "       [[1., 1., 1., 1., 1., 1.],\n",
              "        [2., 2., 2., 2., 2., 2.],\n",
              "        [3., 3., 3., 3., 3., 3.],\n",
              "        [4., 4., 4., 4., 4., 4.],\n",
              "        [5., 5., 5., 5., 5., 5.]]], dtype=float32)>"
            ]
          },
          "execution_count": 160,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMwITTAaetB3"
      },
      "source": [
        "\n",
        "def slicing_window(self, x):\n",
        "  elem_size = x.shape\n",
        "  paddings = tf.constant([[self.k-1, 0]])\n",
        "  x = tf.pad(x, paddings)\n",
        "  # kごとではなく　kごとの奴を作る\n",
        "  tf_list = tf.stack([x[i:i+self.k] for i in range(elem_size[0])])\n",
        "  return tf_list\n",
        "  \n",
        "@tf.function\n",
        "def __call__(self, x):　\n",
        "  #for i in range(x.shape[0]):\n",
        "  #  data_strided.append(tf.map_fn(slicing_window, x[i]))\n",
        "  data_strided = [tf.map_fn(self.slicing_window, x[i]) for i in range(x.shape[0])]\n",
        "  data_strided = tf.stack(data_strided)\n",
        "  return data_strided"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p37m6BIL1Tv"
      },
      "source": [
        "batch_window = batch_window(K)\n",
        "ss=  batch_window(input_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmiX6Sa-rdZt"
      },
      "source": [
        "tf.multiply(ss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_WovSMlrTGl"
      },
      "source": [
        "tf.keras.activations.sigmoid(ss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3QmGTWNHfRZ"
      },
      "source": [
        "ss = tf.reshape(ss, (T*B*H, R, K))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg8eKQ8jF7jX"
      },
      "source": [
        "kernel = np.array(np.ones((C, H*K)) , dtype=np.float32)\n",
        "kernel = tf.constant(value=kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKDDRCvOmQZW",
        "outputId": "10eb209f-eec6-415a-e359-8f666a2e7a2c"
      },
      "source": [
        "kernel.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([6, 6])"
            ]
          },
          "execution_count": 167,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR6mdV2RGnzy",
        "outputId": "1ac4ed1b-c410-488d-aac4-7b0659b6c5cf"
      },
      "source": [
        "kernel = tf.matmul(input_embedding, kernel)\n",
        "print(kernel.shape)\n",
        "kernel = tf.reshape(kernel, (-1, K))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 5, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKwEnVP0HygA",
        "outputId": "313d5ca8-0e36-4e42-a150-b7cf74ea5ad2"
      },
      "source": [
        "ss.shape, kernel.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([30, 2, 2]), TensorShape([30, 2]))"
            ]
          },
          "execution_count": 172,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLct_IyDwJv4"
      },
      "source": [
        "kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ6aPBK_wFKb"
      },
      "source": [
        "tf.nn.softmax(kernel, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDFjaNrUH7pT"
      },
      "source": [
        "result = tf.keras.backend.batch_dot(ss, kernel, [2,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWzGs-ZwIshk",
        "outputId": "5c0f2390-4dd0-43b0-bc41-326575c8c8de"
      },
      "source": [
        "result.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([30, 2])"
            ]
          },
          "execution_count": 175,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-uGI6L-IAit"
      },
      "source": [
        "result = tf.reshape(result, (B, T,C))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iklUk7uazeJf",
        "outputId": "06c83d9d-eb1b-4609-bc91-a2d798f43a07"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 5, 6), dtype=float32, numpy=\n",
              "array([[[  6.,  12.,  12.,  12.,  12.,  12.],\n",
              "        [ 24.,  48.,  48.,  48.,  48.,  48.],\n",
              "        [ 54., 108., 108., 108., 108., 108.],\n",
              "        [ 96., 192., 192., 192., 192., 192.],\n",
              "        [150., 300., 300., 300., 300., 300.]],\n",
              "\n",
              "       [[  6.,  12.,  12.,  12.,  12.,  12.],\n",
              "        [ 24.,  48.,  48.,  48.,  48.,  48.],\n",
              "        [ 54., 108., 108., 108., 108., 108.],\n",
              "        [ 96., 192., 192., 192., 192., 192.],\n",
              "        [150., 300., 300., 300., 300., 300.]]], dtype=float32)>"
            ]
          },
          "execution_count": 180,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEzFzWn_va50",
        "outputId": "f1fb5ee0-5322-4314-8a34-a61f8346eda0"
      },
      "source": [
        "tf.nn.bias_add(result, tf.constant([1]*6, dtype=np.float32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 2, 6), dtype=float32, numpy=\n",
              "array([[[  7.,  13.,  13.,  13.,  13.,  13.],\n",
              "        [  7.,  13.,  13.,  13.,  13.,  13.]],\n",
              "\n",
              "       [[ 25.,  49.,  49.,  49.,  49.,  49.],\n",
              "        [ 25.,  49.,  49.,  49.,  49.,  49.]],\n",
              "\n",
              "       [[ 55., 109., 109., 109., 109., 109.],\n",
              "        [ 55., 109., 109., 109., 109., 109.]],\n",
              "\n",
              "       [[ 97., 193., 193., 193., 193., 193.],\n",
              "        [ 97., 193., 193., 193., 193., 193.]],\n",
              "\n",
              "       [[151., 301., 301., 301., 301., 301.],\n",
              "        [151., 301., 301., 301., 301., 301.]]], dtype=float32)>"
            ]
          },
          "execution_count": 169,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AusIkD0huUdq",
        "outputId": "65c3379c-ad42-43f7-8955-cd05b1b8d1ff"
      },
      "source": [
        "tf.transpose(result, perm=[1,0,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 5, 6), dtype=float32, numpy=\n",
              "array([[[  6.,  12.,  12.,  12.,  12.,  12.],\n",
              "        [ 24.,  48.,  48.,  48.,  48.,  48.],\n",
              "        [ 54., 108., 108., 108., 108., 108.],\n",
              "        [ 96., 192., 192., 192., 192., 192.],\n",
              "        [150., 300., 300., 300., 300., 300.]],\n",
              "\n",
              "       [[  6.,  12.,  12.,  12.,  12.,  12.],\n",
              "        [ 24.,  48.,  48.,  48.,  48.,  48.],\n",
              "        [ 54., 108., 108., 108., 108., 108.],\n",
              "        [ 96., 192., 192., 192., 192., 192.],\n",
              "        [150., 300., 300., 300., 300., 300.]]], dtype=float32)>"
            ]
          },
          "execution_count": 153,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnry6DwH-eiZ"
      },
      "source": [
        "#@title DynamicConv layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hBbvRUeTHKu"
      },
      "source": [
        "\n",
        "class batch_window():\n",
        "  def __init__(self, k):\n",
        "    self.k = k\n",
        "  \n",
        "  @tf.function\n",
        "  def slicing_window(self, x):\n",
        "    elem_size = x.shape\n",
        "    paddings = tf.constant([[self.k-1, 0],[0, 0]])\n",
        "    x = tf.pad(x, paddings)\n",
        "\n",
        "    # kごとではなく　kごとの奴を作る\n",
        "    tf_list = tf.stack([x[i:i+self.k] for i in range(elem_size[0])])\n",
        "    return tf_list\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, x):\n",
        "    data_strided = tf.map_fn(self.slicing_window, x)\n",
        "    data_strided = tf.stack(data_strided)\n",
        "    return data_strided\n",
        "\n",
        "class DynamicConvLayer(Layer):\n",
        "  '''\n",
        "  init : kernel_num, head_num, dropout_rate, interactive_residual=False\n",
        "  '''\n",
        "  def __init__(self, kernel_num, head_num, dropout_rate, interactive_residual=False, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.K_num = kernel_num\n",
        "    self.H_num = head_num if head_num != 0 else 1\n",
        "    self.stride = batch_window(k=kernel_num)\n",
        "    self.IResidual = interactive_residual\n",
        "\n",
        "    self.LN = layers.LayerNormalization(name='ln_dc')\n",
        "    self.dropout = layers.Dropout(rate=dropout_rate, name='dc_dropout')\n",
        "    self.GRU = GatedLinerUnits(name=name)\n",
        "    #self.add = layers.Add()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.B_num = input_shape[0]\n",
        "    self.F_num = input_shape[1]\n",
        "    self.D_num = input_shape[-1]\n",
        "    self.R_num = input_shape[-1] // self.H_num\n",
        "    self.dense = Dense(input_shape[-1])\n",
        "    #assert self.D_num == self.H_num * self.R_num\n",
        "    self.BFH =self.B_num*self.F_num*self.H_num\n",
        "\n",
        "    self.kernel_weight = self.add_weight(shape=(self.D_num, self.K_num*self.H_num), \n",
        "                                         initializer=tf.initializers.HeUniform(), name='kernel_weight')\n",
        "    self.kernel_bias = self.add_weight(shape=[self.D_num], initializer=tf.initializers.HeUniform(), name='bias_weight')\n",
        "    self.value_r = layers.Dense(self.D_num, kernel_initializer=tf.initializers.HeUniform(), name='residual')\n",
        "\n",
        "  #@tf.function\n",
        "  def dynamicconv(self, outputs, kernel_weight, kernel_bias, BFH, B_num, F_num, D_num, R_num, K_num):\n",
        "\n",
        "    query = tf.nn.softmax(tf.matmul(outputs, kernel_weight), axis=1)\n",
        "\n",
        "    # stride inputs \n",
        "    outputs = self.stride(outputs)\n",
        "\n",
        "    outputs = tf.transpose(outputs, perm=[0,1,3,2])\n",
        "    outputs = tf.reshape(outputs, (BFH, R_num, K_num))\n",
        "\n",
        "    query = tf.reshape(query, (BFH,  K_num))\n",
        "    outputs = tf.keras.backend.batch_dot(outputs, query, [2,1])\n",
        "    outputs = tf.reshape(outputs, ( B_num, F_num, D_num))\n",
        "    # back to BFD\n",
        "    outputs = tf.nn.bias_add(outputs, kernel_bias)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    outputs = self.LN(inputs)\n",
        "    outputs = self.dropout(outputs)\n",
        "    outputs = self.GRU(outputs)\n",
        "\n",
        "    outputs = self.dynamicconv(outputs=outputs, kernel_weight=self.kernel_weight, kernel_bias=self.kernel_bias, \n",
        "                     BFH=self.BFH, B_num=self.B_num, F_num=self.F_num, D_num=self.D_num, R_num=self.R_num, K_num=self.K_num)\n",
        "    \n",
        "    outputs = self.dense(outputs)\n",
        "    outputs = self.dropout(outputs)\n",
        "    if self.IResidual:\n",
        "      inputs = self.value_r(inputs)\n",
        "      outputs = tf.nn.relu(outputs +inputs)\n",
        "    else:\n",
        "      outputs += inputs\n",
        "    return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uABWvyeSpwZ4"
      },
      "source": [
        "#DC part\n",
        "    # Batch, Feature, Dimension\n",
        "    # BFD to FBD\n",
        "    #outputs = tf.transpose(outputs, perm=[1, 0, 2])\n",
        "    query = tf.nn.softmax(tf.matmul(outputs, self.kernel_weight), axis=1)\n",
        "\n",
        "    # stride inputs \n",
        "    outputs = self.stride(outputs)\n",
        "    #outputs = tf.reshape(outputs, (self.F_num*self.B_num*self.H_num, self.R_num, self.K_num))\n",
        "    outputs = tf.reshape(outputs, (self.BFH, self.R_num, self.K_num))\n",
        "    query = tf.reshape(query, (self.BFH,  self.K_num))\n",
        "\n",
        "    outputs = tf.keras.backend.batch_dot(outputs, query, [2,1])\n",
        "\n",
        "    #outputs = tf.reshape(outputs, (self.F,self.B_num,self.D_num))\n",
        "    outputs = tf.reshape(outputs, ( self.B_num, self.F_num, self.D_num))\n",
        "\n",
        "    # back to BFD\n",
        "    #outputs = tf.transpose(outputs, perm=[1,0,2])\n",
        "    outputs = tf.nn.bias_add(outputs, self.kernel_bias)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwLeoVG1AuBQ"
      },
      "source": [
        "#@title light conv layer\n",
        "class LightConv_Layer(Layer):\n",
        "  def __init__(self, kernel_num, head_num):\n",
        "    self.K_num = kernel_num\n",
        "    self.H_num = head_num\n",
        "    self.kernel_weight = self.add_weight(shape=(K,1, H,), name='LC_kernel', \n",
        "                                         initializer=tf.keras.initializers.HeUniform(), )\n",
        "    #self.T = tf.transpose\n",
        "\n",
        "    super().__init__()\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    #self.B_num = input_shape[0]\n",
        "    self.C_num = input_shape[-1]\n",
        "    self.F_num = input_shape[1]\n",
        "    self.R_num = self.C_num // self.H_num\n",
        "    assert self.R_num*self.H_num == self.C_num == input_shape[-1]\n",
        "\n",
        "    #self.kernel = self.add_weight(shape=(input_shape[0], self.C_num, ))\n",
        "    self.depthconv = tf.nn.conv1d()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # inpus :Batch Token Channel \n",
        "    # T to BCT\n",
        "    inputs = tf.transpose(inputs, perm=[0, 2, 1])\n",
        "    # shape to BR*kT\n",
        "    inputs = tf.reshape(inputs, [-1, self.R_num, 1,  self.F_num])\n",
        "    inputs = tf.transpose(inputs, perm=[1, 0, 3, 2])\n",
        "    # return to BCT\n",
        "    inputs = tf.reshape(inputs, [-1, self.C_num,  self.F_num])\n",
        "    # reshape to B*C/H, H, T\n",
        "    inputs = tf.reshape(inputs, [-1, self.H_num, self.F_num])\n",
        "    out = tf.nn.conv1d(inputs, self.kernel_weight, 1, padding='SAME', data_format='NCW')\n",
        "    out = tf.reshape(out, [-1, 2, 3,  5])\n",
        "    out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
        "    out = tf.reshape(out, [-1, 6,  5])\n",
        "\n",
        "    return out \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XIztxrX3nNq"
      },
      "source": [
        "#@title GLU\n",
        "class GatedLinerUnits(Layer):\n",
        "  def __init__(self, name):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_num = input_shape[-1]\n",
        "    self.linear = layers.Dense(self.d_num ,kernel_initializer=tf.keras.initializers.HeUniform())\n",
        "    self.sig_linear = layers.Dense(self.d_num, activation='sigmoid', \n",
        "                                   kernel_initializer=tf.keras.initializers.HeUniform())\n",
        "\n",
        "  def call(self, inputs):\n",
        "    outputs = self.linear(inputs)\n",
        "    query = self.sig_linear(inputs)\n",
        "    return outputs * query\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GJpUuKPwxDX"
      },
      "source": [
        "positional_encoding(10, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5X_sdiR73wE"
      },
      "source": [
        "#@title Dynamic Conv hyper model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuHUq6FGkcwJ"
      },
      "source": [
        "from kerastuner import HyperModel\n",
        "import random\n",
        "import math\n",
        "\n",
        "def DynamicConvHyper(feat_size, num_input, num_emb, emb_d, batch_size, num_y=2, Lconv=False, autodis=False):\n",
        "  feat_size = feat_size\n",
        "  num_input = num_input\n",
        "  num_emb = num_emb\n",
        "  emb_d = emb_d\n",
        "  batch_size = batch_size\n",
        "  LN_1 = layers.LayerNormalization(name='lnout')\n",
        "\n",
        "  def DC_build(hp):\n",
        "    value = Input(shape=(num_input,), batch_size=batch_size, name='value', dtype=np.float32)\n",
        "    index = Input(shape=(num_input,), batch_size=batch_size, name='index', dtype=np.int32)\n",
        "    emb_value = Input(shape=(num_emb, emb_d, ), batch_size=batch_size, name='emb_value')\n",
        "    emb_index = Input(shape=(num_emb,), batch_size=batch_size, name='emb_index', dtype=np.int32)\n",
        "    #hp_emb_d = hp.Int('num_of_d', min_value=32, max_value=128, step=32, default=128)\n",
        "    hp_emb_d = hp.Fixed('num_of_d', value=128)\n",
        "    with hp.conditional_scope('num_of_d', 128):\n",
        "      hp_emb_layer = hp.Choice( 'emb_layer_choice', values=['linear', 'dense', 'matrix'])\n",
        "    with hp.conditional_scope('num_of_d', [32,  64,  96]):\n",
        "      hp_emb_layer = hp.Choice('emb_layer_choice', values=['matrix', 'dense'], default='matrix')\n",
        "    \n",
        "    \n",
        "    uid = random.randint(1,9999)\n",
        "    hp_em_dropout_rate = hp.Float('EM_dropout_rate', min_value=0.0, max_value=0.2, step=0.05)\n",
        "    \n",
        "        \n",
        "    K_size = hp.Int('hp_kernel_size', min_value=2, max_value=12, step=2, default=4)\n",
        "    H_size = hp.Choice('hp_head_num', values=[4,8,16,32]) # remove 1, 32\n",
        "    ffn_d = hp.Int('hp_ffn_d_size', min_value=128, max_value=512, step=128) # min 16 to 32\n",
        "    ffn_drop = hp.Float('hp_ffn_drop', min_value=0.0, max_value=0.9, step=0.3)\n",
        "    DConv_droprate = hp.Float('hp_DCconv_drop', min_value=0.1, max_value=0.5, step=0.1)\n",
        "\n",
        "    if autodis:\n",
        "      meta_index =  Input(shape=(num_input,), batch_size=batch_size, name='meta_index', dtype=np.float32)\n",
        "      reverse_meta_index = Input(shape=(num_input,), batch_size=batch_size, \n",
        "                                 name='reverse_meta_index', dtype=np.float32)\n",
        "      \n",
        "    hp_em_dropout_rate = hp.Float('EM_dropout_rate', min_value=0.0, max_value=0.5, step=0.1, default=0.0)\n",
        "    #hp_em_dropout_rate = hp.Fixed('EM_dropout_rate', value=0.0)\n",
        "    if autodis:\n",
        "      hp_meta_size = hp.Int('meta_size', min_value=4, max_value=16, step=4)\n",
        "      hp_t = hp.Float('temperatue_rate', 1e-6, 1e-2, sampling='log', default=1e-3)\n",
        "      outputs = AutoDis_EM(emb_d=hp_emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "              drop_rate=hp_em_dropout_rate, em_em= hp_emb_layer, meta_size=hp_meta_size,\n",
        "              t=hp_t\n",
        "              #name=f'autoint em {hp_emb_d}, {hp_emb_layer} {hp_em_dropout_rate}'\n",
        "              )(value, index, emb_value, emb_index, meta_index, reverse_meta_index) \n",
        "    else:\n",
        "      outputs = AutoInt_EM(emb_d=hp_emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "              drop_rate=hp_em_dropout_rate, em_em= hp_emb_layer, \n",
        "              #name=f'autoint em {hp_emb_d}, {hp_emb_layer} {hp_em_dropout_rate}'\n",
        "              )(value, index, emb_value, emb_index)\n",
        "\n",
        "\n",
        "    #hp_pe = hp.Boolean('positona_encoding')\n",
        "    #positional_emb = PositionalEncoder()(num_input, 128)[:,:,outputs.shape[-1]]\n",
        "    #emb_f = math.sqrt(hp_emb_d)\n",
        "    #emb_f = math.sqrt(128)\n",
        "    #outputs = tf.scalar_mul(emb_f, outputs)\n",
        "    #outputs = self.Add(positional_emb, outputs)\n",
        "    #outputs = layers.Dropout(rate=hp.Float('layer_dropout_rate', min_value = 0.0, max_value=0.99, step=0.01))(outputs)\n",
        "      \n",
        "    #hp.Boolean(f'conv{i}{l_uid}'):\n",
        "    #hp_ffn_bl =  hp.Boolean(f'ffn_{i}{uid}'):\n",
        "    #interactive_residual =  hp.Boolean(f'interac_residu')\n",
        "    interactive_residual =  hp.Fixed(f'interac_residu', value=False)\n",
        "    #for i in range(hp.Int('layers_num', min_value=1, max_value=12, step=1, default=1)):\n",
        "    for i in range(hp.Fixed('layers_num', value=4)):\n",
        "      #l_uid = random.randint(1,999999) # kernel_num, head_num, dropout_rate\n",
        "      outputs = DynamicConvLayer(kernel_num=K_size, head_num=H_size, dropout_rate=DConv_droprate, \n",
        "                                 interactive_residual=interactive_residual, name=f'DynamicConv_{i}_{uid}')(outputs)\n",
        "      #if hp\n",
        "      outputs = FFN(ffn_d, drop_rate=ffn_drop, name=f'ffn_{i}_{uid}')(outputs)\n",
        "    outputs = layers.Flatten(name=f'dc_flatten_{uid}')(outputs)\n",
        "\n",
        "    pred_drop = hp.Float('hp_pred_ffn_drop', min_value=0.0, max_value=0.90, step=0.3)\n",
        "    outputs = layers.LayerNormalization()(outputs)\n",
        "    outputs = layers.Dropout(rate=pred_drop)(outputs)\n",
        "    \n",
        "    pred_ffn_d = hp.Int('hp_pred_ffn_d_size', min_value=64, max_value=256, step=64)\n",
        "    outputs = Dense(pred_ffn_d, activation='relu', kernel_initializer=tf.keras.initializers.he_normal(), bias_initializer='random_normal')(outputs)\n",
        "      \n",
        "    outputs = layers.LayerNormalization()(outputs)\n",
        "    outputs = layers.Dropout(rate=pred_drop)(outputs)\n",
        "\n",
        "    if num_y == 2:\n",
        "      outputs = Dense(num_y, activation='sigmoid', kernel_initializer='glorot_normal', bias_initializer='random_normal')(outputs)\n",
        "    else:\n",
        "      outputs = Dense(num_y, activation='relu', kernel_initializer='glorot_normal', bias_initializer='random_normal')(outputs)\n",
        "      outputs = layers.Softmax()(outputs)\n",
        "    if autodis:\n",
        "      model = Model(inputs=[value, index, emb_value, emb_index, meta_index, reverse_meta_index], outputs=outputs, name=f'DynamicConv_model_{K_size}{hp_emb_d}_{hp_emb_layer}_{uid}')\n",
        "    else:\n",
        "      model = Model(inputs=[value, index, emb_value, emb_index], outputs=outputs, name=f'DynamicConv_model_{K_size}{hp_emb_d}_{hp_emb_layer}_{uid}')\n",
        "\n",
        "    initial_learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
        "    decay_steps=hp.Int('decay_steps', 10000,  110000, step=20000)\n",
        "    #decay_rate=hp.Float('decay_rate', 0.0, 1.0, step=0.05)\n",
        "    decay_rate=hp.Float('decay_rate', 0.2, 1.0, step=0.2)\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        decay_steps=decay_steps,\n",
        "        #decay_steps=steps_each,\n",
        "        decay_rate=decay_rate)\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "    #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "    #optimizer = RAdamOptimizer(total_steps=total_steps, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "    \n",
        "    label_smoothing = hp.Float('label_smoothing', min_value=0.0, max_value=0.8, step=0.2)\n",
        "    \n",
        "    model.compile(optimizer, \n",
        "                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing) if num_y == 2 else tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
        "          metrics=['categorical_accuracy',\n",
        "          #tf.keras.metrics.Precision(),\n",
        "          #tf.keras.metrics.Recall(),\n",
        "          tf.keras.metrics.AUC()])\n",
        "    return model\n",
        "  return DC_build"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtKiSjYHzpxS"
      },
      "source": [
        "from kerastuner import HyperModel\n",
        "import random\n",
        "import math\n",
        "\n",
        "class DynamicConvHyper(HyperModel):\n",
        "\n",
        "    def __init__(self, feat_size, num_input, num_emb, emb_d, batch_size, Lconv=False, AutoDIs=False):\n",
        "        self.feat_size = feat_size\n",
        "        self.num_input = num_input\n",
        "        self.num_emb = num_emb\n",
        "        self.emb_d = emb_d\n",
        "        self.batch_size = batch_size\n",
        "        self.LN_1 = layers.LayerNormalization(name='lnout')\n",
        "\n",
        "\n",
        "    def build(self, hp):\n",
        "      value = Input(shape=(self.num_input,), batch_size=batch_size, name='value', dtype=np.float32)\n",
        "      index = Input(shape=(self.num_input,), batch_size=batch_size, name='index', dtype=np.int32)\n",
        "      emb_value = Input(shape=(self.num_emb, self.emb_d, ), batch_size=batch_size, name='emb_value')\n",
        "      emb_index = Input(shape=(self.num_emb,), batch_size=batch_size, name='emb_index', dtype=np.int32)\n",
        "\n",
        "\n",
        "      hp_emb_d = hp.Int('num_of_d', min_value=32, max_value=128, step=32, default=128)\n",
        "      with hp.conditional_scope('num_of_d', 128):\n",
        "        hp_emb_layer = hp.Choice( 'emb_layer_choice', values=['linear', 'dense', 'matrix'])\n",
        "      with hp.conditional_scope('num_of_d', [32,  64,  96]):\n",
        "        hp_emb_layer = hp.Choice('emb_layer_choice', values=['matrix', 'dense'], default='matrix')\n",
        "      \n",
        "      \n",
        "      uid = random.randint(1,999999)\n",
        "      hp_em_dropout_rate = hp.Float('EM_dropout_rate', min_value=0.0, max_value=0.99, step=0.01)\n",
        "      \n",
        "          \n",
        "      K_size = hp.Int('kernel_size', min_value=2, max_value=12, step=2, default=3)\n",
        "      H_size = hp.Choice('head_num', values=[1,2,4,8,16,32])\n",
        "      ffn_d = hp.Int('ffn_d_size', min_value=16, max_value=1024, step=16)\n",
        "      ffn_drop = hp.Float('ffn_drop', min_value=0.0, max_value=0.99, step=0.01)\n",
        "      DConv_droprate = hp.Float('DCconv_drop', min_value=0.0, max_value=0.99, step=0.01)\n",
        "      #hp_pe = hp.Boolean('positona_encoding')\n",
        "\n",
        "      outputs = AutoInt_EM(emb_d=hp_emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "              drop_rate=hp_em_dropout_rate, em_em= hp_emb_layer, PE=False, name=f'autoEM_{uid}')(value, index, emb_value, emb_index)\n",
        "\n",
        "\n",
        "      #positional_emb = PositionalEncoder()(num_input, 128)[:,:,outputs.shape[-1]]\n",
        "      #emb_f = math.sqrt(hp_emb_d)\n",
        "      #emb_f = math.sqrt(128)\n",
        "      #outputs = tf.scalar_mul(emb_f, outputs)\n",
        "      #outputs = self.Add(positional_emb, outputs)\n",
        "      #outputs = layers.Dropout(rate=hp.Float('layer_dropout_rate', min_value = 0.0, max_value=0.99, step=0.01))(outputs)\n",
        "        \n",
        "\n",
        "\n",
        "      #hp.Boolean(f'conv{i}{l_uid}'):\n",
        "      #hp_ffn_bl =  hp.Boolean(f'ffn_{i}{uid}'):\n",
        "      for i in range(hp.Int('layers_num', min_value=1, max_value=12, step=1, default=1)):\n",
        "        #l_uid = random.randint(1,999999) # kernel_num, head_num, dropout_rate\n",
        "        outputs = DynamicConvLayer(kernel_num=K_size, head_num=H_size, dropout_rate=DConv_droprate, name=f'DynamicConv_{i}_{uid}')(outputs)\n",
        "        #if hp\n",
        "        outputs = FFN(ffn_d, drop_rate=ffn_drop, name=f'ffn_{i}_{uid}')(outputs)\n",
        "\n",
        "      outputs = layers.Flatten(name=f'flatten_{uid}')(outputs)\n",
        "      outputs = Dense(1, activation='relu', kernel_initializer='glorot_normal', \n",
        "                      bias_initializer='random_normal', )(outputs)\n",
        "      outputs = Dense(1, activation='sigmoid', kernel_initializer='glorot_normal', bias_initializer='random_normal')(outputs)\n",
        "\n",
        "      optimizer = 'adam'\n",
        "      model = Model(inputs=[value, index, emb_value, emb_index], outputs=outputs, name=f'DynamicConv_model_{K_size}_{uid}')\n",
        "  \n",
        "      initial_learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
        "      decay_steps=hp.Int('decay_steps', 10000,  100000, step=10000)\n",
        "      decay_rate=hp.Float('decay_rate', 0.0, 1.0, step=0.1)\n",
        "  \n",
        "      lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "          initial_learning_rate=initial_learning_rate,\n",
        "          decay_steps=decay_steps,\n",
        "          #decay_steps=steps_each,\n",
        "          decay_rate=decay_rate)\n",
        "      \n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "      #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "      #optimizer = RAdamOptimizer(total_steps=total_steps, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "      \n",
        "      label_smoothing = hp.Float('label_smoothing', min_value=0.0, max_value=1.0, step=0.01)\n",
        "      \n",
        "\n",
        "      model.compile(optimizer, \n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n",
        "            metrics=['binary_accuracy',\n",
        "            #tf.keras.metrics.Precision(),\n",
        "            #tf.keras.metrics.Recall(),\n",
        "            tf.keras.metrics.AUC()])\n",
        "      return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmtYpHD7jDHP",
        "outputId": "562f42d2-c602-4d26-89b4-e4dde5391c88"
      },
      "source": [
        "df_m_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([], shape=(0, 69), dtype=object)"
            ]
          },
          "execution_count": 92,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJNse0G76cEO"
      },
      "source": [
        "#@title AutoDis_EM \n",
        "class AutoDis_EM(Layer): #multi-valued feature field \n",
        "  def __init__(self, emb_d, feat_size, num_emb, meta_size, t,drop_rate=0.0, em_em='linear', name=None, PE=False):\n",
        "    super().__init__(name=name)\n",
        "    self.feature_embedding = self.add_weight(\n",
        "        shape=(feat_size, emb_d), \n",
        "        #initializer=tf.keras.initializers.GlorotNormal(), \n",
        "        #initializer=tf.keras.initializers.he_normal(),\n",
        "        initializer=tf.keras.initializers.HeUniform(), \n",
        "        #regularizer=tf.keras.regularizers.l2(),\n",
        "        trainable=True, name='embedding_dic', dtype=np.float32)\n",
        "    self.meta_cor = Dense(meta_size, activation='relu', \n",
        "                                #kernel_initializer=tf.keras.initializers.he_normal(),\n",
        "                                kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "                                #kernel_regularizer=tf.keras.regularizers.l2(),\n",
        "                                bias_initializer='random_normal')\n",
        "    \n",
        "    self.t = tf.constant([t], dtype=np.float32)\n",
        "    self.meta_size = meta_size\n",
        "    self.drop = layers.Dropout(rate=drop_rate, name='em_dropout')\n",
        "    self.emb_d = emb_d\n",
        "    self.num_emb = num_emb\n",
        "    self.em_em = em_em\n",
        "    self.pe= PE\n",
        "    if self.em_em == 'linear':\n",
        "      pass\n",
        "    elif self.em_em == 'dense':\n",
        "      self.em_dense = layers.Dense(emb_d, #kernel_regularizer=tf.keras.regularizers.l2(), \n",
        "                                   kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "                                   name='em_dnese')\n",
        "    elif self.em_em == 'matrix':\n",
        "      self.em_nn = self.add_weight(shape=(num_emb,128,emb_d ),\n",
        "                                   initializer = tf.keras.initializers.HeUniform(), \n",
        "                                   #regularizer=tf.keras.regularizers.l2(),\n",
        "                                   trainable=True, name='em_matrix')\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.field_size = input_shape[-1]\n",
        "    self.batch_size = input_shape[0]\n",
        "    self.meta_embedding = self.add_weight(\n",
        "        shape=(self.meta_size, self.emb_d), \n",
        "        #regularizer=tf.keras.regularizers.l2(),\n",
        "        #initializer = tf.keras.initializers.GlorotNormal(), \n",
        "        initializer=tf.keras.initializers.HeUniform(), \n",
        "        trainable=True, name='meta_embedding', dtype=np.float32)\n",
        "    if self.pe:\n",
        "      self.positional_emb = positional_encoding\n",
        "\n",
        "  def tempreture_softmax(self, inputs):\n",
        "    logit = tf.math.divide(inputs, self.t)\n",
        "    # tf.nn.softmax \n",
        "    return tf.nn.softmax(logit, -1)\n",
        "    #return tf.exp(logits) / tf.reduce_sum(tf.exp(logits), -1)\n",
        "\n",
        "  '''\n",
        "  def sample(a, temperature=1.0):\n",
        "    a = np.array(a)**(1/temperature)\n",
        "    p_sum = a.sum()\n",
        "    sample_temp = a/p_sum \n",
        "    return np.argmax(np.random.multinomial(1, sample_temp, 1))\n",
        "  '''\n",
        "\n",
        "  def meta_emb_proj(self, inputs, meta_index):\n",
        "    # inputs : batch, field_size\n",
        "    # meta_iindex : batch, field_size, 1\n",
        "    #inputs = tf.reshape(inputs, [self.batch_size, self.field_size, 1])\n",
        "    inputs = tf.expand_dims(inputs, -1)\n",
        "    meta_out = self.meta_cor(inputs)\n",
        "    meta_out = self.tempreture_softmax(meta_out)\n",
        "    meta_out = tf.matmul(meta_out, self.meta_embedding)\n",
        "    # meta index : Batch, field_size, 1\n",
        "    meta_index = tf.expand_dims(meta_index, -1)\n",
        "    meta_out = tf.multiply(meta_out, meta_index)\n",
        "    return meta_out\n",
        "    # out_emb * meta_index : if numerical :1 categorical :0\n",
        "\n",
        "  def call(self, value, index, value_emb, index_emb, \n",
        "           meta_index, reverse_meta_index):\n",
        "    out = tf.nn.embedding_lookup(self.feature_embedding, index)\n",
        "    feat_value = tf.multiply(value, reverse_meta_index)\n",
        "    feat_value = tf.reshape(feat_value, shape=[-1, self.field_size, 1])\n",
        "    out = tf.multiply(out, feat_value)\n",
        "    autodis_emb = self.meta_emb_proj(value, meta_index)\n",
        "    out = tf.add(out, autodis_emb)\n",
        "    \n",
        "    emb_out =  tf.nn.embedding_lookup(self.feature_embedding, index_emb)\n",
        "    \"\"\"print(\"autodis\", \"emb_out\", emb_out.shape,  \n",
        "          \"emb_weight\", self.feature_embedding.shape,\n",
        "          \"index_emb\", index_emb.shape)\"\"\"\n",
        "    if self.em_em == 'linear':\n",
        "\n",
        "      emb_out = tf.multiply(emb_out, value_emb)\n",
        "\n",
        "    elif self.em_em == 'matrix':\n",
        "      value_emb = tf.einsum('abj, bjk-> abk', value_emb, self.em_nn)\n",
        "      #print(emb_out, value_emb)\n",
        "      emb_out = tf.multiply(emb_out, value_emb)\n",
        "    elif self.em_em == 'dense':\n",
        "      #print(emb_out, value_emb)\n",
        "      value_emb = self.em_dense(value_emb)\n",
        "\n",
        "      emb_out = tf.multiply(emb_out, value_emb)\n",
        "\n",
        "    out= tf.concat([out, emb_out], axis=1)\n",
        "    if self.pe:\n",
        "      out = tf.nn.bias_add(tf.reshape(out, [self.batch_size, -1])*math.sqrt(self.emb_d), \n",
        "                           tf.reshape(self.positional_emb(self.field_size+self.num_emb, self.emb_d), [-1]))\n",
        "      out = tf.reshape(out, shape=[out.shape[0], self.field_size+self.num_emb, self.emb_d])\n",
        "\n",
        "    out = self.drop(out)\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "wkuBa3zUZ7Ad",
        "outputId": "a564aeb2-94ee-42dd-b515-2ae328236a72"
      },
      "source": [
        "#@title AutoMeta_EM \n",
        "class AutoMeta_EM(Layer): #multi-valued feature field \n",
        "  def __init__(self, emb_d, feat_size, num_emb, nume_feat_size,  drop_rate=0.0, em_em='linear', name=None, PE=False):\n",
        "    super().__init__(name=name)\n",
        "    self.feature_embedding = self.add_weight(\n",
        "        shape=(feat_size, emb_d), initializer = tf.keras.initializers.GlorotNormal(), \n",
        "        trainable=True, name='embedding_dic', dtype=np.float32)\n",
        "    self.meta_embedding = \n",
        "    self.drop = layers.Dropout(rate=drop_rate, name='em_dropout')\n",
        "    self.emb_d = emb_d\n",
        "    self.num_emb = num_emb\n",
        "    self.em_em = em_em\n",
        "    self.pe= PE\n",
        "    if self.em_em == 'linear':\n",
        "      pass\n",
        "    elif self.em_em == 'dense':\n",
        "      self.em_dense = layers.Dense(emb_d, name='em_dnese')\n",
        "    elif self.em_em == 'matrix':\n",
        "      self.em_nn = self.add_weight(shape=(num_emb,128,emb_d ),\n",
        "                                   initializer = tf.keras.initializers.HeUniform(), \n",
        "                                   trainable=True, name='em_matrix') \n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.field_size = input_shape[-1]\n",
        "    self.batch_size = input_shape[0]\n",
        "    if self.pe:\n",
        "      self.positional_emb = positional_encoding\n",
        "\n",
        "  def call(self, value, index, value_emb, index_emb, index_emb, index_meta):\n",
        "    out = tf.nn.embedding_lookup(self.feature_embedding, index)\n",
        "    feat_value = tf.reshape(value, shape=[-1, self.field_size, 1])\n",
        "    out = tf.multiply(out, feat_value)\n",
        "    \n",
        "    emb_out =  tf.nn.embedding_lookup(self.feature_embedding, index_emb)\n",
        "\n",
        "    if self.em_em == 'linear':\n",
        "\n",
        "      emb_out = tf.multiply(emb_out, value_emb)\n",
        "\n",
        "    elif self.em_em == 'matrix':\n",
        "      value_emb = tf.einsum('abj, bjk-> abk', value_emb, self.em_nn)\n",
        "\n",
        "      emb_out = tf.multiply(emb_out, value_emb)\n",
        "    elif self.em_em == 'dense':\n",
        "\n",
        "      value_emb = self.em_dense(value_emb)\n",
        "\n",
        "      emb_out = tf.multiply(emb_out, value_emb)\n",
        "\n",
        "    out= tf.concat([out, emb_out], axis=1)\n",
        "    if self.pe:\n",
        "      out = tf.nn.bias_add(tf.reshape(out, [self.batch_size, -1])*math.sqrt(self.emb_d), \n",
        "                           tf.reshape(self.positional_emb(self.field_size+self.num_emb, self.emb_d), [-1]))\n",
        "      out = tf.reshape(out, shape=[out.shape[0], self.field_size+self.num_emb, self.emb_d])\n",
        "\n",
        "    out = self.drop(out)\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-e242944b6781>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    self.meta_embedding =\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3SKqmXvkSCj"
      },
      "source": [
        "## MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l2waDijfhTS"
      },
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def MLP_builder_maker(feat_size, num_input, num_emb, emb_d, batch_size, num_y=2, autodis=False):\n",
        "  def MLP_builder(hp):\n",
        "    \n",
        "    hp_emb_d = hp.Int('num_of_d', min_value=32, max_value=128, step=32, default=128)\n",
        "    with hp.conditional_scope('num_of_d', 128):\n",
        "      #hp_emb_layer = hp.Choice( 'emb_layer_choice', values=['linear', 'dense', 'matrix'])\n",
        "      hp_emb_layer = hp.Choice( 'emb_layer_choice', values=['dense', 'matrix'])\n",
        "    with hp.conditional_scope('num_of_d', [32,  64,  96]):\n",
        "      hp_emb_layer = hp.Choice('emb_layer_choice', values=['matrix', 'dense'], default='matrix')\n",
        "    \n",
        "    #hp_emb_d = 128\n",
        "    #hp_emb_layer = 'linear'\n",
        "\n",
        "    #batch_size needs to implement later \n",
        "    value = Input(shape=(num_input,), batch_size=batch_size, name='value', dtype=np.float32)\n",
        "    index = Input(shape=(num_input,), batch_size=batch_size, name='index', dtype=np.int32)\n",
        "    emb_value = Input(shape=(num_emb, emb_d, ), batch_size=batch_size, name='emb_value')\n",
        "    emb_index = Input(shape=(num_emb,), batch_size=batch_size, name='emb_index', dtype=np.int32)\n",
        "    #hp_em_dropout_rate = hp.Float('EM_dropout_rate', min_value=0.0, max_value=0.90, step=0.1, default=0.0)\n",
        "    hp_em_dropout_rate = hp.Fixed('EM_dropout_rate', value=0.0)\n",
        "\n",
        "    if autodis:\n",
        "      meta_index =  Input(shape=(num_input,), batch_size=batch_size, name='meta_index', dtype=np.float32)\n",
        "      reverse_meta_index = Input(shape=(num_input,), batch_size=batch_size, \n",
        "                                 name='reverse_meta_index', dtype=np.float32)\n",
        "      \n",
        "    #hp_em_dropout_rate = hp.Float('EM_dropout_rate', min_value=0.0, max_value=0.3, step=0.05, default=0.0)\n",
        "    #hp_em_dropout_rate = hp.Fixed('EM_dropout_rate', value=0.0)\n",
        "    if autodis:\n",
        "      hp_meta_size = hp.Int('meta_size', min_value=4, max_value=20, step=4)\n",
        "      hp_t = hp.Float('temperatue_rate', 1e-6, 1e-2, sampling='log', default=1e-3)\n",
        "      outputs = AutoDis_EM(emb_d=hp_emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "              drop_rate=hp_em_dropout_rate, em_em= hp_emb_layer, meta_size=hp_meta_size,\n",
        "              t=hp_t\n",
        "              #name=f'autoint em {hp_emb_d}, {hp_emb_layer} {hp_em_dropout_rate}'\n",
        "              )(value, index, emb_value, emb_index, meta_index, reverse_meta_index) \n",
        "    else:\n",
        "      outputs = AutoInt_EM(emb_d=hp_emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "              drop_rate=hp_em_dropout_rate, em_em= hp_emb_layer, \n",
        "              #name=f'autoint em {hp_emb_d}, {hp_emb_layer} {hp_em_dropout_rate}'\n",
        "              )(value, index, emb_value, emb_index)    \n",
        "\n",
        "    #mha_head = 4\n",
        "    #mha_residual = hp.Boolean('mha_residual', default=True)\n",
        "\n",
        "    uid = random.randint(1,999999)\n",
        "    \n",
        "    ffn_d = hp.Int('ffn_d_size', min_value=32, max_value=512, step=32)\n",
        "    hp_ffn_drop = hp.Float('ffn_emb_drop', min_value=0.0, max_value=0.9, step=0.1)\n",
        "    #for i in range(hp.Int('num_MHA_layer', min_value=1, max_value=12, step=1, default=1)):\n",
        "    for i in range(hp.Fixed('num_mlp_layer', value=4)):\n",
        "      outputs = FFN(ffn_d, drop_rate=hp_ffn_drop, name=f'ffn_{i}_{uid}')(outputs)\n",
        "\n",
        "    outputs = layers.Flatten(name='flatten')(outputs)\n",
        "    outputs = layers.Dense(num_y, activation='relu', kernel_initializer=\"glorot_normal\", \n",
        "                       bias_initializer='random_normal', name='predense')(outputs)\n",
        "    outputs = layers.Dense(num_y, activation='softmax', name='softmax')(outputs)\n",
        "    optimizer = 'adam'\n",
        "    if autodis:\n",
        "      model = Model(inputs=[value, index, emb_value, emb_index, meta_index, reverse_meta_index], outputs=outputs, \n",
        "                  name=f'MLP_ffn_{uid}')\n",
        "    else:  \n",
        "      model = Model(inputs=[value, index, emb_value, emb_index], outputs=outputs, \n",
        "                  name=f'MLP_ffn_{uid}')\n",
        "\n",
        "    initial_learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
        "    decay_steps=hp.Int('decay_steps', 10000,  100000, step=5000)\n",
        "    decay_rate=hp.Float('decay_rate', 0.5, 1.0, step=0.1)\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        decay_steps=decay_steps,\n",
        "        #decay_steps=steps_each,\n",
        "        decay_rate=decay_rate)\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "    #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "    #optimizer = RAdamOptimizer(total_steps=total_steps, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "    \n",
        "    label_smoothing = hp.Float('label_smoothing', min_value=0.0, max_value=0.9, step=0.1)\n",
        "    model.compile(optimizer, \n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing) if num_y == 2 else tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
        "            metrics=['categorical_accuracy',\n",
        "            #tf.keras.metrics.Precision(),\n",
        "            #tf.keras.metrics.Recall(),\n",
        "            tf.keras.metrics.AUC()])\n",
        "    print('model compiled')\n",
        "    return model\n",
        "  return MLP_builder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0P5QgnEkWt7",
        "outputId": "f190def7-1915-4827-9fa8-ab70595e0eff"
      },
      "source": [
        "from kerastuner import HyperModel\n",
        "import random\n",
        "import math\n",
        "\n",
        "class ffn_emHyper(HyperModel):\n",
        "\n",
        "    def __init__(self, feat_size, num_input, num_emb, emb_d, batch_size, embedding=True):\n",
        "        self.feat_size = feat_size\n",
        "        self.num_input = num_input\n",
        "        self.num_emb = num_emb\n",
        "        self.emb_d = emb_d\n",
        "        self.batch_size = batch_size\n",
        "        #self.embedding = embedding\n",
        "\n",
        "\n",
        "    def build(self, hp):\n",
        "      value = Input(shape=(self.num_input,), batch_size=batch_size, name='value', dtype=np.float32)\n",
        "      #if self.embedding:\n",
        "      index = Input(shape=(self.num_input,), batch_size=batch_size, name='index', dtype=np.int32)\n",
        "      emb_value = Input(shape=(self.num_emb, self.emb_d, ), batch_size=batch_size, name='emb_value')\n",
        "      emb_index = Input(shape=(self.num_emb,), batch_size=batch_size, name='emb_index', dtype=np.int32)\n",
        "\n",
        "      uid = random.randint(1,999999)\n",
        "\n",
        "      hp_emb_d = hp.Int('num_of_d', min_value=32, max_value=128, step=32, default=128)\n",
        "      with hp.conditional_scope('num_of_d', 128):\n",
        "        hp_emb_layer = hp.Choice( 'emb_layer_choice', values=['linear', 'dense', 'matrix'])\n",
        "      with hp.conditional_scope('num_of_d', [32,  64,  96]):\n",
        "        hp_emb_layer = hp.Choice('emb_layer_choice', values=['matrix', 'dense'], default='matrix')\n",
        "      \n",
        "      \n",
        "      hp_em_dropout_rate = hp.Float('EM_dropout_rate', min_value=0.0, max_value=0.99, step=0.01)\n",
        "      \n",
        "      ffn_d = hp.Int('ffn_d_size', min_value=16, max_value=1024, step=16)\n",
        "      hp_ffn_drop = hp.Float('ffn_emb_drop', min_value=0.0, max_value=0.99, step=0.01)\n",
        "\n",
        "      #if self.embedding:\n",
        "      outputs = AutoInt_EM(emb_d=hp_emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "              drop_rate=hp_em_dropout_rate, em_em=hp_emb_layer, \n",
        "              name=f'EM_Layer_{uid}')(value, index, emb_value, emb_index)\n",
        "\n",
        "\n",
        "      for i in range(hp.Int('layers_num_ffn', min_value=1, max_value=12, step=1)):\n",
        "        l_uid = random.randint(1,999999)\n",
        "        outputs = FFN(ffn_d, drop_rate=hp_ffn_drop, name=f'ffn_{i}_layer_{l_uid}')(outputs)\n",
        "\n",
        "      outputs = layers.Flatten(name=f'flatten_{uid}')(outputs)\n",
        "      outputs = Dense(1, activation='relu', kernel_initializer='glorot_normal', bias_initializer='random_normal', name=f'predict_dense{uid}')(outputs)\n",
        "      outputs = Dense(1, activation='sigmoid', name=f'predict_dense_sigmoid{uid}')(outputs)\n",
        "\n",
        "      #optimizer = 'adam'\n",
        "      \n",
        "\n",
        "      model = Model(inputs=[value, index, emb_value, emb_index], outputs=outputs, name=f'MLP_EM_model_{uid}')\n",
        "      '''\n",
        "      if self.embedding:\n",
        "        pass\n",
        "        #model = Model(inputs=[value, index, emb_value, emb_index], outputs=outputs, name=f'MLP_EM_model_{uid}')\n",
        "      else:\n",
        "        pass\n",
        "        #model = Model(inputs=[value], outputs=outputs, name=f'MLP_Model_{uid}')'''\n",
        "\n",
        "      #model = Model(inputs=[value, index, emb_value, emb_index], outputs=outputs, name=f'MLP_EM{uid}')\n",
        "  \n",
        "      initial_learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
        "      decay_steps=hp.Int('decay_steps', 10000,  100000, step=5000)\n",
        "      decay_rate=hp.Float('decay_rate', 0.0, 1.0, step=0.01)\n",
        "  \n",
        "      lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "          initial_learning_rate=initial_learning_rate,\n",
        "          decay_steps=decay_steps,\n",
        "          #decay_steps=steps_each,\n",
        "          decay_rate=decay_rate)\n",
        "      \n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "      #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "      #optimizer = RAdamOptimizer(total_steps=total_steps, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "      \n",
        "      label_smoothing = hp.Float('label_smoothing', min_value=0.0, max_value=1.0, step=0.01)\n",
        "      \n",
        "\n",
        "      model.compile(optimizer, \n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n",
        "            metrics=['binary_accuracy',\n",
        "            #tf.keras.metrics.Precision(),\n",
        "            #tf.keras.metrics.Recall(),\n",
        "            tf.keras.metrics.AUC()])\n",
        "      return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5i_REg_Vc2a"
      },
      "source": [
        "from kerastuner import HyperModel\n",
        "import random\n",
        "import math\n",
        "\n",
        "class MLP_EMBHyper(HyperModel):\n",
        "\n",
        "    def __init__(self, feat_size, num_input, num_emb, emb_d, batch_size, Lconv=False, AutoDIs=False):\n",
        "        self.feat_size = feat_size\n",
        "        self.num_input = num_input\n",
        "        self.num_emb = num_emb\n",
        "        self.emb_d = emb_d\n",
        "        self.batch_size = batch_size\n",
        "        self.LN_1 = layers.LayerNormalization(name='lnout')\n",
        "\n",
        "\n",
        "    def build(self, hp):\n",
        "      value = Input(shape=(self.num_input,), batch_size=batch_size, name='value', dtype=np.float32)\n",
        "      index = Input(shape=(self.num_input,), batch_size=batch_size, name='index', dtype=np.int32)\n",
        "      emb_value = Input(shape=(self.num_emb, self.emb_d, ), batch_size=batch_size, name='emb_value')\n",
        "      emb_index = Input(shape=(self.num_emb,), batch_size=batch_size, name='emb_index', dtype=np.int32)\n",
        "\n",
        "\n",
        "      hp_emb_d = hp.Int('num_of_d', min_value=32, max_value=128, step=32, default=128)\n",
        "      with hp.conditional_scope('num_of_d', 128):\n",
        "        hp_emb_layer = hp.Choice( 'emb_layer_choice', values=['linear', 'dense', 'matrix'])\n",
        "      with hp.conditional_scope('num_of_d', [32,  64,  96]):\n",
        "        hp_emb_layer = hp.Choice('emb_layer_choice', values=['matrix', 'dense'], default='matrix')\n",
        "      \n",
        "      \n",
        "      uid = random.randint(1,999999)\n",
        "      hp_em_dropout_rate = hp.Float('EM_dropout_rate', min_value=0.0, max_value=0.99, step=0.01)\n",
        "      \n",
        "      ffn_d = hp.Int('ffn_d_size', min_value=16, max_value=1024, step=16)\n",
        "      ffn_drop = hp.Float('ffn_drop', min_value=0.0, max_value=0.99, step=0.01)\n",
        "\n",
        "      #hp_pe = hp.Boolean('positona_encoding')\n",
        "\n",
        "      outputs = AutoInt_EM(emb_d=hp_emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "              drop_rate=hp_em_dropout_rate, em_em= hp_emb_layer, PE=False, name=f'autoEM_{uid}')(value, index, emb_value, emb_index)\n",
        "\n",
        "\n",
        "      #positional_emb = PositionalEncoder()(num_input, 128)[:,:,outputs.shape[-1]]\n",
        "      #emb_f = math.sqrt(hp_emb_d)\n",
        "      #emb_f = math.sqrt(128)\n",
        "      #outputs = tf.scalar_mul(emb_f, outputs)\n",
        "      #outputs = self.Add(positional_emb, outputs)\n",
        "      #outputs = layers.Dropout(rate=hp.Float('layer_dropout_rate', min_value = 0.0, max_value=0.99, step=0.01))(outputs)\n",
        "        \n",
        "\n",
        "\n",
        "      #hp.Boolean(f'conv{i}{l_uid}'):\n",
        "      #hp_ffn_bl =  hp.Boolean(f'ffn_{i}{uid}'):\n",
        "      for i in range(hp.Int('layers_num', min_value=1, max_value=12, step=1, default=1)):\n",
        "        #l_uid = random.randint(1,999999) # kernel_num, head_num, dropout_rate\n",
        "\n",
        "        #if hp\n",
        "        outputs = FFN(ffn_d, drop_rate=ffn_drop, name=f'ffn_{i}_{uid}')(outputs)\n",
        "\n",
        "      outputs = layers.Flatten(name=f'flatten_{uid}')(outputs)\n",
        "      outputs = Dense(1, activation='relu', kernel_initializer='glorot_normal', bias_initializer='random_normal')(outputs)\n",
        "      outputs = Dense(1, activation='sigmoid', kernel_initializer='glorot_normal', bias_initializer='random_normal')(outputs)\n",
        "\n",
        "      optimizer = 'adam'\n",
        "      model = Model(inputs=[value, index, emb_value, emb_index], outputs=outputs, name=f'DynamicConv_model_{K_size}_{uid}')\n",
        "  \n",
        "      initial_learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
        "      decay_steps=hp.Int('decay_steps', 10000,  100000, step=5000)\n",
        "      decay_rate=hp.Float('decay_rate', 0.0, 1.0, step=0.01)\n",
        "  \n",
        "      lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "          initial_learning_rate=initial_learning_rate,\n",
        "          decay_steps=decay_steps,\n",
        "          #decay_steps=steps_each,\n",
        "          decay_rate=decay_rate)\n",
        "      \n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "      #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "      #optimizer = RAdamOptimizer(total_steps=total_steps, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "      \n",
        "      label_smoothing = hp.Float('label_smoothing', min_value=0.0, max_value=1.0, step=0.01)\n",
        "      \n",
        "\n",
        "      model.compile(optimizer, \n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n",
        "            metrics=['binary_accuracy',\n",
        "            #tf.keras.metrics.Precision(),\n",
        "            #tf.keras.metrics.Recall(),\n",
        "            tf.keras.metrics.AUC()])\n",
        "      return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgWO4TbI5eJ1"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt4uWpjbgkWT"
      },
      "source": [
        "#train_dataset = pre_dataset(train, 1)\n",
        "#vali_dataset = pre_dataset(vali, 1)\n",
        "shape = 83\n",
        "input_d = 32\n",
        "original_inputs = Input(shape=(shape),  name=\"cs_p2p\")\n",
        "#print(original_inputs)\n",
        "#inputw = inputW(32)(x_train)\n",
        "#inputw.shape\n",
        "#print(original_inputs.shape[0])\n",
        "output_inputw = inputW(input_d)(original_inputs)\n",
        "#output_inputw = inputW1(32)(original_inputs)\n",
        "output = Flatten()(output_inputw)\n",
        "output = Dense(input_d*shape)(output)\n",
        "output = Dense(10, activation='relu')(output)\n",
        "output = Dropout(0.1)(output)\n",
        "output = Dense(1, activation='softmax')(output)\n",
        "cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "cs_p2p.summary()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "cs_p2p.compile(optimizer, loss=tf.keras.losses.MeanSquaredError(), metrics=[\n",
        "        tf.keras.metrics.MeanSquaredError(),\n",
        "        tf.keras.metrics.AUC(),\n",
        "    ])\n",
        "cs_p2p.fit(train_dataset, epochs=10, batch_size=32, validation_data=vali_dataset,validation_batch_size=32)\n",
        "s = data[:1]\n",
        "s = s[['annual_inc', 'installment', 'loan_amnt']]\n",
        "#print()\n",
        "cs_p2p.predict(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpKjpWg-UKfP"
      },
      "source": [
        "# Initialize the structured data classifier.\n",
        "clf = ak.StructuredDataClassifier(\n",
        "    overwrite=True,\n",
        "    max_trials=3) # It tries 3 different models.\n",
        "# Feed the structured data classifier with training data.\n",
        "clf.fit(\n",
        "    # The path to the train.csv file.\n",
        "    train_dataset,\n",
        "    # The name of the label column.\n",
        "    #'survived',\n",
        "    epochs=10)\n",
        "# Predict with the best model.\n",
        "predicted_y = clf.predict(test_dataset)\n",
        "# Evaluate the best model with testing data.\n",
        "print(clf.evaluate(test_dataset, 'survived'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKC6DaH-eK0J"
      },
      "source": [
        "XGboost\n",
        "決定木の方が良い結果でるっぽい\n",
        "\n",
        "normalizeしなくても train:74 test:72 aucでた 間違ってたぽい。。。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GFY4wY5gPjq"
      },
      "source": [
        "from sklearn import metrics as skmetrics\n",
        "\n",
        "def GetAUC(model, name:str, X_train, y_train):\n",
        "    '''\n",
        "    To quickly get the AUC of model on the training and testing set\n",
        "    '''\n",
        "    #res = [0.0, 0.0]\n",
        "    #y_train_score = model.predict_proba(X_train)[:, 1]\n",
        "    y_train_score = model.predict(X_train)\n",
        "    acc = skmetrics.accuracy_score(y_train, y_train_score)\n",
        "    auc = skmetrics.roc_auc_score(y_train, y_train_score)\n",
        "\n",
        "    print(f\"In {name}: accuracy\", acc, \"auc\", auc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "S6OGXkafhmEQ",
        "outputId": "57992606-8416-4805-cd77-a00ddaad53b1"
      },
      "source": [
        "! gzip -d 'LC_dataset09-18q4.csv.gz'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gzip: LC_dataset09-18q4.csv.gz: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYVDVzOT-HS8"
      },
      "source": [
        "import re\n",
        "\n",
        "dataset = pd.read_csv('/content/LC_dataset09-18q4.csv')\n",
        "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
        "dataset.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in dataset.columns.values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "yLnfOiW9WFTl",
        "outputId": "3ecf1df5-1a8d-4e1b-951b-d9a2eae5cddb"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "y = dataset[['loan_status']]\n",
        "x = dataset.drop(labels=['loan_status'], axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=101)\n",
        "#scaler = MinMaxScaler()\n",
        "#X_train = scaler.fit_transform(X_train)\n",
        "#X_vali = scaler.transform(X_vali)\n",
        "#X_test = scaler.transform(X_test)\n",
        "with strategy.scope():\n",
        "  xgb = XGBClassifier(max_depth = 6, n_estimators= 200, class_weight = {0: 1, 1:5})\n",
        "  xgb.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "fsLiHx0ptCtN",
        "outputId": "fd259264-3282-4303-d7cd-3503c2d9826d"
      },
      "source": [
        "# min max なし\n",
        "GetAUC(xgb, 'train', X_train, y_train)\n",
        "GetAUC(xgb, 'test', X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In train: accuracy 0.8078659402466981 auc 0.543959131910758\n",
            "In test: accuracy 0.8061947296905977 auc 0.5404561651472131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "1HAk8TVcrl7m",
        "outputId": "19075fbb-2d8e-4502-a87f-97471060db38"
      },
      "source": [
        "# min max した\n",
        "GetAUC(xgb, 'train', X_train, y_train)\n",
        "GetAUC(xgb, 'test', X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In train: accuracy 0.8079457598173431 auc 0.5442005345664778\n",
            "In test: accuracy 0.8063543685355549 auc 0.5406608626680461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtBo3CBvfzVv"
      },
      "source": [
        "def mlp(layers, train_dataset:tf.data.Dataset, vali_dataset:tf.data.Dataset, test_dataset:tf.data.Dataset, batch_size:int, epochs:int, model_d:int=0):\n",
        "  name='cs'\n",
        "  batch_size = batch_size\n",
        "  epochs = epochs\n",
        "  model_d=model_d\n",
        "  layers=layers\n",
        "  \n",
        "  #with tf.device('/device:GPU:0'):\n",
        "  with strategy.scope():\n",
        "    original_inputs = Input(shape=(83,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "    if model_d > 0:\n",
        "      #output_inputw = inputW(model_d)(original_inputs)\n",
        "      output_inputw = inputW1(model_d)(original_inputs)\n",
        "      original_inputs = Flatten()(output_inputw)\n",
        "    for i in layers:\n",
        "      output = Dense(i, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.l2(1e-4),\n",
        "        activity_regularizer=regularizers.l2(1e-5))(original_inputs)\n",
        "      output = Dropout(0.1)(output)\n",
        "\n",
        "    output = Dense(1, activation='sigmoid')(output)\n",
        "    cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "    cs_p2p.summary()\n",
        "    #optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "    cs_p2p.compile(optimizer, \n",
        "                   loss=tf.keras.losses.BinaryCrossentropy()\n",
        "                   #loss=tf.keras.metrics.MeanSquaredError()\n",
        "            , metrics=[\n",
        "            tf.keras.metrics.MeanSquaredError(),\n",
        "            #tf.keras.metrics.Accuracy(),\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.AUC(),\n",
        "        ])\n",
        "    #cs_p2p.fit(train_dataset, epochs=3, batch_size=32, validation_data=vali_dataset,validation_batch_size=4)\n",
        "    if all([i in globals() for i in [\"train_dataset\", \"test_dataset\", \"vali_datase\"]]):\n",
        "      history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=vali_dataset, callbacks=[tensorboard_callback, hparams_callback])\n",
        "    else:\n",
        "      history = cs_p2p.fit(train_dataset, epochs=epochs, verbose=1, validation_split=1, callbacks=[tensorboard_callback, hparams_callback])\n",
        "    results = cs_p2p.evaluate(test_dataset, batch_size=batch_size)\n",
        "    print(history.history)\n",
        "    print(\"test loss, test acc:\", results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AecjMfc-hIe3"
      },
      "source": [
        "def transformer(train_dataset:tf.data.Dataset, vali_dataset:tf.data.Dataset, test_dataset:tf.data.Dataset, \n",
        "                batch_size:int, epochs:int, trans_layers:int, num_heads:int, model_d:int):\n",
        "  #with tf.device('/device:GPU:0'):\n",
        "  with strategy.scope():\n",
        "    name='cs'\n",
        "    #num_heads=num_heads\n",
        "    use_masking=False\n",
        "    #epochs = epochs\n",
        "    original_inputs = Input(shape=(83,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "    #inputw = inputW(32)(x_train)\n",
        "    #inputw.shape\n",
        "    #print(original_inputs.shape[0])\n",
        "    output = inputW0(model_d)(original_inputs)\n",
        "    for i in range(trans_layers):\n",
        "      output_sa = MultiHeadSelfAttention(\n",
        "                num_heads, use_masking=use_masking, \n",
        "                name=f'{name}_self_attention_{i}')(output)\n",
        "      post_residual1 = (Add(name=f'{name}_add_{i}')([output_sa, output]))\n",
        "      norm1_output = LayerNormalization(name=f'{name}_normalization1_{i}')(post_residual1)\n",
        "      output = TransformerTransition(name=f'{name}_transition_{i}', activation='relu')(norm1_output)\n",
        "      post_residual2 = (Add(name=f'{name}_add2_{i}')([norm1_output,output]))\n",
        "      output = LayerNormalization(name=f'{name}_normalization2_{i}')(post_residual2)\n",
        "\n",
        "    output = Flatten()(output)\n",
        "\n",
        "    output = mlpBlock()\n",
        "    output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.l2(1e-4),\n",
        "        activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "    output = Dropout(0.1)(output)\n",
        "    output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.l2(1e-4),\n",
        "        activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "    output = Dropout(0.1)(output)\n",
        "    #output = Dense(2, activation='softmax')(output)\n",
        "    #output = Dense(2, activation='relu')(output)\n",
        "    output = Dense(1, activation='sigmoid')(output)\n",
        "    cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "    cs_p2p.summary()\n",
        "    \"\"\"\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=1e-4,\n",
        "        decay_steps=3000,\n",
        "        decay_rate=0.9)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    \"\"\"\n",
        "    optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.2, min_lr=1e-5, name='RectifiedAdam')\n",
        "    cs_p2p.compile(optimizer, \n",
        "                   loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                   #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "            metrics=[\n",
        "            #tf.keras.metrics.MeanSquaredError(),\n",
        "            #tf.keras.metrics.Accuracy(),\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.AUC(),\n",
        "            ]\n",
        "            )\n",
        "    history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "    print(history.history)\n",
        "    #cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "    result = cs_p2p.evaluate(test_dataset)\n",
        "    print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "hB3uIplLJnnw",
        "outputId": "a19d7461-8b96-4937-b626-34e7fa3417b3"
      },
      "source": [
        "mlp([83, 128, 64, 32], train_dataset, test_dataset, 32, 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a9eaec25355c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m83\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: mlp() missing 1 required positional argument: 'epochs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a_SlP9Jki9AP",
        "outputId": "af1d8f8d-1a32-486d-ef2d-dba35564b4c3"
      },
      "source": [
        "transformer(train_dataset=train_dataset, vali_dataset=vali_dataset, test_dataset=test_dataset,\n",
        "            batch_size=32, epochs=50, trans_layers=6, num_heads=4, model_d=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_size 32\n",
            "Model: \"cs_p2p\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cs_p2p (InputLayer)             [(4, 83)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_w0_9 (inputW0)            (4, 83, 32)          90387       cs_p2p[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_0 (MultiHeadS (4, 83, 32)          3072        input_w0_9[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_0 (Add)                  (4, 83, 32)          0           cs_self_attention_0[0][0]        \n",
            "                                                                 input_w0_9[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_0 (LayerNorma (4, 83, 32)          64          cs_add_0[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_0 (TransformerTra (4, 83, 32)          8352        cs_normalization1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_0 (Add)                 (4, 83, 32)          0           cs_normalization1_0[0][0]        \n",
            "                                                                 cs_transition_0[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_0 (LayerNorma (4, 83, 32)          64          cs_add2_0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_1 (MultiHeadS (4, 83, 32)          3072        cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_1 (Add)                  (4, 83, 32)          0           cs_self_attention_1[0][0]        \n",
            "                                                                 cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_1 (LayerNorma (4, 83, 32)          64          cs_add_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_1 (TransformerTra (4, 83, 32)          8352        cs_normalization1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_1 (Add)                 (4, 83, 32)          0           cs_normalization1_1[0][0]        \n",
            "                                                                 cs_transition_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_1 (LayerNorma (4, 83, 32)          64          cs_add2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_2 (MultiHeadS (4, 83, 32)          3072        cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_2 (Add)                  (4, 83, 32)          0           cs_self_attention_2[0][0]        \n",
            "                                                                 cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_2 (LayerNorma (4, 83, 32)          64          cs_add_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_2 (TransformerTra (4, 83, 32)          8352        cs_normalization1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_2 (Add)                 (4, 83, 32)          0           cs_normalization1_2[0][0]        \n",
            "                                                                 cs_transition_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_2 (LayerNorma (4, 83, 32)          64          cs_add2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_3 (MultiHeadS (4, 83, 32)          3072        cs_normalization2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_3 (Add)                  (4, 83, 32)          0           cs_self_attention_3[0][0]        \n",
            "                                                                 cs_normalization2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_3 (LayerNorma (4, 83, 32)          64          cs_add_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_3 (TransformerTra (4, 83, 32)          8352        cs_normalization1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_3 (Add)                 (4, 83, 32)          0           cs_normalization1_3[0][0]        \n",
            "                                                                 cs_transition_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_3 (LayerNorma (4, 83, 32)          64          cs_add2_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_4 (MultiHeadS (4, 83, 32)          3072        cs_normalization2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_4 (Add)                  (4, 83, 32)          0           cs_self_attention_4[0][0]        \n",
            "                                                                 cs_normalization2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_4 (LayerNorma (4, 83, 32)          64          cs_add_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_4 (TransformerTra (4, 83, 32)          8352        cs_normalization1_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_4 (Add)                 (4, 83, 32)          0           cs_normalization1_4[0][0]        \n",
            "                                                                 cs_transition_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_4 (LayerNorma (4, 83, 32)          64          cs_add2_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_5 (MultiHeadS (4, 83, 32)          3072        cs_normalization2_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_5 (Add)                  (4, 83, 32)          0           cs_self_attention_5[0][0]        \n",
            "                                                                 cs_normalization2_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_5 (LayerNorma (4, 83, 32)          64          cs_add_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_5 (TransformerTra (4, 83, 32)          8352        cs_normalization1_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_5 (Add)                 (4, 83, 32)          0           cs_normalization1_5[0][0]        \n",
            "                                                                 cs_transition_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_5 (LayerNorma (4, 83, 32)          64          cs_add2_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_9 (Flatten)             (4, 2656)            0           cs_normalization2_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (4, 64)              170048      flatten_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (4, 64)              0           dense_34[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_35 (Dense)                (4, 32)              2080        dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (4, 32)              0           dense_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (4, 1)               33          dropout_24[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 331,860\n",
            "Trainable params: 331,860\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "    1/26936 [..............................] - ETA: 44:37:37 - loss: 0.8776 - accuracy: 0.4375 - auc_10: 0.4598WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0226s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0226s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26936/26936 [==============================] - ETA: 0s - loss: 0.4723 - accuracy: 0.7995 - auc_10: 0.6910WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_test_batch_end` time: 0.0110s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_test_batch_end` time: 0.0110s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r26936/26936 [==============================] - 676s 25ms/step - loss: 0.4723 - accuracy: 0.7995 - auc_10: 0.6910 - val_loss: 0.4599 - val_accuracy: 0.8007 - val_auc_10: 0.7075\n",
            "Epoch 2/50\n",
            "26936/26936 [==============================] - 660s 25ms/step - loss: 0.4585 - accuracy: 0.8000 - auc_10: 0.7068 - val_loss: 0.4545 - val_accuracy: 0.8012 - val_auc_10: 0.7124\n",
            "Epoch 3/50\n",
            "26936/26936 [==============================] - 672s 25ms/step - loss: 0.4569 - accuracy: 0.8010 - auc_10: 0.7102 - val_loss: 0.4564 - val_accuracy: 0.8024 - val_auc_10: 0.7115\n",
            "Epoch 4/50\n",
            "26936/26936 [==============================] - 683s 25ms/step - loss: 0.4557 - accuracy: 0.8020 - auc_10: 0.7116 - val_loss: 0.4552 - val_accuracy: 0.8015 - val_auc_10: 0.7162\n",
            "Epoch 5/50\n",
            "26936/26936 [==============================] - 674s 25ms/step - loss: 0.4554 - accuracy: 0.8022 - auc_10: 0.7116 - val_loss: 0.4536 - val_accuracy: 0.8034 - val_auc_10: 0.7137\n",
            "Epoch 6/50\n",
            "24700/26936 [==========================>...] - ETA: 49s - loss: 0.4550 - accuracy: 0.8022 - auc_10: 0.7126Buffered data was truncated after reaching the output size limit."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKb3dHYbOmv3",
        "outputId": "81e296ce-09a1-4095-cce2-61ffe7260823"
      },
      "source": [
        "name='cs' \n",
        "use_masking=False\n",
        "batch_size = batch_size\n",
        "epochs = 50\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "print(loans_raw)\n",
        "data = loans_raw\n",
        "#data = data[data.loan_status != 'Current']\n",
        "#data = data[data.loan_status != 'In Grace Period']\n",
        "data = data[data.loan_status == 'Charged Off']\n",
        "data = data[data.loan_status == 'Fully Paid']\n",
        "#print(data.loc[:100])\n",
        "train_dataset = pre_dataset(data[:1000],32)\n",
        "print(train_dataset)\n",
        "\"\"\"\n",
        "\n",
        "#train_dataset = pre_dataset(train, batch_size)\n",
        "#vali_dataset = pre_dataset(vali, 1)\n",
        "#with tf.device('/device:GPU:0'):\n",
        "with strategy.scope():\n",
        "  original_inputs = Input(shape=(83,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "  \n",
        "  #output_inputw = inputW(32)(original_inputs)\n",
        "  #output = Flatten()(output_inputw)\n",
        "  #output = Dense(32, activation='relu')(output)\n",
        "  output = Dense(83, activation='relu')(original_inputs)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(512, activation='relu')(output)\n",
        "  #output = Dropout(0.1)(output)\n",
        "  #output = Dense(256, activation='relu')(output)\n",
        "  #output = Dropout(0.1)(output)\n",
        "  output = Dense(64, activation='relu')(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(24, activation='relu')(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(1, activation='sigmoid')(output)\n",
        "  cs_p2p_mlp = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "  cs_p2p_mlp.summary()\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "  cs_p2p_mlp.compile(optimizer, \n",
        "                 loss=tf.keras.losses.BinaryCrossentropy()\n",
        "                 #loss=tf.keras.metrics.MeanSquaredError()\n",
        "          , metrics=[\n",
        "          tf.keras.metrics.MeanSquaredError(),\n",
        "          #tf.keras.metrics.Accuracy(),\n",
        "          tf.keras.metrics.Recall(),\n",
        "          tf.keras.metrics.Precision(),\n",
        "          'accuracy',\n",
        "          tf.keras.metrics.AUC(),\n",
        "      ])\n",
        "  #cs_p2p.fit(train_dataset, epochs=3, batch_size=32, validation_data=vali_dataset,validation_batch_size=4)\n",
        "  #history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset)\n",
        "  history = cs_p2p_mlp.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset, validation_batch_size=batch_size)\n",
        "  print(history.history)\n",
        "  results = cs_p2p_mlp.evaluate(test_dataset, batch_size=batch_size)\n",
        "  print(\"test loss, test acc:\", results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cs_p2p\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cs_p2p (InputLayer)          [(4, 83)]                 0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (4, 83)                   6972      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (4, 83)                   0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (4, 64)                   5376      \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (4, 64)                   0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (4, 24)                   1560      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (4, 24)                   0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (4, 1)                    25        \n",
            "=================================================================\n",
            "Total params: 13,933\n",
            "Trainable params: 13,933\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:Model was constructed with shape (4, 83) for input Tensor(\"cs_p2p_3:0\", shape=(4, 83), dtype=float32), but it was called on an input with incompatible shape (128, 83).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (4, 83) for input Tensor(\"cs_p2p_3:0\", shape=(4, 83), dtype=float32), but it was called on an input with incompatible shape (128, 83).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (4, 83) for input Tensor(\"cs_p2p_3:0\", shape=(4, 83), dtype=float32), but it was called on an input with incompatible shape (128, 83).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (4, 83) for input Tensor(\"cs_p2p_3:0\", shape=(4, 83), dtype=float32), but it was called on an input with incompatible shape (128, 83).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r  1/789 [..............................] - ETA: 9:48 - loss: 0.7688 - mean_squared_error: 0.2874 - recall_3: 0.0585 - precision_3: 0.8136 - accuracy: 0.2354 - auc_3: 0.5048WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0228s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0228s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "789/789 [==============================] - ETA: 0s - loss: 0.4671 - mean_squared_error: 0.1497 - recall_3: 0.9923 - precision_3: 0.8021 - accuracy: 0.7980 - auc_3: 0.6824WARNING:tensorflow:Model was constructed with shape (4, 83) for input Tensor(\"cs_p2p_3:0\", shape=(4, 83), dtype=float32), but it was called on an input with incompatible shape (128, 83).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (4, 83) for input Tensor(\"cs_p2p_3:0\", shape=(4, 83), dtype=float32), but it was called on an input with incompatible shape (128, 83).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r789/789 [==============================] - 27s 34ms/step - loss: 0.4671 - mean_squared_error: 0.1497 - recall_3: 0.9923 - precision_3: 0.8021 - accuracy: 0.7980 - auc_3: 0.6824 - val_loss: 0.4555 - val_mean_squared_error: 0.1458 - val_recall_3: 0.9937 - val_precision_3: 0.8035 - val_accuracy: 0.8006 - val_auc_3: 0.7098\n",
            "Epoch 2/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4578 - mean_squared_error: 0.1463 - recall_3: 0.9910 - precision_3: 0.8053 - accuracy: 0.8010 - auc_3: 0.7030 - val_loss: 0.4553 - val_mean_squared_error: 0.1457 - val_recall_3: 0.9853 - val_precision_3: 0.8080 - val_accuracy: 0.8010 - val_auc_3: 0.7113\n",
            "Epoch 3/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4565 - mean_squared_error: 0.1459 - recall_3: 0.9899 - precision_3: 0.8060 - accuracy: 0.8013 - auc_3: 0.7058 - val_loss: 0.4539 - val_mean_squared_error: 0.1452 - val_recall_3: 0.9901 - val_precision_3: 0.8060 - val_accuracy: 0.8015 - val_auc_3: 0.7127\n",
            "Epoch 4/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4557 - mean_squared_error: 0.1456 - recall_3: 0.9884 - precision_3: 0.8070 - accuracy: 0.8016 - auc_3: 0.7075 - val_loss: 0.4538 - val_mean_squared_error: 0.1452 - val_recall_3: 0.9922 - val_precision_3: 0.8051 - val_accuracy: 0.8017 - val_auc_3: 0.7131\n",
            "Epoch 5/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4550 - mean_squared_error: 0.1454 - recall_3: 0.9882 - precision_3: 0.8073 - accuracy: 0.8018 - auc_3: 0.7091 - val_loss: 0.4532 - val_mean_squared_error: 0.1450 - val_recall_3: 0.9922 - val_precision_3: 0.8053 - val_accuracy: 0.8019 - val_auc_3: 0.7143\n",
            "Epoch 6/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4544 - mean_squared_error: 0.1452 - recall_3: 0.9878 - precision_3: 0.8075 - accuracy: 0.8019 - auc_3: 0.7102 - val_loss: 0.4529 - val_mean_squared_error: 0.1449 - val_recall_3: 0.9901 - val_precision_3: 0.8065 - val_accuracy: 0.8020 - val_auc_3: 0.7147\n",
            "Epoch 7/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4540 - mean_squared_error: 0.1451 - recall_3: 0.9877 - precision_3: 0.8078 - accuracy: 0.8021 - auc_3: 0.7110 - val_loss: 0.4532 - val_mean_squared_error: 0.1449 - val_recall_3: 0.9900 - val_precision_3: 0.8065 - val_accuracy: 0.8020 - val_auc_3: 0.7151\n",
            "Epoch 8/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4535 - mean_squared_error: 0.1449 - recall_3: 0.9875 - precision_3: 0.8079 - accuracy: 0.8021 - auc_3: 0.7122 - val_loss: 0.4527 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9887 - val_precision_3: 0.8075 - val_accuracy: 0.8025 - val_auc_3: 0.7151\n",
            "Epoch 9/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4532 - mean_squared_error: 0.1448 - recall_3: 0.9873 - precision_3: 0.8081 - accuracy: 0.8022 - auc_3: 0.7126 - val_loss: 0.4529 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9877 - val_precision_3: 0.8076 - val_accuracy: 0.8020 - val_auc_3: 0.7150\n",
            "Epoch 10/50\n",
            "789/789 [==============================] - 27s 34ms/step - loss: 0.4527 - mean_squared_error: 0.1446 - recall_3: 0.9871 - precision_3: 0.8084 - accuracy: 0.8025 - auc_3: 0.7138 - val_loss: 0.4531 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9887 - val_precision_3: 0.8075 - val_accuracy: 0.8025 - val_auc_3: 0.7156\n",
            "Epoch 11/50\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4524 - mean_squared_error: 0.1446 - recall_3: 0.9866 - precision_3: 0.8085 - accuracy: 0.8023 - auc_3: 0.7143 - val_loss: 0.4529 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9855 - val_precision_3: 0.8089 - val_accuracy: 0.8023 - val_auc_3: 0.7155\n",
            "Epoch 12/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4521 - mean_squared_error: 0.1444 - recall_3: 0.9866 - precision_3: 0.8089 - accuracy: 0.8028 - auc_3: 0.7150 - val_loss: 0.4523 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9884 - val_precision_3: 0.8074 - val_accuracy: 0.8021 - val_auc_3: 0.7158\n",
            "Epoch 13/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4519 - mean_squared_error: 0.1444 - recall_3: 0.9867 - precision_3: 0.8091 - accuracy: 0.8031 - auc_3: 0.7152 - val_loss: 0.4529 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9850 - val_precision_3: 0.8092 - val_accuracy: 0.8023 - val_auc_3: 0.7149\n",
            "Epoch 14/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4518 - mean_squared_error: 0.1443 - recall_3: 0.9862 - precision_3: 0.8094 - accuracy: 0.8031 - auc_3: 0.7155 - val_loss: 0.4522 - val_mean_squared_error: 0.1446 - val_recall_3: 0.9821 - val_precision_3: 0.8107 - val_accuracy: 0.8023 - val_auc_3: 0.7159\n",
            "Epoch 15/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4514 - mean_squared_error: 0.1442 - recall_3: 0.9857 - precision_3: 0.8096 - accuracy: 0.8031 - auc_3: 0.7162 - val_loss: 0.4524 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9903 - val_precision_3: 0.8065 - val_accuracy: 0.8022 - val_auc_3: 0.7162\n",
            "Epoch 16/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4513 - mean_squared_error: 0.1442 - recall_3: 0.9862 - precision_3: 0.8094 - accuracy: 0.8031 - auc_3: 0.7164 - val_loss: 0.4525 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9905 - val_precision_3: 0.8064 - val_accuracy: 0.8022 - val_auc_3: 0.7160\n",
            "Epoch 17/50\n",
            "789/789 [==============================] - 27s 34ms/step - loss: 0.4511 - mean_squared_error: 0.1441 - recall_3: 0.9857 - precision_3: 0.8098 - accuracy: 0.8033 - auc_3: 0.7169 - val_loss: 0.4522 - val_mean_squared_error: 0.1446 - val_recall_3: 0.9889 - val_precision_3: 0.8073 - val_accuracy: 0.8023 - val_auc_3: 0.7162\n",
            "Epoch 18/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4509 - mean_squared_error: 0.1440 - recall_3: 0.9856 - precision_3: 0.8100 - accuracy: 0.8035 - auc_3: 0.7170 - val_loss: 0.4525 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9862 - val_precision_3: 0.8088 - val_accuracy: 0.8024 - val_auc_3: 0.7158\n",
            "Epoch 19/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4508 - mean_squared_error: 0.1439 - recall_3: 0.9858 - precision_3: 0.8101 - accuracy: 0.8037 - auc_3: 0.7173 - val_loss: 0.4525 - val_mean_squared_error: 0.1446 - val_recall_3: 0.9843 - val_precision_3: 0.8098 - val_accuracy: 0.8026 - val_auc_3: 0.7160\n",
            "Epoch 20/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4506 - mean_squared_error: 0.1439 - recall_3: 0.9853 - precision_3: 0.8104 - accuracy: 0.8037 - auc_3: 0.7176 - val_loss: 0.4525 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9850 - val_precision_3: 0.8092 - val_accuracy: 0.8023 - val_auc_3: 0.7160\n",
            "Epoch 21/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4505 - mean_squared_error: 0.1439 - recall_3: 0.9853 - precision_3: 0.8103 - accuracy: 0.8037 - auc_3: 0.7178 - val_loss: 0.4521 - val_mean_squared_error: 0.1446 - val_recall_3: 0.9848 - val_precision_3: 0.8094 - val_accuracy: 0.8024 - val_auc_3: 0.7162\n",
            "Epoch 22/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4502 - mean_squared_error: 0.1438 - recall_3: 0.9854 - precision_3: 0.8103 - accuracy: 0.8037 - auc_3: 0.7185 - val_loss: 0.4521 - val_mean_squared_error: 0.1446 - val_recall_3: 0.9903 - val_precision_3: 0.8067 - val_accuracy: 0.8025 - val_auc_3: 0.7161\n",
            "Epoch 23/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4502 - mean_squared_error: 0.1438 - recall_3: 0.9852 - precision_3: 0.8106 - accuracy: 0.8039 - auc_3: 0.7184 - val_loss: 0.4521 - val_mean_squared_error: 0.1446 - val_recall_3: 0.9869 - val_precision_3: 0.8083 - val_accuracy: 0.8024 - val_auc_3: 0.7158\n",
            "Epoch 24/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4500 - mean_squared_error: 0.1437 - recall_3: 0.9852 - precision_3: 0.8106 - accuracy: 0.8039 - auc_3: 0.7188 - val_loss: 0.4530 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9892 - val_precision_3: 0.8069 - val_accuracy: 0.8021 - val_auc_3: 0.7163\n",
            "Epoch 25/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4500 - mean_squared_error: 0.1437 - recall_3: 0.9850 - precision_3: 0.8107 - accuracy: 0.8040 - auc_3: 0.7187 - val_loss: 0.4530 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9891 - val_precision_3: 0.8072 - val_accuracy: 0.8024 - val_auc_3: 0.7160\n",
            "Epoch 26/50\n",
            "789/789 [==============================] - 27s 34ms/step - loss: 0.4500 - mean_squared_error: 0.1437 - recall_3: 0.9851 - precision_3: 0.8107 - accuracy: 0.8039 - auc_3: 0.7187 - val_loss: 0.4524 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9848 - val_precision_3: 0.8093 - val_accuracy: 0.8023 - val_auc_3: 0.7155\n",
            "Epoch 27/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4497 - mean_squared_error: 0.1436 - recall_3: 0.9850 - precision_3: 0.8108 - accuracy: 0.8040 - auc_3: 0.7192 - val_loss: 0.4525 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9792 - val_precision_3: 0.8119 - val_accuracy: 0.8019 - val_auc_3: 0.7156\n",
            "Epoch 28/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4496 - mean_squared_error: 0.1436 - recall_3: 0.9850 - precision_3: 0.8110 - accuracy: 0.8043 - auc_3: 0.7194 - val_loss: 0.4523 - val_mean_squared_error: 0.1446 - val_recall_3: 0.9864 - val_precision_3: 0.8086 - val_accuracy: 0.8024 - val_auc_3: 0.7157\n",
            "Epoch 29/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4495 - mean_squared_error: 0.1435 - recall_3: 0.9849 - precision_3: 0.8109 - accuracy: 0.8041 - auc_3: 0.7197 - val_loss: 0.4524 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9850 - val_precision_3: 0.8093 - val_accuracy: 0.8024 - val_auc_3: 0.7156\n",
            "Epoch 30/50\n",
            "789/789 [==============================] - 27s 34ms/step - loss: 0.4494 - mean_squared_error: 0.1435 - recall_3: 0.9848 - precision_3: 0.8110 - accuracy: 0.8042 - auc_3: 0.7197 - val_loss: 0.4523 - val_mean_squared_error: 0.1446 - val_recall_3: 0.9857 - val_precision_3: 0.8088 - val_accuracy: 0.8022 - val_auc_3: 0.7156\n",
            "Epoch 31/50\n",
            "789/789 [==============================] - 26s 34ms/step - loss: 0.4494 - mean_squared_error: 0.1435 - recall_3: 0.9848 - precision_3: 0.8110 - accuracy: 0.8042 - auc_3: 0.7200 - val_loss: 0.4527 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9776 - val_precision_3: 0.8127 - val_accuracy: 0.8019 - val_auc_3: 0.7157\n",
            "Epoch 32/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4493 - mean_squared_error: 0.1434 - recall_3: 0.9849 - precision_3: 0.8111 - accuracy: 0.8043 - auc_3: 0.7201 - val_loss: 0.4523 - val_mean_squared_error: 0.1446 - val_recall_3: 0.9871 - val_precision_3: 0.8081 - val_accuracy: 0.8022 - val_auc_3: 0.7157\n",
            "Epoch 33/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4491 - mean_squared_error: 0.1434 - recall_3: 0.9846 - precision_3: 0.8112 - accuracy: 0.8044 - auc_3: 0.7203 - val_loss: 0.4526 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9810 - val_precision_3: 0.8111 - val_accuracy: 0.8021 - val_auc_3: 0.7153\n",
            "Epoch 34/50\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4490 - mean_squared_error: 0.1433 - recall_3: 0.9845 - precision_3: 0.8114 - accuracy: 0.8044 - auc_3: 0.7206 - val_loss: 0.4528 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9812 - val_precision_3: 0.8107 - val_accuracy: 0.8017 - val_auc_3: 0.7151\n",
            "Epoch 35/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4490 - mean_squared_error: 0.1433 - recall_3: 0.9844 - precision_3: 0.8114 - accuracy: 0.8045 - auc_3: 0.7204 - val_loss: 0.4525 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9848 - val_precision_3: 0.8095 - val_accuracy: 0.8024 - val_auc_3: 0.7154\n",
            "Epoch 36/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4488 - mean_squared_error: 0.1433 - recall_3: 0.9844 - precision_3: 0.8115 - accuracy: 0.8045 - auc_3: 0.7209 - val_loss: 0.4528 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9863 - val_precision_3: 0.8084 - val_accuracy: 0.8021 - val_auc_3: 0.7154\n",
            "Epoch 37/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4490 - mean_squared_error: 0.1433 - recall_3: 0.9843 - precision_3: 0.8115 - accuracy: 0.8046 - auc_3: 0.7207 - val_loss: 0.4528 - val_mean_squared_error: 0.1449 - val_recall_3: 0.9802 - val_precision_3: 0.8112 - val_accuracy: 0.8017 - val_auc_3: 0.7154\n",
            "Epoch 38/50\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4490 - mean_squared_error: 0.1433 - recall_3: 0.9843 - precision_3: 0.8115 - accuracy: 0.8044 - auc_3: 0.7207 - val_loss: 0.4524 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9859 - val_precision_3: 0.8088 - val_accuracy: 0.8023 - val_auc_3: 0.7155\n",
            "Epoch 39/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4488 - mean_squared_error: 0.1433 - recall_3: 0.9845 - precision_3: 0.8113 - accuracy: 0.8043 - auc_3: 0.7211 - val_loss: 0.4527 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9806 - val_precision_3: 0.8113 - val_accuracy: 0.8021 - val_auc_3: 0.7152\n",
            "Epoch 40/50\n",
            "789/789 [==============================] - 27s 34ms/step - loss: 0.4487 - mean_squared_error: 0.1432 - recall_3: 0.9840 - precision_3: 0.8117 - accuracy: 0.8046 - auc_3: 0.7210 - val_loss: 0.4526 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9843 - val_precision_3: 0.8095 - val_accuracy: 0.8021 - val_auc_3: 0.7156\n",
            "Epoch 41/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4487 - mean_squared_error: 0.1432 - recall_3: 0.9841 - precision_3: 0.8115 - accuracy: 0.8044 - auc_3: 0.7212 - val_loss: 0.4526 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9880 - val_precision_3: 0.8077 - val_accuracy: 0.8023 - val_auc_3: 0.7151\n",
            "Epoch 42/50\n",
            "789/789 [==============================] - 27s 34ms/step - loss: 0.4486 - mean_squared_error: 0.1432 - recall_3: 0.9842 - precision_3: 0.8116 - accuracy: 0.8046 - auc_3: 0.7213 - val_loss: 0.4530 - val_mean_squared_error: 0.1449 - val_recall_3: 0.9880 - val_precision_3: 0.8076 - val_accuracy: 0.8021 - val_auc_3: 0.7150\n",
            "Epoch 43/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4485 - mean_squared_error: 0.1431 - recall_3: 0.9841 - precision_3: 0.8118 - accuracy: 0.8047 - auc_3: 0.7215 - val_loss: 0.4526 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9829 - val_precision_3: 0.8102 - val_accuracy: 0.8022 - val_auc_3: 0.7155\n",
            "Epoch 44/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4484 - mean_squared_error: 0.1431 - recall_3: 0.9840 - precision_3: 0.8119 - accuracy: 0.8047 - auc_3: 0.7216 - val_loss: 0.4526 - val_mean_squared_error: 0.1447 - val_recall_3: 0.9843 - val_precision_3: 0.8095 - val_accuracy: 0.8022 - val_auc_3: 0.7152\n",
            "Epoch 45/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4484 - mean_squared_error: 0.1431 - recall_3: 0.9842 - precision_3: 0.8117 - accuracy: 0.8047 - auc_3: 0.7218 - val_loss: 0.4526 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9846 - val_precision_3: 0.8093 - val_accuracy: 0.8021 - val_auc_3: 0.7153\n",
            "Epoch 46/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4482 - mean_squared_error: 0.1431 - recall_3: 0.9838 - precision_3: 0.8118 - accuracy: 0.8046 - auc_3: 0.7219 - val_loss: 0.4528 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9852 - val_precision_3: 0.8090 - val_accuracy: 0.8021 - val_auc_3: 0.7148\n",
            "Epoch 47/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4483 - mean_squared_error: 0.1431 - recall_3: 0.9843 - precision_3: 0.8117 - accuracy: 0.8048 - auc_3: 0.7217 - val_loss: 0.4528 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9831 - val_precision_3: 0.8099 - val_accuracy: 0.8020 - val_auc_3: 0.7150\n",
            "Epoch 48/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4481 - mean_squared_error: 0.1430 - recall_3: 0.9837 - precision_3: 0.8120 - accuracy: 0.8048 - auc_3: 0.7222 - val_loss: 0.4529 - val_mean_squared_error: 0.1449 - val_recall_3: 0.9888 - val_precision_3: 0.8072 - val_accuracy: 0.8022 - val_auc_3: 0.7149\n",
            "Epoch 49/50\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4480 - mean_squared_error: 0.1430 - recall_3: 0.9835 - precision_3: 0.8121 - accuracy: 0.8047 - auc_3: 0.7222 - val_loss: 0.4527 - val_mean_squared_error: 0.1448 - val_recall_3: 0.9876 - val_precision_3: 0.8078 - val_accuracy: 0.8022 - val_auc_3: 0.7150\n",
            "Epoch 50/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4482 - mean_squared_error: 0.1430 - recall_3: 0.9838 - precision_3: 0.8120 - accuracy: 0.8048 - auc_3: 0.7220 - val_loss: 0.4530 - val_mean_squared_error: 0.1449 - val_recall_3: 0.9874 - val_precision_3: 0.8080 - val_accuracy: 0.8023 - val_auc_3: 0.7149\n",
            "{'loss': [0.4671173691749573, 0.4578240215778351, 0.45654845237731934, 0.45569175481796265, 0.4549773335456848, 0.45440542697906494, 0.4540467858314514, 0.45345914363861084, 0.4532459080219269, 0.45266813039779663, 0.4524058699607849, 0.4520568549633026, 0.45194387435913086, 0.45175784826278687, 0.45143651962280273, 0.4512925446033478, 0.4510798156261444, 0.45094773173332214, 0.4507569372653961, 0.4506233334541321, 0.4504554569721222, 0.450238972902298, 0.4501737654209137, 0.4500383138656616, 0.45001524686813354, 0.4500158727169037, 0.4497438073158264, 0.4495909810066223, 0.44951942563056946, 0.44942381978034973, 0.44936344027519226, 0.4492960274219513, 0.44912269711494446, 0.44901251792907715, 0.4490479528903961, 0.44884616136550903, 0.44895628094673157, 0.44896095991134644, 0.4488101303577423, 0.44871774315834045, 0.4486837089061737, 0.4486302435398102, 0.44847413897514343, 0.44842904806137085, 0.44838181138038635, 0.4482494294643402, 0.44831475615501404, 0.4480631351470947, 0.4480265974998474, 0.44815412163734436], 'mean_squared_error': [0.14968356490135193, 0.14632944762706757, 0.1459226757287979, 0.14562055468559265, 0.14540454745292664, 0.14521190524101257, 0.14506077766418457, 0.14490999281406403, 0.14481231570243835, 0.1446256935596466, 0.14455561339855194, 0.14441706240177155, 0.1443585455417633, 0.14431293308734894, 0.1441948562860489, 0.14415839314460754, 0.14406590163707733, 0.1440276950597763, 0.1439453810453415, 0.1438954770565033, 0.14386382699012756, 0.14379018545150757, 0.14375126361846924, 0.1437011957168579, 0.1436980664730072, 0.14369332790374756, 0.1435948759317398, 0.14355197548866272, 0.14351175725460052, 0.14349465072155, 0.14347046613693237, 0.14343829452991486, 0.14339885115623474, 0.14333978295326233, 0.14333127439022064, 0.14326302707195282, 0.14331969618797302, 0.14331358671188354, 0.14327295124530792, 0.1432289481163025, 0.1432276964187622, 0.14318858087062836, 0.14313367009162903, 0.1431124061346054, 0.14311672747135162, 0.14307089149951935, 0.14308872818946838, 0.14300194382667542, 0.14299242198467255, 0.1430239975452423], 'recall_3': [0.9922535419464111, 0.9909927845001221, 0.9899193048477173, 0.9883987903594971, 0.9881529808044434, 0.9878435134887695, 0.9877291321754456, 0.9875034093856812, 0.9872511029243469, 0.9870948791503906, 0.9866369962692261, 0.9865550994873047, 0.9867253303527832, 0.9862055778503418, 0.985721230506897, 0.9862148761749268, 0.9856687784194946, 0.9856361150741577, 0.9858155846595764, 0.9852511286735535, 0.9853301644325256, 0.9854430556297302, 0.9851644039154053, 0.9851691126823425, 0.9850360155105591, 0.9850672483444214, 0.9850006699562073, 0.9849571585655212, 0.984855055809021, 0.984844446182251, 0.9848195314407349, 0.9848798513412476, 0.9846400022506714, 0.9844575524330139, 0.9844298362731934, 0.9843783974647522, 0.9843491911888123, 0.9842936396598816, 0.9844683408737183, 0.9840198755264282, 0.9841002821922302, 0.984207034111023, 0.984125018119812, 0.9839779734611511, 0.9841775298118591, 0.983788013458252, 0.9843477010726929, 0.9836921691894531, 0.9835250377655029, 0.9837847352027893], 'precision_3': [0.8021488189697266, 0.8052520751953125, 0.8060263991355896, 0.8070244193077087, 0.8073083758354187, 0.8075200319290161, 0.807792067527771, 0.8079238533973694, 0.8081170320510864, 0.8084108829498291, 0.8085280656814575, 0.8089086413383484, 0.8090898990631104, 0.809356689453125, 0.8095867037773132, 0.8093674182891846, 0.8097946643829346, 0.8100311160087585, 0.8100706934928894, 0.8103505969047546, 0.8103228807449341, 0.8102547526359558, 0.8105548620223999, 0.8105881810188293, 0.810685396194458, 0.8106574416160583, 0.810764491558075, 0.8109797239303589, 0.8108763098716736, 0.8110092282295227, 0.8109817504882812, 0.8110774755477905, 0.8112280368804932, 0.8113769292831421, 0.8114188313484192, 0.8114750385284424, 0.8115262389183044, 0.8114686012268066, 0.8112500905990601, 0.8117272853851318, 0.8114931583404541, 0.8116385340690613, 0.8117724061012268, 0.811869204044342, 0.81172114610672, 0.8118428587913513, 0.8117290735244751, 0.8120412230491638, 0.8120762705802917, 0.8119965195655823], 'accuracy': [0.7979666590690613, 0.8010176420211792, 0.8013110160827637, 0.8016006350517273, 0.8017936944961548, 0.8018630146980286, 0.8021216988563538, 0.8021427392959595, 0.8022243976593018, 0.8024843335151672, 0.80234694480896, 0.8027554154396057, 0.8030759692192078, 0.8030809164047241, 0.8030636310577393, 0.803099513053894, 0.8032801747322083, 0.8035463094711304, 0.8037034869194031, 0.8036923408508301, 0.8037059903144836, 0.8036935925483704, 0.8038867115974426, 0.8039275407791138, 0.8039646744728088, 0.8039448857307434, 0.8040339946746826, 0.8042678833007812, 0.8040810227394104, 0.8042307496070862, 0.8041849732398987, 0.804337203502655, 0.8043718934059143, 0.804436206817627, 0.8044683933258057, 0.8045080304145813, 0.8045501112937927, 0.8044449090957642, 0.8042914271354675, 0.8045859932899475, 0.804355800151825, 0.8045946359634399, 0.8047048449516296, 0.8047308325767517, 0.8046776056289673, 0.8045786023139954, 0.804790198802948, 0.8047568202018738, 0.8046961426734924, 0.8047630190849304], 'auc_3': [0.6824483275413513, 0.7030103802680969, 0.7057595252990723, 0.7075275778770447, 0.7090997695922852, 0.7101847529411316, 0.7110350728034973, 0.7121748328208923, 0.7125980854034424, 0.7138157486915588, 0.7142864465713501, 0.7150172591209412, 0.7152372002601624, 0.7154797315597534, 0.7161985039710999, 0.7164266109466553, 0.716864287853241, 0.7169532775878906, 0.7173147201538086, 0.7175722122192383, 0.7177944183349609, 0.718481183052063, 0.7183992266654968, 0.7187743186950684, 0.7187010049819946, 0.7187333703041077, 0.7191821336746216, 0.7193688154220581, 0.7196829319000244, 0.7196879386901855, 0.7199649810791016, 0.7201213240623474, 0.7202670574188232, 0.7205620408058167, 0.7204182147979736, 0.7208741903305054, 0.7206567525863647, 0.7206736207008362, 0.7210690379142761, 0.7209999561309814, 0.721242368221283, 0.7212842702865601, 0.7214663028717041, 0.7216132879257202, 0.7218165397644043, 0.7219064235687256, 0.7217333316802979, 0.7221626043319702, 0.7222375273704529, 0.7220098376274109], 'val_loss': [0.4555020332336426, 0.4553072452545166, 0.453930139541626, 0.4538222849369049, 0.45323023200035095, 0.45294323563575745, 0.4532355070114136, 0.4527484178543091, 0.4529465436935425, 0.4531274139881134, 0.4528570771217346, 0.45234960317611694, 0.45294857025146484, 0.45216694474220276, 0.452371746301651, 0.4524634778499603, 0.45219266414642334, 0.45254528522491455, 0.4525148272514343, 0.4524986445903778, 0.45212602615356445, 0.45213213562965393, 0.45214521884918213, 0.45301952958106995, 0.452968567609787, 0.4523938000202179, 0.45250874757766724, 0.452306866645813, 0.4524366557598114, 0.4523354470729828, 0.4527449905872345, 0.45228615403175354, 0.4526154100894928, 0.4527963399887085, 0.4524511992931366, 0.45280903577804565, 0.45282745361328125, 0.4524180293083191, 0.4526938199996948, 0.4525983929634094, 0.45260074734687805, 0.45303744077682495, 0.4525814652442932, 0.45256057381629944, 0.4526255130767822, 0.45284155011177063, 0.4527636170387268, 0.452920526266098, 0.45268017053604126, 0.45303764939308167], 'val_mean_squared_error': [0.14575418829917908, 0.14569780230522156, 0.14520181715488434, 0.14517846703529358, 0.14496228098869324, 0.1448671668767929, 0.1448948085308075, 0.14478576183319092, 0.14482906460762024, 0.1448008269071579, 0.14477825164794922, 0.1446550339460373, 0.14482833445072174, 0.14459209144115448, 0.14465811848640442, 0.14467474818229675, 0.14460816979408264, 0.14466355741024017, 0.14463327825069427, 0.14467576146125793, 0.14457356929779053, 0.14456777274608612, 0.1445561796426773, 0.14480619132518768, 0.1448109745979309, 0.14466525614261627, 0.14472581446170807, 0.1446491777896881, 0.14465484023094177, 0.14464262127876282, 0.1448274403810501, 0.14461836218833923, 0.1447535902261734, 0.14482982456684113, 0.14469990134239197, 0.14482975006103516, 0.14485397934913635, 0.1446630358695984, 0.14477626979351044, 0.14471842348575592, 0.1447257101535797, 0.14485356211662292, 0.14475242793560028, 0.14473460614681244, 0.14475594460964203, 0.14481234550476074, 0.1447991281747818, 0.14486823976039886, 0.1447717845439911, 0.14485900104045868], 'val_recall_3': [0.9937461018562317, 0.9853050708770752, 0.9901432991027832, 0.9921952486038208, 0.9921535849571228, 0.9900779724121094, 0.9899852275848389, 0.9887270331382751, 0.9877381920814514, 0.9886574149131775, 0.9855231046676636, 0.9884437918663025, 0.9850449562072754, 0.9820877909660339, 0.9902917742729187, 0.9905330538749695, 0.9889407753944397, 0.9861644506454468, 0.98433518409729, 0.984994113445282, 0.9848408699035645, 0.9903104305267334, 0.9869258999824524, 0.9892375469207764, 0.9890845417976379, 0.9848223328590393, 0.9792368412017822, 0.9864473342895508, 0.9849849343299866, 0.9857093691825867, 0.9776211977005005, 0.9870829582214355, 0.9809965491294861, 0.9812431335449219, 0.9847530722618103, 0.9862940311431885, 0.9802027940750122, 0.9858856797218323, 0.9806159138679504, 0.9842559099197388, 0.9880353212356567, 0.9879934191703796, 0.9829325079917908, 0.9842744469642639, 0.9846319556236267, 0.9852174520492554, 0.9831369519233704, 0.9888291954994202, 0.9875845909118652, 0.9873573780059814], 'val_precision_3': [0.8034933805465698, 0.8080163598060608, 0.8059819340705872, 0.8051465153694153, 0.8053454160690308, 0.8064548969268799, 0.8065125942230225, 0.8074758648872375, 0.8076006174087524, 0.8075346350669861, 0.8089423179626465, 0.8073663115501404, 0.8091704249382019, 0.810748815536499, 0.8064921498298645, 0.8063810467720032, 0.8072851300239563, 0.8087628483772278, 0.8098219037055969, 0.809216320514679, 0.8094207644462585, 0.8067429661750793, 0.8083169460296631, 0.8069117069244385, 0.8072131276130676, 0.8092957735061646, 0.8118910193443298, 0.8085512518882751, 0.8093281388282776, 0.8088255524635315, 0.8126888871192932, 0.8081102967262268, 0.8111081719398499, 0.8106942772865295, 0.8094577193260193, 0.8084334135055542, 0.8111887574195862, 0.8087631464004517, 0.8112682700157166, 0.8094727396965027, 0.8076810836791992, 0.8075542449951172, 0.8101859092712402, 0.8094803690910339, 0.8092559576034546, 0.8089824914932251, 0.8099318146705627, 0.8072402477264404, 0.807816743850708, 0.8080278635025024], 'val_accuracy': [0.8006252646446228, 0.8010225296020508, 0.8014941215515137, 0.8017206192016602, 0.8019359707832336, 0.8020362257957458, 0.8020473718643188, 0.8024521470069885, 0.8019990921020508, 0.8024818301200867, 0.8022738695144653, 0.802147626876831, 0.8022553324699402, 0.8023332953453064, 0.8022070527076721, 0.8022218942642212, 0.8023481369018555, 0.8024410009384155, 0.802596926689148, 0.8022776246070862, 0.8024298548698425, 0.8025226593017578, 0.8023667335510254, 0.8020808100700378, 0.8023518919944763, 0.8022664785385132, 0.8019471168518066, 0.8023629784584045, 0.8024038672447205, 0.8022404909133911, 0.8018988370895386, 0.8022255897521973, 0.8020956516265869, 0.8017466068267822, 0.8024150133132935, 0.8021290898323059, 0.801702082157135, 0.8022738695144653, 0.8020510673522949, 0.802132785320282, 0.8022776246070862, 0.802099347114563, 0.80218106508255, 0.802155077457428, 0.8021031022071838, 0.8021253347396851, 0.8019990921020508, 0.8022293448448181, 0.8021736145019531, 0.8022850155830383], 'val_auc_3': [0.7097710967063904, 0.7113233804702759, 0.7126536965370178, 0.7131422162055969, 0.7142937779426575, 0.7146924734115601, 0.7151439189910889, 0.7151413559913635, 0.7150129079818726, 0.7155871391296387, 0.7155422568321228, 0.7158244252204895, 0.7148653268814087, 0.7159172296524048, 0.7162162661552429, 0.7160484194755554, 0.7162067294120789, 0.715823769569397, 0.7160204648971558, 0.7159580588340759, 0.7161905765533447, 0.7161405682563782, 0.7158012390136719, 0.7162878513336182, 0.7159535884857178, 0.7154766917228699, 0.7155670523643494, 0.7157034873962402, 0.7155843377113342, 0.7155861258506775, 0.715715765953064, 0.7157277464866638, 0.7152546644210815, 0.7151045203208923, 0.7153844237327576, 0.7154161334037781, 0.7153956890106201, 0.7155333757400513, 0.7151541113853455, 0.7156276702880859, 0.7150754928588867, 0.7150378227233887, 0.7155044078826904, 0.7151768207550049, 0.7152862548828125, 0.7148110866546631, 0.7150149345397949, 0.714943528175354, 0.7149946093559265, 0.7149179577827454]}\n",
            "  1/263 [..............................] - ETA: 1:00 - loss: 0.4535 - mean_squared_error: 0.1445 - recall_3: 0.9855 - precision_3: 0.8158 - accuracy: 0.8086 - auc_3: 0.6906WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0237s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0237s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "263/263 [==============================] - 7s 25ms/step - loss: 0.4523 - mean_squared_error: 0.1445 - recall_3: 0.9871 - precision_3: 0.8091 - accuracy: 0.8032 - auc_3: 0.7137\n",
            "test loss, test acc: [0.4523073434829712, 0.14448311924934387, 0.9870915412902832, 0.8091193437576294, 0.8031761646270752, 0.7136754989624023]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8ri-9jnJlW3",
        "outputId": "cc041536-c454-4643-b92f-05f1548de4c2"
      },
      "source": [
        "name='cs' \n",
        "use_masking=False\n",
        "batch_size = batch_size\n",
        "epochs = 50\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "print(loans_raw)\n",
        "data = loans_raw\n",
        "#data = data[data.loan_status != 'Current']\n",
        "#data = data[data.loan_status != 'In Grace Period']\n",
        "data = data[data.loan_status == 'Charged Off']\n",
        "data = data[data.loan_status == 'Fully Paid']\n",
        "#print(data.loc[:100])\n",
        "train_dataset = pre_dataset(data[:1000],32)\n",
        "print(train_dataset)\n",
        "\"\"\"\n",
        "\n",
        "#train_dataset = pre_dataset(train, batch_size)\n",
        "#vali_dataset = pre_dataset(vali, 1)\n",
        "#with tf.device('/device:GPU:0'):\n",
        "with strategy.scope():\n",
        "  original_inputs = Input(shape=(83,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "  \n",
        "  #output_inputw = inputW(32)(original_inputs)\n",
        "  #output = Flatten()(output_inputw)\n",
        "  #output = Dense(32, activation='relu')(output)\n",
        "  output = Dense(83, activation='relu')(original_inputs)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(512, activation='relu')(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(256, activation='relu')(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(64, activation='relu')(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(24, activation='relu')(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(1, activation='sigmoid')(output)\n",
        "  cs_p2p_mlp = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "  cs_p2p_mlp.summary()\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "  cs_p2p_mlp.compile(optimizer, \n",
        "                 loss=tf.keras.losses.BinaryCrossentropy()\n",
        "                 #loss=tf.keras.metrics.MeanSquaredError()\n",
        "          , metrics=[\n",
        "          tf.keras.metrics.MeanSquaredError(),\n",
        "          #tf.keras.metrics.Accuracy(),\n",
        "          tf.keras.metrics.Recall(),\n",
        "          tf.keras.metrics.Precision(),\n",
        "          'accuracy',\n",
        "          tf.keras.metrics.AUC(),\n",
        "      ])\n",
        "  #cs_p2p.fit(train_dataset, epochs=3, batch_size=32, validation_data=vali_dataset,validation_batch_size=4)\n",
        "  #history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset)\n",
        "  history = cs_p2p_mlp.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset, validation_batch_size=batch_size)\n",
        "  print(history.history)\n",
        "  results = cs_p2p_mlp.evaluate(test_dataset, batch_size=batch_size)\n",
        "  print(\"test loss, test acc:\", results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cs_p2p\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cs_p2p (InputLayer)          [(128, 83)]               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (128, 83)                 6972      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (128, 83)                 0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (128, 512)                43008     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (128, 512)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (128, 256)                131328    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (128, 256)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (128, 64)                 16448     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (128, 64)                 0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (128, 24)                 1560      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (128, 24)                 0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (128, 1)                  25        \n",
            "=================================================================\n",
            "Total params: 199,341\n",
            "Trainable params: 199,341\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "  1/789 [..............................] - ETA: 11:47 - loss: 0.6767 - mean_squared_error: 0.2418 - recall_1: 0.9034 - precision_1: 0.8060 - accuracy: 0.7461 - auc_1: 0.4985WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0247s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0247s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "789/789 [==============================] - ETA: 0s - loss: 0.4638 - mean_squared_error: 0.1483 - recall_1: 0.9977 - precision_1: 0.8013 - accuracy: 0.8002 - auc_1: 0.6897WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0118s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0118s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r789/789 [==============================] - 24s 31ms/step - loss: 0.4638 - mean_squared_error: 0.1483 - recall_1: 0.9977 - precision_1: 0.8013 - accuracy: 0.8002 - auc_1: 0.6897 - val_loss: 0.4555 - val_mean_squared_error: 0.1458 - val_recall_1: 0.9950 - val_precision_1: 0.8029 - val_accuracy: 0.8006 - val_auc_1: 0.7095\n",
            "Epoch 2/50\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4579 - mean_squared_error: 0.1463 - recall_1: 0.9910 - precision_1: 0.8052 - accuracy: 0.8010 - auc_1: 0.7029 - val_loss: 0.4547 - val_mean_squared_error: 0.1454 - val_recall_1: 0.9851 - val_precision_1: 0.8081 - val_accuracy: 0.8009 - val_auc_1: 0.7114\n",
            "Epoch 3/50\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4567 - mean_squared_error: 0.1459 - recall_1: 0.9902 - precision_1: 0.8059 - accuracy: 0.8013 - auc_1: 0.7056 - val_loss: 0.4542 - val_mean_squared_error: 0.1453 - val_recall_1: 0.9817 - val_precision_1: 0.8100 - val_accuracy: 0.8012 - val_auc_1: 0.7123\n",
            "Epoch 4/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4558 - mean_squared_error: 0.1456 - recall_1: 0.9899 - precision_1: 0.8062 - accuracy: 0.8015 - auc_1: 0.7074 - val_loss: 0.4538 - val_mean_squared_error: 0.1451 - val_recall_1: 0.9849 - val_precision_1: 0.8087 - val_accuracy: 0.8016 - val_auc_1: 0.7131\n",
            "Epoch 5/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4549 - mean_squared_error: 0.1454 - recall_1: 0.9894 - precision_1: 0.8065 - accuracy: 0.8016 - auc_1: 0.7093 - val_loss: 0.4536 - val_mean_squared_error: 0.1451 - val_recall_1: 0.9980 - val_precision_1: 0.8013 - val_accuracy: 0.8005 - val_auc_1: 0.7139\n",
            "Epoch 6/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4544 - mean_squared_error: 0.1452 - recall_1: 0.9893 - precision_1: 0.8067 - accuracy: 0.8017 - auc_1: 0.7105 - val_loss: 0.4539 - val_mean_squared_error: 0.1452 - val_recall_1: 0.9900 - val_precision_1: 0.8063 - val_accuracy: 0.8017 - val_auc_1: 0.7136\n",
            "Epoch 7/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4538 - mean_squared_error: 0.1450 - recall_1: 0.9888 - precision_1: 0.8071 - accuracy: 0.8020 - auc_1: 0.7116 - val_loss: 0.4537 - val_mean_squared_error: 0.1451 - val_recall_1: 0.9932 - val_precision_1: 0.8046 - val_accuracy: 0.8017 - val_auc_1: 0.7143\n",
            "Epoch 8/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4532 - mean_squared_error: 0.1448 - recall_1: 0.9883 - precision_1: 0.8075 - accuracy: 0.8021 - auc_1: 0.7127 - val_loss: 0.4529 - val_mean_squared_error: 0.1448 - val_recall_1: 0.9882 - val_precision_1: 0.8075 - val_accuracy: 0.8021 - val_auc_1: 0.7146\n",
            "Epoch 9/50\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4528 - mean_squared_error: 0.1447 - recall_1: 0.9881 - precision_1: 0.8078 - accuracy: 0.8023 - auc_1: 0.7137 - val_loss: 0.4527 - val_mean_squared_error: 0.1447 - val_recall_1: 0.9882 - val_precision_1: 0.8079 - val_accuracy: 0.8026 - val_auc_1: 0.7149\n",
            "Epoch 10/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4526 - mean_squared_error: 0.1446 - recall_1: 0.9874 - precision_1: 0.8082 - accuracy: 0.8024 - auc_1: 0.7140 - val_loss: 0.4526 - val_mean_squared_error: 0.1447 - val_recall_1: 0.9901 - val_precision_1: 0.8067 - val_accuracy: 0.8023 - val_auc_1: 0.7151\n",
            "Epoch 11/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4523 - mean_squared_error: 0.1445 - recall_1: 0.9871 - precision_1: 0.8085 - accuracy: 0.8026 - auc_1: 0.7146 - val_loss: 0.4526 - val_mean_squared_error: 0.1447 - val_recall_1: 0.9916 - val_precision_1: 0.8058 - val_accuracy: 0.8022 - val_auc_1: 0.7151\n",
            "Epoch 12/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4519 - mean_squared_error: 0.1444 - recall_1: 0.9867 - precision_1: 0.8088 - accuracy: 0.8028 - auc_1: 0.7153 - val_loss: 0.4527 - val_mean_squared_error: 0.1447 - val_recall_1: 0.9938 - val_precision_1: 0.8043 - val_accuracy: 0.8016 - val_auc_1: 0.7154\n",
            "Epoch 13/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4515 - mean_squared_error: 0.1442 - recall_1: 0.9864 - precision_1: 0.8089 - accuracy: 0.8027 - auc_1: 0.7163 - val_loss: 0.4527 - val_mean_squared_error: 0.1447 - val_recall_1: 0.9856 - val_precision_1: 0.8092 - val_accuracy: 0.8027 - val_auc_1: 0.7148\n",
            "Epoch 14/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4514 - mean_squared_error: 0.1442 - recall_1: 0.9862 - precision_1: 0.8092 - accuracy: 0.8029 - auc_1: 0.7164 - val_loss: 0.4526 - val_mean_squared_error: 0.1447 - val_recall_1: 0.9797 - val_precision_1: 0.8121 - val_accuracy: 0.8024 - val_auc_1: 0.7151\n",
            "Epoch 15/50\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4512 - mean_squared_error: 0.1441 - recall_1: 0.9862 - precision_1: 0.8094 - accuracy: 0.8032 - auc_1: 0.7169 - val_loss: 0.4526 - val_mean_squared_error: 0.1447 - val_recall_1: 0.9931 - val_precision_1: 0.8047 - val_accuracy: 0.8017 - val_auc_1: 0.7151\n",
            "Epoch 16/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4509 - mean_squared_error: 0.1440 - recall_1: 0.9864 - precision_1: 0.8092 - accuracy: 0.8030 - auc_1: 0.7175 - val_loss: 0.4534 - val_mean_squared_error: 0.1449 - val_recall_1: 0.9929 - val_precision_1: 0.8051 - val_accuracy: 0.8021 - val_auc_1: 0.7147\n",
            "Epoch 17/50\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4506 - mean_squared_error: 0.1439 - recall_1: 0.9859 - precision_1: 0.8096 - accuracy: 0.8032 - auc_1: 0.7181 - val_loss: 0.4536 - val_mean_squared_error: 0.1451 - val_recall_1: 0.9757 - val_precision_1: 0.8135 - val_accuracy: 0.8017 - val_auc_1: 0.7144\n",
            "Epoch 18/50\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4503 - mean_squared_error: 0.1438 - recall_1: 0.9858 - precision_1: 0.8097 - accuracy: 0.8033 - auc_1: 0.7185 - val_loss: 0.4528 - val_mean_squared_error: 0.1447 - val_recall_1: 0.9854 - val_precision_1: 0.8094 - val_accuracy: 0.8027 - val_auc_1: 0.7148\n",
            "Epoch 19/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4502 - mean_squared_error: 0.1438 - recall_1: 0.9855 - precision_1: 0.8099 - accuracy: 0.8033 - auc_1: 0.7189 - val_loss: 0.4535 - val_mean_squared_error: 0.1449 - val_recall_1: 0.9910 - val_precision_1: 0.8061 - val_accuracy: 0.8021 - val_auc_1: 0.7140\n",
            "Epoch 20/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4499 - mean_squared_error: 0.1437 - recall_1: 0.9857 - precision_1: 0.8100 - accuracy: 0.8036 - auc_1: 0.7195 - val_loss: 0.4529 - val_mean_squared_error: 0.1448 - val_recall_1: 0.9885 - val_precision_1: 0.8076 - val_accuracy: 0.8024 - val_auc_1: 0.7146\n",
            "Epoch 21/50\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4497 - mean_squared_error: 0.1436 - recall_1: 0.9856 - precision_1: 0.8101 - accuracy: 0.8036 - auc_1: 0.7199 - val_loss: 0.4535 - val_mean_squared_error: 0.1450 - val_recall_1: 0.9924 - val_precision_1: 0.8053 - val_accuracy: 0.8021 - val_auc_1: 0.7135\n",
            "Epoch 22/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4494 - mean_squared_error: 0.1435 - recall_1: 0.9856 - precision_1: 0.8102 - accuracy: 0.8037 - auc_1: 0.7205 - val_loss: 0.4531 - val_mean_squared_error: 0.1448 - val_recall_1: 0.9874 - val_precision_1: 0.8082 - val_accuracy: 0.8025 - val_auc_1: 0.7140\n",
            "Epoch 23/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4491 - mean_squared_error: 0.1434 - recall_1: 0.9850 - precision_1: 0.8105 - accuracy: 0.8037 - auc_1: 0.7209 - val_loss: 0.4531 - val_mean_squared_error: 0.1448 - val_recall_1: 0.9866 - val_precision_1: 0.8085 - val_accuracy: 0.8024 - val_auc_1: 0.7137\n",
            "Epoch 24/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4490 - mean_squared_error: 0.1434 - recall_1: 0.9849 - precision_1: 0.8104 - accuracy: 0.8036 - auc_1: 0.7213 - val_loss: 0.4536 - val_mean_squared_error: 0.1450 - val_recall_1: 0.9935 - val_precision_1: 0.8046 - val_accuracy: 0.8018 - val_auc_1: 0.7134\n",
            "Epoch 25/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4487 - mean_squared_error: 0.1433 - recall_1: 0.9849 - precision_1: 0.8105 - accuracy: 0.8037 - auc_1: 0.7219 - val_loss: 0.4537 - val_mean_squared_error: 0.1450 - val_recall_1: 0.9833 - val_precision_1: 0.8101 - val_accuracy: 0.8023 - val_auc_1: 0.7131\n",
            "Epoch 26/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4485 - mean_squared_error: 0.1432 - recall_1: 0.9850 - precision_1: 0.8105 - accuracy: 0.8037 - auc_1: 0.7222 - val_loss: 0.4534 - val_mean_squared_error: 0.1449 - val_recall_1: 0.9816 - val_precision_1: 0.8110 - val_accuracy: 0.8023 - val_auc_1: 0.7135\n",
            "Epoch 27/50\n",
            "789/789 [==============================] - 23s 30ms/step - loss: 0.4481 - mean_squared_error: 0.1430 - recall_1: 0.9848 - precision_1: 0.8111 - accuracy: 0.8043 - auc_1: 0.7229 - val_loss: 0.4534 - val_mean_squared_error: 0.1450 - val_recall_1: 0.9819 - val_precision_1: 0.8105 - val_accuracy: 0.8019 - val_auc_1: 0.7134\n",
            "Epoch 28/50\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4479 - mean_squared_error: 0.1430 - recall_1: 0.9844 - precision_1: 0.8111 - accuracy: 0.8040 - auc_1: 0.7234 - val_loss: 0.4533 - val_mean_squared_error: 0.1449 - val_recall_1: 0.9884 - val_precision_1: 0.8074 - val_accuracy: 0.8021 - val_auc_1: 0.7132\n",
            "Epoch 29/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4477 - mean_squared_error: 0.1429 - recall_1: 0.9844 - precision_1: 0.8112 - accuracy: 0.8042 - auc_1: 0.7237 - val_loss: 0.4541 - val_mean_squared_error: 0.1452 - val_recall_1: 0.9915 - val_precision_1: 0.8055 - val_accuracy: 0.8017 - val_auc_1: 0.7124\n",
            "Epoch 30/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4474 - mean_squared_error: 0.1428 - recall_1: 0.9840 - precision_1: 0.8114 - accuracy: 0.8042 - auc_1: 0.7241 - val_loss: 0.4539 - val_mean_squared_error: 0.1452 - val_recall_1: 0.9796 - val_precision_1: 0.8115 - val_accuracy: 0.8017 - val_auc_1: 0.7129\n",
            "Epoch 31/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4472 - mean_squared_error: 0.1427 - recall_1: 0.9842 - precision_1: 0.8114 - accuracy: 0.8043 - auc_1: 0.7246 - val_loss: 0.4540 - val_mean_squared_error: 0.1451 - val_recall_1: 0.9861 - val_precision_1: 0.8087 - val_accuracy: 0.8023 - val_auc_1: 0.7121\n",
            "Epoch 32/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4470 - mean_squared_error: 0.1427 - recall_1: 0.9839 - precision_1: 0.8118 - accuracy: 0.8046 - auc_1: 0.7251 - val_loss: 0.4540 - val_mean_squared_error: 0.1451 - val_recall_1: 0.9898 - val_precision_1: 0.8066 - val_accuracy: 0.8021 - val_auc_1: 0.7125\n",
            "Epoch 33/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4466 - mean_squared_error: 0.1425 - recall_1: 0.9836 - precision_1: 0.8120 - accuracy: 0.8047 - auc_1: 0.7258 - val_loss: 0.4545 - val_mean_squared_error: 0.1452 - val_recall_1: 0.9879 - val_precision_1: 0.8075 - val_accuracy: 0.8020 - val_auc_1: 0.7125\n",
            "Epoch 34/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4463 - mean_squared_error: 0.1424 - recall_1: 0.9839 - precision_1: 0.8119 - accuracy: 0.8047 - auc_1: 0.7264 - val_loss: 0.4549 - val_mean_squared_error: 0.1455 - val_recall_1: 0.9796 - val_precision_1: 0.8116 - val_accuracy: 0.8019 - val_auc_1: 0.7106\n",
            "Epoch 35/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4460 - mean_squared_error: 0.1423 - recall_1: 0.9834 - precision_1: 0.8124 - accuracy: 0.8050 - auc_1: 0.7270 - val_loss: 0.4543 - val_mean_squared_error: 0.1452 - val_recall_1: 0.9864 - val_precision_1: 0.8084 - val_accuracy: 0.8022 - val_auc_1: 0.7114\n",
            "Epoch 36/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4458 - mean_squared_error: 0.1423 - recall_1: 0.9835 - precision_1: 0.8121 - accuracy: 0.8047 - auc_1: 0.7273 - val_loss: 0.4551 - val_mean_squared_error: 0.1456 - val_recall_1: 0.9832 - val_precision_1: 0.8094 - val_accuracy: 0.8013 - val_auc_1: 0.7107\n",
            "Epoch 37/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4455 - mean_squared_error: 0.1422 - recall_1: 0.9831 - precision_1: 0.8123 - accuracy: 0.8048 - auc_1: 0.7279 - val_loss: 0.4547 - val_mean_squared_error: 0.1453 - val_recall_1: 0.9874 - val_precision_1: 0.8079 - val_accuracy: 0.8021 - val_auc_1: 0.7108\n",
            "Epoch 38/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4452 - mean_squared_error: 0.1421 - recall_1: 0.9828 - precision_1: 0.8127 - accuracy: 0.8050 - auc_1: 0.7284 - val_loss: 0.4550 - val_mean_squared_error: 0.1454 - val_recall_1: 0.9784 - val_precision_1: 0.8118 - val_accuracy: 0.8014 - val_auc_1: 0.7112\n",
            "Epoch 39/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4448 - mean_squared_error: 0.1419 - recall_1: 0.9828 - precision_1: 0.8129 - accuracy: 0.8052 - auc_1: 0.7292 - val_loss: 0.4549 - val_mean_squared_error: 0.1455 - val_recall_1: 0.9815 - val_precision_1: 0.8106 - val_accuracy: 0.8017 - val_auc_1: 0.7109\n",
            "Epoch 40/50\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4445 - mean_squared_error: 0.1418 - recall_1: 0.9823 - precision_1: 0.8132 - accuracy: 0.8052 - auc_1: 0.7297 - val_loss: 0.4553 - val_mean_squared_error: 0.1456 - val_recall_1: 0.9823 - val_precision_1: 0.8099 - val_accuracy: 0.8015 - val_auc_1: 0.7103\n",
            "Epoch 41/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4444 - mean_squared_error: 0.1418 - recall_1: 0.9823 - precision_1: 0.8134 - accuracy: 0.8055 - auc_1: 0.7297 - val_loss: 0.4553 - val_mean_squared_error: 0.1456 - val_recall_1: 0.9903 - val_precision_1: 0.8061 - val_accuracy: 0.8017 - val_auc_1: 0.7096\n",
            "Epoch 42/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4441 - mean_squared_error: 0.1416 - recall_1: 0.9821 - precision_1: 0.8137 - accuracy: 0.8057 - auc_1: 0.7305 - val_loss: 0.4556 - val_mean_squared_error: 0.1457 - val_recall_1: 0.9813 - val_precision_1: 0.8105 - val_accuracy: 0.8015 - val_auc_1: 0.7096\n",
            "Epoch 43/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4437 - mean_squared_error: 0.1415 - recall_1: 0.9820 - precision_1: 0.8137 - accuracy: 0.8056 - auc_1: 0.7310 - val_loss: 0.4556 - val_mean_squared_error: 0.1456 - val_recall_1: 0.9845 - val_precision_1: 0.8088 - val_accuracy: 0.8014 - val_auc_1: 0.7091\n",
            "Epoch 44/50\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4435 - mean_squared_error: 0.1414 - recall_1: 0.9820 - precision_1: 0.8138 - accuracy: 0.8058 - auc_1: 0.7315 - val_loss: 0.4552 - val_mean_squared_error: 0.1455 - val_recall_1: 0.9864 - val_precision_1: 0.8081 - val_accuracy: 0.8017 - val_auc_1: 0.7098\n",
            "Epoch 45/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4432 - mean_squared_error: 0.1414 - recall_1: 0.9820 - precision_1: 0.8138 - accuracy: 0.8058 - auc_1: 0.7321 - val_loss: 0.4560 - val_mean_squared_error: 0.1458 - val_recall_1: 0.9786 - val_precision_1: 0.8114 - val_accuracy: 0.8010 - val_auc_1: 0.7101\n",
            "Epoch 46/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4431 - mean_squared_error: 0.1413 - recall_1: 0.9817 - precision_1: 0.8140 - accuracy: 0.8058 - auc_1: 0.7323 - val_loss: 0.4560 - val_mean_squared_error: 0.1458 - val_recall_1: 0.9826 - val_precision_1: 0.8095 - val_accuracy: 0.8011 - val_auc_1: 0.7089\n",
            "Epoch 47/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4428 - mean_squared_error: 0.1412 - recall_1: 0.9818 - precision_1: 0.8140 - accuracy: 0.8059 - auc_1: 0.7328 - val_loss: 0.4561 - val_mean_squared_error: 0.1458 - val_recall_1: 0.9840 - val_precision_1: 0.8087 - val_accuracy: 0.8011 - val_auc_1: 0.7084\n",
            "Epoch 48/50\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4424 - mean_squared_error: 0.1411 - recall_1: 0.9818 - precision_1: 0.8141 - accuracy: 0.8061 - auc_1: 0.7334 - val_loss: 0.4562 - val_mean_squared_error: 0.1458 - val_recall_1: 0.9835 - val_precision_1: 0.8094 - val_accuracy: 0.8016 - val_auc_1: 0.7084\n",
            "Epoch 49/50\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4423 - mean_squared_error: 0.1410 - recall_1: 0.9813 - precision_1: 0.8145 - accuracy: 0.8063 - auc_1: 0.7336 - val_loss: 0.4565 - val_mean_squared_error: 0.1459 - val_recall_1: 0.9840 - val_precision_1: 0.8085 - val_accuracy: 0.8009 - val_auc_1: 0.7084\n",
            "Epoch 50/50\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4421 - mean_squared_error: 0.1410 - recall_1: 0.9816 - precision_1: 0.8144 - accuracy: 0.8063 - auc_1: 0.7340 - val_loss: 0.4566 - val_mean_squared_error: 0.1459 - val_recall_1: 0.9818 - val_precision_1: 0.8102 - val_accuracy: 0.8014 - val_auc_1: 0.7085\n",
            "{'loss': [0.463823527097702, 0.4579308331012726, 0.45670703053474426, 0.45581579208374023, 0.45493605732917786, 0.45436444878578186, 0.453804612159729, 0.4532424509525299, 0.45280975103378296, 0.4526281952857971, 0.4523448050022125, 0.45189493894577026, 0.4514888525009155, 0.45137763023376465, 0.4511573910713196, 0.45087265968322754, 0.45059269666671753, 0.4503389894962311, 0.4501768946647644, 0.4498744308948517, 0.4496593475341797, 0.44935327768325806, 0.44914114475250244, 0.448971152305603, 0.44872400164604187, 0.44849827885627747, 0.44810107350349426, 0.4479391872882843, 0.4476722776889801, 0.4474368393421173, 0.4471859931945801, 0.44699472188949585, 0.44660162925720215, 0.44629958271980286, 0.4459691047668457, 0.44584962725639343, 0.44552284479141235, 0.4452299475669861, 0.4448256492614746, 0.4445495903491974, 0.4444258511066437, 0.4440847635269165, 0.44373881816864014, 0.44346898794174194, 0.4432232975959778, 0.4430547058582306, 0.4427570402622223, 0.44240739941596985, 0.442295640707016, 0.44209611415863037], 'mean_squared_error': [0.14829348027706146, 0.14633424580097198, 0.14591661095619202, 0.14563217759132385, 0.14536112546920776, 0.1451747864484787, 0.14500463008880615, 0.14482922852039337, 0.14468052983283997, 0.14459986984729767, 0.14448656141757965, 0.1443670690059662, 0.14421774446964264, 0.14415761828422546, 0.14408841729164124, 0.14398884773254395, 0.14390479028224945, 0.14382274448871613, 0.14376389980316162, 0.14366194605827332, 0.14359541237354279, 0.1434914767742157, 0.14340445399284363, 0.14336222410202026, 0.14325706660747528, 0.14319677650928497, 0.14303258061408997, 0.1429997831583023, 0.14290925860404968, 0.14281535148620605, 0.14273881912231445, 0.14266476035118103, 0.14253778755664825, 0.14243194460868835, 0.14229989051818848, 0.14228880405426025, 0.14216147363185883, 0.14205658435821533, 0.1419135481119156, 0.1418399065732956, 0.14177972078323364, 0.1416400820016861, 0.14154092967510223, 0.1414385735988617, 0.14135755598545074, 0.1412891000509262, 0.1412290781736374, 0.1410866528749466, 0.1410181075334549, 0.14095407724380493], 'recall_1': [0.9976736307144165, 0.9909990429878235, 0.9901793003082275, 0.9898962378501892, 0.9894275665283203, 0.989329993724823, 0.9888228178024292, 0.9882579445838928, 0.9880709052085876, 0.9874289035797119, 0.9870671033859253, 0.9866756200790405, 0.9864190816879272, 0.9861978888511658, 0.9861993789672852, 0.9863957166671753, 0.9859007000923157, 0.9858189225196838, 0.9855356216430664, 0.9857274293899536, 0.9855635762214661, 0.985568106174469, 0.9849509596824646, 0.9848875999450684, 0.9849326014518738, 0.9849957823753357, 0.9847718477249146, 0.9844388961791992, 0.9844360947608948, 0.9840322136878967, 0.9842226505279541, 0.983874499797821, 0.983597457408905, 0.9838743805885315, 0.9833765029907227, 0.9834706783294678, 0.9831365942955017, 0.9828253984451294, 0.9827590584754944, 0.9823136329650879, 0.9823229312896729, 0.9820567965507507, 0.981951892375946, 0.9819890260696411, 0.9819562435150146, 0.9816780686378479, 0.9817860722541809, 0.9817925095558167, 0.9813470840454102, 0.9816422462463379], 'precision_1': [0.8012887835502625, 0.8052495718002319, 0.8058823943138123, 0.8062250018119812, 0.8064797520637512, 0.8066623210906982, 0.8071415424346924, 0.8075327277183533, 0.8077965378761292, 0.8081980347633362, 0.8085204362869263, 0.808845579624176, 0.808928370475769, 0.809246301651001, 0.8094459772109985, 0.8092240691184998, 0.8096296191215515, 0.8097164034843445, 0.809913694858551, 0.8100154995918274, 0.810117781162262, 0.8102259039878845, 0.8105214834213257, 0.810445249080658, 0.8105403184890747, 0.8104878067970276, 0.811071515083313, 0.8110592365264893, 0.8112159371376038, 0.811428964138031, 0.811406672000885, 0.8117735385894775, 0.8119988441467285, 0.8118832111358643, 0.8123958110809326, 0.8121113777160645, 0.8123335242271423, 0.8127047419548035, 0.8128573298454285, 0.8131590485572815, 0.813366174697876, 0.8137120604515076, 0.8136729001998901, 0.813835084438324, 0.8137789368629456, 0.8139943480491638, 0.8140203952789307, 0.8141130208969116, 0.8145245909690857, 0.8144384622573853], 'accuracy': [0.8001648783683777, 0.8010176420211792, 0.8012911677360535, 0.8015375137329102, 0.8015622496604919, 0.8017268776893616, 0.8019979000091553, 0.8021328449249268, 0.8023370504379272, 0.8024323582649231, 0.8025994300842285, 0.8027541637420654, 0.8026947379112244, 0.8029423356056213, 0.8031849265098572, 0.8030388355255127, 0.803224503993988, 0.8032764792442322, 0.8033433556556702, 0.8035834431648254, 0.8036044836044312, 0.8037381768226624, 0.8037146329879761, 0.8035834431648254, 0.8037233352661133, 0.8037022948265076, 0.8042604923248291, 0.8040463328361511, 0.8042283058166504, 0.8042369484901428, 0.8043248653411865, 0.804551362991333, 0.8046516180038452, 0.8046838045120239, 0.8049870133399963, 0.8047072887420654, 0.8047667145729065, 0.8050204515457153, 0.8051590919494629, 0.8052432537078857, 0.805494487285614, 0.8057420253753662, 0.8056281805038452, 0.8058435320854187, 0.8057593703269958, 0.8058410286903381, 0.8059412837028503, 0.8060526847839355, 0.806264340877533, 0.8063472509384155], 'auc_1': [0.6896603107452393, 0.7028778791427612, 0.7055752277374268, 0.70740807056427, 0.7092973589897156, 0.7104724049568176, 0.7116118669509888, 0.7127171158790588, 0.7136693000793457, 0.7140101790428162, 0.7145639061927795, 0.7153236865997314, 0.7162668704986572, 0.7163926959037781, 0.7168573141098022, 0.7175493240356445, 0.7180982828140259, 0.7184873819351196, 0.7189276814460754, 0.7195463180541992, 0.7199113965034485, 0.7205414772033691, 0.7209180593490601, 0.7212936282157898, 0.7218650579452515, 0.7222254872322083, 0.7228518128395081, 0.7233632206916809, 0.7236618995666504, 0.7241232991218567, 0.7246274352073669, 0.7251120805740356, 0.7257897853851318, 0.726433277130127, 0.7269958257675171, 0.7272896766662598, 0.7278677225112915, 0.7284453511238098, 0.7292041778564453, 0.72967529296875, 0.7297171950340271, 0.7304531931877136, 0.7310283184051514, 0.7314989566802979, 0.7321137189865112, 0.7323306202888489, 0.7328107953071594, 0.7333518266677856, 0.7335573434829712, 0.7339638471603394], 'val_loss': [0.4555230736732483, 0.4547403156757355, 0.4542361795902252, 0.45381298661231995, 0.45364493131637573, 0.4539499282836914, 0.4536871016025543, 0.45293551683425903, 0.45266276597976685, 0.4526136517524719, 0.45264074206352234, 0.4526882469654083, 0.45269107818603516, 0.45263203978538513, 0.45262396335601807, 0.4534108638763428, 0.4536259174346924, 0.45276185870170593, 0.45349085330963135, 0.4529256224632263, 0.4534614086151123, 0.4530757963657379, 0.4530508816242218, 0.45357006788253784, 0.4536857306957245, 0.4533652365207672, 0.4534169137477875, 0.4532904326915741, 0.45408785343170166, 0.4539108872413635, 0.4540041983127594, 0.4539782404899597, 0.45451655983924866, 0.4549280107021332, 0.45433005690574646, 0.45507416129112244, 0.45466718077659607, 0.4550304114818573, 0.45487070083618164, 0.45532679557800293, 0.45533525943756104, 0.45556557178497314, 0.4555717706680298, 0.45521625876426697, 0.45602649450302124, 0.4559871554374695, 0.4561382830142975, 0.45620736479759216, 0.4564563035964966, 0.45660537481307983], 'val_mean_squared_error': [0.14575402438640594, 0.14544051885604858, 0.14529091119766235, 0.1451248824596405, 0.14511309564113617, 0.14515678584575653, 0.14509651064872742, 0.1448131501674652, 0.14471109211444855, 0.14470183849334717, 0.14470374584197998, 0.14474701881408691, 0.14472344517707825, 0.14472833275794983, 0.14473608136177063, 0.14493705332279205, 0.1451379358768463, 0.14473529160022736, 0.1449197232723236, 0.14478398859500885, 0.14499258995056152, 0.14480727910995483, 0.1448412835597992, 0.14503848552703857, 0.14501258730888367, 0.14492858946323395, 0.14498212933540344, 0.14490669965744019, 0.14518818259239197, 0.14517337083816528, 0.1450974941253662, 0.14508193731307983, 0.14523296058177948, 0.14545293152332306, 0.1452256590127945, 0.1455773562192917, 0.14529173076152802, 0.1454327255487442, 0.14548233151435852, 0.14561010897159576, 0.14556623995304108, 0.14565801620483398, 0.14563022553920746, 0.14547723531723022, 0.14576350152492523, 0.14579692482948303, 0.14584361016750336, 0.14583612978458405, 0.1459357887506485, 0.14589840173721313], 'val_recall_1': [0.9949623942375183, 0.9850916266441345, 0.9816837310791016, 0.9848779439926147, 0.9979943037033081, 0.9899712800979614, 0.9931749105453491, 0.9882346987724304, 0.9881792664527893, 0.9900782108306885, 0.9915870428085327, 0.9937600493431091, 0.98562091588974, 0.9797061681747437, 0.9931192994117737, 0.9928591847419739, 0.9757314324378967, 0.9853703379631042, 0.9909926056861877, 0.98848557472229, 0.9923577308654785, 0.9873573780059814, 0.9866467714309692, 0.9934674501419067, 0.9833458662033081, 0.9815863966941833, 0.9819112420082092, 0.9883834719657898, 0.9915032982826233, 0.979631245136261, 0.9861221313476562, 0.989813506603241, 0.9878960251808167, 0.9796411395072937, 0.9864056706428528, 0.9831562042236328, 0.987371563911438, 0.9784383177757263, 0.9814611673355103, 0.9822826981544495, 0.9903333187103271, 0.9812986850738525, 0.9844555854797363, 0.9863918423652649, 0.9785637259483337, 0.9825519919395447, 0.9840283989906311, 0.9835221767425537, 0.9840239882469177, 0.9817954301834106], 'val_precision_1': [0.8028877973556519, 0.8080618977546692, 0.8100258111953735, 0.8087192177772522, 0.8013166785240173, 0.8062710762023926, 0.8046318888664246, 0.8074575066566467, 0.8078508377075195, 0.8066965341567993, 0.8058484792709351, 0.8042821288108826, 0.8092418909072876, 0.8120514154434204, 0.8047070503234863, 0.8050855398178101, 0.8134626746177673, 0.8093668818473816, 0.8060537576675415, 0.8075872659683228, 0.8053421378135681, 0.808220624923706, 0.8084757328033447, 0.804591953754425, 0.8100876808166504, 0.8109845519065857, 0.8104681968688965, 0.8073751330375671, 0.8054949641227722, 0.8114514946937561, 0.8086519241333008, 0.8066251873970032, 0.8075150847434998, 0.8116102814674377, 0.8084138035774231, 0.8093764185905457, 0.807852566242218, 0.811829686164856, 0.810573935508728, 0.8099374771118164, 0.8060897588729858, 0.8104652762413025, 0.8087814450263977, 0.808050274848938, 0.8114030361175537, 0.8095118403434753, 0.8086958527565002, 0.8093802332878113, 0.8085330128669739, 0.8101580142974854], 'val_accuracy': [0.8006178140640259, 0.8009445667266846, 0.8012230396270752, 0.801609218120575, 0.8004990220069885, 0.8017466068267822, 0.8016834855079651, 0.802132785320282, 0.8025709390640259, 0.8023258447647095, 0.8022107481956482, 0.8016055226325989, 0.802689790725708, 0.8024224042892456, 0.8017392158508301, 0.8020510673522949, 0.8016500473022461, 0.8026860356330872, 0.8021031022071838, 0.8024410009384155, 0.8020585179328918, 0.8025189638137817, 0.8023964166641235, 0.8018134832382202, 0.8023147583007812, 0.8023073077201843, 0.8018914461135864, 0.802121639251709, 0.8017317652702332, 0.801668643951416, 0.8022887110710144, 0.8020808100700378, 0.8019917011260986, 0.8018580079078674, 0.802169919013977, 0.8013381958007812, 0.8020771145820618, 0.8013827204704285, 0.8017392158508301, 0.8014829754829407, 0.8017466068267822, 0.8015089631080627, 0.8014235496520996, 0.8017206192016602, 0.8009520173072815, 0.8011376857757568, 0.8010596632957458, 0.8015758395195007, 0.8008591532707214, 0.8014458417892456], 'val_auc_1': [0.7095139026641846, 0.7113569974899292, 0.7122865915298462, 0.7130779027938843, 0.7138991355895996, 0.7136093974113464, 0.7143282294273376, 0.7146097421646118, 0.714892566204071, 0.7151365280151367, 0.715107798576355, 0.7153884172439575, 0.7147584557533264, 0.7151428461074829, 0.7151085138320923, 0.7147263288497925, 0.7144477367401123, 0.7148085236549377, 0.7140424847602844, 0.7145869731903076, 0.7134582996368408, 0.7140300273895264, 0.7137411236763, 0.7134295105934143, 0.7131467461585999, 0.7134828567504883, 0.7133573889732361, 0.7132411599159241, 0.7123664021492004, 0.7129087448120117, 0.7120700478553772, 0.7125129103660583, 0.7124575972557068, 0.7106499075889587, 0.711434543132782, 0.710715115070343, 0.7108038067817688, 0.7111958861351013, 0.7108895182609558, 0.7103066444396973, 0.7095942497253418, 0.7096458077430725, 0.7091477513313293, 0.7097536325454712, 0.7100701332092285, 0.7089027166366577, 0.7084205746650696, 0.7083730697631836, 0.7084309458732605, 0.7085320949554443]}\n",
            "  1/263 [..............................] - ETA: 47s - loss: 0.4498 - mean_squared_error: 0.1446 - recall_1: 0.9767 - precision_1: 0.8149 - accuracy: 0.8047 - auc_1: 0.7270WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0240s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0240s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "263/263 [==============================] - 6s 24ms/step - loss: 0.4558 - mean_squared_error: 0.1455 - recall_1: 0.9817 - precision_1: 0.8111 - accuracy: 0.8023 - auc_1: 0.7073\n",
            "test loss, test acc: [0.4558262825012207, 0.14552924036979675, 0.981745183467865, 0.8111001253128052, 0.8022813200950623, 0.7072579860687256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfA7HTBYGR_A"
      },
      "source": [
        " #a= tf.ones([32, 83, 21248])\n",
        " a= tf.ones([4, 4])\n",
        " denses = [Dense(1) for i in range(4)]\n",
        " \n",
        " #K.reshape(a , shape=(-1, 32,83 ,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxWFagdgShJh"
      },
      "source": [
        "\n",
        "512 * 512 はaccuracy 80, auc 50 mlp 型は普通に失敗\n",
        "\"\"\"\n",
        "input_em + layer normalization\n",
        "inpput_heads : 8 heads : 4 model_d 64 sa layers 3\n",
        "Epoch 50/50\n",
        "1684/1684 [==============================] - 70s 42ms/step - loss: 0.4069 - binary_accuracy: 0.8227 - precision_2: 0.8426 - recall_2: 0.9572 - accuracy: 0.8227 - auc_2: 0.7929 - val_loss: 0.4930 - val_binary_accuracy: 0.7849 - val_precision_2: 0.8169 - val_recall_2: 0.9425 - val_accuracy: 0.7849 - val_auc_2: 0.6709\n",
        "  1/526 [..............................] - ETA: 1:53 - loss: 0.4574 - binary_accuracy: 0.8027 - precision_2: 0.8337 - recall_2: 0.9502 - accuracy: 0.8027 - auc_2: 0.6684WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0246s). Check your callbacks.\n",
        "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0246s). Check your callbacks.\n",
        "526/526 [==============================] - 15s 29ms/step - loss: 0.4928 - binary_accuracy: 0.7851 - precision_2: 0.8171 - recall_2: 0.9426 - accuracy: 0.7851 - auc_2: 0.6704\n",
        "\"\"\"\n",
        "\n",
        "hueads 8 in heads 8 model_d 64 sa_layer 3 inputW_em + norm layer\n",
        "  loss: 0.4527 - binary_accuracy: 0.8028 - precision_4: 0.8073 - recall_4: 0.9895 - accuracy: 0.8028 - auc_4: 0.7226\n",
        "vali_dataを使ったのが関係あるの？\n",
        "\n",
        "\n",
        "heads:16 in_heads:8, model_d, sa_layer 3 inputW_em + normlayer epo 50 overfited \n",
        "loss: 0.4179 - binary_accuracy: 0.8159 accuracy: 0.8159 - auc_5: 0.7744 - val_loss: 0.4721 - val_binary_accuracy: 0.7953 -  val_auc_5: 0.6920\n",
        "best accuracy: 0.8033 - auc_5: 0.7211 - val_loss: 0.4512 - val_binary_accuracy: 0.8039 val_auc_5: 0.7190\n",
        "\n",
        "heads:16 in_heads:16, model_d, sa_layer 3 inputW_em + normlayer epo 50 overfited \n",
        "loss: 0.4490 accuracy: 0.8041 - auc_6: 0.7240 val_accuracy: 0.8039 - val_auc_6: 0.7190\n",
        "\n",
        "heads:８ in_heads:8, model_d 128, sa_layer 3 inputW_em + normlayer dense32 droptout 0.1 epo 50 best 71.2\n",
        "\n",
        "heads:８ in_heads:8, model_d 128, sa_layer 3 inputW_em + normlayer droptout 0.1 s dense: sigmoid epo 50 \n",
        "best : 71.9\n",
        "\n",
        "classifier dense の中にnorilization したけど意味梨\n",
        "\n",
        "heads 8 input h 8 d 64 \n",
        "dense 32 + sigmoid 二つ共の前に0.3 のdropout あんま意味梨かつ途中からoverfitting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1eRET5qTuNr",
        "outputId": "d9d9b293-e14e-489f-d91a-59ee0fc8e2df"
      },
      "source": [
        "# random sumpling \n",
        "\n",
        "name='cs' \n",
        "num_heads= 8\n",
        "input_heads = 8\n",
        "use_masking=False\n",
        "shape_v = 83\n",
        "model_d = 16\n",
        "SA_layers = 3\n",
        "\n",
        "batch_size = batch_size\n",
        "epochs = 200\n",
        "total_steps  = len(train_dataset) * epochs\n",
        "\n",
        "#with tf.device('/device:GPU:0'):\n",
        "with strategy.scope():\n",
        "  original_inputs = Input(shape=(shape_v,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "  #output_w = inputW(model_d)(original_inputs)\n",
        "  #output = mlpBlock([83, 128], original_inputs)\n",
        "  \n",
        "  output = inputW_Em(model_d, input_heads)(original_inputs)\n",
        "  output = LayerNormalization(name=f'{name}_normalization_before')(output)\n",
        "\n",
        "  for i in range(SA_layers):\n",
        "    output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention_{i}')(output)\n",
        "    post_residual1 = (Add(name=f'{name}_add_{i}')([output_sa, output]))\n",
        "    norm1_output = LayerNormalization(name=f'{name}_normalization1_{i}')(post_residual1)\n",
        "    #output = TransformerTransition(name=f'{name}_transition_{i}', activation='relu')(norm1_output)\n",
        "    outpu = point_wise_feed_forward_network(model_d, model_d*4)(norm1_output)\n",
        "    post_residual2 = (Add(name=f'{name}_add2_{i}')([norm1_output,output]))\n",
        "    output = LayerNormalization(name=f'{name}_normalization2_{i}')(post_residual2)\n",
        "\n",
        "  output = Flatten()(output)\n",
        "\n",
        "  \"\"\"\n",
        "  output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  \"\"\"\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(1, activation='sigmoid')(output)\n",
        "  cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "  cs_p2p.summary()\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-4,\n",
        "      decay_steps=10000,\n",
        "      decay_rate=0.9)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "  #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "  #optimizer = RAdamOptimizer(total_steps=total_steps, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "  cs_p2p.compile(optimizer, \n",
        "                 loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                 #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "          metrics=[\n",
        "          #tf.keras.metrics.MeanSquaredError(),\n",
        "          tf.keras.metrics.BinaryAccuracy(),\n",
        "          #tf.keras.metrics.Precision(),\n",
        "          #tf.keras.metrics.Recall(),\n",
        "          #'accuracy',\n",
        "          tf.keras.metrics.AUC(),]\n",
        "          )\n",
        "  history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=test_dataset, validation_batch_size=batch_size)\n",
        "  #history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "  #history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  print(history.history)\n",
        "  #cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  result = cs_p2p.evaluate(test_dataset)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cs_p2p\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cs_p2p (InputLayer)             [(128, 83)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_w__em_1 (inputW_Em)       (128, 83, 16)        23987       cs_p2p[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization_before (LayerN (128, 83, 16)        32          input_w__em_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_0 (MultiHeadS (128, 83, 16)        768         cs_normalization_before[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_0 (Add)                  (128, 83, 16)        0           cs_self_attention_0[0][0]        \n",
            "                                                                 cs_normalization_before[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_0 (LayerNorma (128, 83, 16)        32          cs_add_0[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_0 (Add)                 (128, 83, 16)        0           cs_normalization1_0[0][0]        \n",
            "                                                                 cs_normalization_before[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_0 (LayerNorma (128, 83, 16)        32          cs_add2_0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_1 (MultiHeadS (128, 83, 16)        768         cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_1 (Add)                  (128, 83, 16)        0           cs_self_attention_1[0][0]        \n",
            "                                                                 cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_1 (LayerNorma (128, 83, 16)        32          cs_add_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_1 (Add)                 (128, 83, 16)        0           cs_normalization1_1[0][0]        \n",
            "                                                                 cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_1 (LayerNorma (128, 83, 16)        32          cs_add2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_2 (MultiHeadS (128, 83, 16)        768         cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_2 (Add)                  (128, 83, 16)        0           cs_self_attention_2[0][0]        \n",
            "                                                                 cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_2 (LayerNorma (128, 83, 16)        32          cs_add_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_2 (Add)                 (128, 83, 16)        0           cs_normalization1_2[0][0]        \n",
            "                                                                 cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_2 (LayerNorma (128, 83, 16)        32          cs_add2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (128, 1328)          0           cs_normalization2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (128, 1328)          0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (128, 32)            42528       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (128, 32)            0           dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (128, 1)             33          dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 69,076\n",
            "Trainable params: 69,076\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "789/789 [==============================] - 31s 30ms/step - loss: 0.5448 - binary_accuracy: 0.7810 - auc_1: 0.5934 - val_loss: 0.4760 - val_binary_accuracy: 0.8009 - val_auc_1: 0.6968\n",
            "Epoch 2/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4815 - binary_accuracy: 0.8002 - auc_1: 0.6850 - val_loss: 0.4727 - val_binary_accuracy: 0.8021 - val_auc_1: 0.7054\n",
            "Epoch 3/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4767 - binary_accuracy: 0.8006 - auc_1: 0.6940 - val_loss: 0.4689 - val_binary_accuracy: 0.8019 - val_auc_1: 0.7082\n",
            "Epoch 4/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4728 - binary_accuracy: 0.8014 - auc_1: 0.6994 - val_loss: 0.4669 - val_binary_accuracy: 0.8027 - val_auc_1: 0.7100\n",
            "Epoch 5/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4706 - binary_accuracy: 0.8013 - auc_1: 0.7029 - val_loss: 0.4658 - val_binary_accuracy: 0.8030 - val_auc_1: 0.7111\n",
            "Epoch 6/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4702 - binary_accuracy: 0.8012 - auc_1: 0.7026 - val_loss: 0.4640 - val_binary_accuracy: 0.8031 - val_auc_1: 0.7120\n",
            "Epoch 7/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4683 - binary_accuracy: 0.8015 - auc_1: 0.7046 - val_loss: 0.4640 - val_binary_accuracy: 0.8030 - val_auc_1: 0.7120\n",
            "Epoch 8/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4666 - binary_accuracy: 0.8021 - auc_1: 0.7045 - val_loss: 0.4617 - val_binary_accuracy: 0.8032 - val_auc_1: 0.7122\n",
            "Epoch 9/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4657 - binary_accuracy: 0.8025 - auc_1: 0.7034 - val_loss: 0.4607 - val_binary_accuracy: 0.8032 - val_auc_1: 0.7127\n",
            "Epoch 10/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4641 - binary_accuracy: 0.8023 - auc_1: 0.7061 - val_loss: 0.4622 - val_binary_accuracy: 0.8033 - val_auc_1: 0.7127\n",
            "Epoch 11/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4645 - binary_accuracy: 0.8014 - auc_1: 0.7049 - val_loss: 0.4588 - val_binary_accuracy: 0.8033 - val_auc_1: 0.7130\n",
            "Epoch 12/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4614 - binary_accuracy: 0.8031 - auc_1: 0.7057 - val_loss: 0.4580 - val_binary_accuracy: 0.8034 - val_auc_1: 0.7130\n",
            "Epoch 13/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4624 - binary_accuracy: 0.8018 - auc_1: 0.7056 - val_loss: 0.4575 - val_binary_accuracy: 0.8034 - val_auc_1: 0.7132\n",
            "Epoch 14/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4612 - binary_accuracy: 0.8019 - auc_1: 0.7069 - val_loss: 0.4570 - val_binary_accuracy: 0.8035 - val_auc_1: 0.7135\n",
            "Epoch 15/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4617 - binary_accuracy: 0.8020 - auc_1: 0.7051 - val_loss: 0.4570 - val_binary_accuracy: 0.8034 - val_auc_1: 0.7136\n",
            "Epoch 16/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4601 - binary_accuracy: 0.8023 - auc_1: 0.7077 - val_loss: 0.4564 - val_binary_accuracy: 0.8032 - val_auc_1: 0.7139\n",
            "Epoch 17/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4597 - binary_accuracy: 0.8028 - auc_1: 0.7068 - val_loss: 0.4561 - val_binary_accuracy: 0.8035 - val_auc_1: 0.7140\n",
            "Epoch 18/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4597 - binary_accuracy: 0.8025 - auc_1: 0.7074 - val_loss: 0.4558 - val_binary_accuracy: 0.8033 - val_auc_1: 0.7142\n",
            "Epoch 19/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4602 - binary_accuracy: 0.8011 - auc_1: 0.7087 - val_loss: 0.4557 - val_binary_accuracy: 0.8036 - val_auc_1: 0.7142\n",
            "Epoch 20/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4594 - binary_accuracy: 0.8023 - auc_1: 0.7076 - val_loss: 0.4557 - val_binary_accuracy: 0.8033 - val_auc_1: 0.7144\n",
            "Epoch 21/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4590 - binary_accuracy: 0.8022 - auc_1: 0.7076 - val_loss: 0.4552 - val_binary_accuracy: 0.8036 - val_auc_1: 0.7144\n",
            "Epoch 22/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4592 - binary_accuracy: 0.8023 - auc_1: 0.7079 - val_loss: 0.4550 - val_binary_accuracy: 0.8036 - val_auc_1: 0.7145\n",
            "Epoch 23/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4582 - binary_accuracy: 0.8023 - auc_1: 0.7090 - val_loss: 0.4550 - val_binary_accuracy: 0.8031 - val_auc_1: 0.7147\n",
            "Epoch 24/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4592 - binary_accuracy: 0.8018 - auc_1: 0.7084 - val_loss: 0.4581 - val_binary_accuracy: 0.8025 - val_auc_1: 0.7147\n",
            "Epoch 25/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4581 - binary_accuracy: 0.8023 - auc_1: 0.7097 - val_loss: 0.4549 - val_binary_accuracy: 0.8035 - val_auc_1: 0.7149\n",
            "Epoch 26/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4578 - binary_accuracy: 0.8026 - auc_1: 0.7081 - val_loss: 0.4544 - val_binary_accuracy: 0.8036 - val_auc_1: 0.7149\n",
            "Epoch 27/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4574 - binary_accuracy: 0.8028 - auc_1: 0.7087 - val_loss: 0.4548 - val_binary_accuracy: 0.8034 - val_auc_1: 0.7149\n",
            "Epoch 28/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4564 - binary_accuracy: 0.8031 - auc_1: 0.7103 - val_loss: 0.4541 - val_binary_accuracy: 0.8036 - val_auc_1: 0.7153\n",
            "Epoch 29/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4567 - binary_accuracy: 0.8031 - auc_1: 0.7101 - val_loss: 0.4542 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7163\n",
            "Epoch 30/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4567 - binary_accuracy: 0.8029 - auc_1: 0.7108 - val_loss: 0.4534 - val_binary_accuracy: 0.8036 - val_auc_1: 0.7168\n",
            "Epoch 31/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4570 - binary_accuracy: 0.8019 - auc_1: 0.7115 - val_loss: 0.4539 - val_binary_accuracy: 0.8036 - val_auc_1: 0.7166\n",
            "Epoch 32/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4560 - binary_accuracy: 0.8027 - auc_1: 0.7116 - val_loss: 0.4530 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7171\n",
            "Epoch 33/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4560 - binary_accuracy: 0.8028 - auc_1: 0.7117 - val_loss: 0.4543 - val_binary_accuracy: 0.8035 - val_auc_1: 0.7165\n",
            "Epoch 34/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4561 - binary_accuracy: 0.8028 - auc_1: 0.7112 - val_loss: 0.4528 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7172\n",
            "Epoch 35/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4561 - binary_accuracy: 0.8022 - auc_1: 0.7128 - val_loss: 0.4531 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7173\n",
            "Epoch 36/200\n",
            "789/789 [==============================] - 23s 29ms/step - loss: 0.4561 - binary_accuracy: 0.8026 - auc_1: 0.7129 - val_loss: 0.4523 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7177\n",
            "Epoch 37/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4558 - binary_accuracy: 0.8028 - auc_1: 0.7122 - val_loss: 0.4531 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7177\n",
            "Epoch 38/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4556 - binary_accuracy: 0.8028 - auc_1: 0.7119 - val_loss: 0.4552 - val_binary_accuracy: 0.8032 - val_auc_1: 0.7175\n",
            "Epoch 39/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4559 - binary_accuracy: 0.8024 - auc_1: 0.7126 - val_loss: 0.4521 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7178\n",
            "Epoch 40/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4566 - binary_accuracy: 0.8015 - auc_1: 0.7126 - val_loss: 0.4522 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7178\n",
            "Epoch 41/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4560 - binary_accuracy: 0.8023 - auc_1: 0.7127 - val_loss: 0.4523 - val_binary_accuracy: 0.8029 - val_auc_1: 0.7179\n",
            "Epoch 42/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4557 - binary_accuracy: 0.8022 - auc_1: 0.7130 - val_loss: 0.4520 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7178\n",
            "Epoch 43/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4548 - binary_accuracy: 0.8027 - auc_1: 0.7129 - val_loss: 0.4519 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7181\n",
            "Epoch 44/200\n",
            "789/789 [==============================] - 23s 29ms/step - loss: 0.4554 - binary_accuracy: 0.8023 - auc_1: 0.7128 - val_loss: 0.4517 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7183\n",
            "Epoch 45/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4553 - binary_accuracy: 0.8026 - auc_1: 0.7119 - val_loss: 0.4520 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7182\n",
            "Epoch 46/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4542 - binary_accuracy: 0.8029 - auc_1: 0.7143 - val_loss: 0.4515 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7184\n",
            "Epoch 47/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4542 - binary_accuracy: 0.8034 - auc_1: 0.7137 - val_loss: 0.4523 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7184\n",
            "Epoch 48/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4546 - binary_accuracy: 0.8029 - auc_1: 0.7139 - val_loss: 0.4524 - val_binary_accuracy: 0.8036 - val_auc_1: 0.7181\n",
            "Epoch 49/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4539 - binary_accuracy: 0.8031 - auc_1: 0.7144 - val_loss: 0.4517 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7179\n",
            "Epoch 50/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4538 - binary_accuracy: 0.8029 - auc_1: 0.7145 - val_loss: 0.4514 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7185\n",
            "Epoch 51/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4539 - binary_accuracy: 0.8037 - auc_1: 0.7134 - val_loss: 0.4514 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7184\n",
            "Epoch 52/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4537 - binary_accuracy: 0.8028 - auc_1: 0.7150 - val_loss: 0.4515 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7188\n",
            "Epoch 53/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4546 - binary_accuracy: 0.8028 - auc_1: 0.7139 - val_loss: 0.4512 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7189\n",
            "Epoch 54/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4532 - binary_accuracy: 0.8038 - auc_1: 0.7140 - val_loss: 0.4513 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7184\n",
            "Epoch 55/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4542 - binary_accuracy: 0.8027 - auc_1: 0.7149 - val_loss: 0.4511 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7187\n",
            "Epoch 56/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4532 - binary_accuracy: 0.8034 - auc_1: 0.7147 - val_loss: 0.4511 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7188\n",
            "Epoch 57/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4537 - binary_accuracy: 0.8031 - auc_1: 0.7145 - val_loss: 0.4513 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7185\n",
            "Epoch 58/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4525 - binary_accuracy: 0.8037 - auc_1: 0.7158 - val_loss: 0.4511 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7187\n",
            "Epoch 59/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4535 - binary_accuracy: 0.8031 - auc_1: 0.7145 - val_loss: 0.4516 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7187\n",
            "Epoch 60/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4521 - binary_accuracy: 0.8036 - auc_1: 0.7173 - val_loss: 0.4512 - val_binary_accuracy: 0.8035 - val_auc_1: 0.7188\n",
            "Epoch 61/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4533 - binary_accuracy: 0.8033 - auc_1: 0.7155 - val_loss: 0.4509 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7189\n",
            "Epoch 62/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4542 - binary_accuracy: 0.8024 - auc_1: 0.7159 - val_loss: 0.4508 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7192\n",
            "Epoch 63/200\n",
            "789/789 [==============================] - 23s 29ms/step - loss: 0.4536 - binary_accuracy: 0.8028 - auc_1: 0.7154 - val_loss: 0.4516 - val_binary_accuracy: 0.8033 - val_auc_1: 0.7186\n",
            "Epoch 64/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4534 - binary_accuracy: 0.8034 - auc_1: 0.7149 - val_loss: 0.4508 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7191\n",
            "Epoch 65/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4528 - binary_accuracy: 0.8033 - auc_1: 0.7160 - val_loss: 0.4507 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7191\n",
            "Epoch 66/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4528 - binary_accuracy: 0.8029 - auc_1: 0.7172 - val_loss: 0.4507 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7191\n",
            "Epoch 67/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4536 - binary_accuracy: 0.8029 - auc_1: 0.7151 - val_loss: 0.4505 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7194\n",
            "Epoch 68/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4519 - binary_accuracy: 0.8040 - auc_1: 0.7163 - val_loss: 0.4506 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7193\n",
            "Epoch 69/200\n",
            "789/789 [==============================] - 23s 29ms/step - loss: 0.4519 - binary_accuracy: 0.8038 - auc_1: 0.7168 - val_loss: 0.4507 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7189\n",
            "Epoch 70/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4542 - binary_accuracy: 0.8016 - auc_1: 0.7166 - val_loss: 0.4508 - val_binary_accuracy: 0.8035 - val_auc_1: 0.7191\n",
            "Epoch 71/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4524 - binary_accuracy: 0.8033 - auc_1: 0.7166 - val_loss: 0.4506 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7193\n",
            "Epoch 72/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4526 - binary_accuracy: 0.8031 - auc_1: 0.7166 - val_loss: 0.4524 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7189\n",
            "Epoch 73/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4527 - binary_accuracy: 0.8032 - auc_1: 0.7153 - val_loss: 0.4504 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7195\n",
            "Epoch 74/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4528 - binary_accuracy: 0.8033 - auc_1: 0.7161 - val_loss: 0.4511 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7195\n",
            "Epoch 75/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4518 - binary_accuracy: 0.8034 - auc_1: 0.7172 - val_loss: 0.4511 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7194\n",
            "Epoch 76/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4519 - binary_accuracy: 0.8037 - auc_1: 0.7164 - val_loss: 0.4511 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7197\n",
            "Epoch 77/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4532 - binary_accuracy: 0.8025 - auc_1: 0.7167 - val_loss: 0.4502 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7194\n",
            "Epoch 78/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4524 - binary_accuracy: 0.8031 - auc_1: 0.7172 - val_loss: 0.4502 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7194\n",
            "Epoch 79/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4513 - binary_accuracy: 0.8036 - auc_1: 0.7173 - val_loss: 0.4502 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7196\n",
            "Epoch 80/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4517 - binary_accuracy: 0.8041 - auc_1: 0.7166 - val_loss: 0.4509 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7193\n",
            "Epoch 81/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4528 - binary_accuracy: 0.8026 - auc_1: 0.7166 - val_loss: 0.4503 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7195\n",
            "Epoch 82/200\n",
            "789/789 [==============================] - 23s 29ms/step - loss: 0.4523 - binary_accuracy: 0.8032 - auc_1: 0.7168 - val_loss: 0.4504 - val_binary_accuracy: 0.8036 - val_auc_1: 0.7195\n",
            "Epoch 83/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4522 - binary_accuracy: 0.8031 - auc_1: 0.7165 - val_loss: 0.4501 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7195\n",
            "Epoch 84/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4524 - binary_accuracy: 0.8032 - auc_1: 0.7173 - val_loss: 0.4504 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7193\n",
            "Epoch 85/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4530 - binary_accuracy: 0.8027 - auc_1: 0.7163 - val_loss: 0.4501 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7196\n",
            "Epoch 86/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4506 - binary_accuracy: 0.8038 - auc_1: 0.7186 - val_loss: 0.4502 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7194\n",
            "Epoch 87/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4513 - binary_accuracy: 0.8035 - auc_1: 0.7182 - val_loss: 0.4504 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7194\n",
            "Epoch 88/200\n",
            "789/789 [==============================] - 23s 29ms/step - loss: 0.4525 - binary_accuracy: 0.8031 - auc_1: 0.7171 - val_loss: 0.4506 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7193\n",
            "Epoch 89/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4530 - binary_accuracy: 0.8024 - auc_1: 0.7174 - val_loss: 0.4505 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7194\n",
            "Epoch 90/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4519 - binary_accuracy: 0.8036 - auc_1: 0.7169 - val_loss: 0.4499 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7198\n",
            "Epoch 91/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4512 - binary_accuracy: 0.8033 - auc_1: 0.7184 - val_loss: 0.4501 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7195\n",
            "Epoch 92/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4517 - binary_accuracy: 0.8032 - auc_1: 0.7188 - val_loss: 0.4502 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7197\n",
            "Epoch 93/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4529 - binary_accuracy: 0.8031 - auc_1: 0.7156 - val_loss: 0.4502 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7193\n",
            "Epoch 94/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4531 - binary_accuracy: 0.8027 - auc_1: 0.7163 - val_loss: 0.4502 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7194\n",
            "Epoch 95/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4515 - binary_accuracy: 0.8035 - auc_1: 0.7172 - val_loss: 0.4500 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7195\n",
            "Epoch 96/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4518 - binary_accuracy: 0.8030 - auc_1: 0.7183 - val_loss: 0.4516 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7196\n",
            "Epoch 97/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4509 - binary_accuracy: 0.8043 - auc_1: 0.7170 - val_loss: 0.4501 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7197\n",
            "Epoch 98/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4522 - binary_accuracy: 0.8029 - auc_1: 0.7171 - val_loss: 0.4501 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7197\n",
            "Epoch 99/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4517 - binary_accuracy: 0.8034 - auc_1: 0.7180 - val_loss: 0.4504 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7196\n",
            "Epoch 100/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4516 - binary_accuracy: 0.8032 - auc_1: 0.7179 - val_loss: 0.4499 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7198\n",
            "Epoch 101/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4511 - binary_accuracy: 0.8032 - auc_1: 0.7177 - val_loss: 0.4499 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7196\n",
            "Epoch 102/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4524 - binary_accuracy: 0.8027 - auc_1: 0.7170 - val_loss: 0.4500 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7195\n",
            "Epoch 103/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4512 - binary_accuracy: 0.8036 - auc_1: 0.7177 - val_loss: 0.4499 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7197\n",
            "Epoch 104/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4516 - binary_accuracy: 0.8031 - auc_1: 0.7172 - val_loss: 0.4498 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7198\n",
            "Epoch 105/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4517 - binary_accuracy: 0.8031 - auc_1: 0.7176 - val_loss: 0.4508 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7197\n",
            "Epoch 106/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4523 - binary_accuracy: 0.8026 - auc_1: 0.7174 - val_loss: 0.4499 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7196\n",
            "Epoch 107/200\n",
            "789/789 [==============================] - 23s 29ms/step - loss: 0.4503 - binary_accuracy: 0.8040 - auc_1: 0.7184 - val_loss: 0.4498 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7198\n",
            "Epoch 108/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4514 - binary_accuracy: 0.8030 - auc_1: 0.7188 - val_loss: 0.4499 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7200\n",
            "Epoch 109/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4530 - binary_accuracy: 0.8021 - auc_1: 0.7178 - val_loss: 0.4500 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7199\n",
            "Epoch 110/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4516 - binary_accuracy: 0.8035 - auc_1: 0.7165 - val_loss: 0.4506 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7195\n",
            "Epoch 111/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4523 - binary_accuracy: 0.8027 - auc_1: 0.7172 - val_loss: 0.4500 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7200\n",
            "Epoch 112/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4515 - binary_accuracy: 0.8033 - auc_1: 0.7179 - val_loss: 0.4502 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7198\n",
            "Epoch 113/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4519 - binary_accuracy: 0.8030 - auc_1: 0.7178 - val_loss: 0.4497 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7200\n",
            "Epoch 114/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4502 - binary_accuracy: 0.8038 - auc_1: 0.7184 - val_loss: 0.4497 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7198\n",
            "Epoch 115/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4512 - binary_accuracy: 0.8031 - auc_1: 0.7186 - val_loss: 0.4508 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7200\n",
            "Epoch 116/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4520 - binary_accuracy: 0.8026 - auc_1: 0.7185 - val_loss: 0.4497 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7200\n",
            "Epoch 117/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4522 - binary_accuracy: 0.8027 - auc_1: 0.7179 - val_loss: 0.4497 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7199\n",
            "Epoch 118/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4518 - binary_accuracy: 0.8031 - auc_1: 0.7184 - val_loss: 0.4500 - val_binary_accuracy: 0.8036 - val_auc_1: 0.7199\n",
            "Epoch 119/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4508 - binary_accuracy: 0.8037 - auc_1: 0.7171 - val_loss: 0.4500 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7198\n",
            "Epoch 120/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4506 - binary_accuracy: 0.8035 - auc_1: 0.7188 - val_loss: 0.4497 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7199\n",
            "Epoch 121/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4514 - binary_accuracy: 0.8032 - auc_1: 0.7185 - val_loss: 0.4501 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7200\n",
            "Epoch 122/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4504 - binary_accuracy: 0.8042 - auc_1: 0.7174 - val_loss: 0.4498 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7198\n",
            "Epoch 123/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4507 - binary_accuracy: 0.8035 - auc_1: 0.7195 - val_loss: 0.4497 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7198\n",
            "Epoch 124/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4499 - binary_accuracy: 0.8037 - auc_1: 0.7201 - val_loss: 0.4509 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7200\n",
            "Epoch 125/200\n",
            "789/789 [==============================] - 23s 29ms/step - loss: 0.4507 - binary_accuracy: 0.8036 - auc_1: 0.7187 - val_loss: 0.4496 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7200\n",
            "Epoch 126/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4520 - binary_accuracy: 0.8024 - auc_1: 0.7186 - val_loss: 0.4499 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7199\n",
            "Epoch 127/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4521 - binary_accuracy: 0.8028 - auc_1: 0.7175 - val_loss: 0.4498 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7199\n",
            "Epoch 128/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4493 - binary_accuracy: 0.8042 - auc_1: 0.7199 - val_loss: 0.4502 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7199\n",
            "Epoch 129/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4497 - binary_accuracy: 0.8042 - auc_1: 0.7198 - val_loss: 0.4498 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7200\n",
            "Epoch 130/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4512 - binary_accuracy: 0.8032 - auc_1: 0.7193 - val_loss: 0.4495 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7202\n",
            "Epoch 131/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4504 - binary_accuracy: 0.8037 - auc_1: 0.7189 - val_loss: 0.4495 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7202\n",
            "Epoch 132/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4508 - binary_accuracy: 0.8033 - auc_1: 0.7184 - val_loss: 0.4499 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7200\n",
            "Epoch 133/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4522 - binary_accuracy: 0.8024 - auc_1: 0.7184 - val_loss: 0.4496 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 134/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4504 - binary_accuracy: 0.8039 - auc_1: 0.7194 - val_loss: 0.4496 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7201\n",
            "Epoch 135/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4512 - binary_accuracy: 0.8030 - auc_1: 0.7183 - val_loss: 0.4496 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 136/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4502 - binary_accuracy: 0.8038 - auc_1: 0.7199 - val_loss: 0.4496 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 137/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4512 - binary_accuracy: 0.8028 - auc_1: 0.7195 - val_loss: 0.4500 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7201\n",
            "Epoch 138/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4519 - binary_accuracy: 0.8029 - auc_1: 0.7179 - val_loss: 0.4499 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7199\n",
            "Epoch 139/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4514 - binary_accuracy: 0.8029 - auc_1: 0.7189 - val_loss: 0.4495 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7201\n",
            "Epoch 140/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4506 - binary_accuracy: 0.8036 - auc_1: 0.7188 - val_loss: 0.4495 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7201\n",
            "Epoch 141/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4520 - binary_accuracy: 0.8027 - auc_1: 0.7176 - val_loss: 0.4495 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 142/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4501 - binary_accuracy: 0.8037 - auc_1: 0.7203 - val_loss: 0.4495 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7200\n",
            "Epoch 143/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4507 - binary_accuracy: 0.8036 - auc_1: 0.7183 - val_loss: 0.4494 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7202\n",
            "Epoch 144/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4511 - binary_accuracy: 0.8031 - auc_1: 0.7197 - val_loss: 0.4495 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7200\n",
            "Epoch 145/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4498 - binary_accuracy: 0.8039 - auc_1: 0.7199 - val_loss: 0.4497 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7201\n",
            "Epoch 146/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4507 - binary_accuracy: 0.8037 - auc_1: 0.7186 - val_loss: 0.4505 - val_binary_accuracy: 0.8038 - val_auc_1: 0.7201\n",
            "Epoch 147/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4499 - binary_accuracy: 0.8037 - auc_1: 0.7198 - val_loss: 0.4495 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 148/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4510 - binary_accuracy: 0.8032 - auc_1: 0.7192 - val_loss: 0.4496 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 149/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4504 - binary_accuracy: 0.8040 - auc_1: 0.7189 - val_loss: 0.4497 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7200\n",
            "Epoch 150/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4513 - binary_accuracy: 0.8033 - auc_1: 0.7180 - val_loss: 0.4497 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7201\n",
            "Epoch 151/200\n",
            "789/789 [==============================] - 23s 29ms/step - loss: 0.4510 - binary_accuracy: 0.8032 - auc_1: 0.7193 - val_loss: 0.4496 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7202\n",
            "Epoch 152/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4500 - binary_accuracy: 0.8036 - auc_1: 0.7202 - val_loss: 0.4497 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7201\n",
            "Epoch 153/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4509 - binary_accuracy: 0.8034 - auc_1: 0.7193 - val_loss: 0.4494 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7202\n",
            "Epoch 154/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4495 - binary_accuracy: 0.8041 - auc_1: 0.7202 - val_loss: 0.4494 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 155/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4505 - binary_accuracy: 0.8032 - auc_1: 0.7202 - val_loss: 0.4498 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7200\n",
            "Epoch 156/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4512 - binary_accuracy: 0.8032 - auc_1: 0.7188 - val_loss: 0.4495 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7202\n",
            "Epoch 157/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4509 - binary_accuracy: 0.8032 - auc_1: 0.7196 - val_loss: 0.4495 - val_binary_accuracy: 0.8043 - val_auc_1: 0.7201\n",
            "Epoch 158/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4506 - binary_accuracy: 0.8037 - auc_1: 0.7191 - val_loss: 0.4495 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7201\n",
            "Epoch 159/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4499 - binary_accuracy: 0.8038 - auc_1: 0.7203 - val_loss: 0.4496 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7200\n",
            "Epoch 160/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4493 - binary_accuracy: 0.8038 - auc_1: 0.7213 - val_loss: 0.4499 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7201\n",
            "Epoch 161/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4508 - binary_accuracy: 0.8029 - auc_1: 0.7206 - val_loss: 0.4495 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7200\n",
            "Epoch 162/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4497 - binary_accuracy: 0.8042 - auc_1: 0.7195 - val_loss: 0.4495 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7201\n",
            "Epoch 163/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4510 - binary_accuracy: 0.8031 - auc_1: 0.7191 - val_loss: 0.4499 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7201\n",
            "Epoch 164/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4515 - binary_accuracy: 0.8028 - auc_1: 0.7183 - val_loss: 0.4495 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7201\n",
            "Epoch 165/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4507 - binary_accuracy: 0.8033 - auc_1: 0.7188 - val_loss: 0.4499 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7202\n",
            "Epoch 166/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4507 - binary_accuracy: 0.8031 - auc_1: 0.7203 - val_loss: 0.4496 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7201\n",
            "Epoch 167/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4497 - binary_accuracy: 0.8037 - auc_1: 0.7209 - val_loss: 0.4496 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7199\n",
            "Epoch 168/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4488 - binary_accuracy: 0.8044 - auc_1: 0.7200 - val_loss: 0.4499 - val_binary_accuracy: 0.8037 - val_auc_1: 0.7201\n",
            "Epoch 169/200\n",
            "789/789 [==============================] - 23s 29ms/step - loss: 0.4507 - binary_accuracy: 0.8032 - auc_1: 0.7199 - val_loss: 0.4495 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7202\n",
            "Epoch 170/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4492 - binary_accuracy: 0.8043 - auc_1: 0.7207 - val_loss: 0.4493 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7201\n",
            "Epoch 171/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4505 - binary_accuracy: 0.8033 - auc_1: 0.7202 - val_loss: 0.4494 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7200\n",
            "Epoch 172/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4501 - binary_accuracy: 0.8036 - auc_1: 0.7200 - val_loss: 0.4494 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7201\n",
            "Epoch 173/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4497 - binary_accuracy: 0.8040 - auc_1: 0.7209 - val_loss: 0.4495 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 174/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4498 - binary_accuracy: 0.8036 - auc_1: 0.7198 - val_loss: 0.4496 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 175/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4501 - binary_accuracy: 0.8039 - auc_1: 0.7201 - val_loss: 0.4495 - val_binary_accuracy: 0.8039 - val_auc_1: 0.7202\n",
            "Epoch 176/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4510 - binary_accuracy: 0.8030 - auc_1: 0.7197 - val_loss: 0.4501 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7201\n",
            "Epoch 177/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4504 - binary_accuracy: 0.8036 - auc_1: 0.7199 - val_loss: 0.4495 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7201\n",
            "Epoch 178/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4500 - binary_accuracy: 0.8039 - auc_1: 0.7204 - val_loss: 0.4495 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7202\n",
            "Epoch 179/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4506 - binary_accuracy: 0.8035 - auc_1: 0.7199 - val_loss: 0.4493 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7202\n",
            "Epoch 180/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4496 - binary_accuracy: 0.8038 - auc_1: 0.7210 - val_loss: 0.4493 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7203\n",
            "Epoch 181/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4498 - binary_accuracy: 0.8035 - auc_1: 0.7207 - val_loss: 0.4495 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7201\n",
            "Epoch 182/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4500 - binary_accuracy: 0.8037 - auc_1: 0.7201 - val_loss: 0.4493 - val_binary_accuracy: 0.8043 - val_auc_1: 0.7202\n",
            "Epoch 183/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4494 - binary_accuracy: 0.8040 - auc_1: 0.7207 - val_loss: 0.4494 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 184/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4495 - binary_accuracy: 0.8043 - auc_1: 0.7197 - val_loss: 0.4493 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7202\n",
            "Epoch 185/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4499 - binary_accuracy: 0.8038 - auc_1: 0.7208 - val_loss: 0.4493 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7202\n",
            "Epoch 186/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4507 - binary_accuracy: 0.8026 - auc_1: 0.7208 - val_loss: 0.4493 - val_binary_accuracy: 0.8044 - val_auc_1: 0.7202\n",
            "Epoch 187/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4507 - binary_accuracy: 0.8035 - auc_1: 0.7200 - val_loss: 0.4493 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7203\n",
            "Epoch 188/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4502 - binary_accuracy: 0.8039 - auc_1: 0.7200 - val_loss: 0.4493 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7202\n",
            "Epoch 189/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4497 - binary_accuracy: 0.8039 - auc_1: 0.7209 - val_loss: 0.4493 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7201\n",
            "Epoch 190/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4490 - binary_accuracy: 0.8043 - auc_1: 0.7218 - val_loss: 0.4494 - val_binary_accuracy: 0.8043 - val_auc_1: 0.7201\n",
            "Epoch 191/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4495 - binary_accuracy: 0.8040 - auc_1: 0.7210 - val_loss: 0.4499 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 192/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4495 - binary_accuracy: 0.8039 - auc_1: 0.7201 - val_loss: 0.4501 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7201\n",
            "Epoch 193/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4509 - binary_accuracy: 0.8031 - auc_1: 0.7201 - val_loss: 0.4494 - val_binary_accuracy: 0.8043 - val_auc_1: 0.7202\n",
            "Epoch 194/200\n",
            "789/789 [==============================] - 22s 27ms/step - loss: 0.4493 - binary_accuracy: 0.8042 - auc_1: 0.7208 - val_loss: 0.4494 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7202\n",
            "Epoch 195/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4492 - binary_accuracy: 0.8038 - auc_1: 0.7213 - val_loss: 0.4494 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7202\n",
            "Epoch 196/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4504 - binary_accuracy: 0.8032 - auc_1: 0.7206 - val_loss: 0.4499 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7200\n",
            "Epoch 197/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4501 - binary_accuracy: 0.8033 - auc_1: 0.7211 - val_loss: 0.4494 - val_binary_accuracy: 0.8040 - val_auc_1: 0.7202\n",
            "Epoch 198/200\n",
            "789/789 [==============================] - 23s 29ms/step - loss: 0.4494 - binary_accuracy: 0.8042 - auc_1: 0.7203 - val_loss: 0.4494 - val_binary_accuracy: 0.8042 - val_auc_1: 0.7203\n",
            "Epoch 199/200\n",
            "789/789 [==============================] - 23s 28ms/step - loss: 0.4499 - binary_accuracy: 0.8033 - auc_1: 0.7213 - val_loss: 0.4495 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7202\n",
            "Epoch 200/200\n",
            "789/789 [==============================] - 22s 28ms/step - loss: 0.4494 - binary_accuracy: 0.8041 - auc_1: 0.7210 - val_loss: 0.4494 - val_binary_accuracy: 0.8041 - val_auc_1: 0.7202\n",
            "{'loss': [0.5030480027198792, 0.47971466183662415, 0.47522926330566406, 0.47282713651657104, 0.4709262251853943, 0.46939361095428467, 0.467716783285141, 0.4667036533355713, 0.4655415415763855, 0.4641875922679901, 0.4635617733001709, 0.4622577130794525, 0.46170300245285034, 0.46119147539138794, 0.46067380905151367, 0.4600837528705597, 0.4598968029022217, 0.45965775847435, 0.45916423201560974, 0.4591462016105652, 0.45892903208732605, 0.4587251543998718, 0.4582931399345398, 0.458292692899704, 0.45804837346076965, 0.45784395933151245, 0.4577348530292511, 0.4575803577899933, 0.45732539892196655, 0.45681390166282654, 0.4564628601074219, 0.4562443792819977, 0.4563150703907013, 0.45613330602645874, 0.45597341656684875, 0.4556582570075989, 0.4556688666343689, 0.45563802123069763, 0.45537614822387695, 0.455113023519516, 0.4551350474357605, 0.4549652636051178, 0.45486053824424744, 0.45470643043518066, 0.4547911286354065, 0.45455795526504517, 0.4544391334056854, 0.4543622136116028, 0.45407745242118835, 0.45421528816223145, 0.4540073275566101, 0.45404312014579773, 0.45392265915870667, 0.4538734555244446, 0.45373234152793884, 0.4535084068775177, 0.45367202162742615, 0.45358774065971375, 0.45353031158447266, 0.45331278443336487, 0.45337605476379395, 0.45328211784362793, 0.45330578088760376, 0.453062504529953, 0.453135222196579, 0.45298057794570923, 0.45303529500961304, 0.45288321375846863, 0.45278382301330566, 0.45285218954086304, 0.45256295800209045, 0.4527149200439453, 0.45265430212020874, 0.45260193943977356, 0.4525166451931, 0.45248618721961975, 0.45256558060646057, 0.4524751901626587, 0.45243361592292786, 0.4524119794368744, 0.4522082209587097, 0.45227932929992676, 0.45225638151168823, 0.452092707157135, 0.45218372344970703, 0.45199355483055115, 0.4519796371459961, 0.4520571529865265, 0.4519561231136322, 0.45197245478630066, 0.4518985450267792, 0.45190107822418213, 0.4519193470478058, 0.45184585452079773, 0.4518430233001709, 0.45178917050361633, 0.45176759362220764, 0.4519093930721283, 0.45179834961891174, 0.45168936252593994, 0.45161736011505127, 0.45162320137023926, 0.4516264498233795, 0.4515715539455414, 0.4515402615070343, 0.4516274333000183, 0.451384574174881, 0.4514995217323303, 0.45137032866477966, 0.4514932632446289, 0.45131537318229675, 0.4513522982597351, 0.451310932636261, 0.45132824778556824, 0.4513176381587982, 0.4512068033218384, 0.45134812593460083, 0.45133575797080994, 0.45113977789878845, 0.4512483477592468, 0.45118448138237, 0.4510808289051056, 0.45120877027511597, 0.45110252499580383, 0.45116737484931946, 0.45102933049201965, 0.4511289894580841, 0.4509006142616272, 0.45095187425613403, 0.4509768784046173, 0.45093709230422974, 0.45098185539245605, 0.4509789049625397, 0.4509354531764984, 0.4509377181529999, 0.4508894383907318, 0.4510114789009094, 0.4507589638233185, 0.45099321007728577, 0.4508860111236572, 0.45082902908325195, 0.45087921619415283, 0.4508086144924164, 0.45068997144699097, 0.45078200101852417, 0.4507269859313965, 0.45067504048347473, 0.4506695568561554, 0.4506884217262268, 0.4508233666419983, 0.4506446123123169, 0.45057839155197144, 0.4506032168865204, 0.4505844712257385, 0.4505613148212433, 0.4506514370441437, 0.4505378007888794, 0.4506027102470398, 0.4504641592502594, 0.45043566823005676, 0.4504622220993042, 0.45044755935668945, 0.4505276679992676, 0.4505043029785156, 0.4504978060722351, 0.4503181278705597, 0.45035308599472046, 0.4503510594367981, 0.45039987564086914, 0.4503512680530548, 0.45052507519721985, 0.4502544105052948, 0.4503210783004761, 0.4502701163291931, 0.4502299129962921, 0.45027220249176025, 0.45025312900543213, 0.4502708613872528, 0.4502142369747162, 0.4502134621143341, 0.45022645592689514, 0.4502466320991516, 0.4500664174556732, 0.45013129711151123, 0.45006391406059265, 0.45010536909103394, 0.45009273290634155, 0.4501171410083771, 0.4500473141670227, 0.44997888803482056, 0.4500673711299896, 0.4500989317893982, 0.45004209876060486, 0.4500049352645874, 0.4499287009239197, 0.4500928819179535, 0.45002901554107666, 0.4500497281551361, 0.44993433356285095, 0.4499538838863373], 'binary_accuracy': [0.796886146068573, 0.8005114197731018, 0.8009842038154602, 0.8013283014297485, 0.8014793395996094, 0.801724374294281, 0.8018778562545776, 0.8018221855163574, 0.801871657371521, 0.8019818067550659, 0.8018729090690613, 0.8021947145462036, 0.8021563291549683, 0.8022392988204956, 0.8020474314689636, 0.8021031022071838, 0.8024781346321106, 0.802266538143158, 0.8021390438079834, 0.8022120594978333, 0.802018940448761, 0.8023346066474915, 0.8023655414581299, 0.8022850751876831, 0.8024930357933044, 0.8024632930755615, 0.8022912740707397, 0.8024360537528992, 0.8025895357131958, 0.8027306795120239, 0.8026155233383179, 0.8027318716049194, 0.8026167750358582, 0.8027182817459106, 0.8027850985527039, 0.8028395771980286, 0.8028494715690613, 0.8026390671730042, 0.8027083873748779, 0.8027578592300415, 0.8028717637062073, 0.8028420209884644, 0.8026972413063049, 0.8026922941207886, 0.8027541637420654, 0.8029918074607849, 0.8028234839439392, 0.802837073802948, 0.8028804063796997, 0.8029905557632446, 0.8029769659042358, 0.8028903007507324, 0.8030450344085693, 0.8031094074249268, 0.8030017018318176, 0.8030202984809875, 0.803084671497345, 0.8030363917350769, 0.8030673265457153, 0.8031514883041382, 0.803040087223053, 0.8031613826751709, 0.8029856085777283, 0.8032492399215698, 0.8030796647071838, 0.8031663298606873, 0.8030140995979309, 0.8031898736953735, 0.8032888770103455, 0.8030214905738831, 0.8032009601593018, 0.8031131029129028, 0.8032195568084717, 0.8032109141349792, 0.8032071590423584, 0.8033470511436462, 0.8031886219978333, 0.8030574321746826, 0.8030363917350769, 0.8032814264297485, 0.803110659122467, 0.8031279444694519, 0.8030079007148743, 0.80326908826828, 0.8032653331756592, 0.8031588792800903, 0.8033804297447205, 0.8033099174499512, 0.8032059669494629, 0.8034151196479797, 0.8033631443977356, 0.8032665848731995, 0.8032009601593018, 0.803312361240387, 0.8033062219619751, 0.8030858635902405, 0.8032517433166504, 0.8031861186027527, 0.803345799446106, 0.8032232522964478, 0.8032047152519226, 0.8033173084259033, 0.8032504916191101, 0.8031923174858093, 0.8032665848731995, 0.803243100643158, 0.8033198118209839, 0.8032653331756592, 0.8032071590423584, 0.8033074140548706, 0.8033655881881714, 0.8033297061920166, 0.8033408522605896, 0.8032307028770447, 0.8033643960952759, 0.8032084107398987, 0.8032529950141907, 0.8034336566925049, 0.8032888770103455, 0.8031712770462036, 0.8034287095069885, 0.8034670948982239, 0.8033519983291626, 0.8031551837921143, 0.8032814264297485, 0.803394079208374, 0.803334653377533, 0.8033359050750732, 0.8035104274749756, 0.8035277724266052, 0.8032294511795044, 0.8032801747322083, 0.8033693432807922, 0.8034039735794067, 0.8033322095870972, 0.8034720420837402, 0.8033148646354675, 0.803481936454773, 0.8033631443977356, 0.8034262657165527, 0.8033322095870972, 0.8034782409667969, 0.803459644317627, 0.8034374117851257, 0.8033136129379272, 0.8034200668334961, 0.8033631443977356, 0.8034262657165527, 0.803655207157135, 0.8033334016799927, 0.8034757375717163, 0.8034473061561584, 0.8034831881523132, 0.8033841848373413, 0.8035054802894592, 0.8035054802894592, 0.8035821914672852, 0.8035277724266052, 0.8034349083900452, 0.8035611510276794, 0.8035549521446228, 0.803552508354187, 0.8034535050392151, 0.8034559488296509, 0.8035821914672852, 0.8036143779754639, 0.8037691116333008, 0.8035759925842285, 0.8035128712654114, 0.8035970330238342, 0.8035029768943787, 0.8036577105522156, 0.8036119341850281, 0.8035005331039429, 0.8037084341049194, 0.8035574555397034, 0.8036218285560608, 0.8035995364189148, 0.8036218285560608, 0.8036218285560608, 0.8036143779754639, 0.8036329746246338, 0.8036428689956665, 0.8035549521446228, 0.8035512566566467, 0.8034473061561584, 0.8036465644836426, 0.8037592172622681, 0.8036873936653137, 0.803699791431427, 0.8037616610527039, 0.8035970330238342, 0.8036354184150696, 0.8037233352661133, 0.8036032319068909, 0.8035426139831543, 0.8036577105522156, 0.8035958409309387, 0.8035067319869995, 0.8037418723106384], 'auc_1': [0.6414090991020203, 0.6882766485214233, 0.6962090730667114, 0.6995256543159485, 0.701579213142395, 0.7027886509895325, 0.7041594982147217, 0.7042206525802612, 0.704708993434906, 0.7056285738945007, 0.7052509784698486, 0.7063490748405457, 0.7063334584236145, 0.7063471078872681, 0.7067842483520508, 0.7075638175010681, 0.7074933648109436, 0.7075469493865967, 0.7083286643028259, 0.7080366611480713, 0.7082840204238892, 0.708195686340332, 0.7090990543365479, 0.7087417244911194, 0.7090157866477966, 0.7092556953430176, 0.7094817757606506, 0.7094199657440186, 0.7098665833473206, 0.7106932997703552, 0.7112700343132019, 0.7116438746452332, 0.7113592028617859, 0.7116668820381165, 0.7117881178855896, 0.7123775482177734, 0.7121725678443909, 0.7121534943580627, 0.7126169800758362, 0.7131121158599854, 0.7129011750221252, 0.7132105827331543, 0.7132390141487122, 0.7135589718818665, 0.7133632898330688, 0.7137672901153564, 0.713874340057373, 0.7139757871627808, 0.7145047187805176, 0.7140282392501831, 0.7144858837127686, 0.7143458724021912, 0.7146371603012085, 0.7146574258804321, 0.7147684693336487, 0.7151774168014526, 0.7148312926292419, 0.7149200439453125, 0.7150854468345642, 0.7154623866081238, 0.7152955532073975, 0.7153881192207336, 0.7153430581092834, 0.715664803981781, 0.7155883312225342, 0.7158873677253723, 0.7157840132713318, 0.7159537672996521, 0.7161115407943726, 0.7159730792045593, 0.7165402770042419, 0.7160939574241638, 0.7162903547286987, 0.7163047790527344, 0.7164779305458069, 0.7165431976318359, 0.7163467407226562, 0.7165901064872742, 0.7165151834487915, 0.7164963483810425, 0.716861367225647, 0.716751754283905, 0.716813862323761, 0.7171048521995544, 0.7168719172477722, 0.717294454574585, 0.7171930074691772, 0.7171850204467773, 0.717271625995636, 0.7171353101730347, 0.717308759689331, 0.7173116207122803, 0.7172767519950867, 0.7174466252326965, 0.7173769474029541, 0.7174405455589294, 0.7174646854400635, 0.717093288898468, 0.7173406481742859, 0.7177594900131226, 0.7177502512931824, 0.7176221609115601, 0.7177179455757141, 0.717792809009552, 0.7177643179893494, 0.7176656723022461, 0.7181448936462402, 0.7179092764854431, 0.7181972861289978, 0.7178574800491333, 0.7181754112243652, 0.7181401252746582, 0.7182068824768066, 0.718200147151947, 0.7181604504585266, 0.7184249758720398, 0.7181926369667053, 0.7180871367454529, 0.7184585332870483, 0.718319833278656, 0.7184011340141296, 0.7186703085899353, 0.7183924913406372, 0.7186487317085266, 0.7183970808982849, 0.7186129093170166, 0.7184567451477051, 0.7189785838127136, 0.7187582850456238, 0.7187413573265076, 0.7188336849212646, 0.7187427282333374, 0.7188119292259216, 0.7188113927841187, 0.7187983989715576, 0.7189262509346008, 0.7186608910560608, 0.7190677523612976, 0.7186180353164673, 0.7188495993614197, 0.7190523147583008, 0.7189313173294067, 0.7190194129943848, 0.7191648483276367, 0.7190918922424316, 0.7191349864006042, 0.7192579507827759, 0.719221830368042, 0.7192728519439697, 0.7190510630607605, 0.7192398309707642, 0.7194429636001587, 0.7193963527679443, 0.7193822860717773, 0.7194147706031799, 0.7193450927734375, 0.7194538116455078, 0.7193577885627747, 0.7196820974349976, 0.7196103930473328, 0.7196155190467834, 0.7195944786071777, 0.7194703221321106, 0.7195401191711426, 0.719406247138977, 0.7197883129119873, 0.7197413444519043, 0.7198073267936707, 0.7196565270423889, 0.7198403477668762, 0.7194709777832031, 0.7199409008026123, 0.7198410630226135, 0.7199786901473999, 0.7199569344520569, 0.7199490666389465, 0.7199079394340515, 0.7199559211730957, 0.7200255990028381, 0.7200933694839478, 0.720000684261322, 0.7198986411094666, 0.7202914357185364, 0.7201322913169861, 0.720276951789856, 0.7202567458152771, 0.7202115058898926, 0.7201856970787048, 0.7202988862991333, 0.7204687595367432, 0.720240592956543, 0.7201946973800659, 0.7203278541564941, 0.720357358455658, 0.7205414772033691, 0.720198392868042, 0.7204185724258423, 0.7203806638717651, 0.720675528049469, 0.7203967571258545], 'val_loss': [0.47595906257629395, 0.47273460030555725, 0.4688850939273834, 0.46689900755882263, 0.46580955386161804, 0.46397820115089417, 0.4639585614204407, 0.46172547340393066, 0.46073412895202637, 0.4621690511703491, 0.4588333070278168, 0.4580201208591461, 0.45749467611312866, 0.4570402204990387, 0.4569685459136963, 0.4563917815685272, 0.4561189115047455, 0.4557980000972748, 0.4557334780693054, 0.45568767189979553, 0.45521655678749084, 0.4549511969089508, 0.4550366997718811, 0.4581131935119629, 0.45488131046295166, 0.45437508821487427, 0.45484596490859985, 0.4540500044822693, 0.454188734292984, 0.4533746838569641, 0.45385128259658813, 0.4530012011528015, 0.4542750120162964, 0.45276954770088196, 0.4530593156814575, 0.45231765508651733, 0.4531305730342865, 0.4552050232887268, 0.45208534598350525, 0.4522373378276825, 0.4523390531539917, 0.45204389095306396, 0.451894074678421, 0.4517090618610382, 0.45200562477111816, 0.45153701305389404, 0.45233386754989624, 0.45237359404563904, 0.45166832208633423, 0.45140138268470764, 0.4513518214225769, 0.451482892036438, 0.45122888684272766, 0.4512767195701599, 0.451127827167511, 0.45111146569252014, 0.45130589604377747, 0.4510943591594696, 0.4515749216079712, 0.4512239098548889, 0.45090219378471375, 0.45084837079048157, 0.45157596468925476, 0.45076704025268555, 0.45072001218795776, 0.45073825120925903, 0.45052218437194824, 0.45057716965675354, 0.450736939907074, 0.4508250057697296, 0.45055055618286133, 0.45243528485298157, 0.4504234790802002, 0.45105820894241333, 0.45113667845726013, 0.4510962665081024, 0.45024624466896057, 0.4501953721046448, 0.4501582980155945, 0.45091551542282104, 0.45025479793548584, 0.45040035247802734, 0.45013129711151123, 0.4504242539405823, 0.4501335620880127, 0.4502235949039459, 0.4503970742225647, 0.45064330101013184, 0.4504617154598236, 0.4499269425868988, 0.45007652044296265, 0.45022687315940857, 0.4502294361591339, 0.45018380880355835, 0.45002955198287964, 0.45155489444732666, 0.45008668303489685, 0.45012807846069336, 0.4504024386405945, 0.44987568259239197, 0.4498540461063385, 0.4499565362930298, 0.44991686940193176, 0.44983047246932983, 0.4507577121257782, 0.4499167203903198, 0.4498113691806793, 0.44989243149757385, 0.45004335045814514, 0.4505557417869568, 0.4500282406806946, 0.4502255916595459, 0.4496857225894928, 0.449664443731308, 0.45083585381507874, 0.4496609568595886, 0.44968634843826294, 0.45000284910202026, 0.45004814863204956, 0.4496561884880066, 0.4500771164894104, 0.44981926679611206, 0.4496699571609497, 0.450861394405365, 0.4495828151702881, 0.44985902309417725, 0.4497665762901306, 0.4502111077308655, 0.4498385787010193, 0.4494648873806, 0.44946810603141785, 0.44991686940193176, 0.4496142268180847, 0.4496302008628845, 0.4496467411518097, 0.44958174228668213, 0.4500190317630768, 0.4498930275440216, 0.44954535365104675, 0.4495176672935486, 0.4494989216327667, 0.44950583577156067, 0.44941118359565735, 0.4494883716106415, 0.4496507942676544, 0.450490802526474, 0.44949209690093994, 0.44959861040115356, 0.4496825039386749, 0.4497024416923523, 0.4495956003665924, 0.44969725608825684, 0.44944092631340027, 0.44942519068717957, 0.4498283863067627, 0.44946160912513733, 0.44951844215393066, 0.4495195150375366, 0.4495994448661804, 0.44985339045524597, 0.44951552152633667, 0.4494893550872803, 0.44985976815223694, 0.449538916349411, 0.4498812258243561, 0.44955259561538696, 0.44955015182495117, 0.44990137219429016, 0.44948306679725647, 0.44934624433517456, 0.449440062046051, 0.44940176606178284, 0.44952043890953064, 0.44955068826675415, 0.4494851529598236, 0.4500606656074524, 0.4494500458240509, 0.44947078824043274, 0.4493114650249481, 0.44932255148887634, 0.4495093524456024, 0.4493458569049835, 0.44939425587654114, 0.4493022859096527, 0.4493228793144226, 0.4493280053138733, 0.449307918548584, 0.4493425786495209, 0.4493488371372223, 0.4494386315345764, 0.44985464215278625, 0.45014673471450806, 0.4494364857673645, 0.4494204819202423, 0.4494057297706604, 0.44993287324905396, 0.4494435489177704, 0.4493623971939087, 0.4495009481906891, 0.4494057595729828], 'val_binary_accuracy': [0.8009222745895386, 0.8021179437637329, 0.8018728494644165, 0.8027008771896362, 0.8030462265014648, 0.8030722141265869, 0.8030313849449158, 0.8032244443893433, 0.8032430410385132, 0.8032541871070862, 0.8032764196395874, 0.8033878207206726, 0.8034249544143677, 0.8034620881080627, 0.8034287095069885, 0.8031836152076721, 0.8035289645195007, 0.8032504320144653, 0.8035586476325989, 0.8033024072647095, 0.8035697937011719, 0.8035549521446228, 0.8030908107757568, 0.8025375008583069, 0.8035475015640259, 0.803644061088562, 0.8033581376075745, 0.803632915019989, 0.8036774396896362, 0.8036069273948669, 0.8036403059959412, 0.8039039373397827, 0.8034732341766357, 0.8037999868392944, 0.8036514520645142, 0.8039299845695496, 0.8039596676826477, 0.8031947612762451, 0.8038111329078674, 0.8039039373397827, 0.8028791546821594, 0.8040116429328918, 0.8038779497146606, 0.8037368655204773, 0.8037666082382202, 0.8039708137512207, 0.8038519620895386, 0.803632915019989, 0.8036514520645142, 0.8038408160209656, 0.8039522171020508, 0.8040339350700378, 0.8038779497146606, 0.8037220239639282, 0.8039485216140747, 0.8039373755455017, 0.8036625981330872, 0.8037554621696472, 0.8036625981330872, 0.8035029172897339, 0.8038111329078674, 0.8038408160209656, 0.8033246994018555, 0.8038705587387085, 0.8040339350700378, 0.8039596676826477, 0.8038111329078674, 0.8039671182632446, 0.8039596676826477, 0.8035289645195007, 0.8037925958633423, 0.8039485216140747, 0.8037962913513184, 0.8037517070770264, 0.8036514520645142, 0.8037220239639282, 0.804119348526001, 0.8039002418518066, 0.8039522171020508, 0.8038965463638306, 0.8039893507957458, 0.803551197052002, 0.8039633631706238, 0.8039485216140747, 0.8040339350700378, 0.8039225339889526, 0.8040710687637329, 0.8037257194519043, 0.8039485216140747, 0.804108202457428, 0.8039076924324036, 0.8038594126701355, 0.8037108778953552, 0.8039782047271729, 0.8039559721946716, 0.8039819598197937, 0.8038817048072815, 0.8039968013763428, 0.8041155934333801, 0.8040562272071838, 0.8040673732757568, 0.8041750192642212, 0.8040004968643188, 0.8040599226951599, 0.8039633631706238, 0.8039262294769287, 0.8039522171020508, 0.8039262294769287, 0.804097056388855, 0.8037628531455994, 0.8039856553077698, 0.8039745092391968, 0.8040302395820618, 0.8040264844894409, 0.8039299845695496, 0.80404132604599, 0.8041267395019531, 0.803632915019989, 0.804108202457428, 0.804085910320282, 0.8038519620895386, 0.804100751876831, 0.804108202457428, 0.8039745092391968, 0.8040710687637329, 0.8040376305580139, 0.8040302395820618, 0.8039225339889526, 0.8040153384208679, 0.8041045069694519, 0.80413419008255, 0.804108202457428, 0.8040933609008789, 0.8039188385009766, 0.8041267395019531, 0.8041155934333801, 0.8039373755455017, 0.8040673732757568, 0.8039968013763428, 0.8041787147521973, 0.804123044013977, 0.80404132604599, 0.8040227890014648, 0.8040933609008789, 0.8039559721946716, 0.8037925958633423, 0.804111897945404, 0.8040599226951599, 0.804052472114563, 0.8039633631706238, 0.8041750192642212, 0.8038965463638306, 0.8042084574699402, 0.8041415810585022, 0.8039485216140747, 0.8040153384208679, 0.8042752742767334, 0.8039633631706238, 0.8038817048072815, 0.8038890957832336, 0.8040116429328918, 0.8039671182632446, 0.8040264844894409, 0.8039633631706238, 0.8040227890014648, 0.8038890957832336, 0.8041601777076721, 0.8037183284759521, 0.8039002418518066, 0.8042121529579163, 0.8042492866516113, 0.8042232990264893, 0.80413419008255, 0.8041155934333801, 0.8038631081581116, 0.8042121529579163, 0.8039708137512207, 0.8040302395820618, 0.8041155934333801, 0.8041490316390991, 0.8039931058883667, 0.8042604327201843, 0.80413419008255, 0.8041490316390991, 0.8042381405830383, 0.8043569922447205, 0.804119348526001, 0.8040264844894409, 0.804167628288269, 0.8043198585510254, 0.804100751876831, 0.804123044013977, 0.8042604327201843, 0.8042010068893433, 0.8040339350700378, 0.8041527271270752, 0.8040116429328918, 0.8041750192642212, 0.8040599226951599, 0.8041415810585022], 'val_auc_1': [0.6968113780021667, 0.7053870558738708, 0.7082394957542419, 0.7100293636322021, 0.711103618144989, 0.7119654417037964, 0.7120454907417297, 0.7121989130973816, 0.7126619219779968, 0.7126873731613159, 0.7129937410354614, 0.7129815220832825, 0.713230550289154, 0.7135046720504761, 0.7136468887329102, 0.7138921618461609, 0.7139853239059448, 0.7141614556312561, 0.7141874432563782, 0.7143824100494385, 0.7143641710281372, 0.7145151495933533, 0.7146555781364441, 0.7147181034088135, 0.7148797512054443, 0.7149463295936584, 0.714897871017456, 0.7153060436248779, 0.7163376212120056, 0.7167693376541138, 0.7165924906730652, 0.717079758644104, 0.7165283560752869, 0.7172262072563171, 0.7173007726669312, 0.7177025675773621, 0.7176558375358582, 0.7174668312072754, 0.7178144454956055, 0.7177650928497314, 0.7179111838340759, 0.717814564704895, 0.7181047201156616, 0.7183371782302856, 0.7181516289710999, 0.7184107303619385, 0.7183542251586914, 0.7181410193443298, 0.717933714389801, 0.7184654474258423, 0.7184450626373291, 0.7187942266464233, 0.7188577055931091, 0.7184447646141052, 0.7187114357948303, 0.7187982201576233, 0.7185478806495667, 0.7187461256980896, 0.7187487483024597, 0.7188363075256348, 0.7189188003540039, 0.7191919088363647, 0.7185685038566589, 0.7190899848937988, 0.7191075682640076, 0.7191361784934998, 0.7194237112998962, 0.7192738056182861, 0.718927264213562, 0.7190544009208679, 0.7192714214324951, 0.7189448475837708, 0.7194583415985107, 0.7194789052009583, 0.7194156646728516, 0.7197068929672241, 0.719437301158905, 0.7194238901138306, 0.7195533514022827, 0.719257652759552, 0.7194675803184509, 0.719534695148468, 0.7194840312004089, 0.7192844152450562, 0.7196094989776611, 0.7194362282752991, 0.7194100618362427, 0.719302773475647, 0.7194089293479919, 0.7197670936584473, 0.7194972038269043, 0.7196712493896484, 0.719291090965271, 0.7193537950515747, 0.7195417284965515, 0.7195722460746765, 0.7196965217590332, 0.7197263240814209, 0.7195895910263062, 0.7198010087013245, 0.7196078896522522, 0.7194995284080505, 0.7196664214134216, 0.7198455929756165, 0.7197160720825195, 0.7196134924888611, 0.7198066711425781, 0.7200068831443787, 0.719878077507019, 0.7195174098014832, 0.7199660539627075, 0.7198294401168823, 0.719969630241394, 0.719810426235199, 0.7199779748916626, 0.719977617263794, 0.7198526859283447, 0.7198625802993774, 0.7198010087013245, 0.7199218273162842, 0.7200430631637573, 0.7197694182395935, 0.7198245525360107, 0.7199945449829102, 0.720016598701477, 0.7198982238769531, 0.7198842763900757, 0.71993488073349, 0.7199639081954956, 0.7201526165008545, 0.7201578617095947, 0.7200427055358887, 0.720089852809906, 0.720103919506073, 0.7200950384140015, 0.7200575470924377, 0.7201189398765564, 0.719880223274231, 0.7201176285743713, 0.7200717329978943, 0.7200771570205688, 0.7200002670288086, 0.7201942801475525, 0.720038652420044, 0.7201393842697144, 0.7200660109519958, 0.7200894355773926, 0.720109224319458, 0.7199723720550537, 0.720093846321106, 0.720151424407959, 0.7201188206672668, 0.7202373147010803, 0.7200602889060974, 0.7199819087982178, 0.7201759815216064, 0.7201249003410339, 0.7200863361358643, 0.7200401425361633, 0.7200914025306702, 0.7199882864952087, 0.7200691103935242, 0.7201400995254517, 0.7200894355773926, 0.720231831073761, 0.720112144947052, 0.719937264919281, 0.7201053500175476, 0.7202092409133911, 0.7201462984085083, 0.7200055718421936, 0.720057487487793, 0.7200562953948975, 0.7200928926467896, 0.7202435731887817, 0.7200807332992554, 0.7201110124588013, 0.7201710343360901, 0.720215380191803, 0.7202618718147278, 0.7200947999954224, 0.7202206254005432, 0.7201144099235535, 0.720212996006012, 0.7201966047286987, 0.7201929688453674, 0.720258891582489, 0.7201566100120544, 0.7201176285743713, 0.7200514674186707, 0.7201381921768188, 0.7200760245323181, 0.7201984524726868, 0.7202408909797668, 0.7201940417289734, 0.7199556827545166, 0.7201932668685913, 0.7203487157821655, 0.7201583981513977, 0.7202057838439941]}\n",
            "263/263 [==============================] - 5s 17ms/step - loss: 0.4494 - binary_accuracy: 0.8042 - auc_1: 0.7202\n",
            "[0.4493977129459381, 0.8041527271270752, 0.7202260494232178]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFqPVwy1J-Gs",
        "outputId": "c4c8330d-bdeb-4075-dd81-132920ae4a9f"
      },
      "source": [
        "# random sumpling \n",
        "\n",
        "name='cs' \n",
        "num_heads= 8\n",
        "input_heads = 8\n",
        "use_masking=False\n",
        "shape_v = 83\n",
        "model_d = 16\n",
        "SA_layers = 3\n",
        "\n",
        "batch_size = batch_size\n",
        "epochs = 200\n",
        "total_steps  = len(train_dataset) * epochs\n",
        "\n",
        "#with tf.device('/device:GPU:0'):\n",
        "with strategy.scope():\n",
        "  original_inputs = Input(shape=(shape_v,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "  #output_w = inputW(model_d)(original_inputs)\n",
        "  #output = mlpBlock([83, 128], original_inputs)\n",
        "  output = layers.batchNormalization()(original_inputs)\n",
        "  output = inputW_Em(model_d, input_heads)(output)\n",
        "  output = LayerNormalization(name=f'{name}_normalization_before')(output)\n",
        "  #output = inputW0_4(model_d)(original_inputs)\n",
        "\n",
        "  for i in range(SA_layers):\n",
        "    output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention_{i}')(output)\n",
        "    post_residual1 = (Add(name=f'{name}_add_{i}')([output_sa, output]))\n",
        "    norm1_output = LayerNormalization(name=f'{name}_normalization1_{i}')(post_residual1)\n",
        "    output = TransformerTransition(name=f'{name}_transition_{i}', activation='relu')(norm1_output)\n",
        "    post_residual2 = (Add(name=f'{name}_add2_{i}')([norm1_output,output]))\n",
        "    output = LayerNormalization(name=f'{name}_normalization2_{i}')(post_residual2)\n",
        "\n",
        "  output = Flatten()(output)\n",
        "\n",
        "  \"\"\"\n",
        "  output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  \"\"\"\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(1, activation='sigmoid')(output)\n",
        "  cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "  cs_p2p.summary()\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-4,\n",
        "      decay_steps=10000,\n",
        "      decay_rate=0.9)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "  #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "  #optimizer = RAdamOptimizer(total_steps=total_steps, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "  cs_p2p.compile(optimizer, \n",
        "                 loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                 #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "          metrics=[\n",
        "          #tf.keras.metrics.MeanSquaredError(),\n",
        "          tf.keras.metrics.BinaryAccuracy(),\n",
        "          #tf.keras.metrics.Precision(),\n",
        "          #tf.keras.metrics.Recall(),\n",
        "          #'accuracy',\n",
        "          tf.keras.metrics.AUC(),]\n",
        "          )\n",
        "  history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=test_dataset, validation_batch_size=batch_size)\n",
        "  #history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "  #history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  print(history.history)\n",
        "  #cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  result = cs_p2p.evaluate(test_dataset)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cs_p2p\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cs_p2p (InputLayer)             [(128, 83)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_w__em (inputW_Em)         (128, 83, 16)        23987       cs_p2p[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization_before (LayerN (128, 83, 16)        32          input_w__em[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_0 (MultiHeadS (128, 83, 16)        768         cs_normalization_before[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_0 (Add)                  (128, 83, 16)        0           cs_self_attention_0[0][0]        \n",
            "                                                                 cs_normalization_before[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_0 (LayerNorma (128, 83, 16)        32          cs_add_0[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_0 (TransformerTra (128, 83, 16)        2128        cs_normalization1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_0 (Add)                 (128, 83, 16)        0           cs_normalization1_0[0][0]        \n",
            "                                                                 cs_transition_0[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_0 (LayerNorma (128, 83, 16)        32          cs_add2_0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_1 (MultiHeadS (128, 83, 16)        768         cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_1 (Add)                  (128, 83, 16)        0           cs_self_attention_1[0][0]        \n",
            "                                                                 cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_1 (LayerNorma (128, 83, 16)        32          cs_add_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_1 (TransformerTra (128, 83, 16)        2128        cs_normalization1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_1 (Add)                 (128, 83, 16)        0           cs_normalization1_1[0][0]        \n",
            "                                                                 cs_transition_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_1 (LayerNorma (128, 83, 16)        32          cs_add2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_2 (MultiHeadS (128, 83, 16)        768         cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_2 (Add)                  (128, 83, 16)        0           cs_self_attention_2[0][0]        \n",
            "                                                                 cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_2 (LayerNorma (128, 83, 16)        32          cs_add_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_2 (TransformerTra (128, 83, 16)        2128        cs_normalization1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_2 (Add)                 (128, 83, 16)        0           cs_normalization1_2[0][0]        \n",
            "                                                                 cs_transition_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_2 (LayerNorma (128, 83, 16)        32          cs_add2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (128, 1328)          0           cs_normalization2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (128, 1328)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (128, 32)            42528       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (128, 32)            0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (128, 1)             33          dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 75,460\n",
            "Trainable params: 75,460\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "  1/789 [..............................] - ETA: 48:04 - loss: 1.0199 - binary_accuracy: 0.3379 - auc: 0.4982WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0270s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0270s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "787/789 [============================>.] - ETA: 0s - loss: 0.4954 - binary_accuracy: 0.7983 - auc: 0.6579WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0117s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0117s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r789/789 [==============================] - 30s 38ms/step - loss: 0.4955 - binary_accuracy: 0.7983 - auc: 0.6579 - val_loss: 0.4738 - val_binary_accuracy: 0.8017 - val_auc: 0.7045\n",
            "Epoch 2/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4785 - binary_accuracy: 0.8007 - auc: 0.6936 - val_loss: 0.4702 - val_binary_accuracy: 0.8023 - val_auc: 0.7081\n",
            "Epoch 3/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4757 - binary_accuracy: 0.8011 - auc: 0.6961 - val_loss: 0.4687 - val_binary_accuracy: 0.8021 - val_auc: 0.7099\n",
            "Epoch 4/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4728 - binary_accuracy: 0.8015 - auc: 0.6995 - val_loss: 0.4659 - val_binary_accuracy: 0.8028 - val_auc: 0.7115\n",
            "Epoch 5/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4706 - binary_accuracy: 0.8015 - auc: 0.7011 - val_loss: 0.4656 - val_binary_accuracy: 0.8028 - val_auc: 0.7116\n",
            "Epoch 6/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4684 - binary_accuracy: 0.8016 - auc: 0.7026 - val_loss: 0.4624 - val_binary_accuracy: 0.8030 - val_auc: 0.7120\n",
            "Epoch 7/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4667 - binary_accuracy: 0.8018 - auc: 0.7032 - val_loss: 0.4611 - val_binary_accuracy: 0.8032 - val_auc: 0.7122\n",
            "Epoch 8/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4651 - binary_accuracy: 0.8019 - auc: 0.7041 - val_loss: 0.4597 - val_binary_accuracy: 0.8031 - val_auc: 0.7124\n",
            "Epoch 9/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4638 - binary_accuracy: 0.8020 - auc: 0.7044 - val_loss: 0.4589 - val_binary_accuracy: 0.8034 - val_auc: 0.7127\n",
            "Epoch 10/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4627 - binary_accuracy: 0.8022 - auc: 0.7048 - val_loss: 0.4599 - val_binary_accuracy: 0.8032 - val_auc: 0.7129\n",
            "Epoch 11/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4617 - binary_accuracy: 0.8023 - auc: 0.7056 - val_loss: 0.4571 - val_binary_accuracy: 0.8033 - val_auc: 0.7132\n",
            "Epoch 12/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4609 - binary_accuracy: 0.8020 - auc: 0.7061 - val_loss: 0.4583 - val_binary_accuracy: 0.8034 - val_auc: 0.7135\n",
            "Epoch 13/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4605 - binary_accuracy: 0.8022 - auc: 0.7063 - val_loss: 0.4567 - val_binary_accuracy: 0.8034 - val_auc: 0.7133\n",
            "Epoch 14/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4601 - binary_accuracy: 0.8024 - auc: 0.7066 - val_loss: 0.4559 - val_binary_accuracy: 0.8035 - val_auc: 0.7137\n",
            "Epoch 15/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4595 - binary_accuracy: 0.8021 - auc: 0.7075 - val_loss: 0.4561 - val_binary_accuracy: 0.8035 - val_auc: 0.7138\n",
            "Epoch 16/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4592 - binary_accuracy: 0.8025 - auc: 0.7076 - val_loss: 0.4559 - val_binary_accuracy: 0.8031 - val_auc: 0.7141\n",
            "Epoch 17/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4589 - binary_accuracy: 0.8023 - auc: 0.7080 - val_loss: 0.4557 - val_binary_accuracy: 0.8033 - val_auc: 0.7145\n",
            "Epoch 18/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4584 - binary_accuracy: 0.8022 - auc: 0.7089 - val_loss: 0.4551 - val_binary_accuracy: 0.8036 - val_auc: 0.7146\n",
            "Epoch 19/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4579 - binary_accuracy: 0.8024 - auc: 0.7097 - val_loss: 0.4544 - val_binary_accuracy: 0.8034 - val_auc: 0.7157\n",
            "Epoch 20/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4578 - binary_accuracy: 0.8025 - auc: 0.7095 - val_loss: 0.4547 - val_binary_accuracy: 0.8037 - val_auc: 0.7157\n",
            "Epoch 21/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4574 - binary_accuracy: 0.8025 - auc: 0.7102 - val_loss: 0.4537 - val_binary_accuracy: 0.8038 - val_auc: 0.7164\n",
            "Epoch 22/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4570 - binary_accuracy: 0.8026 - auc: 0.7108 - val_loss: 0.4537 - val_binary_accuracy: 0.8036 - val_auc: 0.7162\n",
            "Epoch 23/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4567 - binary_accuracy: 0.8027 - auc: 0.7111 - val_loss: 0.4535 - val_binary_accuracy: 0.8036 - val_auc: 0.7164\n",
            "Epoch 24/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4567 - binary_accuracy: 0.8026 - auc: 0.7109 - val_loss: 0.4534 - val_binary_accuracy: 0.8032 - val_auc: 0.7163\n",
            "Epoch 25/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4564 - binary_accuracy: 0.8028 - auc: 0.7111 - val_loss: 0.4549 - val_binary_accuracy: 0.8036 - val_auc: 0.7168\n",
            "Epoch 26/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4564 - binary_accuracy: 0.8026 - auc: 0.7111 - val_loss: 0.4540 - val_binary_accuracy: 0.8036 - val_auc: 0.7168\n",
            "Epoch 27/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4558 - binary_accuracy: 0.8027 - auc: 0.7121 - val_loss: 0.4532 - val_binary_accuracy: 0.8037 - val_auc: 0.7169\n",
            "Epoch 28/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4557 - binary_accuracy: 0.8028 - auc: 0.7121 - val_loss: 0.4531 - val_binary_accuracy: 0.8037 - val_auc: 0.7170\n",
            "Epoch 29/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4555 - binary_accuracy: 0.8029 - auc: 0.7124 - val_loss: 0.4525 - val_binary_accuracy: 0.8038 - val_auc: 0.7172\n",
            "Epoch 30/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4552 - binary_accuracy: 0.8027 - auc: 0.7128 - val_loss: 0.4535 - val_binary_accuracy: 0.8039 - val_auc: 0.7173\n",
            "Epoch 31/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4552 - binary_accuracy: 0.8027 - auc: 0.7128 - val_loss: 0.4523 - val_binary_accuracy: 0.8039 - val_auc: 0.7173\n",
            "Epoch 32/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4551 - binary_accuracy: 0.8027 - auc: 0.7130 - val_loss: 0.4521 - val_binary_accuracy: 0.8037 - val_auc: 0.7176\n",
            "Epoch 33/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4549 - binary_accuracy: 0.8025 - auc: 0.7131 - val_loss: 0.4520 - val_binary_accuracy: 0.8038 - val_auc: 0.7176\n",
            "Epoch 34/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4546 - binary_accuracy: 0.8028 - auc: 0.7136 - val_loss: 0.4528 - val_binary_accuracy: 0.8038 - val_auc: 0.7177\n",
            "Epoch 35/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4546 - binary_accuracy: 0.8030 - auc: 0.7134 - val_loss: 0.4516 - val_binary_accuracy: 0.8039 - val_auc: 0.7181\n",
            "Epoch 36/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4544 - binary_accuracy: 0.8030 - auc: 0.7137 - val_loss: 0.4530 - val_binary_accuracy: 0.8036 - val_auc: 0.7179\n",
            "Epoch 37/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4543 - binary_accuracy: 0.8030 - auc: 0.7139 - val_loss: 0.4515 - val_binary_accuracy: 0.8037 - val_auc: 0.7183\n",
            "Epoch 38/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4543 - binary_accuracy: 0.8029 - auc: 0.7139 - val_loss: 0.4515 - val_binary_accuracy: 0.8038 - val_auc: 0.7183\n",
            "Epoch 39/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4540 - binary_accuracy: 0.8029 - auc: 0.7144 - val_loss: 0.4513 - val_binary_accuracy: 0.8039 - val_auc: 0.7185\n",
            "Epoch 40/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4540 - binary_accuracy: 0.8026 - auc: 0.7144 - val_loss: 0.4521 - val_binary_accuracy: 0.8036 - val_auc: 0.7181\n",
            "Epoch 41/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4539 - binary_accuracy: 0.8028 - auc: 0.7145 - val_loss: 0.4513 - val_binary_accuracy: 0.8037 - val_auc: 0.7184\n",
            "Epoch 42/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4537 - binary_accuracy: 0.8028 - auc: 0.7149 - val_loss: 0.4511 - val_binary_accuracy: 0.8040 - val_auc: 0.7184\n",
            "Epoch 43/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4537 - binary_accuracy: 0.8028 - auc: 0.7146 - val_loss: 0.4512 - val_binary_accuracy: 0.8038 - val_auc: 0.7186\n",
            "Epoch 44/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4535 - binary_accuracy: 0.8029 - auc: 0.7150 - val_loss: 0.4512 - val_binary_accuracy: 0.8040 - val_auc: 0.7184\n",
            "Epoch 45/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4537 - binary_accuracy: 0.8028 - auc: 0.7147 - val_loss: 0.4512 - val_binary_accuracy: 0.8040 - val_auc: 0.7185\n",
            "Epoch 46/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4534 - binary_accuracy: 0.8029 - auc: 0.7151 - val_loss: 0.4512 - val_binary_accuracy: 0.8037 - val_auc: 0.7186\n",
            "Epoch 47/200\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4533 - binary_accuracy: 0.8029 - auc: 0.7153 - val_loss: 0.4513 - val_binary_accuracy: 0.8037 - val_auc: 0.7183\n",
            "Epoch 48/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4531 - binary_accuracy: 0.8028 - auc: 0.7156 - val_loss: 0.4512 - val_binary_accuracy: 0.8040 - val_auc: 0.7189\n",
            "Epoch 49/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4532 - binary_accuracy: 0.8030 - auc: 0.7153 - val_loss: 0.4511 - val_binary_accuracy: 0.8037 - val_auc: 0.7187\n",
            "Epoch 50/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4531 - binary_accuracy: 0.8029 - auc: 0.7154 - val_loss: 0.4509 - val_binary_accuracy: 0.8042 - val_auc: 0.7186\n",
            "Epoch 51/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4530 - binary_accuracy: 0.8030 - auc: 0.7156 - val_loss: 0.4512 - val_binary_accuracy: 0.8038 - val_auc: 0.7188\n",
            "Epoch 52/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4528 - binary_accuracy: 0.8030 - auc: 0.7159 - val_loss: 0.4510 - val_binary_accuracy: 0.8038 - val_auc: 0.7189\n",
            "Epoch 53/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4530 - binary_accuracy: 0.8028 - auc: 0.7157 - val_loss: 0.4506 - val_binary_accuracy: 0.8041 - val_auc: 0.7189\n",
            "Epoch 54/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4528 - binary_accuracy: 0.8030 - auc: 0.7160 - val_loss: 0.4510 - val_binary_accuracy: 0.8037 - val_auc: 0.7191\n",
            "Epoch 55/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4527 - binary_accuracy: 0.8029 - auc: 0.7161 - val_loss: 0.4507 - val_binary_accuracy: 0.8035 - val_auc: 0.7193\n",
            "Epoch 56/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4526 - binary_accuracy: 0.8032 - auc: 0.7161 - val_loss: 0.4508 - val_binary_accuracy: 0.8041 - val_auc: 0.7187\n",
            "Epoch 57/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4527 - binary_accuracy: 0.8032 - auc: 0.7161 - val_loss: 0.4509 - val_binary_accuracy: 0.8039 - val_auc: 0.7193\n",
            "Epoch 58/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4527 - binary_accuracy: 0.8030 - auc: 0.7159 - val_loss: 0.4506 - val_binary_accuracy: 0.8041 - val_auc: 0.7189\n",
            "Epoch 59/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4525 - binary_accuracy: 0.8031 - auc: 0.7164 - val_loss: 0.4504 - val_binary_accuracy: 0.8038 - val_auc: 0.7194\n",
            "Epoch 60/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4524 - binary_accuracy: 0.8032 - auc: 0.7165 - val_loss: 0.4514 - val_binary_accuracy: 0.8036 - val_auc: 0.7191\n",
            "Epoch 61/200\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4523 - binary_accuracy: 0.8030 - auc: 0.7168 - val_loss: 0.4508 - val_binary_accuracy: 0.8040 - val_auc: 0.7192\n",
            "Epoch 62/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4523 - binary_accuracy: 0.8031 - auc: 0.7166 - val_loss: 0.4504 - val_binary_accuracy: 0.8042 - val_auc: 0.7194\n",
            "Epoch 63/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4523 - binary_accuracy: 0.8032 - auc: 0.7168 - val_loss: 0.4507 - val_binary_accuracy: 0.8040 - val_auc: 0.7192\n",
            "Epoch 64/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4522 - binary_accuracy: 0.8034 - auc: 0.7166 - val_loss: 0.4525 - val_binary_accuracy: 0.8036 - val_auc: 0.7191\n",
            "Epoch 65/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4521 - binary_accuracy: 0.8032 - auc: 0.7170 - val_loss: 0.4503 - val_binary_accuracy: 0.8040 - val_auc: 0.7193\n",
            "Epoch 66/200\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4523 - binary_accuracy: 0.8032 - auc: 0.7166 - val_loss: 0.4510 - val_binary_accuracy: 0.8036 - val_auc: 0.7189\n",
            "Epoch 67/200\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4521 - binary_accuracy: 0.8032 - auc: 0.7170 - val_loss: 0.4502 - val_binary_accuracy: 0.8042 - val_auc: 0.7195\n",
            "Epoch 68/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4520 - binary_accuracy: 0.8032 - auc: 0.7171 - val_loss: 0.4503 - val_binary_accuracy: 0.8039 - val_auc: 0.7194\n",
            "Epoch 69/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4519 - binary_accuracy: 0.8032 - auc: 0.7174 - val_loss: 0.4506 - val_binary_accuracy: 0.8039 - val_auc: 0.7194\n",
            "Epoch 70/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4519 - binary_accuracy: 0.8034 - auc: 0.7173 - val_loss: 0.4501 - val_binary_accuracy: 0.8041 - val_auc: 0.7195\n",
            "Epoch 71/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4518 - binary_accuracy: 0.8033 - auc: 0.7175 - val_loss: 0.4522 - val_binary_accuracy: 0.8024 - val_auc: 0.7194\n",
            "Epoch 72/200\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4520 - binary_accuracy: 0.8033 - auc: 0.7170 - val_loss: 0.4503 - val_binary_accuracy: 0.8040 - val_auc: 0.7196\n",
            "Epoch 73/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4518 - binary_accuracy: 0.8033 - auc: 0.7173 - val_loss: 0.4500 - val_binary_accuracy: 0.8042 - val_auc: 0.7195\n",
            "Epoch 74/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4517 - binary_accuracy: 0.8035 - auc: 0.7176 - val_loss: 0.4500 - val_binary_accuracy: 0.8042 - val_auc: 0.7196\n",
            "Epoch 75/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4517 - binary_accuracy: 0.8032 - auc: 0.7175 - val_loss: 0.4505 - val_binary_accuracy: 0.8042 - val_auc: 0.7194\n",
            "Epoch 76/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4515 - binary_accuracy: 0.8035 - auc: 0.7178 - val_loss: 0.4501 - val_binary_accuracy: 0.8040 - val_auc: 0.7197\n",
            "Epoch 77/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4515 - binary_accuracy: 0.8032 - auc: 0.7178 - val_loss: 0.4501 - val_binary_accuracy: 0.8043 - val_auc: 0.7193\n",
            "Epoch 78/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4514 - binary_accuracy: 0.8033 - auc: 0.7178 - val_loss: 0.4499 - val_binary_accuracy: 0.8042 - val_auc: 0.7197\n",
            "Epoch 79/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4516 - binary_accuracy: 0.8032 - auc: 0.7175 - val_loss: 0.4501 - val_binary_accuracy: 0.8040 - val_auc: 0.7194\n",
            "Epoch 80/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4514 - binary_accuracy: 0.8033 - auc: 0.7180 - val_loss: 0.4500 - val_binary_accuracy: 0.8040 - val_auc: 0.7195\n",
            "Epoch 81/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4514 - binary_accuracy: 0.8033 - auc: 0.7181 - val_loss: 0.4501 - val_binary_accuracy: 0.8041 - val_auc: 0.7194\n",
            "Epoch 82/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4514 - binary_accuracy: 0.8033 - auc: 0.7179 - val_loss: 0.4498 - val_binary_accuracy: 0.8042 - val_auc: 0.7198\n",
            "Epoch 83/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4514 - binary_accuracy: 0.8033 - auc: 0.7180 - val_loss: 0.4498 - val_binary_accuracy: 0.8041 - val_auc: 0.7196\n",
            "Epoch 84/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4513 - binary_accuracy: 0.8034 - auc: 0.7181 - val_loss: 0.4501 - val_binary_accuracy: 0.8041 - val_auc: 0.7194\n",
            "Epoch 85/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4513 - binary_accuracy: 0.8036 - auc: 0.7183 - val_loss: 0.4503 - val_binary_accuracy: 0.8042 - val_auc: 0.7199\n",
            "Epoch 86/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4511 - binary_accuracy: 0.8034 - auc: 0.7185 - val_loss: 0.4499 - val_binary_accuracy: 0.8040 - val_auc: 0.7198\n",
            "Epoch 87/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4510 - binary_accuracy: 0.8034 - auc: 0.7187 - val_loss: 0.4498 - val_binary_accuracy: 0.8042 - val_auc: 0.7198\n",
            "Epoch 88/200\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4511 - binary_accuracy: 0.8034 - auc: 0.7185 - val_loss: 0.4500 - val_binary_accuracy: 0.8042 - val_auc: 0.7197\n",
            "Epoch 89/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4511 - binary_accuracy: 0.8034 - auc: 0.7185 - val_loss: 0.4504 - val_binary_accuracy: 0.8035 - val_auc: 0.7198\n",
            "Epoch 90/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4510 - binary_accuracy: 0.8035 - auc: 0.7185 - val_loss: 0.4498 - val_binary_accuracy: 0.8042 - val_auc: 0.7197\n",
            "Epoch 91/200\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4509 - binary_accuracy: 0.8036 - auc: 0.7189 - val_loss: 0.4497 - val_binary_accuracy: 0.8042 - val_auc: 0.7198\n",
            "Epoch 92/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4510 - binary_accuracy: 0.8036 - auc: 0.7186 - val_loss: 0.4504 - val_binary_accuracy: 0.8043 - val_auc: 0.7198\n",
            "Epoch 93/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4509 - binary_accuracy: 0.8035 - auc: 0.7189 - val_loss: 0.4501 - val_binary_accuracy: 0.8041 - val_auc: 0.7199\n",
            "Epoch 94/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4509 - binary_accuracy: 0.8034 - auc: 0.7189 - val_loss: 0.4497 - val_binary_accuracy: 0.8043 - val_auc: 0.7200\n",
            "Epoch 95/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4508 - binary_accuracy: 0.8033 - auc: 0.7190 - val_loss: 0.4498 - val_binary_accuracy: 0.8040 - val_auc: 0.7197\n",
            "Epoch 96/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4508 - binary_accuracy: 0.8036 - auc: 0.7189 - val_loss: 0.4501 - val_binary_accuracy: 0.8044 - val_auc: 0.7195\n",
            "Epoch 97/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4508 - binary_accuracy: 0.8035 - auc: 0.7191 - val_loss: 0.4498 - val_binary_accuracy: 0.8043 - val_auc: 0.7198\n",
            "Epoch 98/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4508 - binary_accuracy: 0.8035 - auc: 0.7189 - val_loss: 0.4506 - val_binary_accuracy: 0.8042 - val_auc: 0.7199\n",
            "Epoch 99/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4509 - binary_accuracy: 0.8033 - auc: 0.7189 - val_loss: 0.4505 - val_binary_accuracy: 0.8042 - val_auc: 0.7198\n",
            "Epoch 100/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4507 - binary_accuracy: 0.8037 - auc: 0.7193 - val_loss: 0.4497 - val_binary_accuracy: 0.8042 - val_auc: 0.7199\n",
            "Epoch 101/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4507 - binary_accuracy: 0.8036 - auc: 0.7192 - val_loss: 0.4497 - val_binary_accuracy: 0.8043 - val_auc: 0.7199\n",
            "Epoch 102/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4506 - binary_accuracy: 0.8036 - auc: 0.7194 - val_loss: 0.4505 - val_binary_accuracy: 0.8040 - val_auc: 0.7200\n",
            "Epoch 103/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4506 - binary_accuracy: 0.8035 - auc: 0.7194 - val_loss: 0.4496 - val_binary_accuracy: 0.8041 - val_auc: 0.7199\n",
            "Epoch 104/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4506 - binary_accuracy: 0.8036 - auc: 0.7193 - val_loss: 0.4496 - val_binary_accuracy: 0.8043 - val_auc: 0.7201\n",
            "Epoch 105/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4505 - binary_accuracy: 0.8036 - auc: 0.7195 - val_loss: 0.4497 - val_binary_accuracy: 0.8041 - val_auc: 0.7198\n",
            "Epoch 106/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4506 - binary_accuracy: 0.8036 - auc: 0.7195 - val_loss: 0.4499 - val_binary_accuracy: 0.8039 - val_auc: 0.7198\n",
            "Epoch 107/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4506 - binary_accuracy: 0.8033 - auc: 0.7194 - val_loss: 0.4497 - val_binary_accuracy: 0.8044 - val_auc: 0.7197\n",
            "Epoch 108/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4506 - binary_accuracy: 0.8036 - auc: 0.7193 - val_loss: 0.4497 - val_binary_accuracy: 0.8040 - val_auc: 0.7200\n",
            "Epoch 109/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4503 - binary_accuracy: 0.8035 - auc: 0.7200 - val_loss: 0.4510 - val_binary_accuracy: 0.8042 - val_auc: 0.7200\n",
            "Epoch 110/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4505 - binary_accuracy: 0.8036 - auc: 0.7195 - val_loss: 0.4506 - val_binary_accuracy: 0.8037 - val_auc: 0.7201\n",
            "Epoch 111/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4504 - binary_accuracy: 0.8037 - auc: 0.7197 - val_loss: 0.4495 - val_binary_accuracy: 0.8045 - val_auc: 0.7200\n",
            "Epoch 112/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4504 - binary_accuracy: 0.8037 - auc: 0.7197 - val_loss: 0.4496 - val_binary_accuracy: 0.8045 - val_auc: 0.7199\n",
            "Epoch 113/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4504 - binary_accuracy: 0.8036 - auc: 0.7197 - val_loss: 0.4496 - val_binary_accuracy: 0.8043 - val_auc: 0.7197\n",
            "Epoch 114/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4504 - binary_accuracy: 0.8035 - auc: 0.7197 - val_loss: 0.4497 - val_binary_accuracy: 0.8041 - val_auc: 0.7200\n",
            "Epoch 115/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4504 - binary_accuracy: 0.8036 - auc: 0.7197 - val_loss: 0.4495 - val_binary_accuracy: 0.8043 - val_auc: 0.7200\n",
            "Epoch 116/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4503 - binary_accuracy: 0.8036 - auc: 0.7200 - val_loss: 0.4497 - val_binary_accuracy: 0.8040 - val_auc: 0.7197\n",
            "Epoch 117/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4503 - binary_accuracy: 0.8035 - auc: 0.7200 - val_loss: 0.4497 - val_binary_accuracy: 0.8042 - val_auc: 0.7198\n",
            "Epoch 118/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4502 - binary_accuracy: 0.8037 - auc: 0.7200 - val_loss: 0.4501 - val_binary_accuracy: 0.8042 - val_auc: 0.7198\n",
            "Epoch 119/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4501 - binary_accuracy: 0.8036 - auc: 0.7202 - val_loss: 0.4495 - val_binary_accuracy: 0.8045 - val_auc: 0.7198\n",
            "Epoch 120/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4503 - binary_accuracy: 0.8037 - auc: 0.7199 - val_loss: 0.4498 - val_binary_accuracy: 0.8042 - val_auc: 0.7199\n",
            "Epoch 121/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4501 - binary_accuracy: 0.8035 - auc: 0.7202 - val_loss: 0.4496 - val_binary_accuracy: 0.8042 - val_auc: 0.7201\n",
            "Epoch 122/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4501 - binary_accuracy: 0.8036 - auc: 0.7201 - val_loss: 0.4502 - val_binary_accuracy: 0.8040 - val_auc: 0.7200\n",
            "Epoch 123/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4500 - binary_accuracy: 0.8037 - auc: 0.7204 - val_loss: 0.4494 - val_binary_accuracy: 0.8040 - val_auc: 0.7201\n",
            "Epoch 124/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4501 - binary_accuracy: 0.8036 - auc: 0.7203 - val_loss: 0.4495 - val_binary_accuracy: 0.8044 - val_auc: 0.7200\n",
            "Epoch 125/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4500 - binary_accuracy: 0.8037 - auc: 0.7204 - val_loss: 0.4498 - val_binary_accuracy: 0.8043 - val_auc: 0.7201\n",
            "Epoch 126/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4500 - binary_accuracy: 0.8038 - auc: 0.7204 - val_loss: 0.4494 - val_binary_accuracy: 0.8042 - val_auc: 0.7201\n",
            "Epoch 127/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4501 - binary_accuracy: 0.8036 - auc: 0.7203 - val_loss: 0.4494 - val_binary_accuracy: 0.8042 - val_auc: 0.7201\n",
            "Epoch 128/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4500 - binary_accuracy: 0.8037 - auc: 0.7204 - val_loss: 0.4505 - val_binary_accuracy: 0.8041 - val_auc: 0.7199\n",
            "Epoch 129/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4501 - binary_accuracy: 0.8037 - auc: 0.7202 - val_loss: 0.4494 - val_binary_accuracy: 0.8043 - val_auc: 0.7200\n",
            "Epoch 130/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4500 - binary_accuracy: 0.8038 - auc: 0.7204 - val_loss: 0.4496 - val_binary_accuracy: 0.8042 - val_auc: 0.7201\n",
            "Epoch 131/200\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4500 - binary_accuracy: 0.8037 - auc: 0.7202 - val_loss: 0.4502 - val_binary_accuracy: 0.8041 - val_auc: 0.7200\n",
            "Epoch 132/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4499 - binary_accuracy: 0.8036 - auc: 0.7205 - val_loss: 0.4494 - val_binary_accuracy: 0.8044 - val_auc: 0.7200\n",
            "Epoch 133/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4499 - binary_accuracy: 0.8039 - auc: 0.7206 - val_loss: 0.4496 - val_binary_accuracy: 0.8042 - val_auc: 0.7200\n",
            "Epoch 134/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4500 - binary_accuracy: 0.8037 - auc: 0.7205 - val_loss: 0.4494 - val_binary_accuracy: 0.8043 - val_auc: 0.7200\n",
            "Epoch 135/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4499 - binary_accuracy: 0.8037 - auc: 0.7206 - val_loss: 0.4498 - val_binary_accuracy: 0.8043 - val_auc: 0.7199\n",
            "Epoch 136/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4500 - binary_accuracy: 0.8040 - auc: 0.7204 - val_loss: 0.4494 - val_binary_accuracy: 0.8044 - val_auc: 0.7201\n",
            "Epoch 137/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4499 - binary_accuracy: 0.8037 - auc: 0.7206 - val_loss: 0.4496 - val_binary_accuracy: 0.8039 - val_auc: 0.7198\n",
            "Epoch 138/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4498 - binary_accuracy: 0.8039 - auc: 0.7206 - val_loss: 0.4494 - val_binary_accuracy: 0.8046 - val_auc: 0.7200\n",
            "Epoch 139/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4498 - binary_accuracy: 0.8038 - auc: 0.7207 - val_loss: 0.4495 - val_binary_accuracy: 0.8043 - val_auc: 0.7202\n",
            "Epoch 140/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4497 - binary_accuracy: 0.8037 - auc: 0.7209 - val_loss: 0.4497 - val_binary_accuracy: 0.8042 - val_auc: 0.7200\n",
            "Epoch 141/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4497 - binary_accuracy: 0.8038 - auc: 0.7209 - val_loss: 0.4500 - val_binary_accuracy: 0.8042 - val_auc: 0.7202\n",
            "Epoch 142/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4497 - binary_accuracy: 0.8038 - auc: 0.7209 - val_loss: 0.4494 - val_binary_accuracy: 0.8044 - val_auc: 0.7201\n",
            "Epoch 143/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4498 - binary_accuracy: 0.8038 - auc: 0.7208 - val_loss: 0.4497 - val_binary_accuracy: 0.8039 - val_auc: 0.7199\n",
            "Epoch 144/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4497 - binary_accuracy: 0.8039 - auc: 0.7209 - val_loss: 0.4498 - val_binary_accuracy: 0.8041 - val_auc: 0.7199\n",
            "Epoch 145/200\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4497 - binary_accuracy: 0.8039 - auc: 0.7210 - val_loss: 0.4495 - val_binary_accuracy: 0.8045 - val_auc: 0.7199\n",
            "Epoch 146/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4497 - binary_accuracy: 0.8039 - auc: 0.7209 - val_loss: 0.4496 - val_binary_accuracy: 0.8044 - val_auc: 0.7201\n",
            "Epoch 147/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4495 - binary_accuracy: 0.8038 - auc: 0.7212 - val_loss: 0.4495 - val_binary_accuracy: 0.8042 - val_auc: 0.7201\n",
            "Epoch 148/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4496 - binary_accuracy: 0.8038 - auc: 0.7211 - val_loss: 0.4504 - val_binary_accuracy: 0.8044 - val_auc: 0.7201\n",
            "Epoch 149/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4495 - binary_accuracy: 0.8040 - auc: 0.7212 - val_loss: 0.4498 - val_binary_accuracy: 0.8042 - val_auc: 0.7198\n",
            "Epoch 150/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4496 - binary_accuracy: 0.8038 - auc: 0.7212 - val_loss: 0.4494 - val_binary_accuracy: 0.8041 - val_auc: 0.7202\n",
            "Epoch 151/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4496 - binary_accuracy: 0.8038 - auc: 0.7213 - val_loss: 0.4496 - val_binary_accuracy: 0.8044 - val_auc: 0.7201\n",
            "Epoch 152/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4496 - binary_accuracy: 0.8040 - auc: 0.7211 - val_loss: 0.4494 - val_binary_accuracy: 0.8043 - val_auc: 0.7201\n",
            "Epoch 153/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4496 - binary_accuracy: 0.8039 - auc: 0.7211 - val_loss: 0.4493 - val_binary_accuracy: 0.8043 - val_auc: 0.7202\n",
            "Epoch 154/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4495 - binary_accuracy: 0.8040 - auc: 0.7212 - val_loss: 0.4497 - val_binary_accuracy: 0.8041 - val_auc: 0.7202\n",
            "Epoch 155/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4494 - binary_accuracy: 0.8039 - auc: 0.7214 - val_loss: 0.4498 - val_binary_accuracy: 0.8045 - val_auc: 0.7199\n",
            "Epoch 156/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4495 - binary_accuracy: 0.8038 - auc: 0.7212 - val_loss: 0.4494 - val_binary_accuracy: 0.8044 - val_auc: 0.7202\n",
            "Epoch 157/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4494 - binary_accuracy: 0.8039 - auc: 0.7214 - val_loss: 0.4493 - val_binary_accuracy: 0.8043 - val_auc: 0.7203\n",
            "Epoch 158/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4494 - binary_accuracy: 0.8037 - auc: 0.7215 - val_loss: 0.4493 - val_binary_accuracy: 0.8043 - val_auc: 0.7202\n",
            "Epoch 159/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4495 - binary_accuracy: 0.8040 - auc: 0.7214 - val_loss: 0.4493 - val_binary_accuracy: 0.8043 - val_auc: 0.7200\n",
            "Epoch 160/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4494 - binary_accuracy: 0.8037 - auc: 0.7215 - val_loss: 0.4495 - val_binary_accuracy: 0.8045 - val_auc: 0.7202\n",
            "Epoch 161/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4494 - binary_accuracy: 0.8040 - auc: 0.7214 - val_loss: 0.4495 - val_binary_accuracy: 0.8044 - val_auc: 0.7202\n",
            "Epoch 162/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4494 - binary_accuracy: 0.8038 - auc: 0.7215 - val_loss: 0.4493 - val_binary_accuracy: 0.8043 - val_auc: 0.7202\n",
            "Epoch 163/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4494 - binary_accuracy: 0.8039 - auc: 0.7215 - val_loss: 0.4495 - val_binary_accuracy: 0.8044 - val_auc: 0.7200\n",
            "Epoch 164/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4493 - binary_accuracy: 0.8039 - auc: 0.7216 - val_loss: 0.4494 - val_binary_accuracy: 0.8043 - val_auc: 0.7200\n",
            "Epoch 165/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4494 - binary_accuracy: 0.8039 - auc: 0.7216 - val_loss: 0.4494 - val_binary_accuracy: 0.8044 - val_auc: 0.7201\n",
            "Epoch 166/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4493 - binary_accuracy: 0.8040 - auc: 0.7217 - val_loss: 0.4493 - val_binary_accuracy: 0.8044 - val_auc: 0.7202\n",
            "Epoch 167/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4493 - binary_accuracy: 0.8041 - auc: 0.7214 - val_loss: 0.4493 - val_binary_accuracy: 0.8045 - val_auc: 0.7202\n",
            "Epoch 168/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4494 - binary_accuracy: 0.8040 - auc: 0.7215 - val_loss: 0.4496 - val_binary_accuracy: 0.8043 - val_auc: 0.7200\n",
            "Epoch 169/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4493 - binary_accuracy: 0.8040 - auc: 0.7217 - val_loss: 0.4494 - val_binary_accuracy: 0.8042 - val_auc: 0.7199\n",
            "Epoch 170/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4493 - binary_accuracy: 0.8038 - auc: 0.7217 - val_loss: 0.4493 - val_binary_accuracy: 0.8042 - val_auc: 0.7202\n",
            "Epoch 171/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4493 - binary_accuracy: 0.8039 - auc: 0.7218 - val_loss: 0.4495 - val_binary_accuracy: 0.8042 - val_auc: 0.7202\n",
            "Epoch 172/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4493 - binary_accuracy: 0.8039 - auc: 0.7217 - val_loss: 0.4494 - val_binary_accuracy: 0.8044 - val_auc: 0.7200\n",
            "Epoch 173/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4492 - binary_accuracy: 0.8038 - auc: 0.7219 - val_loss: 0.4493 - val_binary_accuracy: 0.8044 - val_auc: 0.7203\n",
            "Epoch 174/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4492 - binary_accuracy: 0.8040 - auc: 0.7217 - val_loss: 0.4499 - val_binary_accuracy: 0.8042 - val_auc: 0.7202\n",
            "Epoch 175/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4493 - binary_accuracy: 0.8038 - auc: 0.7218 - val_loss: 0.4499 - val_binary_accuracy: 0.8044 - val_auc: 0.7200\n",
            "Epoch 176/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4491 - binary_accuracy: 0.8040 - auc: 0.7219 - val_loss: 0.4493 - val_binary_accuracy: 0.8045 - val_auc: 0.7203\n",
            "Epoch 177/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4492 - binary_accuracy: 0.8040 - auc: 0.7219 - val_loss: 0.4494 - val_binary_accuracy: 0.8046 - val_auc: 0.7202\n",
            "Epoch 178/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4493 - binary_accuracy: 0.8039 - auc: 0.7217 - val_loss: 0.4495 - val_binary_accuracy: 0.8046 - val_auc: 0.7202\n",
            "Epoch 179/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4491 - binary_accuracy: 0.8038 - auc: 0.7220 - val_loss: 0.4493 - val_binary_accuracy: 0.8043 - val_auc: 0.7203\n",
            "Epoch 180/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4492 - binary_accuracy: 0.8040 - auc: 0.7218 - val_loss: 0.4496 - val_binary_accuracy: 0.8046 - val_auc: 0.7202\n",
            "Epoch 181/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4491 - binary_accuracy: 0.8040 - auc: 0.7220 - val_loss: 0.4495 - val_binary_accuracy: 0.8043 - val_auc: 0.7201\n",
            "Epoch 182/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4491 - binary_accuracy: 0.8040 - auc: 0.7220 - val_loss: 0.4498 - val_binary_accuracy: 0.8046 - val_auc: 0.7202\n",
            "Epoch 183/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4491 - binary_accuracy: 0.8040 - auc: 0.7221 - val_loss: 0.4494 - val_binary_accuracy: 0.8046 - val_auc: 0.7199\n",
            "Epoch 184/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4492 - binary_accuracy: 0.8040 - auc: 0.7219 - val_loss: 0.4501 - val_binary_accuracy: 0.8042 - val_auc: 0.7200\n",
            "Epoch 185/200\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4491 - binary_accuracy: 0.8041 - auc: 0.7219 - val_loss: 0.4494 - val_binary_accuracy: 0.8045 - val_auc: 0.7201\n",
            "Epoch 186/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4490 - binary_accuracy: 0.8040 - auc: 0.7221 - val_loss: 0.4493 - val_binary_accuracy: 0.8045 - val_auc: 0.7202\n",
            "Epoch 187/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4490 - binary_accuracy: 0.8040 - auc: 0.7223 - val_loss: 0.4494 - val_binary_accuracy: 0.8045 - val_auc: 0.7201\n",
            "Epoch 188/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4491 - binary_accuracy: 0.8041 - auc: 0.7221 - val_loss: 0.4493 - val_binary_accuracy: 0.8045 - val_auc: 0.7200\n",
            "Epoch 189/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4491 - binary_accuracy: 0.8040 - auc: 0.7220 - val_loss: 0.4493 - val_binary_accuracy: 0.8044 - val_auc: 0.7203\n",
            "Epoch 190/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4490 - binary_accuracy: 0.8039 - auc: 0.7221 - val_loss: 0.4494 - val_binary_accuracy: 0.8044 - val_auc: 0.7202\n",
            "Epoch 191/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4491 - binary_accuracy: 0.8039 - auc: 0.7221 - val_loss: 0.4494 - val_binary_accuracy: 0.8045 - val_auc: 0.7201\n",
            "Epoch 192/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4491 - binary_accuracy: 0.8040 - auc: 0.7220 - val_loss: 0.4492 - val_binary_accuracy: 0.8045 - val_auc: 0.7203\n",
            "Epoch 193/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4490 - binary_accuracy: 0.8041 - auc: 0.7222 - val_loss: 0.4495 - val_binary_accuracy: 0.8046 - val_auc: 0.7201\n",
            "Epoch 194/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4490 - binary_accuracy: 0.8041 - auc: 0.7222 - val_loss: 0.4494 - val_binary_accuracy: 0.8044 - val_auc: 0.7201\n",
            "Epoch 195/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4489 - binary_accuracy: 0.8040 - auc: 0.7224 - val_loss: 0.4494 - val_binary_accuracy: 0.8045 - val_auc: 0.7203\n",
            "Epoch 196/200\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4489 - binary_accuracy: 0.8041 - auc: 0.7222 - val_loss: 0.4494 - val_binary_accuracy: 0.8045 - val_auc: 0.7202\n",
            "Epoch 197/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4489 - binary_accuracy: 0.8040 - auc: 0.7223 - val_loss: 0.4494 - val_binary_accuracy: 0.8045 - val_auc: 0.7200\n",
            "Epoch 198/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4490 - binary_accuracy: 0.8040 - auc: 0.7222 - val_loss: 0.4492 - val_binary_accuracy: 0.8046 - val_auc: 0.7203\n",
            "Epoch 199/200\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4489 - binary_accuracy: 0.8041 - auc: 0.7223 - val_loss: 0.4492 - val_binary_accuracy: 0.8045 - val_auc: 0.7203\n",
            "Epoch 200/200\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4489 - binary_accuracy: 0.8041 - auc: 0.7224 - val_loss: 0.4495 - val_binary_accuracy: 0.8045 - val_auc: 0.7200\n",
            "{'loss': [0.4954568147659302, 0.4784752428531647, 0.47573569416999817, 0.47280821204185486, 0.4706020951271057, 0.46843141317367554, 0.4666651785373688, 0.46506214141845703, 0.463768869638443, 0.4626591205596924, 0.46169060468673706, 0.46092644333839417, 0.46049320697784424, 0.46009618043899536, 0.4595392048358917, 0.4592238962650299, 0.4589029848575592, 0.4583956003189087, 0.4578738212585449, 0.4577891528606415, 0.45738232135772705, 0.4569680690765381, 0.45667508244514465, 0.4566866457462311, 0.4564330279827118, 0.45637667179107666, 0.45579081773757935, 0.45571112632751465, 0.4555160403251648, 0.45523178577423096, 0.4551773965358734, 0.45509111881256104, 0.45494112372398376, 0.45457080006599426, 0.45462867617607117, 0.4543805420398712, 0.45428335666656494, 0.45429372787475586, 0.45401689410209656, 0.4539702832698822, 0.4539079964160919, 0.4536784887313843, 0.45371678471565247, 0.45352110266685486, 0.45365047454833984, 0.4534046947956085, 0.453291654586792, 0.45312032103538513, 0.453209787607193, 0.4531300663948059, 0.45299413800239563, 0.4528498649597168, 0.4530044198036194, 0.45281052589416504, 0.4527219533920288, 0.45262375473976135, 0.4526768624782562, 0.45272141695022583, 0.4525168836116791, 0.45241403579711914, 0.45228612422943115, 0.4523341953754425, 0.45227885246276855, 0.45224469900131226, 0.45208561420440674, 0.45225295424461365, 0.4520625174045563, 0.45195433497428894, 0.4518606960773468, 0.45185577869415283, 0.45177626609802246, 0.4519664943218231, 0.45180174708366394, 0.45166051387786865, 0.45167437195777893, 0.4515218734741211, 0.45154982805252075, 0.45144203305244446, 0.4516468346118927, 0.4513775408267975, 0.451355516910553, 0.45141828060150146, 0.4513670802116394, 0.4512891173362732, 0.45125994086265564, 0.451122909784317, 0.45100268721580505, 0.45110201835632324, 0.45110201835632324, 0.45103341341018677, 0.45088186860084534, 0.451042115688324, 0.4508807063102722, 0.4508669376373291, 0.4508187472820282, 0.4508197009563446, 0.45078244805336, 0.45082688331604004, 0.450885534286499, 0.45066896080970764, 0.45068350434303284, 0.45056799054145813, 0.45059409737586975, 0.4505777955055237, 0.450541228055954, 0.45055636763572693, 0.45058929920196533, 0.45058274269104004, 0.4502764642238617, 0.4504667818546295, 0.4503650665283203, 0.4504202902317047, 0.4503748416900635, 0.45040664076805115, 0.45037779211997986, 0.45027464628219604, 0.45027387142181396, 0.4501727223396301, 0.4501242935657501, 0.4502675235271454, 0.4501189589500427, 0.45013830065727234, 0.4500376880168915, 0.4500928223133087, 0.45003238320350647, 0.44999828934669495, 0.45008420944213867, 0.4500332176685333, 0.4500678479671478, 0.4499666094779968, 0.45004570484161377, 0.44994932413101196, 0.4499277174472809, 0.44995757937431335, 0.4499153196811676, 0.4499610960483551, 0.44991689920425415, 0.4498366415500641, 0.44976311922073364, 0.4497299790382385, 0.4496753215789795, 0.4496724009513855, 0.4497758746147156, 0.4496886432170868, 0.4496561586856842, 0.44967663288116455, 0.4495460093021393, 0.44958630204200745, 0.4495477080345154, 0.44960817694664, 0.44955942034721375, 0.44956138730049133, 0.44955891370773315, 0.44950658082962036, 0.4494205415248871, 0.4495287537574768, 0.4494180977344513, 0.4494241774082184, 0.44946637749671936, 0.4494381248950958, 0.44941988587379456, 0.4494253695011139, 0.4494105577468872, 0.44928744435310364, 0.4493519961833954, 0.44930028915405273, 0.4493196904659271, 0.4493861198425293, 0.4492637813091278, 0.44931578636169434, 0.44927096366882324, 0.4492833912372589, 0.44915643334388733, 0.44924578070640564, 0.4492657780647278, 0.4491274654865265, 0.44918060302734375, 0.44926807284355164, 0.44914984703063965, 0.44924983382225037, 0.4491422474384308, 0.4491240978240967, 0.4490790367126465, 0.44917750358581543, 0.4491425156593323, 0.4490494132041931, 0.4489870071411133, 0.4490809738636017, 0.4491451382637024, 0.44902685284614563, 0.44905924797058105, 0.4490773379802704, 0.44896748661994934, 0.44901904463768005, 0.4488963186740875, 0.44893592596054077, 0.4489419460296631, 0.44898849725723267, 0.44893231987953186, 0.4489261507987976], 'binary_accuracy': [0.7982983589172363, 0.8006500601768494, 0.8011277914047241, 0.8014508485794067, 0.8014545440673828, 0.8015931844711304, 0.8018258810043335, 0.801860511302948, 0.8020090460777283, 0.8021873235702515, 0.8022764325141907, 0.8020066022872925, 0.8022157549858093, 0.8023518919944763, 0.8021266460418701, 0.8025264143943787, 0.802281379699707, 0.8022232055664062, 0.8024100661277771, 0.802520215511322, 0.8024818897247314, 0.8026019334793091, 0.8026935458183289, 0.8025932908058167, 0.8027727603912354, 0.8026365637779236, 0.8026959896087646, 0.8027888536453247, 0.8028643131256104, 0.8027306795120239, 0.8027108311653137, 0.8027133345603943, 0.8024855852127075, 0.8028333783149719, 0.8029509782791138, 0.8029571771621704, 0.803006649017334, 0.8029484748840332, 0.8028754591941833, 0.8026279211044312, 0.8028148412704468, 0.8028296828269958, 0.8028358817100525, 0.8028791546821594, 0.8028420209884644, 0.8029497265815735, 0.8029076457023621, 0.8027925491333008, 0.8030214905738831, 0.8029274344444275, 0.803006649017334, 0.8030165433883667, 0.8028470277786255, 0.8029608726501465, 0.8029237389564514, 0.8031836748123169, 0.8031861186027527, 0.8030140995979309, 0.8031118512153625, 0.8032281994819641, 0.8030115962028503, 0.803073525428772, 0.803187370300293, 0.8033655881881714, 0.8032418489456177, 0.8031588792800903, 0.803246796131134, 0.8032480478286743, 0.8032220602035522, 0.803382933139801, 0.8032987713813782, 0.8033260107040405, 0.8032901287078857, 0.8035029768943787, 0.8032034635543823, 0.8034559488296509, 0.8032195568084717, 0.80332350730896, 0.803153932094574, 0.8032975196838379, 0.8033223152160645, 0.8032641410827637, 0.8032839298248291, 0.8033593893051147, 0.8035649061203003, 0.803405225276947, 0.8034225702285767, 0.8034448027610779, 0.8034485578536987, 0.8034893870353699, 0.8036341667175293, 0.8035784959793091, 0.8034918308258057, 0.8034386038780212, 0.8033371567726135, 0.8035722970962524, 0.8035054802894592, 0.8035091757774353, 0.8032740354537964, 0.8036527633666992, 0.8035611510276794, 0.8035537600517273, 0.8035289645195007, 0.8036044836044312, 0.8035784959793091, 0.8035649061203003, 0.8033210635185242, 0.8035549521446228, 0.8034584522247314, 0.8036081790924072, 0.8037109375, 0.8036564588546753, 0.8036317229270935, 0.8034646511077881, 0.803585946559906, 0.8036032319068909, 0.803459644317627, 0.8037183284759521, 0.8035772442817688, 0.8036911487579346, 0.8035475611686707, 0.8035995364189148, 0.8037480711936951, 0.8036193251609802, 0.8036836981773376, 0.8037901520729065, 0.8036193251609802, 0.8037109375, 0.8036502599716187, 0.8038334846496582, 0.8037059903144836, 0.8035933375358582, 0.8038817644119263, 0.8036923408508301, 0.8037121891975403, 0.8040104508399963, 0.8036972880363464, 0.8038780093193054, 0.8038000464439392, 0.8037146329879761, 0.8038371801376343, 0.8038433790206909, 0.8038062453269958, 0.8038668632507324, 0.8039225935935974, 0.8039374351501465, 0.8037579655647278, 0.8038483262062073, 0.8039535284042358, 0.8037715554237366, 0.8038074970245361, 0.8040475845336914, 0.8038730621337891, 0.8039956092834473, 0.8039052486419678, 0.8038421273231506, 0.8038706183433533, 0.8036701083183289, 0.8039881587028503, 0.8037480711936951, 0.8039510250091553, 0.8037715554237366, 0.8039126992225647, 0.8038718104362488, 0.8039324879646301, 0.8039869666099548, 0.8040946125984192, 0.803972065448761, 0.8040141463279724, 0.803835928440094, 0.8039312362670898, 0.8038904070854187, 0.8038371801376343, 0.8039857149124146, 0.8038210868835449, 0.8039535284042358, 0.8039572238922119, 0.8039374351501465, 0.8038037419319153, 0.8039646744728088, 0.8040241003036499, 0.8039683699607849, 0.8039956092834473, 0.8039881587028503, 0.8040735721588135, 0.804038941860199, 0.8039683699607849, 0.8040587306022644, 0.8040290474891663, 0.8039114475250244, 0.8039448857307434, 0.8039745688438416, 0.8041132092475891, 0.8040785193443298, 0.8040413856506348, 0.8040562272071838, 0.8040191531181335, 0.8040351867675781, 0.8041267991065979, 0.8041317462921143], 'auc': [0.6579046845436096, 0.6935926675796509, 0.6961238980293274, 0.6995131969451904, 0.7011154294013977, 0.7025742530822754, 0.7031610012054443, 0.7040596008300781, 0.7044216394424438, 0.7047903537750244, 0.7056165337562561, 0.7061492800712585, 0.7062903642654419, 0.7066311836242676, 0.7074611783027649, 0.7076463103294373, 0.707995593547821, 0.7089282870292664, 0.7096925973892212, 0.709486722946167, 0.7102124094963074, 0.710822582244873, 0.7110738158226013, 0.7108755111694336, 0.7110910415649414, 0.7111110091209412, 0.7121043801307678, 0.7121267318725586, 0.712445080280304, 0.7128496766090393, 0.712780237197876, 0.7129591107368469, 0.7131102085113525, 0.7136452198028564, 0.713386595249176, 0.71368408203125, 0.7139015793800354, 0.7138704061508179, 0.7143933773040771, 0.7144263982772827, 0.7144672274589539, 0.7148839235305786, 0.7146342992782593, 0.7150430083274841, 0.7147009372711182, 0.7150572538375854, 0.7152702808380127, 0.715599000453949, 0.7152622938156128, 0.7154316306114197, 0.7156228423118591, 0.7159317135810852, 0.7156552672386169, 0.7159554958343506, 0.7160789370536804, 0.7161499261856079, 0.7161485552787781, 0.7158576250076294, 0.716397762298584, 0.7165133953094482, 0.7167736887931824, 0.7165767550468445, 0.7167844772338867, 0.7166204452514648, 0.7169503569602966, 0.7165977954864502, 0.7169938087463379, 0.7171168923377991, 0.7173771858215332, 0.7172826528549194, 0.7174904942512512, 0.7170084714889526, 0.7172987461090088, 0.7175516486167908, 0.7174988985061646, 0.717835009098053, 0.7177530527114868, 0.7178248763084412, 0.7175487279891968, 0.7180432081222534, 0.7181298136711121, 0.7179102897644043, 0.7180207967758179, 0.7181414365768433, 0.7182585000991821, 0.7185155153274536, 0.7186899781227112, 0.7185482382774353, 0.7184824347496033, 0.7185219526290894, 0.718927264213562, 0.7185953855514526, 0.7189232707023621, 0.7188521027565002, 0.7190338373184204, 0.7189360857009888, 0.7190718650817871, 0.7189187407493591, 0.7188839912414551, 0.719302773475647, 0.7191727161407471, 0.7194069027900696, 0.7193979620933533, 0.7193295955657959, 0.7194717526435852, 0.719474196434021, 0.7194160223007202, 0.71933913230896, 0.7199877500534058, 0.7195187211036682, 0.7197261452674866, 0.7196836471557617, 0.7196909785270691, 0.7197284698486328, 0.7197021842002869, 0.7199536561965942, 0.7199651598930359, 0.72002112865448, 0.7202454209327698, 0.7198547720909119, 0.7201648354530334, 0.7201259136199951, 0.7204448580741882, 0.720256507396698, 0.7203705906867981, 0.720374584197998, 0.7202675342559814, 0.7204426527023315, 0.7202377319335938, 0.720387876033783, 0.7202194929122925, 0.7205134034156799, 0.7205571532249451, 0.7204686999320984, 0.7205638289451599, 0.7204391956329346, 0.7206075191497803, 0.7206152677536011, 0.7206518054008484, 0.7208582162857056, 0.7209462523460388, 0.7209241390228271, 0.7207537293434143, 0.7209300398826599, 0.7209863662719727, 0.7209131717681885, 0.7211833596229553, 0.7210971713066101, 0.7211967706680298, 0.7211637496948242, 0.7212574481964111, 0.7211146950721741, 0.7211240530014038, 0.721240758895874, 0.7214027643203735, 0.7211933135986328, 0.7214069366455078, 0.7215001583099365, 0.7213732600212097, 0.7214542031288147, 0.7214071750640869, 0.7214603424072266, 0.7214665412902832, 0.7216494083404541, 0.7215881943702698, 0.7216539978981018, 0.7214482426643372, 0.7215099930763245, 0.7216585874557495, 0.7216845750808716, 0.7218207120895386, 0.7217061519622803, 0.721933126449585, 0.7217320203781128, 0.7217604517936707, 0.7218900322914124, 0.7218879461288452, 0.7216581106185913, 0.7220032811164856, 0.7217519879341125, 0.7220077514648438, 0.7219981551170349, 0.7221047878265381, 0.7219055891036987, 0.7218684554100037, 0.7221082448959351, 0.7223215103149414, 0.7221136689186096, 0.7219812870025635, 0.7221113443374634, 0.722130537033081, 0.7220348119735718, 0.7221859693527222, 0.7222053408622742, 0.7224458456039429, 0.7222159504890442, 0.7222719192504883, 0.7222257852554321, 0.7223213315010071, 0.722366213798523], 'val_loss': [0.47378504276275635, 0.4702240526676178, 0.46868982911109924, 0.4658505320549011, 0.4655910134315491, 0.4624214768409729, 0.46114251017570496, 0.4597422480583191, 0.45894530415534973, 0.45991918444633484, 0.4571043848991394, 0.458276629447937, 0.45669642090797424, 0.4559246599674225, 0.4561136066913605, 0.45592957735061646, 0.45570895075798035, 0.45509859919548035, 0.4543538987636566, 0.45473504066467285, 0.4536600112915039, 0.4537421762943268, 0.45346909761428833, 0.45343032479286194, 0.4549257159233093, 0.4540014863014221, 0.4532334804534912, 0.4530978500843048, 0.45250409841537476, 0.45351096987724304, 0.45231369137763977, 0.45206132531166077, 0.45200198888778687, 0.4527987241744995, 0.4516483247280121, 0.4529922604560852, 0.4514906406402588, 0.4514511525630951, 0.4513223469257355, 0.45205774903297424, 0.451296329498291, 0.45110008120536804, 0.45118778944015503, 0.45120060443878174, 0.45117294788360596, 0.4512206017971039, 0.45130637288093567, 0.45116499066352844, 0.4510962665081024, 0.4508707821369171, 0.4512150287628174, 0.45099833607673645, 0.45061153173446655, 0.4509744644165039, 0.45070141553878784, 0.45084911584854126, 0.45086154341697693, 0.45056119561195374, 0.4503802955150604, 0.45140913128852844, 0.4507938325405121, 0.45040175318717957, 0.45066744089126587, 0.45245659351348877, 0.4503386914730072, 0.45096299052238464, 0.45024558901786804, 0.4502686858177185, 0.4505864083766937, 0.4501027464866638, 0.4522399306297302, 0.4502710998058319, 0.44998258352279663, 0.4500407576560974, 0.45049360394477844, 0.45011547207832336, 0.4500560462474823, 0.44994890689849854, 0.4501149356365204, 0.4499957263469696, 0.4500548243522644, 0.44976547360420227, 0.4498389959335327, 0.4500933289527893, 0.45026636123657227, 0.4498889446258545, 0.449802964925766, 0.44996875524520874, 0.4503750503063202, 0.4497857689857483, 0.449714332818985, 0.4504193663597107, 0.45010673999786377, 0.4496927559375763, 0.4497738778591156, 0.45006150007247925, 0.44980090856552124, 0.45063602924346924, 0.45046377182006836, 0.4496760070323944, 0.4496765434741974, 0.4504665732383728, 0.44959601759910583, 0.4495646357536316, 0.44972726702690125, 0.44988545775413513, 0.44974640011787415, 0.4497043192386627, 0.4509667158126831, 0.4506336450576782, 0.4494960606098175, 0.44960328936576843, 0.4496273696422577, 0.44974735379219055, 0.4495011568069458, 0.449663907289505, 0.4496845006942749, 0.45009851455688477, 0.4495267868041992, 0.4498353600502014, 0.44960305094718933, 0.45021170377731323, 0.44943109154701233, 0.4495352506637573, 0.44983601570129395, 0.44942766427993774, 0.4494064152240753, 0.45045506954193115, 0.44943106174468994, 0.44958460330963135, 0.4502317011356354, 0.44940030574798584, 0.44964349269866943, 0.4494428038597107, 0.4497698247432709, 0.4494071304798126, 0.4496452510356903, 0.4493856132030487, 0.4494739770889282, 0.44966885447502136, 0.450002521276474, 0.44943705201148987, 0.44973087310791016, 0.4497554302215576, 0.4495200216770172, 0.44959527254104614, 0.44954895973205566, 0.4503762125968933, 0.4498296082019806, 0.44941815733909607, 0.4496190845966339, 0.44942042231559753, 0.44933226704597473, 0.44970810413360596, 0.4498254954814911, 0.4493974447250366, 0.4492711126804352, 0.4493391215801239, 0.4493408799171448, 0.4495307207107544, 0.44953852891921997, 0.44934287667274475, 0.44949787855148315, 0.4493841826915741, 0.4493732154369354, 0.44933584332466125, 0.44932547211647034, 0.4496103525161743, 0.4493570029735565, 0.4493396580219269, 0.4495261013507843, 0.4493867754936218, 0.4492722749710083, 0.44985923171043396, 0.44987672567367554, 0.44930338859558105, 0.44944170117378235, 0.44954612851142883, 0.44928911328315735, 0.4495517611503601, 0.4494757354259491, 0.4498409032821655, 0.449389785528183, 0.45005354285240173, 0.4494312107563019, 0.4493347406387329, 0.44942888617515564, 0.44929513335227966, 0.4493200182914734, 0.4494273066520691, 0.4493871331214905, 0.4492166042327881, 0.4495486319065094, 0.4493904709815979, 0.4493885636329651, 0.44936439394950867, 0.44940465688705444, 0.44920679926872253, 0.44923046231269836, 0.4494921863079071], 'val_binary_accuracy': [0.801713228225708, 0.8022553324699402, 0.802132785320282, 0.8028160333633423, 0.8027566075325012, 0.8029534220695496, 0.8032059073448181, 0.803120493888855, 0.8033915758132935, 0.8032096028327942, 0.8033024072647095, 0.8034212589263916, 0.8033878207206726, 0.8035289645195007, 0.8035029172897339, 0.8030685186386108, 0.8033246994018555, 0.8035549521446228, 0.8034324049949646, 0.8036514520645142, 0.8038148283958435, 0.8035549521446228, 0.803610622882843, 0.8032096028327942, 0.8036180734634399, 0.8035586476325989, 0.8036848902702332, 0.8037480115890503, 0.8037628531455994, 0.8039002418518066, 0.8038705587387085, 0.8036700487136841, 0.8037814497947693, 0.8038259744644165, 0.8039150834083557, 0.8036180734634399, 0.8036625981330872, 0.8037925958633423, 0.8038594126701355, 0.803588330745697, 0.8036589026451111, 0.8040302395820618, 0.8038148283958435, 0.8040153384208679, 0.8040004968643188, 0.8036997318267822, 0.8036997318267822, 0.8039819598197937, 0.8036885857582092, 0.8041638731956482, 0.8038074374198914, 0.8037999868392944, 0.8041155934333801, 0.8036848902702332, 0.8035066723823547, 0.8041155934333801, 0.8038779497146606, 0.8040933609008789, 0.8037925958633423, 0.803584635257721, 0.8039782047271729, 0.8041973114013672, 0.8039745092391968, 0.803551197052002, 0.8040041923522949, 0.8036292195320129, 0.8042344450950623, 0.8039188385009766, 0.8038557171821594, 0.804085910320282, 0.8024483919143677, 0.8039856553077698, 0.8042010068893433, 0.8042196035385132, 0.8041713237762451, 0.8040079474449158, 0.8042827248573303, 0.8041713237762451, 0.8040487766265869, 0.8040339350700378, 0.8041267395019531, 0.8042047619819641, 0.80413419008255, 0.804111897945404, 0.8042084574699402, 0.8040302395820618, 0.8041936159133911, 0.8041601777076721, 0.8034806847572327, 0.8042269945144653, 0.8041936159133911, 0.8043087124824524, 0.8040562272071838, 0.8042975664138794, 0.80404132604599, 0.8043903708457947, 0.8043420910835266, 0.8042307496070862, 0.8041936159133911, 0.804156482219696, 0.8042938709259033, 0.8039596676826477, 0.8041415810585022, 0.8043420910835266, 0.804074764251709, 0.8039485216140747, 0.8043606877326965, 0.8039633631706238, 0.8042232990264893, 0.8036960363388062, 0.8045352101325989, 0.804538905620575, 0.8042975664138794, 0.804130494594574, 0.8043420910835266, 0.8039931058883667, 0.8041898608207703, 0.8042455911636353, 0.8045463562011719, 0.8042418360710144, 0.8042269945144653, 0.8039596676826477, 0.80404132604599, 0.8044052720069885, 0.8043198585510254, 0.8042381405830383, 0.8042158484458923, 0.8041415810585022, 0.8043124079704285, 0.8041861653327942, 0.8040673732757568, 0.8044163584709167, 0.8041787147521973, 0.8043124079704285, 0.8043050169944763, 0.8044052720069885, 0.8039299845695496, 0.804609477519989, 0.8042827248573303, 0.8041936159133911, 0.8041787147521973, 0.8044015169143677, 0.8039150834083557, 0.8041267395019531, 0.8045203685760498, 0.8043978214263916, 0.8042010068893433, 0.8043829798698425, 0.8041787147521973, 0.8040710687637329, 0.8044349551200867, 0.8043383955955505, 0.8043495416641235, 0.8040710687637329, 0.8044609427452087, 0.8044238090515137, 0.8042975664138794, 0.8043087124824524, 0.8043310046195984, 0.8045463562011719, 0.8043978214263916, 0.8043347001075745, 0.8044461011886597, 0.8043124079704285, 0.8043569922447205, 0.8043532371520996, 0.8045315146446228, 0.8043124079704285, 0.8042047619819641, 0.804156482219696, 0.8041861653327942, 0.8043681383132935, 0.8044349551200867, 0.8042158484458923, 0.8044089674949646, 0.8045055270195007, 0.804553747177124, 0.8045946359634399, 0.8043383955955505, 0.8045686483383179, 0.8043272495269775, 0.8045946359634399, 0.804550051689148, 0.8041824698448181, 0.8044906258583069, 0.8045092225074768, 0.8044794797897339, 0.8045129179954529, 0.8044126629829407, 0.8044423460960388, 0.8044869303703308, 0.8044869303703308, 0.8046466112136841, 0.8044275045394897, 0.8045203685760498, 0.8044794797897339, 0.8044794797897339, 0.8046354651451111, 0.8044869303703308, 0.8045463562011719], 'val_auc': [0.7044553756713867, 0.7080780863761902, 0.7099108695983887, 0.7115287184715271, 0.7116231918334961, 0.7120205163955688, 0.7122354507446289, 0.7124483585357666, 0.7126917839050293, 0.7128658294677734, 0.7131628394126892, 0.7134650945663452, 0.7133058309555054, 0.7136611938476562, 0.7138145565986633, 0.7140733599662781, 0.7145364284515381, 0.7146211862564087, 0.7157467603683472, 0.7156968116760254, 0.7163726091384888, 0.7161669731140137, 0.7164087295532227, 0.7163207530975342, 0.7167957425117493, 0.7168457508087158, 0.7169113159179688, 0.716988205909729, 0.7171516418457031, 0.7172724008560181, 0.7173102498054504, 0.7176432609558105, 0.7176315784454346, 0.7177350521087646, 0.7181418538093567, 0.7179402709007263, 0.7182843685150146, 0.7182876467704773, 0.7184903621673584, 0.7181152701377869, 0.7184073328971863, 0.7184402346611023, 0.7186405062675476, 0.7184183597564697, 0.7184923887252808, 0.7186124920845032, 0.7183022499084473, 0.7188695073127747, 0.7186527252197266, 0.7185847163200378, 0.7187585830688477, 0.7188552021980286, 0.7188957929611206, 0.7190781235694885, 0.7192652821540833, 0.7186780571937561, 0.7192599773406982, 0.7189361453056335, 0.7193980813026428, 0.7190548777580261, 0.719174861907959, 0.7193653583526611, 0.7191951274871826, 0.7190588712692261, 0.7192952036857605, 0.7188668847084045, 0.7195004224777222, 0.7194149494171143, 0.7193759083747864, 0.7194698452949524, 0.7194392681121826, 0.7196257710456848, 0.7195488214492798, 0.7196260690689087, 0.7193905115127563, 0.7196904420852661, 0.7192590236663818, 0.7196754217147827, 0.7193586826324463, 0.7195217609405518, 0.7193745374679565, 0.7198434472084045, 0.7196253538131714, 0.7194231152534485, 0.7198697328567505, 0.719795823097229, 0.7198439240455627, 0.7196525931358337, 0.7198474407196045, 0.719657838344574, 0.7197903990745544, 0.7198056578636169, 0.7198653221130371, 0.7200208306312561, 0.7196764349937439, 0.719525933265686, 0.7197533845901489, 0.7198552489280701, 0.7197606563568115, 0.7199018597602844, 0.7199269533157349, 0.7200483679771423, 0.7199128866195679, 0.7200558185577393, 0.7198447585105896, 0.7197986245155334, 0.7197492122650146, 0.719964325428009, 0.719994843006134, 0.7200673222541809, 0.7200274467468262, 0.7198573350906372, 0.7196784615516663, 0.719951868057251, 0.7200130820274353, 0.7196676135063171, 0.7198290824890137, 0.7198148965835571, 0.7198213338851929, 0.7199040651321411, 0.7201402187347412, 0.7199983596801758, 0.7201029062271118, 0.7200172543525696, 0.7200657725334167, 0.7200565934181213, 0.7201120853424072, 0.7199277281761169, 0.720027506351471, 0.7200613021850586, 0.7199956178665161, 0.7199852466583252, 0.7200168371200562, 0.7199921011924744, 0.7199313640594482, 0.7200999855995178, 0.7197594046592712, 0.7199926972389221, 0.7201518416404724, 0.7199692130088806, 0.720238983631134, 0.7201130390167236, 0.719919741153717, 0.7199035286903381, 0.7198780179023743, 0.720061182975769, 0.720108687877655, 0.7200669050216675, 0.7198191285133362, 0.7201619744300842, 0.7201244831085205, 0.7200743556022644, 0.7201712131500244, 0.7202001214027405, 0.719943106174469, 0.7202052474021912, 0.7202537655830383, 0.7201729416847229, 0.7200191020965576, 0.720222532749176, 0.7201859354972839, 0.720176637172699, 0.7199753522872925, 0.720035970211029, 0.7201367020606995, 0.7202045321464539, 0.7201500535011292, 0.7199617624282837, 0.7199486494064331, 0.7201969623565674, 0.7202177047729492, 0.7200294137001038, 0.7202608585357666, 0.7201532125473022, 0.7199681997299194, 0.7202523350715637, 0.7202216982841492, 0.7201501131057739, 0.7202681303024292, 0.7201935648918152, 0.7201341986656189, 0.720225989818573, 0.7199440598487854, 0.7200338840484619, 0.7200515866279602, 0.720221757888794, 0.7200881242752075, 0.7200494408607483, 0.7203080654144287, 0.720178484916687, 0.7200793623924255, 0.7202920317649841, 0.7201380133628845, 0.7200888395309448, 0.7202511429786682, 0.7201794981956482, 0.7199865579605103, 0.7202821969985962, 0.7203381657600403, 0.7200214862823486]}\n",
            "  1/263 [..............................] - ETA: 46s - loss: 0.4889 - binary_accuracy: 0.7793 - auc: 0.7067WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0159s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0159s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "263/263 [==============================] - 4s 17ms/step - loss: 0.4495 - binary_accuracy: 0.8045 - auc: 0.7200\n",
            "[0.44949808716773987, 0.804527759552002, 0.7200208902359009]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJZzJuxU4gLB",
        "outputId": "e6ea51cd-be96-4bb4-a171-352df04d6771"
      },
      "source": [
        "len(train_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "789"
            ]
          },
          "execution_count": 14,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oxUnYgRoHARx",
        "outputId": "97be848b-4bea-4669-d5b9-9204ea7c2bd9"
      },
      "source": [
        "\"\"\"\n",
        "72.00 auc\n",
        "name='cs' \n",
        "num_heads= 8\n",
        "input_heads = 8\n",
        "use_masking=False\n",
        "shape_v = 83\n",
        "model_d = 32\n",
        "SA_layers = 3\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-4,\n",
        "      decay_steps=10000,\n",
        "      #decay_steps=steps_each,\n",
        "      decay_rate=0.9)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\"\"\"\n",
        "#上に３２の前に６４足したけどあんま変わらず\n",
        "\n",
        "\n",
        "name='cs' \n",
        "num_heads= 4\n",
        "input_heads = 8\n",
        "use_masking=False\n",
        "shape_v = 83\n",
        "model_d = 32\n",
        "SA_layers = 3\n",
        "\n",
        "batch_size = batch_size\n",
        "epochs = 300\n",
        "steps_each = len(train_dataset)\n",
        "total_steps  = steps_each * epochs\n",
        "\n",
        "#with tf.device('/device:GPU:0'):\n",
        "with strategy.scope():\n",
        "  original_inputs = Input(shape=(shape_v,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "  #output_w = inputW(model_d)(original_inputs)\n",
        "  #output = mlpBlock([83, 128], original_inputs)\n",
        "  output = inputW_Em(model_d, input_heads)(original_inputs)\n",
        "  output = LayerNormalization(name=f'{name}_normalization_before')(output)\n",
        "  #output = inputW0_4(model_d)(original_inputs)\n",
        "\n",
        "  for i in range(SA_layers):\n",
        "    output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention_{i}')(output)\n",
        "    post_residual1 = (Add(name=f'{name}_add_{i}')([output_sa, output]))\n",
        "    norm1_output = LayerNormalization(name=f'{name}_normalization1_{i}')(post_residual1)\n",
        "    output = TransformerTransition(name=f'{name}_transition_{i}', activation='relu')(norm1_output)\n",
        "    post_residual2 = (Add(name=f'{name}_add2_{i}')([norm1_output,output]))\n",
        "    output = LayerNormalization(name=f'{name}_normalization2_{i}')(post_residual2)\n",
        "\n",
        "  output = Flatten()(output)\n",
        "\n",
        "  \"\"\"\n",
        "  output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  \"\"\"\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(1, activation='sigmoid')(output)\n",
        "  cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "  cs_p2p.summary()\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-4,\n",
        "      decay_steps=10000,\n",
        "      #decay_steps=steps_each,\n",
        "      decay_rate=0.9)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "  #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "  #optimizer = RAdamOptimizer(total_steps=total_steps, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "  cs_p2p.compile(optimizer, \n",
        "                 loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                 #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "          metrics=[\n",
        "          #tf.keras.metrics.MeanSquaredError(),\n",
        "          tf.keras.metrics.BinaryAccuracy(),\n",
        "          #tf.keras.metrics.Precision(),\n",
        "          #tf.keras.metrics.Recall(),\n",
        "          #'accuracy',\n",
        "          tf.keras.metrics.AUC(),]\n",
        "          )\n",
        "  history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=test_dataset, validation_batch_size=batch_size)\n",
        "  #history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "  #history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  print(history.history)\n",
        "  #cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  result = cs_p2p.evaluate(test_dataset)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cs_p2p\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cs_p2p (InputLayer)             [(128, 83)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_w__em_7 (inputW_Em)       (128, 83, 32)        90387       cs_p2p[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization_before (LayerN (128, 83, 32)        64          input_w__em_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_0 (MultiHeadS (128, 83, 32)        3072        cs_normalization_before[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_0 (Add)                  (128, 83, 32)        0           cs_self_attention_0[0][0]        \n",
            "                                                                 cs_normalization_before[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_0 (LayerNorma (128, 83, 32)        64          cs_add_0[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_0 (TransformerTra (128, 83, 32)        8352        cs_normalization1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_0 (Add)                 (128, 83, 32)        0           cs_normalization1_0[0][0]        \n",
            "                                                                 cs_transition_0[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_0 (LayerNorma (128, 83, 32)        64          cs_add2_0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_1 (MultiHeadS (128, 83, 32)        3072        cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_1 (Add)                  (128, 83, 32)        0           cs_self_attention_1[0][0]        \n",
            "                                                                 cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_1 (LayerNorma (128, 83, 32)        64          cs_add_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_1 (TransformerTra (128, 83, 32)        8352        cs_normalization1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_1 (Add)                 (128, 83, 32)        0           cs_normalization1_1[0][0]        \n",
            "                                                                 cs_transition_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_1 (LayerNorma (128, 83, 32)        64          cs_add2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_2 (MultiHeadS (128, 83, 32)        3072        cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_2 (Add)                  (128, 83, 32)        0           cs_self_attention_2[0][0]        \n",
            "                                                                 cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_2 (LayerNorma (128, 83, 32)        64          cs_add_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_2 (TransformerTra (128, 83, 32)        8352        cs_normalization1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_2 (Add)                 (128, 83, 32)        0           cs_normalization1_2[0][0]        \n",
            "                                                                 cs_transition_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_2 (LayerNorma (128, 83, 32)        64          cs_add2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (128, 2656)          0           cs_normalization2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (128, 2656)          0           flatten_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (128, 32)            85024       dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (128, 32)            0           dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (128, 1)             33          dropout_16[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 210,164\n",
            "Trainable params: 210,164\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "  1/789 [..............................] - ETA: 51:18 - loss: 0.5761 - binary_accuracy: 0.8125 - auc_7: 0.4574WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0244s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0244s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "788/789 [============================>.] - ETA: 0s - loss: 0.4921 - binary_accuracy: 0.8001 - auc_7: 0.6701WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_test_batch_end` time: 0.0111s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_test_batch_end` time: 0.0111s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r789/789 [==============================] - 31s 39ms/step - loss: 0.4921 - binary_accuracy: 0.8001 - auc_7: 0.6702 - val_loss: 0.4746 - val_binary_accuracy: 0.8009 - val_auc_7: 0.7072\n",
            "Epoch 2/300\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4765 - binary_accuracy: 0.8005 - auc_7: 0.6968 - val_loss: 0.4690 - val_binary_accuracy: 0.8026 - val_auc_7: 0.7098\n",
            "Epoch 3/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4719 - binary_accuracy: 0.8012 - auc_7: 0.7006 - val_loss: 0.4660 - val_binary_accuracy: 0.8025 - val_auc_7: 0.7112\n",
            "Epoch 4/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4691 - binary_accuracy: 0.8015 - auc_7: 0.7024 - val_loss: 0.4630 - val_binary_accuracy: 0.8029 - val_auc_7: 0.7118\n",
            "Epoch 5/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4669 - binary_accuracy: 0.8016 - auc_7: 0.7032 - val_loss: 0.4612 - val_binary_accuracy: 0.8032 - val_auc_7: 0.7122\n",
            "Epoch 6/300\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4652 - binary_accuracy: 0.8019 - auc_7: 0.7042 - val_loss: 0.4601 - val_binary_accuracy: 0.8031 - val_auc_7: 0.7123\n",
            "Epoch 7/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4639 - binary_accuracy: 0.8017 - auc_7: 0.7051 - val_loss: 0.4593 - val_binary_accuracy: 0.8033 - val_auc_7: 0.7128\n",
            "Epoch 8/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4631 - binary_accuracy: 0.8019 - auc_7: 0.7052 - val_loss: 0.4602 - val_binary_accuracy: 0.8027 - val_auc_7: 0.7127\n",
            "Epoch 9/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4625 - binary_accuracy: 0.8021 - auc_7: 0.7053 - val_loss: 0.4578 - val_binary_accuracy: 0.8035 - val_auc_7: 0.7132\n",
            "Epoch 10/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4616 - binary_accuracy: 0.8022 - auc_7: 0.7062 - val_loss: 0.4577 - val_binary_accuracy: 0.8033 - val_auc_7: 0.7135\n",
            "Epoch 11/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4610 - binary_accuracy: 0.8021 - auc_7: 0.7066 - val_loss: 0.4576 - val_binary_accuracy: 0.8031 - val_auc_7: 0.7137\n",
            "Epoch 12/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4605 - binary_accuracy: 0.8022 - auc_7: 0.7073 - val_loss: 0.4565 - val_binary_accuracy: 0.8032 - val_auc_7: 0.7139\n",
            "Epoch 13/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4599 - binary_accuracy: 0.8024 - auc_7: 0.7077 - val_loss: 0.4565 - val_binary_accuracy: 0.8036 - val_auc_7: 0.7142\n",
            "Epoch 14/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4596 - binary_accuracy: 0.8025 - auc_7: 0.7077 - val_loss: 0.4572 - val_binary_accuracy: 0.8027 - val_auc_7: 0.7144\n",
            "Epoch 15/300\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4591 - binary_accuracy: 0.8022 - auc_7: 0.7084 - val_loss: 0.4599 - val_binary_accuracy: 0.8032 - val_auc_7: 0.7146\n",
            "Epoch 16/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4585 - binary_accuracy: 0.8023 - auc_7: 0.7093 - val_loss: 0.4553 - val_binary_accuracy: 0.8032 - val_auc_7: 0.7152\n",
            "Epoch 17/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4579 - binary_accuracy: 0.8026 - auc_7: 0.7100 - val_loss: 0.4552 - val_binary_accuracy: 0.8037 - val_auc_7: 0.7157\n",
            "Epoch 18/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4574 - binary_accuracy: 0.8026 - auc_7: 0.7106 - val_loss: 0.4554 - val_binary_accuracy: 0.8038 - val_auc_7: 0.7166\n",
            "Epoch 19/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4568 - binary_accuracy: 0.8027 - auc_7: 0.7115 - val_loss: 0.4543 - val_binary_accuracy: 0.8038 - val_auc_7: 0.7166\n",
            "Epoch 20/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4565 - binary_accuracy: 0.8028 - auc_7: 0.7119 - val_loss: 0.4531 - val_binary_accuracy: 0.8038 - val_auc_7: 0.7173\n",
            "Epoch 21/300\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4562 - binary_accuracy: 0.8028 - auc_7: 0.7124 - val_loss: 0.4532 - val_binary_accuracy: 0.8038 - val_auc_7: 0.7175\n",
            "Epoch 22/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4561 - binary_accuracy: 0.8028 - auc_7: 0.7122 - val_loss: 0.4533 - val_binary_accuracy: 0.8037 - val_auc_7: 0.7173\n",
            "Epoch 23/300\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4557 - binary_accuracy: 0.8027 - auc_7: 0.7130 - val_loss: 0.4552 - val_binary_accuracy: 0.8035 - val_auc_7: 0.7177\n",
            "Epoch 24/300\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4555 - binary_accuracy: 0.8029 - auc_7: 0.7129 - val_loss: 0.4526 - val_binary_accuracy: 0.8038 - val_auc_7: 0.7177\n",
            "Epoch 25/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4553 - binary_accuracy: 0.8028 - auc_7: 0.7135 - val_loss: 0.4523 - val_binary_accuracy: 0.8039 - val_auc_7: 0.7180\n",
            "Epoch 26/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4551 - binary_accuracy: 0.8029 - auc_7: 0.7134 - val_loss: 0.4522 - val_binary_accuracy: 0.8035 - val_auc_7: 0.7182\n",
            "Epoch 27/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4547 - binary_accuracy: 0.8030 - auc_7: 0.7140 - val_loss: 0.4525 - val_binary_accuracy: 0.8037 - val_auc_7: 0.7180\n",
            "Epoch 28/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4547 - binary_accuracy: 0.8029 - auc_7: 0.7140 - val_loss: 0.4523 - val_binary_accuracy: 0.8037 - val_auc_7: 0.7182\n",
            "Epoch 29/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4544 - binary_accuracy: 0.8030 - auc_7: 0.7144 - val_loss: 0.4520 - val_binary_accuracy: 0.8039 - val_auc_7: 0.7187\n",
            "Epoch 30/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4541 - binary_accuracy: 0.8030 - auc_7: 0.7150 - val_loss: 0.4518 - val_binary_accuracy: 0.8041 - val_auc_7: 0.7187\n",
            "Epoch 31/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4540 - binary_accuracy: 0.8029 - auc_7: 0.7150 - val_loss: 0.4551 - val_binary_accuracy: 0.8034 - val_auc_7: 0.7186\n",
            "Epoch 32/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4540 - binary_accuracy: 0.8030 - auc_7: 0.7150 - val_loss: 0.4516 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7185\n",
            "Epoch 33/300\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4538 - binary_accuracy: 0.8031 - auc_7: 0.7150 - val_loss: 0.4529 - val_binary_accuracy: 0.8034 - val_auc_7: 0.7184\n",
            "Epoch 34/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4535 - binary_accuracy: 0.8031 - auc_7: 0.7155 - val_loss: 0.4527 - val_binary_accuracy: 0.8028 - val_auc_7: 0.7186\n",
            "Epoch 35/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4537 - binary_accuracy: 0.8031 - auc_7: 0.7151 - val_loss: 0.4518 - val_binary_accuracy: 0.8037 - val_auc_7: 0.7184\n",
            "Epoch 36/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4533 - binary_accuracy: 0.8031 - auc_7: 0.7160 - val_loss: 0.4531 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7188\n",
            "Epoch 37/300\n",
            "789/789 [==============================] - 24s 30ms/step - loss: 0.4532 - binary_accuracy: 0.8030 - auc_7: 0.7160 - val_loss: 0.4522 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7190\n",
            "Epoch 38/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4531 - binary_accuracy: 0.8032 - auc_7: 0.7162 - val_loss: 0.4510 - val_binary_accuracy: 0.8041 - val_auc_7: 0.7194\n",
            "Epoch 39/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4529 - binary_accuracy: 0.8032 - auc_7: 0.7165 - val_loss: 0.4515 - val_binary_accuracy: 0.8038 - val_auc_7: 0.7190\n",
            "Epoch 40/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4528 - binary_accuracy: 0.8031 - auc_7: 0.7166 - val_loss: 0.4510 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7191\n",
            "Epoch 41/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4527 - binary_accuracy: 0.8032 - auc_7: 0.7166 - val_loss: 0.4509 - val_binary_accuracy: 0.8039 - val_auc_7: 0.7192\n",
            "Epoch 42/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4528 - binary_accuracy: 0.8031 - auc_7: 0.7165 - val_loss: 0.4507 - val_binary_accuracy: 0.8039 - val_auc_7: 0.7192\n",
            "Epoch 43/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4527 - binary_accuracy: 0.8031 - auc_7: 0.7166 - val_loss: 0.4514 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7194\n",
            "Epoch 44/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4524 - binary_accuracy: 0.8032 - auc_7: 0.7171 - val_loss: 0.4508 - val_binary_accuracy: 0.8038 - val_auc_7: 0.7193\n",
            "Epoch 45/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4526 - binary_accuracy: 0.8032 - auc_7: 0.7167 - val_loss: 0.4512 - val_binary_accuracy: 0.8037 - val_auc_7: 0.7195\n",
            "Epoch 46/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4523 - binary_accuracy: 0.8033 - auc_7: 0.7172 - val_loss: 0.4508 - val_binary_accuracy: 0.8042 - val_auc_7: 0.7195\n",
            "Epoch 47/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4521 - binary_accuracy: 0.8033 - auc_7: 0.7176 - val_loss: 0.4505 - val_binary_accuracy: 0.8038 - val_auc_7: 0.7193\n",
            "Epoch 48/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4523 - binary_accuracy: 0.8033 - auc_7: 0.7172 - val_loss: 0.4508 - val_binary_accuracy: 0.8039 - val_auc_7: 0.7192\n",
            "Epoch 49/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4521 - binary_accuracy: 0.8032 - auc_7: 0.7176 - val_loss: 0.4508 - val_binary_accuracy: 0.8041 - val_auc_7: 0.7195\n",
            "Epoch 50/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4518 - binary_accuracy: 0.8033 - auc_7: 0.7179 - val_loss: 0.4505 - val_binary_accuracy: 0.8041 - val_auc_7: 0.7194\n",
            "Epoch 51/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4519 - binary_accuracy: 0.8032 - auc_7: 0.7179 - val_loss: 0.4516 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7198\n",
            "Epoch 52/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4518 - binary_accuracy: 0.8033 - auc_7: 0.7181 - val_loss: 0.4512 - val_binary_accuracy: 0.8041 - val_auc_7: 0.7197\n",
            "Epoch 53/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4518 - binary_accuracy: 0.8031 - auc_7: 0.7178 - val_loss: 0.4512 - val_binary_accuracy: 0.8038 - val_auc_7: 0.7195\n",
            "Epoch 54/300\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4515 - binary_accuracy: 0.8034 - auc_7: 0.7184 - val_loss: 0.4503 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7196\n",
            "Epoch 55/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4516 - binary_accuracy: 0.8032 - auc_7: 0.7183 - val_loss: 0.4504 - val_binary_accuracy: 0.8039 - val_auc_7: 0.7200\n",
            "Epoch 56/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4517 - binary_accuracy: 0.8034 - auc_7: 0.7182 - val_loss: 0.4508 - val_binary_accuracy: 0.8041 - val_auc_7: 0.7195\n",
            "Epoch 57/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4515 - binary_accuracy: 0.8034 - auc_7: 0.7183 - val_loss: 0.4511 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7191\n",
            "Epoch 58/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4516 - binary_accuracy: 0.8032 - auc_7: 0.7182 - val_loss: 0.4504 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7196\n",
            "Epoch 59/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4514 - binary_accuracy: 0.8035 - auc_7: 0.7186 - val_loss: 0.4503 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7197\n",
            "Epoch 60/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4513 - binary_accuracy: 0.8034 - auc_7: 0.7188 - val_loss: 0.4503 - val_binary_accuracy: 0.8039 - val_auc_7: 0.7194\n",
            "Epoch 61/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4511 - binary_accuracy: 0.8034 - auc_7: 0.7193 - val_loss: 0.4503 - val_binary_accuracy: 0.8038 - val_auc_7: 0.7198\n",
            "Epoch 62/300\n",
            "789/789 [==============================] - 26s 33ms/step - loss: 0.4512 - binary_accuracy: 0.8034 - auc_7: 0.7189 - val_loss: 0.4502 - val_binary_accuracy: 0.8043 - val_auc_7: 0.7197\n",
            "Epoch 63/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4513 - binary_accuracy: 0.8034 - auc_7: 0.7190 - val_loss: 0.4504 - val_binary_accuracy: 0.8042 - val_auc_7: 0.7195\n",
            "Epoch 64/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4511 - binary_accuracy: 0.8033 - auc_7: 0.7191 - val_loss: 0.4511 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7189\n",
            "Epoch 65/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4510 - binary_accuracy: 0.8034 - auc_7: 0.7194 - val_loss: 0.4502 - val_binary_accuracy: 0.8042 - val_auc_7: 0.7194\n",
            "Epoch 66/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4509 - binary_accuracy: 0.8035 - auc_7: 0.7194 - val_loss: 0.4500 - val_binary_accuracy: 0.8040 - val_auc_7: 0.7197\n",
            "Epoch 67/300\n",
            "789/789 [==============================] - 26s 32ms/step - loss: 0.4507 - binary_accuracy: 0.8036 - auc_7: 0.7198 - val_loss: 0.4506 - val_binary_accuracy: 0.8037 - val_auc_7: 0.7194\n",
            "Epoch 68/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4507 - binary_accuracy: 0.8036 - auc_7: 0.7199 - val_loss: 0.4515 - val_binary_accuracy: 0.8042 - val_auc_7: 0.7195\n",
            "Epoch 69/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4508 - binary_accuracy: 0.8036 - auc_7: 0.7197 - val_loss: 0.4504 - val_binary_accuracy: 0.8041 - val_auc_7: 0.7194\n",
            "Epoch 70/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4506 - binary_accuracy: 0.8037 - auc_7: 0.7199 - val_loss: 0.4502 - val_binary_accuracy: 0.8041 - val_auc_7: 0.7197\n",
            "Epoch 71/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4506 - binary_accuracy: 0.8037 - auc_7: 0.7200 - val_loss: 0.4505 - val_binary_accuracy: 0.8043 - val_auc_7: 0.7198\n",
            "Epoch 72/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4506 - binary_accuracy: 0.8034 - auc_7: 0.7200 - val_loss: 0.4508 - val_binary_accuracy: 0.8041 - val_auc_7: 0.7194\n",
            "Epoch 73/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4505 - binary_accuracy: 0.8036 - auc_7: 0.7201 - val_loss: 0.4515 - val_binary_accuracy: 0.8036 - val_auc_7: 0.7195\n",
            "Epoch 74/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4504 - binary_accuracy: 0.8037 - auc_7: 0.7203 - val_loss: 0.4506 - val_binary_accuracy: 0.8041 - val_auc_7: 0.7193\n",
            "Epoch 75/300\n",
            "789/789 [==============================] - 25s 32ms/step - loss: 0.4502 - binary_accuracy: 0.8037 - auc_7: 0.7207 - val_loss: 0.4503 - val_binary_accuracy: 0.8039 - val_auc_7: 0.7194\n",
            "Epoch 76/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4504 - binary_accuracy: 0.8037 - auc_7: 0.7204 - val_loss: 0.4504 - val_binary_accuracy: 0.8043 - val_auc_7: 0.7197\n",
            "Epoch 77/300\n",
            "789/789 [==============================] - 24s 31ms/step - loss: 0.4501 - binary_accuracy: 0.8039 - auc_7: 0.7209 - val_loss: 0.4504 - val_binary_accuracy: 0.8041 - val_auc_7: 0.7187\n",
            "Epoch 78/300\n",
            "789/789 [==============================] - 25s 31ms/step - loss: 0.4501 - binary_accuracy: 0.8036 - auc_7: 0.7207 - val_loss: 0.4517 - val_binary_accuracy: 0.8039 - val_auc_7: 0.7189\n",
            "Epoch 79/300\n",
            "202/789 [======>.......................] - ETA: 16s - loss: 0.4478 - binary_accuracy: 0.8055 - auc_7: 0.7213"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0b26073aa065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m           tf.keras.metrics.AUC(),]\n\u001b[1;32m     95\u001b[0m           )\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcs_p2p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m   \u001b[0;31m#history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;31m#history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rBg7jPkXSGpN",
        "outputId": "3041b5e6-d960-47d8-9397-a927b930f3e8"
      },
      "source": [
        "name='cs' \n",
        "num_heads=4\n",
        "use_masking=False\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "shape_v = 83\n",
        "model_d = 16\n",
        "SA_layers = 3\n",
        "\n",
        "\"\"\"\n",
        "print(loans_raw)\n",
        "data = loans_raw\n",
        "#data = data[data.loan_status != 'Current']\n",
        "#data = data[data.loan_status != 'In Grace Period']\n",
        "data = data[data.loan_status == 'Charged Off']\n",
        "data = data[data.loan_status == 'Fully Paid']\n",
        "#print(data.loc[:100])\n",
        "train_dataset = pre_dataset(data[:1000],32)\n",
        "print(train_dataset)\n",
        "\"\"\"\n",
        "\n",
        "#train_dataset = pre_dataset(train[:256000], batch_size)\n",
        "#print(train_dataset.element_spec[0])\n",
        "#vali_dataset = pre_dataset(vali, 1)\n",
        "#print(len(train[256000:288000]))\n",
        "#vali_dataset = pre_dataset(train[256000:307200], batch_size)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  original_inputs = Input(shape=(shape_v,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "  #output_w = inputW(model_d)(original_inputs)\n",
        "  output = inputW0_4(model_d)(original_inputs)\n",
        "  \"\"\"\n",
        "  output_1 = Dense(model_d, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output_w)\n",
        "  output = Add()([output_1, output_w])\n",
        "  output = Dropout(0.1)(output)\n",
        "  output_2 = Dense(model_d, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Add()([output_2, output_w])\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = K.reshape(output, shape=(-1, shape_v, model_d))\n",
        "  \"\"\"\n",
        "  for i in range(SA_layers):\n",
        "    output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention_{i}')(output)\n",
        "    post_residual1 = (Add(name=f'{name}_add_{i}')([output_sa, output]))\n",
        "    norm1_output = LayerNormalization(name=f'{name}_normalization1_{i}')(post_residual1)\n",
        "    output = TransformerTransition(name=f'{name}_transition_{i}', activation='relu')(norm1_output)\n",
        "    post_residual2 = (Add(name=f'{name}_add2_{i}')([norm1_output,output]))\n",
        "    output = LayerNormalization(name=f'{name}_normalization2_{i}')(post_residual2)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention')(output_inputw)\n",
        "  post_residual1 = (Add(name=f'{name}_add')([output_sa, output_inputw]))\n",
        "  norm1_output = LayerNormalization(name=f'{name}_normalization1')(post_residual1)\n",
        "  output = TransformerTransition(name=f'{name}_transition', activation='relu')(norm1_output)\n",
        "  post_residual2 = (Add(name=f'{name}_add2')([norm1_output,output]))\n",
        "  output = LayerNormalization(name=f'{name}_normalization2')(post_residual2)\n",
        "  output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention2')(output)\n",
        "  post_residual1 = (Add(name=f'{name}_add12')([output_sa, output_inputw]))\n",
        "  norm1_output = LayerNormalization(name=f'{name}_normalization12')(post_residual1)\n",
        "  output = TransformerTransition(name=f'{name}_transition2', activation='relu')(norm1_output)\n",
        "  post_residual2 = (Add(name=f'{name}_add22')([norm1_output,output]))\n",
        "  output = LayerNormalization(name=f'{name}_normalization22')(post_residual2)\n",
        "  \"\"\"\n",
        "  output = Flatten()(output)\n",
        "  \"\"\"\n",
        "  output = Dense(1536, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dense(768, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(256, activation='relu')(output)\n",
        "  #output = Dropout(0.1)(output)\n",
        "  output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(128, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  \"\"\"\n",
        "  output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(2, activation='softmax')(output)\n",
        "  #output = Dense(2, activation='relu')(output)\n",
        "  output = Dense(1, activation='sigmoid')(output)\n",
        "  cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "  cs_p2p.summary()\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-4,\n",
        "      decay_steps=10000,\n",
        "      decay_rate=0.9)\n",
        "  #optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "  #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "  optimizer = RAdamOptimizer(total_steps=100000, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "  cs_p2p.compile(optimizer, \n",
        "                 loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                 #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "          metrics=[\n",
        "          #tf.keras.metrics.MeanSquaredError(),\n",
        "          tf.keras.metrics.BinaryAccuracy(),\n",
        "          tf.keras.metrics.Precision(),\n",
        "          tf.keras.metrics.Recall(),\n",
        "          'accuracy',\n",
        "          tf.keras.metrics.AUC(),]\n",
        "          )\n",
        "  #history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "  history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  print(history.history)\n",
        "  #cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  result = cs_p2p.evaluate(test_dataset)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cs_p2p\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cs_p2p (InputLayer)             [(128, 83)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_w0_4 (inputW0_4)          (128, 83, 16)        23987       cs_p2p[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_0 (MultiHeadS (128, 83, 16)        768         input_w0_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_0 (Add)                  (128, 83, 16)        0           cs_self_attention_0[0][0]        \n",
            "                                                                 input_w0_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_0 (LayerNorma (128, 83, 16)        32          cs_add_0[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_0 (TransformerTra (128, 83, 16)        2128        cs_normalization1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_0 (Add)                 (128, 83, 16)        0           cs_normalization1_0[0][0]        \n",
            "                                                                 cs_transition_0[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_0 (LayerNorma (128, 83, 16)        32          cs_add2_0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_1 (MultiHeadS (128, 83, 16)        768         cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_1 (Add)                  (128, 83, 16)        0           cs_self_attention_1[0][0]        \n",
            "                                                                 cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_1 (LayerNorma (128, 83, 16)        32          cs_add_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_1 (TransformerTra (128, 83, 16)        2128        cs_normalization1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_1 (Add)                 (128, 83, 16)        0           cs_normalization1_1[0][0]        \n",
            "                                                                 cs_transition_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_1 (LayerNorma (128, 83, 16)        32          cs_add2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_2 (MultiHeadS (128, 83, 16)        768         cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_2 (Add)                  (128, 83, 16)        0           cs_self_attention_2[0][0]        \n",
            "                                                                 cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_2 (LayerNorma (128, 83, 16)        32          cs_add_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_2 (TransformerTra (128, 83, 16)        2128        cs_normalization1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_2 (Add)                 (128, 83, 16)        0           cs_normalization1_2[0][0]        \n",
            "                                                                 cs_transition_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_2 (LayerNorma (128, 83, 16)        32          cs_add2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (128, 1328)          0           cs_normalization2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (128, 64)            85056       flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (128, 64)            0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (128, 32)            2080        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (128, 32)            0           dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (128, 1)             33          dropout_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 120,036\n",
            "Trainable params: 120,036\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "6734/6734 [==============================] - 132s 20ms/step - loss: 0.5092 - binary_accuracy: 0.7892 - precision_4: 0.8020 - recall_4: 0.9778 - accuracy: 0.7892 - auc_4: 0.6633\n",
            "Epoch 2/50\n",
            "6734/6734 [==============================] - 131s 19ms/step - loss: 0.4668 - binary_accuracy: 0.8000 - precision_4: 0.8003 - recall_4: 0.9992 - accuracy: 0.8000 - auc_4: 0.6987\n",
            "Epoch 3/50\n",
            "6734/6734 [==============================] - 132s 20ms/step - loss: 0.4611 - binary_accuracy: 0.7999 - precision_4: 0.8002 - recall_4: 0.9995 - accuracy: 0.7999 - auc_4: 0.7029\n",
            "Epoch 4/50\n",
            "6734/6734 [==============================] - 135s 20ms/step - loss: 0.4586 - binary_accuracy: 0.8001 - precision_4: 0.8027 - recall_4: 0.9946 - accuracy: 0.8001 - auc_4: 0.7061\n",
            "Epoch 5/50\n",
            "6734/6734 [==============================] - 133s 20ms/step - loss: 0.4570 - binary_accuracy: 0.8010 - precision_4: 0.8069 - recall_4: 0.9876 - accuracy: 0.8010 - auc_4: 0.7090\n",
            "Epoch 6/50\n",
            "6734/6734 [==============================] - 135s 20ms/step - loss: 0.4559 - binary_accuracy: 0.8015 - precision_4: 0.8070 - recall_4: 0.9882 - accuracy: 0.8015 - auc_4: 0.7109\n",
            "Epoch 7/50\n",
            "6734/6734 [==============================] - 136s 20ms/step - loss: 0.4550 - binary_accuracy: 0.8018 - precision_4: 0.8074 - recall_4: 0.9879 - accuracy: 0.8018 - auc_4: 0.7124\n",
            "Epoch 8/50\n",
            "6734/6734 [==============================] - 138s 20ms/step - loss: 0.4543 - binary_accuracy: 0.8021 - precision_4: 0.8079 - recall_4: 0.9874 - accuracy: 0.8021 - auc_4: 0.7136\n",
            "Epoch 9/50\n",
            "6734/6734 [==============================] - 137s 20ms/step - loss: 0.4538 - binary_accuracy: 0.8024 - precision_4: 0.8081 - recall_4: 0.9874 - accuracy: 0.8024 - auc_4: 0.7147\n",
            "Epoch 10/50\n",
            "6734/6734 [==============================] - 137s 20ms/step - loss: 0.4532 - binary_accuracy: 0.8023 - precision_4: 0.8084 - recall_4: 0.9866 - accuracy: 0.8023 - auc_4: 0.7159\n",
            "Epoch 11/50\n",
            "6734/6734 [==============================] - 139s 21ms/step - loss: 0.4525 - binary_accuracy: 0.8026 - precision_4: 0.8088 - recall_4: 0.9864 - accuracy: 0.8026 - auc_4: 0.7170\n",
            "Epoch 12/50\n",
            "6734/6734 [==============================] - 137s 20ms/step - loss: 0.4520 - binary_accuracy: 0.8026 - precision_4: 0.8088 - recall_4: 0.9864 - accuracy: 0.8026 - auc_4: 0.7179\n",
            "Epoch 13/50\n",
            "6734/6734 [==============================] - 138s 20ms/step - loss: 0.4514 - binary_accuracy: 0.8031 - precision_4: 0.8095 - recall_4: 0.9859 - accuracy: 0.8031 - auc_4: 0.7190\n",
            "Epoch 14/50\n",
            "6734/6734 [==============================] - 136s 20ms/step - loss: 0.4508 - binary_accuracy: 0.8032 - precision_4: 0.8096 - recall_4: 0.9858 - accuracy: 0.8032 - auc_4: 0.7201\n",
            "Epoch 15/50\n",
            "6734/6734 [==============================] - 139s 21ms/step - loss: 0.4501 - binary_accuracy: 0.8034 - precision_4: 0.8099 - recall_4: 0.9855 - accuracy: 0.8034 - auc_4: 0.7213\n",
            "Epoch 16/50\n",
            "6734/6734 [==============================] - 136s 20ms/step - loss: 0.4498 - binary_accuracy: 0.8035 - precision_4: 0.8101 - recall_4: 0.9854 - accuracy: 0.8035 - auc_4: 0.7217\n",
            "Epoch 17/50\n",
            "6734/6734 [==============================] - 138s 20ms/step - loss: 0.4496 - binary_accuracy: 0.8036 - precision_4: 0.8102 - recall_4: 0.9854 - accuracy: 0.8036 - auc_4: 0.7221\n",
            "Epoch 18/50\n",
            "6734/6734 [==============================] - 138s 20ms/step - loss: 0.4495 - binary_accuracy: 0.8034 - precision_4: 0.8101 - recall_4: 0.9852 - accuracy: 0.8034 - auc_4: 0.7223\n",
            "Epoch 19/50\n",
            "6734/6734 [==============================] - 135s 20ms/step - loss: 0.4493 - binary_accuracy: 0.8035 - precision_4: 0.8101 - recall_4: 0.9852 - accuracy: 0.8035 - auc_4: 0.7226\n",
            "Epoch 20/50\n",
            "6734/6734 [==============================] - 137s 20ms/step - loss: 0.4492 - binary_accuracy: 0.8037 - precision_4: 0.8104 - recall_4: 0.9851 - accuracy: 0.8037 - auc_4: 0.7228\n",
            "Epoch 21/50\n",
            "6734/6734 [==============================] - 135s 20ms/step - loss: 0.4492 - binary_accuracy: 0.8036 - precision_4: 0.8103 - recall_4: 0.9851 - accuracy: 0.8036 - auc_4: 0.7229\n",
            "Epoch 22/50\n",
            "6734/6734 [==============================] - 137s 20ms/step - loss: 0.4490 - binary_accuracy: 0.8037 - precision_4: 0.8103 - recall_4: 0.9852 - accuracy: 0.8037 - auc_4: 0.7232\n",
            "Epoch 23/50\n",
            "6734/6734 [==============================] - 135s 20ms/step - loss: 0.4489 - binary_accuracy: 0.8038 - precision_4: 0.8105 - recall_4: 0.9850 - accuracy: 0.8038 - auc_4: 0.7234\n",
            "Epoch 24/50\n",
            "6734/6734 [==============================] - 136s 20ms/step - loss: 0.4488 - binary_accuracy: 0.8038 - precision_4: 0.8104 - recall_4: 0.9851 - accuracy: 0.8038 - auc_4: 0.7236\n",
            "Epoch 25/50\n",
            "6734/6734 [==============================] - 134s 20ms/step - loss: 0.4487 - binary_accuracy: 0.8038 - precision_4: 0.8107 - recall_4: 0.9847 - accuracy: 0.8038 - auc_4: 0.7239\n",
            "Epoch 26/50\n",
            "6734/6734 [==============================] - 138s 20ms/step - loss: 0.4487 - binary_accuracy: 0.8038 - precision_4: 0.8104 - recall_4: 0.9850 - accuracy: 0.8038 - auc_4: 0.7239\n",
            "Epoch 27/50\n",
            "6734/6734 [==============================] - 137s 20ms/step - loss: 0.4485 - binary_accuracy: 0.8036 - precision_4: 0.8105 - recall_4: 0.9847 - accuracy: 0.8036 - auc_4: 0.7243\n",
            "Epoch 28/50\n",
            "6734/6734 [==============================] - 134s 20ms/step - loss: 0.4484 - binary_accuracy: 0.8040 - precision_4: 0.8108 - recall_4: 0.9848 - accuracy: 0.8040 - auc_4: 0.7243\n",
            "Epoch 29/50\n",
            "6734/6734 [==============================] - 137s 20ms/step - loss: 0.4482 - binary_accuracy: 0.8039 - precision_4: 0.8107 - recall_4: 0.9847 - accuracy: 0.8039 - auc_4: 0.7246\n",
            "Epoch 30/50\n",
            "6734/6734 [==============================] - 135s 20ms/step - loss: 0.4481 - binary_accuracy: 0.8042 - precision_4: 0.8110 - recall_4: 0.9846 - accuracy: 0.8042 - auc_4: 0.7248\n",
            "Epoch 31/50\n",
            "6734/6734 [==============================] - 136s 20ms/step - loss: 0.4481 - binary_accuracy: 0.8040 - precision_4: 0.8109 - recall_4: 0.9846 - accuracy: 0.8040 - auc_4: 0.7250\n",
            "Epoch 32/50\n",
            "6734/6734 [==============================] - 137s 20ms/step - loss: 0.4479 - binary_accuracy: 0.8043 - precision_4: 0.8111 - recall_4: 0.9847 - accuracy: 0.8043 - auc_4: 0.7251\n",
            "Epoch 33/50\n",
            "6734/6734 [==============================] - 139s 21ms/step - loss: 0.4478 - binary_accuracy: 0.8042 - precision_4: 0.8111 - recall_4: 0.9844 - accuracy: 0.8042 - auc_4: 0.7254\n",
            "Epoch 34/50\n",
            "6734/6734 [==============================] - 138s 21ms/step - loss: 0.4477 - binary_accuracy: 0.8042 - precision_4: 0.8111 - recall_4: 0.9844 - accuracy: 0.8042 - auc_4: 0.7256\n",
            "Epoch 35/50\n",
            "6734/6734 [==============================] - 140s 21ms/step - loss: 0.4476 - binary_accuracy: 0.8042 - precision_4: 0.8112 - recall_4: 0.9843 - accuracy: 0.8042 - auc_4: 0.7258\n",
            "Epoch 36/50\n",
            "6734/6734 [==============================] - 140s 21ms/step - loss: 0.4475 - binary_accuracy: 0.8042 - precision_4: 0.8111 - recall_4: 0.9844 - accuracy: 0.8042 - auc_4: 0.7260\n",
            "Epoch 37/50\n",
            "6734/6734 [==============================] - 138s 21ms/step - loss: 0.4474 - binary_accuracy: 0.8043 - precision_4: 0.8113 - recall_4: 0.9843 - accuracy: 0.8043 - auc_4: 0.7263\n",
            "Epoch 38/50\n",
            "6734/6734 [==============================] - 141s 21ms/step - loss: 0.4473 - binary_accuracy: 0.8042 - precision_4: 0.8112 - recall_4: 0.9844 - accuracy: 0.8042 - auc_4: 0.7264\n",
            "Epoch 39/50\n",
            "6734/6734 [==============================] - 140s 21ms/step - loss: 0.4472 - binary_accuracy: 0.8043 - precision_4: 0.8113 - recall_4: 0.9843 - accuracy: 0.8043 - auc_4: 0.7265\n",
            "Epoch 40/50\n",
            "6734/6734 [==============================] - 139s 21ms/step - loss: 0.4471 - binary_accuracy: 0.8042 - precision_4: 0.8113 - recall_4: 0.9841 - accuracy: 0.8042 - auc_4: 0.7267\n",
            "Epoch 41/50\n",
            "6734/6734 [==============================] - 140s 21ms/step - loss: 0.4470 - binary_accuracy: 0.8044 - precision_4: 0.8114 - recall_4: 0.9842 - accuracy: 0.8044 - auc_4: 0.7269\n",
            "Epoch 42/50\n",
            "6734/6734 [==============================] - 138s 20ms/step - loss: 0.4469 - binary_accuracy: 0.8045 - precision_4: 0.8114 - recall_4: 0.9842 - accuracy: 0.8045 - auc_4: 0.7273\n",
            "Epoch 43/50\n",
            "6734/6734 [==============================] - 138s 20ms/step - loss: 0.4469 - binary_accuracy: 0.8044 - precision_4: 0.8114 - recall_4: 0.9842 - accuracy: 0.8044 - auc_4: 0.7273\n",
            "Epoch 44/50\n",
            "6734/6734 [==============================] - 139s 21ms/step - loss: 0.4467 - binary_accuracy: 0.8044 - precision_4: 0.8114 - recall_4: 0.9842 - accuracy: 0.8044 - auc_4: 0.7275\n",
            "Epoch 45/50\n",
            "6734/6734 [==============================] - 138s 21ms/step - loss: 0.4466 - binary_accuracy: 0.8045 - precision_4: 0.8116 - recall_4: 0.9840 - accuracy: 0.8045 - auc_4: 0.7276\n",
            "Epoch 46/50\n",
            "6734/6734 [==============================] - 139s 21ms/step - loss: 0.4465 - binary_accuracy: 0.8046 - precision_4: 0.8117 - recall_4: 0.9840 - accuracy: 0.8046 - auc_4: 0.7278\n",
            "Epoch 47/50\n",
            "6734/6734 [==============================] - 137s 20ms/step - loss: 0.4465 - binary_accuracy: 0.8046 - precision_4: 0.8117 - recall_4: 0.9839 - accuracy: 0.8046 - auc_4: 0.7278\n",
            "Epoch 48/50\n",
            "6734/6734 [==============================] - 138s 20ms/step - loss: 0.4463 - binary_accuracy: 0.8046 - precision_4: 0.8118 - recall_4: 0.9838 - accuracy: 0.8046 - auc_4: 0.7281\n",
            "Epoch 49/50\n",
            "6734/6734 [==============================] - 137s 20ms/step - loss: 0.4463 - binary_accuracy: 0.8048 - precision_4: 0.8119 - recall_4: 0.9838 - accuracy: 0.8048 - auc_4: 0.7283\n",
            "Epoch 50/50\n",
            "6734/6734 [==============================] - 136s 20ms/step - loss: 0.4462 - binary_accuracy: 0.8048 - precision_4: 0.8119 - recall_4: 0.9839 - accuracy: 0.8048 - auc_4: 0.7285\n",
            "{'loss': [0.5092452764511108, 0.4667898416519165, 0.46106386184692383, 0.4586198627948761, 0.45701348781585693, 0.45591872930526733, 0.45504480600357056, 0.45432141423225403, 0.4537753462791443, 0.4531531035900116, 0.45250579714775085, 0.4520234167575836, 0.4513554275035858, 0.45076701045036316, 0.4500660300254822, 0.44979026913642883, 0.44963592290878296, 0.4495408833026886, 0.449336439371109, 0.44924500584602356, 0.44917088747024536, 0.44902899861335754, 0.44894400238990784, 0.4488116502761841, 0.448650985956192, 0.4486565887928009, 0.44847944378852844, 0.44840410351753235, 0.4482448995113373, 0.4481406509876251, 0.4481127858161926, 0.44793418049812317, 0.44780394434928894, 0.4477137327194214, 0.4476209282875061, 0.4475255310535431, 0.44737082719802856, 0.4472840130329132, 0.44723761081695557, 0.44713330268859863, 0.4469994306564331, 0.44686460494995117, 0.44685783982276917, 0.44673866033554077, 0.44662386178970337, 0.44654127955436707, 0.4465045630931854, 0.44632023572921753, 0.4462544620037079, 0.44615307450294495], 'binary_accuracy': [0.7891680598258972, 0.7999505996704102, 0.7999030351638794, 0.8001234531402588, 0.8010330200195312, 0.8015028834342957, 0.8017882704734802, 0.8021003603935242, 0.8023706674575806, 0.8022859692573547, 0.8025597929954529, 0.802581787109375, 0.8030830025672913, 0.8031944036483765, 0.8033881187438965, 0.8034809231758118, 0.8036143779754639, 0.8034449815750122, 0.8034751415252686, 0.8037118315696716, 0.8035818934440613, 0.8037315011024475, 0.8038208484649658, 0.8037524223327637, 0.8038440942764282, 0.8037512302398682, 0.8036178350448608, 0.8040065169334412, 0.8038800358772278, 0.8041619658470154, 0.8039635419845581, 0.8042791485786438, 0.8041654229164124, 0.8041549921035767, 0.8041607737541199, 0.8041526675224304, 0.8042779564857483, 0.8042361736297607, 0.8042837381362915, 0.8042361736297607, 0.8043603301048279, 0.8044508099555969, 0.8043580055236816, 0.8044078946113586, 0.8044775128364563, 0.804578423500061, 0.804578423500061, 0.8045923709869385, 0.804783821105957, 0.8047547936439514], 'precision_4': [0.8020427823066711, 0.80032879114151, 0.8001595139503479, 0.8026978373527527, 0.8068890571594238, 0.8070027828216553, 0.8073872923851013, 0.8078638911247253, 0.8080828785896301, 0.8084360361099243, 0.8087766170501709, 0.8088125586509705, 0.8094764947891235, 0.8095919489860535, 0.8098999261856079, 0.8100651502609253, 0.8101599812507629, 0.8100980520248413, 0.8101291060447693, 0.810370683670044, 0.8102668523788452, 0.8103345036506653, 0.810527503490448, 0.8104474544525146, 0.8106796145439148, 0.8104487061500549, 0.8105101585388184, 0.8107883930206299, 0.8107348084449768, 0.810999870300293, 0.8108765482902527, 0.8110843896865845, 0.8111143112182617, 0.8110988736152649, 0.8111624717712402, 0.811137855052948, 0.8112537264823914, 0.8111741542816162, 0.8112771511077881, 0.8113349080085754, 0.8113687038421631, 0.8114414215087891, 0.8113935589790344, 0.8114211559295654, 0.8115799427032471, 0.8116756081581116, 0.8117166757583618, 0.8118051886558533, 0.8119363784790039, 0.8118986487388611], 'recall_4': [0.9777542352676392, 0.9991936087608337, 0.9994662404060364, 0.9945944547653198, 0.9876283407211304, 0.9881736636161804, 0.9878763556480408, 0.9874412417411804, 0.9874499440193176, 0.9866116046905518, 0.9863868355751038, 0.9863520264625549, 0.9858632683753967, 0.9858182668685913, 0.9855296611785889, 0.9853570461273193, 0.985388994216919, 0.9852337837219238, 0.9852221608161926, 0.9851351380348206, 0.9851264357566833, 0.9852381348609924, 0.9850060939788818, 0.9850510358810425, 0.9847464561462402, 0.9850466847419739, 0.9847087264060974, 0.9847986698150635, 0.984697163105011, 0.9846391081809998, 0.9845564365386963, 0.9846652150154114, 0.9844215512275696, 0.9844346046447754, 0.9843200445175171, 0.9843548536300659, 0.9843330979347229, 0.984420120716095, 0.9842968583106995, 0.9841068387031555, 0.9842431545257568, 0.9842489957809448, 0.9841909408569336, 0.9842185378074646, 0.9840227365493774, 0.9840009808540344, 0.9839211702346802, 0.983771800994873, 0.9838283658027649, 0.9838544726371765], 'accuracy': [0.7891680598258972, 0.7999505996704102, 0.7999030351638794, 0.8001234531402588, 0.8010330200195312, 0.8015028834342957, 0.8017882704734802, 0.8021003603935242, 0.8023706674575806, 0.8022859692573547, 0.8025597929954529, 0.802581787109375, 0.8030830025672913, 0.8031944036483765, 0.8033881187438965, 0.8034809231758118, 0.8036143779754639, 0.8034449815750122, 0.8034751415252686, 0.8037118315696716, 0.8035818934440613, 0.8037315011024475, 0.8038208484649658, 0.8037524223327637, 0.8038440942764282, 0.8037512302398682, 0.8036178350448608, 0.8040065169334412, 0.8038800358772278, 0.8041619658470154, 0.8039635419845581, 0.8042791485786438, 0.8041654229164124, 0.8041549921035767, 0.8041607737541199, 0.8041526675224304, 0.8042779564857483, 0.8042361736297607, 0.8042837381362915, 0.8042361736297607, 0.8043603301048279, 0.8044508099555969, 0.8043580055236816, 0.8044078946113586, 0.8044775128364563, 0.804578423500061, 0.804578423500061, 0.8045923709869385, 0.804783821105957, 0.8047547936439514], 'auc_4': [0.663301944732666, 0.6987231373786926, 0.7029246687889099, 0.7061156630516052, 0.7089924812316895, 0.710882842540741, 0.7124350070953369, 0.7135956287384033, 0.7146921753883362, 0.7158606052398682, 0.7170001268386841, 0.7178654670715332, 0.7190190553665161, 0.720062792301178, 0.7212525010108948, 0.7217411994934082, 0.7220683693885803, 0.722277045249939, 0.722615659236908, 0.7227550745010376, 0.7229204177856445, 0.7232154011726379, 0.7234188914299011, 0.723606288433075, 0.7238858938217163, 0.7238598465919495, 0.7242879867553711, 0.7243452668190002, 0.7246361970901489, 0.7248010635375977, 0.7250173091888428, 0.7251444458961487, 0.7254238128662109, 0.7256254553794861, 0.7257629036903381, 0.7260313630104065, 0.7262611389160156, 0.7264447212219238, 0.7265376448631287, 0.7267328500747681, 0.7269240617752075, 0.7272536158561707, 0.7272990345954895, 0.727498471736908, 0.7276427745819092, 0.7278127074241638, 0.727849006652832, 0.7281368374824524, 0.7282848358154297, 0.7285192608833313]}\n",
            "2104/2104 [==============================] - 10s 5ms/step - loss: 0.4524 - binary_accuracy: 0.8037 - precision_4: 0.8093 - recall_4: 0.9876 - accuracy: 0.8037 - auc_4: 0.7148\n",
            "[0.4523681402206421, 0.803699791431427, 0.809278130531311, 0.987629234790802, 0.803699791431427, 0.7147910594940186]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "mAeE735Qy7N2",
        "outputId": "355285a5-67e3-46fa-a8d6-2f6d7895fd80"
      },
      "source": [
        "# auc 73ごえ 単なる過学習　batch size 関係あるのか\n",
        "name='cs' \n",
        "num_heads=4\n",
        "use_masking=False\n",
        "batch_size = batch_size\n",
        "epochs = 100\n",
        "shape_v = 83\n",
        "model_d = 32\n",
        "SA_layers = 3\n",
        "\n",
        "#train_dataset = pre_dataset(train[:256000], batch_size)\n",
        "#print(train_dataset.element_spec[0])\n",
        "#vali_dataset = pre_dataset(vali, 1)\n",
        "#print(len(train[256000:288000]))\n",
        "#vali_dataset = pre_dataset(train[256000:307200], batch_size)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  original_inputs = Input(shape=(shape_v,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "  #output_w = inputW(model_d)(original_inputs)\n",
        "  output = inputW0(model_d)(original_inputs)\n",
        "  \"\"\"\n",
        "  output_1 = Dense(model_d, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output_w)\n",
        "  output = Add()([output_1, output_w])\n",
        "  output = Dropout(0.1)(output)\n",
        "  output_2 = Dense(model_d, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Add()([output_2, output_w])\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = K.reshape(output, shape=(-1, shape_v, model_d))\n",
        "  \"\"\"\n",
        "  for i in range(SA_layers):\n",
        "    output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention_{i}')(output)\n",
        "    post_residual1 = (Add(name=f'{name}_add_{i}')([output_sa, output]))\n",
        "    norm1_output = LayerNormalization(name=f'{name}_normalization1_{i}')(post_residual1)\n",
        "    output = TransformerTransition(name=f'{name}_transition_{i}', activation='relu')(norm1_output)\n",
        "    post_residual2 = (Add(name=f'{name}_add2_{i}')([norm1_output,output]))\n",
        "    output = LayerNormalization(name=f'{name}_normalization2_{i}')(post_residual2)\n",
        "  output = Flatten()(output)\n",
        "\n",
        "  output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(2, activation='softmax')(output)\n",
        "  #output = Dense(2, activation='relu')(output)\n",
        "  output = Dense(1, activation='sigmoid')(output)\n",
        "  cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "  cs_p2p.summary()\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-4,\n",
        "      decay_steps=10000,\n",
        "      decay_rate=0.9)\n",
        "  #optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "  #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "  optimizer = RAdamOptimizer(total_steps=100000, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "  cs_p2p.compile(optimizer, \n",
        "                 loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                 #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "          metrics=[\n",
        "          #tf.keras.metrics.MeanSquaredError(),\n",
        "          tf.keras.metrics.BinaryAccuracy(),\n",
        "          tf.keras.metrics.Precision(),\n",
        "          tf.keras.metrics.Recall(),\n",
        "          'accuracy',\n",
        "          tf.keras.metrics.AUC(),]\n",
        "          )\n",
        "  #history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "  history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  print(history.history)\n",
        "  #cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  result = cs_p2p.evaluate(test_dataset)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-836a460e70be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0moriginal_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cs_p2p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m#output_w = inputW(model_d)(original_inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputW0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \"\"\"\n\u001b[1;32m     21\u001b[0m   output_1 = Dense(model_d, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
            "\u001b[0;31mNameError\u001b[0m: name 'inputW0' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "dL8swhBCgwdj",
        "outputId": "570fe622-90e0-418b-8c05-9d602dfb9240"
      },
      "source": [
        "cs_p2p.evaluate(vali_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1683/1683 [==============================] - 8s 5ms/step - loss: 0.4687 - binary_accuracy: 0.7979 - precision_3: 0.8119 - recall_3: 0.9732 - accuracy: 0.7979 - auc_3: 0.6966\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.4686896502971649,\n",
              " 0.7979472875595093,\n",
              " 0.8118764162063599,\n",
              " 0.9731587767601013,\n",
              " 0.7979472875595093,\n",
              " 0.6966391205787659]"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hO9WQ_Tx1AWU",
        "outputId": "435fa597-aee7-4ba8-e1cb-927e804863f3"
      },
      "source": [
        "# auc 73ごえ\n",
        "name='cs' \n",
        "num_heads=4\n",
        "use_masking=False\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "shape_v = 83\n",
        "model_d = 32\n",
        "SA_layers = 3\n",
        "\n",
        "\"\"\"\n",
        "print(loans_raw)\n",
        "data = loans_raw\n",
        "#data = data[data.loan_status != 'Current']\n",
        "#data = data[data.loan_status != 'In Grace Period']\n",
        "data = data[data.loan_status == 'Charged Off']\n",
        "data = data[data.loan_status == 'Fully Paid']\n",
        "#print(data.loc[:100])\n",
        "train_dataset = pre_dataset(data[:1000],32)\n",
        "print(train_dataset)\n",
        "\"\"\"\n",
        "\n",
        "#train_dataset = pre_dataset(train[:256000], batch_size)\n",
        "#print(train_dataset.element_spec[0])\n",
        "#vali_dataset = pre_dataset(vali, 1)\n",
        "#print(len(train[256000:288000]))\n",
        "#vali_dataset = pre_dataset(train[256000:307200], batch_size)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  original_inputs = Input(shape=(shape_v,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "  #output_w = inputW(model_d)(original_inputs)\n",
        "  output = inputW0(model_d)(original_inputs)\n",
        "  \"\"\"\n",
        "  output_1 = Dense(model_d, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output_w)\n",
        "  output = Add()([output_1, output_w])\n",
        "  output = Dropout(0.1)(output)\n",
        "  output_2 = Dense(model_d, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Add()([output_2, output_w])\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = K.reshape(output, shape=(-1, shape_v, model_d))\n",
        "  \"\"\"\n",
        "  for i in range(SA_layers):\n",
        "    output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention_{i}')(output)\n",
        "    post_residual1 = (Add(name=f'{name}_add_{i}')([output_sa, output]))\n",
        "    norm1_output = LayerNormalization(name=f'{name}_normalization1_{i}')(post_residual1)\n",
        "    output = TransformerTransition(name=f'{name}_transition_{i}', activation='relu')(norm1_output)\n",
        "    post_residual2 = (Add(name=f'{name}_add2_{i}')([norm1_output,output]))\n",
        "    output = LayerNormalization(name=f'{name}_normalization2_{i}')(post_residual2)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention')(output_inputw)\n",
        "  post_residual1 = (Add(name=f'{name}_add')([output_sa, output_inputw]))\n",
        "  norm1_output = LayerNormalization(name=f'{name}_normalization1')(post_residual1)\n",
        "  output = TransformerTransition(name=f'{name}_transition', activation='relu')(norm1_output)\n",
        "  post_residual2 = (Add(name=f'{name}_add2')([norm1_output,output]))\n",
        "  output = LayerNormalization(name=f'{name}_normalization2')(post_residual2)\n",
        "  output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention2')(output)\n",
        "  post_residual1 = (Add(name=f'{name}_add12')([output_sa, output_inputw]))\n",
        "  norm1_output = LayerNormalization(name=f'{name}_normalization12')(post_residual1)\n",
        "  output = TransformerTransition(name=f'{name}_transition2', activation='relu')(norm1_output)\n",
        "  post_residual2 = (Add(name=f'{name}_add22')([norm1_output,output]))\n",
        "  output = LayerNormalization(name=f'{name}_normalization22')(post_residual2)\n",
        "  \"\"\"\n",
        "  output = Flatten()(output)\n",
        "  \"\"\"\n",
        "  output = Dense(1536, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dense(768, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(256, activation='relu')(output)\n",
        "  #output = Dropout(0.1)(output)\n",
        "  output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(128, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  \"\"\"\n",
        "  output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(2, activation='softmax')(output)\n",
        "  #output = Dense(2, activation='relu')(output)\n",
        "  output = Dense(1, activation='sigmoid')(output)\n",
        "  cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "  cs_p2p.summary()\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-4,\n",
        "      decay_steps=10000,\n",
        "      decay_rate=0.9)\n",
        "  #optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "  #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "  optimizer = RAdamOptimizer(total_steps=100000, warmup_proportion=0.2, min_lr=1e-4, name='RectifiedAdam')\n",
        "  cs_p2p.compile(optimizer, \n",
        "                 loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                 #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "          metrics=[\n",
        "          #tf.keras.metrics.MeanSquaredError(),\n",
        "          tf.keras.metrics.BinaryAccuracy(),\n",
        "          tf.keras.metrics.Precision(),\n",
        "          tf.keras.metrics.Recall(),\n",
        "          'accuracy',\n",
        "          tf.keras.metrics.AUC(),]\n",
        "          )\n",
        "  #history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "  history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  print(history.history)\n",
        "  #cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  result = cs_p2p.evaluate(test_dataset)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cs_p2p\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cs_p2p (InputLayer)             [(32, 83)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_w0_22 (inputW0)           (32, 83, 32)         90387       cs_p2p[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_0 (MultiHeadS (32, 83, 32)         3072        input_w0_22[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_0 (Add)                  (32, 83, 32)         0           cs_self_attention_0[0][0]        \n",
            "                                                                 input_w0_22[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_0 (LayerNorma (32, 83, 32)         64          cs_add_0[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_0 (TransformerTra (32, 83, 32)         8352        cs_normalization1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_0 (Add)                 (32, 83, 32)         0           cs_normalization1_0[0][0]        \n",
            "                                                                 cs_transition_0[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_0 (LayerNorma (32, 83, 32)         64          cs_add2_0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_1 (MultiHeadS (32, 83, 32)         3072        cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_1 (Add)                  (32, 83, 32)         0           cs_self_attention_1[0][0]        \n",
            "                                                                 cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_1 (LayerNorma (32, 83, 32)         64          cs_add_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_1 (TransformerTra (32, 83, 32)         8352        cs_normalization1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_1 (Add)                 (32, 83, 32)         0           cs_normalization1_1[0][0]        \n",
            "                                                                 cs_transition_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_1 (LayerNorma (32, 83, 32)         64          cs_add2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_2 (MultiHeadS (32, 83, 32)         3072        cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_2 (Add)                  (32, 83, 32)         0           cs_self_attention_2[0][0]        \n",
            "                                                                 cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_2 (LayerNorma (32, 83, 32)         64          cs_add_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_2 (TransformerTra (32, 83, 32)         8352        cs_normalization1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_2 (Add)                 (32, 83, 32)         0           cs_normalization1_2[0][0]        \n",
            "                                                                 cs_transition_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_2 (LayerNorma (32, 83, 32)         64          cs_add2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_16 (Flatten)            (32, 2656)           0           cs_normalization2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_97 (Dense)                (32, 64)             170048      flatten_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (32, 64)             0           dense_97[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_98 (Dense)                (32, 32)             2080        dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (32, 32)             0           dense_98[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_99 (Dense)                (32, 1)              33          dropout_64[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 297,204\n",
            "Trainable params: 297,204\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "33669/33669 [==============================] - 668s 20ms/step - loss: 0.4776 - binary_accuracy: 0.7998 - precision_6: 0.8006 - recall_6: 0.9985 - accuracy: 0.7998 - auc_21: 0.6855\n",
            "Epoch 2/50\n",
            "33669/33669 [==============================] - 661s 20ms/step - loss: 0.4578 - binary_accuracy: 0.8011 - precision_6: 0.8053 - recall_6: 0.9911 - accuracy: 0.8011 - auc_21: 0.7073\n",
            "Epoch 3/50\n",
            "33669/33669 [==============================] - 661s 20ms/step - loss: 0.4543 - binary_accuracy: 0.8021 - precision_6: 0.8075 - recall_6: 0.9881 - accuracy: 0.8021 - auc_21: 0.7134\n",
            "Epoch 4/50\n",
            "33669/33669 [==============================] - 658s 20ms/step - loss: 0.4522 - binary_accuracy: 0.8028 - precision_6: 0.8088 - recall_6: 0.9869 - accuracy: 0.8028 - auc_21: 0.7168\n",
            "Epoch 5/50\n",
            "33669/33669 [==============================] - 664s 20ms/step - loss: 0.4517 - binary_accuracy: 0.8029 - precision_6: 0.8090 - recall_6: 0.9865 - accuracy: 0.8029 - auc_21: 0.7177\n",
            "Epoch 6/50\n",
            "33669/33669 [==============================] - 664s 20ms/step - loss: 0.4514 - binary_accuracy: 0.8031 - precision_6: 0.8094 - recall_6: 0.9861 - accuracy: 0.8031 - auc_21: 0.7183\n",
            "Epoch 7/50\n",
            "33669/33669 [==============================] - 656s 19ms/step - loss: 0.4512 - binary_accuracy: 0.8032 - precision_6: 0.8095 - recall_6: 0.9861 - accuracy: 0.8032 - auc_21: 0.7187\n",
            "Epoch 8/50\n",
            "33669/33669 [==============================] - 658s 20ms/step - loss: 0.4509 - binary_accuracy: 0.8033 - precision_6: 0.8097 - recall_6: 0.9859 - accuracy: 0.8033 - auc_21: 0.7192\n",
            "Epoch 9/50\n",
            "33669/33669 [==============================] - 660s 20ms/step - loss: 0.4507 - binary_accuracy: 0.8033 - precision_6: 0.8098 - recall_6: 0.9857 - accuracy: 0.8033 - auc_21: 0.7195\n",
            "Epoch 10/50\n",
            "33669/33669 [==============================] - 658s 20ms/step - loss: 0.4504 - binary_accuracy: 0.8034 - precision_6: 0.8100 - recall_6: 0.9855 - accuracy: 0.8034 - auc_21: 0.7200\n",
            "Epoch 11/50\n",
            "33669/33669 [==============================] - 640s 19ms/step - loss: 0.4502 - binary_accuracy: 0.8035 - precision_6: 0.8101 - recall_6: 0.9854 - accuracy: 0.8035 - auc_21: 0.7204\n",
            "Epoch 12/50\n",
            "33669/33669 [==============================] - 642s 19ms/step - loss: 0.4500 - binary_accuracy: 0.8034 - precision_6: 0.8098 - recall_6: 0.9857 - accuracy: 0.8034 - auc_21: 0.7208\n",
            "Epoch 13/50\n",
            "33669/33669 [==============================] - 632s 19ms/step - loss: 0.4498 - binary_accuracy: 0.8035 - precision_6: 0.8101 - recall_6: 0.9854 - accuracy: 0.8035 - auc_21: 0.7211\n",
            "Epoch 14/50\n",
            "33669/33669 [==============================] - 628s 19ms/step - loss: 0.4497 - binary_accuracy: 0.8035 - precision_6: 0.8101 - recall_6: 0.9853 - accuracy: 0.8035 - auc_21: 0.7214\n",
            "Epoch 15/50\n",
            "33669/33669 [==============================] - 632s 19ms/step - loss: 0.4495 - binary_accuracy: 0.8035 - precision_6: 0.8102 - recall_6: 0.9852 - accuracy: 0.8035 - auc_21: 0.7217\n",
            "Epoch 16/50\n",
            "33669/33669 [==============================] - 632s 19ms/step - loss: 0.4493 - binary_accuracy: 0.8035 - precision_6: 0.8103 - recall_6: 0.9850 - accuracy: 0.8035 - auc_21: 0.7221\n",
            "Epoch 17/50\n",
            "33669/33669 [==============================] - 637s 19ms/step - loss: 0.4491 - binary_accuracy: 0.8037 - precision_6: 0.8104 - recall_6: 0.9851 - accuracy: 0.8037 - auc_21: 0.7225\n",
            "Epoch 18/50\n",
            "33669/33669 [==============================] - 635s 19ms/step - loss: 0.4489 - binary_accuracy: 0.8036 - precision_6: 0.8103 - recall_6: 0.9851 - accuracy: 0.8036 - auc_21: 0.7228\n",
            "Epoch 19/50\n",
            "33669/33669 [==============================] - 637s 19ms/step - loss: 0.4486 - binary_accuracy: 0.8040 - precision_6: 0.8107 - recall_6: 0.9850 - accuracy: 0.8040 - auc_21: 0.7233\n",
            "Epoch 20/50\n",
            "33669/33669 [==============================] - 637s 19ms/step - loss: 0.4485 - binary_accuracy: 0.8040 - precision_6: 0.8107 - recall_6: 0.9850 - accuracy: 0.8040 - auc_21: 0.7235\n",
            "Epoch 21/50\n",
            "33669/33669 [==============================] - 645s 19ms/step - loss: 0.4483 - binary_accuracy: 0.8039 - precision_6: 0.8107 - recall_6: 0.9847 - accuracy: 0.8039 - auc_21: 0.7240\n",
            "Epoch 22/50\n",
            "33669/33669 [==============================] - 649s 19ms/step - loss: 0.4480 - binary_accuracy: 0.8041 - precision_6: 0.8109 - recall_6: 0.9848 - accuracy: 0.8041 - auc_21: 0.7245\n",
            "Epoch 23/50\n",
            "33669/33669 [==============================] - 647s 19ms/step - loss: 0.4479 - binary_accuracy: 0.8040 - precision_6: 0.8108 - recall_6: 0.9849 - accuracy: 0.8040 - auc_21: 0.7247\n",
            "Epoch 24/50\n",
            "33669/33669 [==============================] - 656s 19ms/step - loss: 0.4477 - binary_accuracy: 0.8040 - precision_6: 0.8110 - recall_6: 0.9844 - accuracy: 0.8040 - auc_21: 0.7251\n",
            "Epoch 25/50\n",
            "33669/33669 [==============================] - 654s 19ms/step - loss: 0.4475 - binary_accuracy: 0.8040 - precision_6: 0.8109 - recall_6: 0.9847 - accuracy: 0.8040 - auc_21: 0.7255\n",
            "Epoch 26/50\n",
            "33669/33669 [==============================] - 655s 19ms/step - loss: 0.4472 - binary_accuracy: 0.8043 - precision_6: 0.8112 - recall_6: 0.9845 - accuracy: 0.8043 - auc_21: 0.7260\n",
            "Epoch 27/50\n",
            "33669/33669 [==============================] - 649s 19ms/step - loss: 0.4471 - binary_accuracy: 0.8041 - precision_6: 0.8112 - recall_6: 0.9843 - accuracy: 0.8041 - auc_21: 0.7263\n",
            "Epoch 28/50\n",
            "33669/33669 [==============================] - 645s 19ms/step - loss: 0.4469 - binary_accuracy: 0.8042 - precision_6: 0.8112 - recall_6: 0.9844 - accuracy: 0.8042 - auc_21: 0.7267\n",
            "Epoch 29/50\n",
            "33669/33669 [==============================] - 640s 19ms/step - loss: 0.4467 - binary_accuracy: 0.8043 - precision_6: 0.8114 - recall_6: 0.9842 - accuracy: 0.8043 - auc_21: 0.7270\n",
            "Epoch 30/50\n",
            "33669/33669 [==============================] - 647s 19ms/step - loss: 0.4464 - binary_accuracy: 0.8044 - precision_6: 0.8115 - recall_6: 0.9842 - accuracy: 0.8044 - auc_21: 0.7275\n",
            "Epoch 31/50\n",
            "33669/33669 [==============================] - 648s 19ms/step - loss: 0.4462 - binary_accuracy: 0.8045 - precision_6: 0.8117 - recall_6: 0.9838 - accuracy: 0.8045 - auc_21: 0.7279\n",
            "Epoch 32/50\n",
            "33669/33669 [==============================] - 668s 20ms/step - loss: 0.4460 - binary_accuracy: 0.8045 - precision_6: 0.8117 - recall_6: 0.9840 - accuracy: 0.8045 - auc_21: 0.7281\n",
            "Epoch 33/50\n",
            "33669/33669 [==============================] - 666s 20ms/step - loss: 0.4458 - binary_accuracy: 0.8046 - precision_6: 0.8118 - recall_6: 0.9838 - accuracy: 0.8046 - auc_21: 0.7287\n",
            "Epoch 34/50\n",
            "33669/33669 [==============================] - 656s 19ms/step - loss: 0.4456 - binary_accuracy: 0.8046 - precision_6: 0.8119 - recall_6: 0.9837 - accuracy: 0.8046 - auc_21: 0.7290\n",
            "Epoch 35/50\n",
            "33669/33669 [==============================] - 642s 19ms/step - loss: 0.4454 - binary_accuracy: 0.8048 - precision_6: 0.8120 - recall_6: 0.9837 - accuracy: 0.8048 - auc_21: 0.7293\n",
            "Epoch 36/50\n",
            "33669/33669 [==============================] - 659s 20ms/step - loss: 0.4451 - binary_accuracy: 0.8050 - precision_6: 0.8121 - recall_6: 0.9838 - accuracy: 0.8050 - auc_21: 0.7298\n",
            "Epoch 37/50\n",
            "33669/33669 [==============================] - 660s 20ms/step - loss: 0.4450 - binary_accuracy: 0.8048 - precision_6: 0.8121 - recall_6: 0.9837 - accuracy: 0.8048 - auc_21: 0.7300\n",
            "Epoch 38/50\n",
            "33669/33669 [==============================] - 629s 19ms/step - loss: 0.4447 - binary_accuracy: 0.8050 - precision_6: 0.8122 - recall_6: 0.9837 - accuracy: 0.8050 - auc_21: 0.7305\n",
            "Epoch 39/50\n",
            "33669/33669 [==============================] - 624s 19ms/step - loss: 0.4446 - binary_accuracy: 0.8051 - precision_6: 0.8125 - recall_6: 0.9834 - accuracy: 0.8051 - auc_21: 0.7309\n",
            "Epoch 40/50\n",
            "33669/33669 [==============================] - 629s 19ms/step - loss: 0.4445 - binary_accuracy: 0.8049 - precision_6: 0.8122 - recall_6: 0.9835 - accuracy: 0.8049 - auc_21: 0.7310\n",
            "Epoch 41/50\n",
            "33669/33669 [==============================] - 654s 19ms/step - loss: 0.4441 - binary_accuracy: 0.8052 - precision_6: 0.8125 - recall_6: 0.9835 - accuracy: 0.8052 - auc_21: 0.7316\n",
            "Epoch 42/50\n",
            "33669/33669 [==============================] - 646s 19ms/step - loss: 0.4441 - binary_accuracy: 0.8051 - precision_6: 0.8125 - recall_6: 0.9834 - accuracy: 0.8051 - auc_21: 0.7317\n",
            "Epoch 43/50\n",
            "33669/33669 [==============================] - 641s 19ms/step - loss: 0.4438 - binary_accuracy: 0.8054 - precision_6: 0.8127 - recall_6: 0.9833 - accuracy: 0.8054 - auc_21: 0.7320\n",
            "Epoch 44/50\n",
            "33669/33669 [==============================] - 627s 19ms/step - loss: 0.4436 - binary_accuracy: 0.8052 - precision_6: 0.8126 - recall_6: 0.9832 - accuracy: 0.8052 - auc_21: 0.7326\n",
            "Epoch 45/50\n",
            " 3214/33669 [=>............................] - ETA: 9:31 - loss: 0.4439 - binary_accuracy: 0.8051 - precision_6: 0.8130 - recall_6: 0.9823 - accuracy: 0.8051 - auc_21: 0.7321"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OHfhsSR0AT9p",
        "outputId": "5c6e367d-9cce-4474-cfaf-bb2c7d40ef02"
      },
      "source": [
        "name='cs' \n",
        "num_heads=4\n",
        "use_masking=False\n",
        "batch_size = batch_size\n",
        "epochs = 100\n",
        "model_d = 128\n",
        "SA_layers = 2\n",
        "\n",
        "#train_dataset = pre_dataset(train[:256000], batch_size)\n",
        "#print(train_dataset.element_spec[0])\n",
        "#vali_dataset = pre_dataset(vali, 1)\n",
        "#print(len(train[256000:288000]))\n",
        "#vali_dataset = pre_dataset(train[256000:307200], batch_size)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  original_inputs = Input(shape=(83,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "  #inputw = inputW(32)(x_train)\n",
        "  #inputw.shape\n",
        "  #print(original_inputs.shape[0])\n",
        "  #output = inputW(model_d)(original_inputs)\n",
        "  output = inputW1(model_d)(original_inputs)\n",
        "  for i in range(SA_layers):\n",
        "    output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention_{i}')(output)\n",
        "    post_residual1 = (Add(name=f'{name}_add_{i}')([output_sa, output]))\n",
        "    norm1_output = LayerNormalization(name=f'{name}_normalization1_{i}')(post_residual1)\n",
        "    output = TransformerTransition(name=f'{name}_transition_{i}', activation='relu')(norm1_output)\n",
        "    post_residual2 = (Add(name=f'{name}_add2_{i}')([norm1_output,output]))\n",
        "    output = LayerNormalization(name=f'{name}_normalization2_{i}')(post_residual2)\n",
        "\n",
        "  \"\"\"\n",
        "  output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention')(output_inputw)\n",
        "  post_residual1 = (Add(name=f'{name}_add')([output_sa, output_inputw]))\n",
        "  norm1_output = LayerNormalization(name=f'{name}_normalization1')(post_residual1)\n",
        "  output = TransformerTransition(name=f'{name}_transition', activation='relu')(norm1_output)\n",
        "  post_residual2 = (Add(name=f'{name}_add2')([norm1_output,output]))\n",
        "  output = LayerNormalization(name=f'{name}_normalization2')(post_residual2)\n",
        "  output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention2')(output)\n",
        "  post_residual1 = (Add(name=f'{name}_add12')([output_sa, output_inputw]))\n",
        "  norm1_output = LayerNormalization(name=f'{name}_normalization12')(post_residual1)\n",
        "  output = TransformerTransition(name=f'{name}_transition2', activation='relu')(norm1_output)\n",
        "  post_residual2 = (Add(name=f'{name}_add22')([norm1_output,output]))\n",
        "  output = LayerNormalization(name=f'{name}_normalization22')(post_residual2)\n",
        "  \"\"\"\n",
        "  output = Flatten()(output)\n",
        "  \"\"\"\n",
        "  output = Dense(1536, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dense(768, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(256, activation='relu')(output)\n",
        "  #output = Dropout(0.1)(output)\n",
        "  output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(128, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  \"\"\"\n",
        "  output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(2, activation='softmax')(output)\n",
        "  #output = Dense(2, activation='relu')(output)\n",
        "  output = Dense(1, activation='sigmoid')(output)\n",
        "  cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "  cs_p2p.summary()\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-4,\n",
        "      decay_steps=10000,\n",
        "      decay_rate=0.9)\n",
        "  #optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "  optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "  cs_p2p.compile(optimizer, \n",
        "                 loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                 #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "          metrics=[\n",
        "          #tf.keras.metrics.MeanSquaredError(),\n",
        "          #tf.keras.metrics.Accuracy(),\n",
        "          'accuracy',\n",
        "          tf.keras.metrics.AUC(),]\n",
        "          )\n",
        "  history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "  print(history.history)\n",
        "  #cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  result = cs_p2p.evaluate(test_dataset)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"input_w1/dense/Relu:0\", shape=(32, 10624), dtype=float32)\n",
            "Tensor(\"input_w1/Reshape:0\", shape=(32, 83, 128), dtype=float32)\n",
            "Model: \"cs_p2p\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cs_p2p (InputLayer)             [(32, 83)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_w1 (inputW1)              (32, 83, 128)        1370579     cs_p2p[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_0 (MultiHeadS (32, 83, 128)        49152       input_w1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_0 (Add)                  (32, 83, 128)        0           cs_self_attention_0[0][0]        \n",
            "                                                                 input_w1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_0 (LayerNorma (32, 83, 128)        256         cs_add_0[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_0 (TransformerTra (32, 83, 128)        131712      cs_normalization1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_0 (Add)                 (32, 83, 128)        0           cs_normalization1_0[0][0]        \n",
            "                                                                 cs_transition_0[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_0 (LayerNorma (32, 83, 128)        256         cs_add2_0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention_1 (MultiHeadS (32, 83, 128)        49152       cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add_1 (Add)                  (32, 83, 128)        0           cs_self_attention_1[0][0]        \n",
            "                                                                 cs_normalization2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1_1 (LayerNorma (32, 83, 128)        256         cs_add_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition_1 (TransformerTra (32, 83, 128)        131712      cs_normalization1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2_1 (Add)                 (32, 83, 128)        0           cs_normalization1_1[0][0]        \n",
            "                                                                 cs_transition_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2_1 (LayerNorma (32, 83, 128)        256         cs_add2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (32, 10624)          0           cs_normalization2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (32, 64)             680000      flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (32, 64)             0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (32, 32)             2080        dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (32, 32)             0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (32, 1)              33          dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,415,444\n",
            "Trainable params: 2,415,444\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "Tensor(\"cs_p2p/input_w1/dense/Relu:0\", shape=(32, 10624), dtype=float32)\n",
            "Tensor(\"cs_p2p/input_w1/Reshape:0\", shape=(32, 83, 128), dtype=float32)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5ee99b63f0be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m           tf.keras.metrics.AUC(),]\n\u001b[1;32m    112\u001b[0m           )\n\u001b[0;32m--> 113\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcs_p2p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvali_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m   \u001b[0;31m#cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-11-31dc48242882>:51 call  *\n        result = Dense(self.units*input.shape[1],activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:982 __call__  **\n        self._maybe_build(inputs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:2643 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:1178 build\n        trainable=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:614 add_weight\n        caching_device=caching_device)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py:750 _add_variable_with_custom_getter\n        **kwargs_for_getter)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:145 make_variable\n        shape=variable_shape if variable_shape else None)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:260 __call__\n        return cls._variable_v1_call(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call\n        shape=shape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:702 invalid_creator_scope\n        \"tf.function-decorated function tried to create \"\n\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1ohVLt27bOMX",
        "outputId": "f2d07d11-f852-4979-b7f4-581e67a86706"
      },
      "source": [
        "mlp(train_dataset, vali_dataset, test_dataset, 32, 100, [1536, 512, 64])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cs_p2p\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cs_p2p (InputLayer)          [(32, 83)]                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (32, 64)                  5376      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (32, 64)                  0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (32, 1)                   65        \n",
            "=================================================================\n",
            "Total params: 5,441\n",
            "Trainable params: 5,441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "29456/29456 [==============================] - 148s 5ms/step - loss: 0.7429 - mean_squared_error: 0.1550 - accuracy: 0.7958 - auc: 0.6646 - val_loss: 0.4664 - val_mean_squared_error: 0.1482 - val_accuracy: 0.7989 - val_auc: 0.6953\n",
            "Epoch 2/100\n",
            "29456/29456 [==============================] - 146s 5ms/step - loss: 0.4679 - mean_squared_error: 0.1483 - accuracy: 0.8006 - auc: 0.6860 - val_loss: 0.4661 - val_mean_squared_error: 0.1482 - val_accuracy: 0.7989 - val_auc: 0.6962\n",
            "Epoch 3/100\n",
            "29456/29456 [==============================] - 147s 5ms/step - loss: 0.4673 - mean_squared_error: 0.1482 - accuracy: 0.8006 - auc: 0.6874 - val_loss: 0.4656 - val_mean_squared_error: 0.1481 - val_accuracy: 0.7989 - val_auc: 0.6971\n",
            "Epoch 4/100\n",
            "29456/29456 [==============================] - 147s 5ms/step - loss: 0.4667 - mean_squared_error: 0.1480 - accuracy: 0.8006 - auc: 0.6890 - val_loss: 0.4651 - val_mean_squared_error: 0.1479 - val_accuracy: 0.7988 - val_auc: 0.6979\n",
            "Epoch 5/100\n",
            "29456/29456 [==============================] - 147s 5ms/step - loss: 0.4662 - mean_squared_error: 0.1479 - accuracy: 0.8005 - auc: 0.6900 - val_loss: 0.4647 - val_mean_squared_error: 0.1478 - val_accuracy: 0.7988 - val_auc: 0.6986\n",
            "Epoch 6/100\n",
            "29456/29456 [==============================] - 145s 5ms/step - loss: 0.4660 - mean_squared_error: 0.1478 - accuracy: 0.8006 - auc: 0.6905 - val_loss: 0.4646 - val_mean_squared_error: 0.1478 - val_accuracy: 0.7988 - val_auc: 0.6992\n",
            "Epoch 7/100\n",
            "29456/29456 [==============================] - 146s 5ms/step - loss: 0.4653 - mean_squared_error: 0.1476 - accuracy: 0.8006 - auc: 0.6921 - val_loss: 0.4641 - val_mean_squared_error: 0.1477 - val_accuracy: 0.7988 - val_auc: 0.7002\n",
            "Epoch 8/100\n",
            "29456/29456 [==============================] - 145s 5ms/step - loss: 0.4647 - mean_squared_error: 0.1474 - accuracy: 0.8006 - auc: 0.6936 - val_loss: 0.4637 - val_mean_squared_error: 0.1475 - val_accuracy: 0.7987 - val_auc: 0.7011\n",
            "Epoch 9/100\n",
            "29456/29456 [==============================] - 147s 5ms/step - loss: 0.4641 - mean_squared_error: 0.1472 - accuracy: 0.8006 - auc: 0.6950 - val_loss: 0.4632 - val_mean_squared_error: 0.1474 - val_accuracy: 0.7989 - val_auc: 0.7020\n",
            "Epoch 10/100\n",
            "29456/29456 [==============================] - 146s 5ms/step - loss: 0.4634 - mean_squared_error: 0.1470 - accuracy: 0.8006 - auc: 0.6965 - val_loss: 0.4628 - val_mean_squared_error: 0.1472 - val_accuracy: 0.7988 - val_auc: 0.7030\n",
            "Epoch 11/100\n",
            "29456/29456 [==============================] - 145s 5ms/step - loss: 0.4630 - mean_squared_error: 0.1469 - accuracy: 0.8007 - auc: 0.6974 - val_loss: 0.4624 - val_mean_squared_error: 0.1471 - val_accuracy: 0.7989 - val_auc: 0.7038\n",
            "Epoch 12/100\n",
            "29456/29456 [==============================] - 147s 5ms/step - loss: 0.4625 - mean_squared_error: 0.1467 - accuracy: 0.8006 - auc: 0.6985 - val_loss: 0.4620 - val_mean_squared_error: 0.1470 - val_accuracy: 0.7989 - val_auc: 0.7047\n",
            "Epoch 13/100\n",
            "29456/29456 [==============================] - 146s 5ms/step - loss: 0.4621 - mean_squared_error: 0.1466 - accuracy: 0.8007 - auc: 0.6996 - val_loss: 0.4616 - val_mean_squared_error: 0.1468 - val_accuracy: 0.7989 - val_auc: 0.7053\n",
            "Epoch 14/100\n",
            "29456/29456 [==============================] - 146s 5ms/step - loss: 0.4615 - mean_squared_error: 0.1464 - accuracy: 0.8007 - auc: 0.7007 - val_loss: 0.4613 - val_mean_squared_error: 0.1468 - val_accuracy: 0.7990 - val_auc: 0.7063\n",
            "Epoch 15/100\n",
            "29456/29456 [==============================] - 147s 5ms/step - loss: 0.4611 - mean_squared_error: 0.1463 - accuracy: 0.8007 - auc: 0.7016 - val_loss: 0.4610 - val_mean_squared_error: 0.1467 - val_accuracy: 0.7990 - val_auc: 0.7069\n",
            "Epoch 16/100\n",
            "29456/29456 [==============================] - 148s 5ms/step - loss: 0.4608 - mean_squared_error: 0.1462 - accuracy: 0.8008 - auc: 0.7023 - val_loss: 0.4604 - val_mean_squared_error: 0.1465 - val_accuracy: 0.7990 - val_auc: 0.7077\n",
            "Epoch 17/100\n",
            "29456/29456 [==============================] - 146s 5ms/step - loss: 0.4604 - mean_squared_error: 0.1461 - accuracy: 0.8009 - auc: 0.7030 - val_loss: 0.4603 - val_mean_squared_error: 0.1465 - val_accuracy: 0.7990 - val_auc: 0.7081\n",
            "Epoch 18/100\n",
            "29456/29456 [==============================] - 146s 5ms/step - loss: 0.4600 - mean_squared_error: 0.1460 - accuracy: 0.8012 - auc: 0.7037 - val_loss: 0.4599 - val_mean_squared_error: 0.1464 - val_accuracy: 0.7990 - val_auc: 0.7088\n",
            "Epoch 19/100\n",
            "29456/29456 [==============================] - 143s 5ms/step - loss: 0.4598 - mean_squared_error: 0.1459 - accuracy: 0.8014 - auc: 0.7042 - val_loss: 0.4595 - val_mean_squared_error: 0.1462 - val_accuracy: 0.7992 - val_auc: 0.7093\n",
            "Epoch 20/100\n",
            "29456/29456 [==============================] - 144s 5ms/step - loss: 0.4594 - mean_squared_error: 0.1458 - accuracy: 0.8015 - auc: 0.7050 - val_loss: 0.4594 - val_mean_squared_error: 0.1462 - val_accuracy: 0.7990 - val_auc: 0.7097\n",
            "Epoch 21/100\n",
            "15092/29456 [==============>...............] - ETA: 1:01 - loss: 0.4581 - mean_squared_error: 0.1453 - accuracy: 0.8024 - auc: 0.7053Buffered data was truncated after reaching the output size limit."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "th4lwqttSSWk",
        "outputId": "6546993e-b175-4aee-9eeb-c2336a33cdf2"
      },
      "source": [
        "name='cs' \n",
        "num_heads=4\n",
        "use_masking=False\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "model_d = 128\n",
        "SA_layers = 2\n",
        "\n",
        "\"\"\"\n",
        "print(loans_raw)\n",
        "data = loans_raw\n",
        "#data = data[data.loan_status != 'Current']\n",
        "#data = data[data.loan_status != 'In Grace Period']\n",
        "data = data[data.loan_status == 'Charged Off']\n",
        "data = data[data.loan_status == 'Fully Paid']\n",
        "#print(data.loc[:100])\n",
        "train_dataset = pre_dataset(data[:1000],32)\n",
        "print(train_dataset)\n",
        "\"\"\"\n",
        "\n",
        "#train_dataset = pre_dataset(train[:256000], batch_size)\n",
        "#print(train_dataset.element_spec[0])\n",
        "#vali_dataset = pre_dataset(vali, 1)\n",
        "#print(len(train[256000:288000]))\n",
        "#vali_dataset = pre_dataset(train[256000:307200], batch_size)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  original_inputs = Input(shape=(83,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "  #inputw = inputW(32)(x_train)\n",
        "  #inputw.shape\n",
        "  #print(original_inputs.shape[0])\n",
        "  output = inputW(model_d)(original_inputs)\n",
        "  for i in range(SA_layers):\n",
        "    output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention_{i}')(output)\n",
        "    post_residual1 = (Add(name=f'{name}_add_{i}')([output_sa, output]))\n",
        "    norm1_output = LayerNormalization(name=f'{name}_normalization1_{i}')(post_residual1)\n",
        "    output = TransformerTransition(name=f'{name}_transition_{i}', activation='relu')(norm1_output)\n",
        "    post_residual2 = (Add(name=f'{name}_add2_{i}')([norm1_output,output]))\n",
        "    output = LayerNormalization(name=f'{name}_normalization2_{i}')(post_residual2)\n",
        "\n",
        "  output = Flatten()(output)\n",
        "  output = Dense(1536, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  \"\"\"\n",
        "  output = Dense(768, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(256, activation='relu')(output)\n",
        "  #output = Dropout(0.1)(output)\n",
        "  output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(128, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  \"\"\"\n",
        "  output = Dense(512, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(2, activation='softmax')(output)\n",
        "  #output = Dense(2, activation='relu')(output)\n",
        "  output = Dense(1, activation='sigmoid')(output)\n",
        "  cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "  cs_p2p.summary()\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-4,\n",
        "      decay_steps=10000,\n",
        "      decay_rate=0.9)\n",
        "  #optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "  optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "  cs_p2p.compile(optimizer, \n",
        "                 loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                 #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "          metrics=[\n",
        "          #tf.keras.metrics.MeanSquaredError(),\n",
        "          #tf.keras.metrics.Accuracy(),\n",
        "          'accuracy',\n",
        "          tf.keras.metrics.AUC(),]\n",
        "          )\n",
        "  history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "  print(history.history)\n",
        "  #cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  result = cs_p2p.evaluate(test_dataset)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-19d952d856c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#print(len(train[256000:288000]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#vali_dataset = pre_dataset(train[256000:307200], batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0moriginal_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m83\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cs_p2p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m#inputw = inputW(32)(x_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0D0tnRjk1H3y",
        "outputId": "8896d825-84c7-4b48-c852-f97cf721b755"
      },
      "source": [
        "name='cs' \n",
        "num_heads=4\n",
        "use_masking=False\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "\n",
        "\"\"\"\n",
        "print(loans_raw)\n",
        "data = loans_raw\n",
        "#data = data[data.loan_status != 'Current']\n",
        "#data = data[data.loan_status != 'In Grace Period']\n",
        "data = data[data.loan_status == 'Charged Off']\n",
        "data = data[data.loan_status == 'Fully Paid']\n",
        "#print(data.loc[:100])\n",
        "train_dataset = pre_dataset(data[:1000],32)\n",
        "print(train_dataset)\n",
        "\"\"\"\n",
        "\n",
        "#train_dataset = pre_dataset(train[:256000], batch_size)\n",
        "#print(train_dataset.element_spec[0])\n",
        "#vali_dataset = pre_dataset(vali, 1)\n",
        "#print(len(train[256000:288000]))\n",
        "#vali_dataset = pre_dataset(train[256000:307200], batch_size)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  original_inputs = Input(shape=(83,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "  #inputw = inputW(32)(x_train)\n",
        "  #inputw.shape\n",
        "  #print(original_inputs.shape[0])\n",
        "  output_inputw = inputW(64)(original_inputs)\n",
        "  output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention')(output_inputw)\n",
        "  post_residual1 = (Add(name=f'{name}_add')([output_sa, output_inputw]))\n",
        "  norm1_output = LayerNormalization(name=f'{name}_normalization1')(post_residual1)\n",
        "  output = TransformerTransition(name=f'{name}_transition', activation='relu')(norm1_output)\n",
        "  post_residual2 = (Add(name=f'{name}_add2')([norm1_output,output]))\n",
        "  output = LayerNormalization(name=f'{name}_normalization2')(post_residual2)\n",
        "  \"\"\"\n",
        "  output_sa = MultiHeadSelfAttention(\n",
        "              num_heads, use_masking=use_masking, \n",
        "              name=f'{name}_self_attention2')(output)\n",
        "  post_residual1 = (Add(name=f'{name}_add12')([output_sa, output_inputw]))\n",
        "  norm1_output = LayerNormalization(name=f'{name}_normalization12')(post_residual1)\n",
        "  output = TransformerTransition(name=f'{name}_transition2', activation='relu')(norm1_output)\n",
        "  post_residual2 = (Add(name=f'{name}_add22')([norm1_output,output]))\n",
        "  output = LayerNormalization(name=f'{name}_normalization22')(post_residual2)\n",
        "  \"\"\"\n",
        "  output = Flatten()(output)\n",
        "  \"\"\"\n",
        "  output = Dense(1536, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dense(768, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(256, activation='relu')(output)\n",
        "  #output = Dropout(0.1)(output)\n",
        "  output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(128, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  \"\"\"\n",
        "  output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "      bias_regularizer=regularizers.l2(1e-4),\n",
        "      activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "  output = Dropout(0.1)(output)\n",
        "  #output = Dense(2, activation='softmax')(output)\n",
        "  #output = Dense(2, activation='relu')(output)\n",
        "  output = Dense(1, activation='sigmoid')(output)\n",
        "  cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "  cs_p2p.summary()\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-3,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.9)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "  cs_p2p.compile(optimizer, \n",
        "                 loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                 #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "          metrics=[\n",
        "          #tf.keras.metrics.MeanSquaredError(),\n",
        "          #tf.keras.metrics.Accuracy(),\n",
        "          'accuracy',\n",
        "          tf.keras.metrics.AUC(),\n",
        "          ]\n",
        "          )\n",
        "  history = cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "  print(history.history)\n",
        "  #cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "  result = cs_p2p.evaluate(test_dataset)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<tf.Variable 'input_w_18/Variable:0' shape=(83, 64, 64) dtype=float32>\n",
            "finished inputw (32, 83, 64)\n",
            "test\n",
            "Tensor(\"cs_self_attention/einsum_19/Einsum:0\", shape=(128, 83, 16), dtype=float32)\n",
            "Model: \"cs_p2p\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cs_p2p (InputLayer)             [(32, 83)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_w_18 (inputW)             (32, 83, 64)         345363      cs_p2p[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention (MultiHeadSel (32, 83, 64)         12288       input_w_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "cs_add (Add)                    (32, 83, 64)         0           cs_self_attention[0][0]          \n",
            "                                                                 input_w_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1 (LayerNormali (32, 83, 64)         128         cs_add[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition (TransformerTrans (32, 83, 64)         33088       cs_normalization1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2 (Add)                   (32, 83, 64)         0           cs_normalization1[0][0]          \n",
            "                                                                 cs_transition[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2 (LayerNormali (32, 83, 64)         128         cs_add2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_9 (Flatten)             (32, 5312)           0           cs_normalization2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (32, 64)             340032      flatten_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (32, 64)             0           dense_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (32, 32)             2080        dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (32, 32)             0           dense_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_32 (Dense)                (32, 1)              33          dropout_21[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 733,140\n",
            "Trainable params: 733,140\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "<tf.Variable 'input_w_18/Variable:0' shape=(83, 64, 64) dtype=float32>\n",
            "finished inputw (32, 83, 64)\n",
            "test\n",
            "Tensor(\"cs_p2p/cs_self_attention/einsum_1/Einsum:0\", shape=(128, 83, 16), dtype=float32)\n",
            "<tf.Variable 'input_w_18/Variable:0' shape=(83, 64, 64) dtype=float32>\n",
            "finished inputw (32, 83, 64)\n",
            "test\n",
            "Tensor(\"cs_p2p/cs_self_attention/einsum_1/Einsum:0\", shape=(128, 83, 16), dtype=float32)\n",
            "29450/29456 [============================>.] - ETA: 0s - loss: 0.4743 - accuracy: 0.8001 - auc_10: 0.6801<tf.Variable 'input_w_18/Variable:0' shape=(83, 64, 64) dtype=float32>\n",
            "finished inputw (32, 83, 64)\n",
            "test\n",
            "Tensor(\"cs_p2p/cs_self_attention/einsum_1/Einsum:0\", shape=(128, 83, 16), dtype=float32)\n",
            "29456/29456 [==============================] - 177s 6ms/step - loss: 0.4743 - accuracy: 0.8001 - auc_10: 0.6801 - val_loss: 0.4630 - val_accuracy: 0.8021 - val_auc_10: 0.7035\n",
            "Epoch 2/50\n",
            "29456/29456 [==============================] - 177s 6ms/step - loss: 0.4628 - accuracy: 0.8009 - auc_10: 0.7004 - val_loss: 0.4591 - val_accuracy: 0.8014 - val_auc_10: 0.7085\n",
            "Epoch 3/50\n",
            "29456/29456 [==============================] - 175s 6ms/step - loss: 0.4607 - accuracy: 0.8011 - auc_10: 0.7037 - val_loss: 0.4572 - val_accuracy: 0.8026 - val_auc_10: 0.7087\n",
            "Epoch 4/50\n",
            "29456/29456 [==============================] - 176s 6ms/step - loss: 0.4626 - accuracy: 0.8005 - auc_10: 0.6998 - val_loss: 0.4613 - val_accuracy: 0.8014 - val_auc_10: 0.7060\n",
            "Epoch 5/50\n",
            "29456/29456 [==============================] - 173s 6ms/step - loss: 0.4606 - accuracy: 0.8011 - auc_10: 0.7030 - val_loss: 0.4579 - val_accuracy: 0.8026 - val_auc_10: 0.7081\n",
            "Epoch 6/50\n",
            "11945/29456 [===========>..................] - ETA: 1:30 - loss: 0.4594 - accuracy: 0.8013 - auc_10: 0.7048"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-5d6ca8f08b85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m           ]\n\u001b[1;32m     99\u001b[0m           )\n\u001b[0;32m--> 100\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcs_p2p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvali_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m#cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "lWJOlL_WaDSW",
        "outputId": "c4ea052f-3c8c-414e-b993-5c597d6daa6c"
      },
      "source": [
        "eval_dataset = pre_dataset(train[307200:358400], batch_size=batch_size)\n",
        "cs_p2p.evaluate(eval_dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "default: 51200\n",
            "fully paid: 51200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 0.4571 - accuracy: 0.8023 - auc_27: 0.7116\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.45711785554885864, 0.8023242354393005, 0.7116186618804932]"
            ]
          },
          "execution_count": 61,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LqLeRABT8GSm",
        "outputId": "18c0336a-c07a-47f0-e124-aa3a208212ab"
      },
      "source": [
        "name='cs' \n",
        "num_heads=4\n",
        "use_masking=False\n",
        "batch_size = 512\n",
        "epochs = 50\n",
        "\n",
        "\"\"\"\n",
        "print(loans_raw)\n",
        "data = loans_raw\n",
        "#data = data[data.loan_status != 'Current']\n",
        "#data = data[data.loan_status != 'In Grace Period']\n",
        "data = data[data.loan_status == 'Charged Off']\n",
        "data = data[data.loan_status == 'Fully Paid']\n",
        "#print(data.loc[:100])\n",
        "train_dataset = pre_dataset(data[:1000],32)\n",
        "print(train_dataset)\n",
        "\"\"\"\n",
        "\n",
        "train_dataset = pre_dataset(train[:256000], batch_size)\n",
        "print(train_dataset.element_spec[0])\n",
        "#vali_dataset = pre_dataset(vali, 1)\n",
        "print(len(train[256000:288000]))\n",
        "vali_dataset = pre_dataset(train[256000:307200], batch_size)\n",
        "\n",
        "original_inputs = Input(shape=(5,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "#inputw = inputW(32)(x_train)\n",
        "#inputw.shape\n",
        "#print(original_inputs.shape[0])\n",
        "output_inputw = inputW(256)(original_inputs)\n",
        "output_sa = MultiHeadSelfAttention(\n",
        "            num_heads, use_masking=use_masking, \n",
        "            name=f'{name}_self_attention')(output_inputw)\n",
        "post_residual1 = (Add(name=f'{name}_add')([output_sa, output_inputw]))\n",
        "norm1_output = LayerNormalization(name=f'{name}_normalization1')(post_residual1)\n",
        "output = TransformerTransition(name=f'{name}_transition', activation='relu')(norm1_output)\n",
        "post_residual2 = (Add(name=f'{name}_add2')([norm1_output,output]))\n",
        "output = LayerNormalization(name=f'{name}_normalization2')(post_residual2)\n",
        "\n",
        "output_sa = MultiHeadSelfAttention(\n",
        "            num_heads, use_masking=use_masking, \n",
        "            name=f'{name}_self_attention2')(output)\n",
        "post_residual1 = (Add(name=f'{name}_add12')([output_sa, output_inputw]))\n",
        "norm1_output = LayerNormalization(name=f'{name}_normalization12')(post_residual1)\n",
        "output = TransformerTransition(name=f'{name}_transition2', activation='relu')(norm1_output)\n",
        "post_residual2 = (Add(name=f'{name}_add22')([norm1_output,output]))\n",
        "output = LayerNormalization(name=f'{name}_normalization22')(post_residual2)\n",
        "\n",
        "output = Flatten()(output)\n",
        "\"\"\"\n",
        "output = Dense(1536, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dense(768, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "\"\"\"\n",
        "output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "#output = Dense(256, activation='relu')(output)\n",
        "#output = Dropout(0.1)(output)\n",
        "output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "output = Dense(128, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "#output = Dense(2, activation='softmax')(output)\n",
        "#output = Dense(2, activation='relu')(output)\n",
        "output = Dense(1, activation='sigmoid')(output)\n",
        "cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "cs_p2p.summary()\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "cs_p2p.compile(optimizer, \n",
        "               loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "               #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "        metrics=[\n",
        "        #tf.keras.metrics.MeanSquaredError(),\n",
        "        #tf.keras.metrics.Accuracy(),\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.AUC(),\n",
        "        ]\n",
        "        )\n",
        "cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "#cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "print(cs_p2p.predict(train_dataset))\n",
        "train[['loan_status']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "default: 256000\n",
            "fully paid: 256000\n",
            "TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
            "32000\n",
            "default: 51200\n",
            "fully paid: 51200\n",
            "finished inputw (512, 5, 256)\n",
            "test\n",
            "Tensor(\"cs_self_attention/einsum_57/Einsum:0\", shape=(2048, 5, 64), dtype=float32)\n",
            "test\n",
            "Tensor(\"cs_self_attention2/einsum_1/Einsum:0\", shape=(2048, 5, 64), dtype=float32)\n",
            "Model: \"cs_p2p\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cs_p2p (InputLayer)             [(512, 5)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_w_28 (inputW)             (512, 5, 256)        328960      cs_p2p[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention (MultiHeadSel (512, 5, 256)        196608      input_w_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "cs_add (Add)                    (512, 5, 256)        0           cs_self_attention[0][0]          \n",
            "                                                                 input_w_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization1 (LayerNormali (512, 5, 256)        512         cs_add[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition (TransformerTrans (512, 5, 256)        525568      cs_normalization1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "cs_add2 (Add)                   (512, 5, 256)        0           cs_normalization1[0][0]          \n",
            "                                                                 cs_transition[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization2 (LayerNormali (512, 5, 256)        512         cs_add2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "cs_self_attention2 (MultiHeadSe (512, 5, 256)        196608      cs_normalization2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "cs_add12 (Add)                  (512, 5, 256)        0           cs_self_attention2[0][0]         \n",
            "                                                                 input_w_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization12 (LayerNormal (512, 5, 256)        512         cs_add12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cs_transition2 (TransformerTran (512, 5, 256)        525568      cs_normalization12[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "cs_add22 (Add)                  (512, 5, 256)        0           cs_normalization12[0][0]         \n",
            "                                                                 cs_transition2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "cs_normalization22 (LayerNormal (512, 5, 256)        512         cs_add22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_28 (Flatten)            (512, 1280)          0           cs_normalization22[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dense_163 (Dense)               (512, 256)           327936      flatten_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_135 (Dropout)           (512, 256)           0           dense_163[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_164 (Dense)               (512, 256)           65792       dropout_135[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_136 (Dropout)           (512, 256)           0           dense_164[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_165 (Dense)               (512, 128)           32896       dropout_136[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_137 (Dropout)           (512, 128)           0           dense_165[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_166 (Dense)               (512, 64)            8256        dropout_137[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_138 (Dropout)           (512, 64)            0           dense_166[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_167 (Dense)               (512, 32)            2080        dropout_138[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_139 (Dropout)           (512, 32)            0           dense_167[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_168 (Dense)               (512, 1)             33          dropout_139[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 2,212,353\n",
            "Trainable params: 2,212,353\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "finished inputw (512, 5, 256)\n",
            "test\n",
            "Tensor(\"cs_p2p/cs_self_attention/einsum_1/Einsum:0\", shape=(2048, 5, 64), dtype=float32)\n",
            "test\n",
            "Tensor(\"cs_p2p/cs_self_attention2/einsum_1/Einsum:0\", shape=(2048, 5, 64), dtype=float32)\n",
            "finished inputw (512, 5, 256)\n",
            "test\n",
            "Tensor(\"cs_p2p/cs_self_attention/einsum_1/Einsum:0\", shape=(2048, 5, 64), dtype=float32)\n",
            "test\n",
            "Tensor(\"cs_p2p/cs_self_attention2/einsum_1/Einsum:0\", shape=(2048, 5, 64), dtype=float32)\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.7023 - accuracy: 0.7990 - auc_28: 0.5605finished inputw (512, 5, 256)\n",
            "test\n",
            "Tensor(\"cs_p2p/cs_self_attention/einsum_1/Einsum:0\", shape=(2048, 5, 64), dtype=float32)\n",
            "test\n",
            "Tensor(\"cs_p2p/cs_self_attention2/einsum_1/Einsum:0\", shape=(2048, 5, 64), dtype=float32)\n",
            "500/500 [==============================] - 174s 349ms/step - loss: 0.7023 - accuracy: 0.7990 - auc_28: 0.5605 - val_loss: 0.6594 - val_accuracy: 0.7913 - val_auc_28: 0.5937\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 173s 347ms/step - loss: 0.6228 - accuracy: 0.7990 - auc_28: 0.5878 - val_loss: 0.6117 - val_accuracy: 0.7913 - val_auc_28: 0.6067\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 176s 352ms/step - loss: 0.5867 - accuracy: 0.7990 - auc_28: 0.5992 - val_loss: 0.5813 - val_accuracy: 0.7913 - val_auc_28: 0.6134\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 172s 343ms/step - loss: 0.5625 - accuracy: 0.7990 - auc_28: 0.6109 - val_loss: 0.5617 - val_accuracy: 0.7913 - val_auc_28: 0.6214\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 172s 345ms/step - loss: 0.5441 - accuracy: 0.7990 - auc_28: 0.6224 - val_loss: 0.5495 - val_accuracy: 0.7913 - val_auc_28: 0.6236\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 172s 345ms/step - loss: 0.5296 - accuracy: 0.7990 - auc_28: 0.6334 - val_loss: 0.5384 - val_accuracy: 0.7913 - val_auc_28: 0.6242\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 173s 345ms/step - loss: 0.5184 - accuracy: 0.7990 - auc_28: 0.6429 - val_loss: 0.5284 - val_accuracy: 0.7913 - val_auc_28: 0.6361\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 172s 344ms/step - loss: 0.5067 - accuracy: 0.7990 - auc_28: 0.6605 - val_loss: 0.5306 - val_accuracy: 0.7913 - val_auc_28: 0.6727\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 171s 342ms/step - loss: 0.5157 - accuracy: 0.7990 - auc_28: 0.6181 - val_loss: 0.5233 - val_accuracy: 0.7913 - val_auc_28: 0.6269\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 173s 345ms/step - loss: 0.5087 - accuracy: 0.7990 - auc_28: 0.6313 - val_loss: 0.5160 - val_accuracy: 0.7913 - val_auc_28: 0.6367\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 170s 340ms/step - loss: 0.5023 - accuracy: 0.7990 - auc_28: 0.6437 - val_loss: 0.5311 - val_accuracy: 0.7913 - val_auc_28: 0.6412\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 172s 343ms/step - loss: 0.4972 - accuracy: 0.7990 - auc_28: 0.6528 - val_loss: 0.5108 - val_accuracy: 0.7913 - val_auc_28: 0.6571\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 169s 337ms/step - loss: 0.4926 - accuracy: 0.7990 - auc_28: 0.6623 - val_loss: 0.5117 - val_accuracy: 0.7913 - val_auc_28: 0.6347\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 174s 348ms/step - loss: 0.4961 - accuracy: 0.7990 - auc_28: 0.6477 - val_loss: 0.5025 - val_accuracy: 0.7913 - val_auc_28: 0.6584\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 170s 340ms/step - loss: 0.4935 - accuracy: 0.7990 - auc_28: 0.6513 - val_loss: 0.5043 - val_accuracy: 0.7913 - val_auc_28: 0.6490\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 169s 339ms/step - loss: 0.4892 - accuracy: 0.7990 - auc_28: 0.6617 - val_loss: 0.4926 - val_accuracy: 0.7913 - val_auc_28: 0.6807\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 173s 345ms/step - loss: 0.4782 - accuracy: 0.7990 - auc_28: 0.6910 - val_loss: 0.4886 - val_accuracy: 0.7913 - val_auc_28: 0.6912\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 169s 337ms/step - loss: 0.4714 - accuracy: 0.7990 - auc_28: 0.7060 - val_loss: 0.4805 - val_accuracy: 0.7913 - val_auc_28: 0.7077\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 170s 341ms/step - loss: 0.4700 - accuracy: 0.7990 - auc_28: 0.7076 - val_loss: 0.4894 - val_accuracy: 0.7913 - val_auc_28: 0.6885\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 169s 339ms/step - loss: 0.4693 - accuracy: 0.7990 - auc_28: 0.7082 - val_loss: 0.4800 - val_accuracy: 0.7913 - val_auc_28: 0.7073\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 172s 343ms/step - loss: 0.4850 - accuracy: 0.7990 - auc_28: 0.6646 - val_loss: 0.4918 - val_accuracy: 0.7913 - val_auc_28: 0.6743\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 169s 338ms/step - loss: 0.4764 - accuracy: 0.7990 - auc_28: 0.6866 - val_loss: 0.4837 - val_accuracy: 0.7913 - val_auc_28: 0.6968\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 170s 340ms/step - loss: 0.4696 - accuracy: 0.7990 - auc_28: 0.7038 - val_loss: 0.4862 - val_accuracy: 0.7913 - val_auc_28: 0.7045\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 171s 342ms/step - loss: 0.4689 - accuracy: 0.7990 - auc_28: 0.7045 - val_loss: 0.4785 - val_accuracy: 0.7913 - val_auc_28: 0.7066\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 171s 343ms/step - loss: 0.4676 - accuracy: 0.7990 - auc_28: 0.7066 - val_loss: 0.4762 - val_accuracy: 0.7913 - val_auc_28: 0.7097\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 170s 340ms/step - loss: 0.4662 - accuracy: 0.7990 - auc_28: 0.7090 - val_loss: 0.4790 - val_accuracy: 0.7913 - val_auc_28: 0.7036\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 170s 339ms/step - loss: 0.4754 - accuracy: 0.7990 - auc_28: 0.6862 - val_loss: 0.4860 - val_accuracy: 0.7913 - val_auc_28: 0.6909\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 172s 343ms/step - loss: 0.4697 - accuracy: 0.7990 - auc_28: 0.7002 - val_loss: 0.4851 - val_accuracy: 0.7913 - val_auc_28: 0.6878\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 169s 338ms/step - loss: 0.4668 - accuracy: 0.7990 - auc_28: 0.7062 - val_loss: 0.4797 - val_accuracy: 0.7913 - val_auc_28: 0.7084\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 173s 347ms/step - loss: 0.4659 - accuracy: 0.7990 - auc_28: 0.7072 - val_loss: 0.4798 - val_accuracy: 0.7913 - val_auc_28: 0.7038\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 171s 342ms/step - loss: 0.4662 - accuracy: 0.7990 - auc_28: 0.7061 - val_loss: 0.4877 - val_accuracy: 0.7913 - val_auc_28: 0.6904\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 170s 341ms/step - loss: 0.4643 - accuracy: 0.7990 - auc_28: 0.7098 - val_loss: 0.4909 - val_accuracy: 0.7913 - val_auc_28: 0.6889\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 170s 340ms/step - loss: 0.4637 - accuracy: 0.7990 - auc_28: 0.7106 - val_loss: 0.4792 - val_accuracy: 0.7913 - val_auc_28: 0.7072\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 170s 340ms/step - loss: 0.4637 - accuracy: 0.7990 - auc_28: 0.7102 - val_loss: 0.4801 - val_accuracy: 0.7913 - val_auc_28: 0.7070\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 171s 343ms/step - loss: 0.4633 - accuracy: 0.7990 - auc_28: 0.7108 - val_loss: 0.4776 - val_accuracy: 0.7913 - val_auc_28: 0.7042\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 172s 344ms/step - loss: 0.4639 - accuracy: 0.7990 - auc_28: 0.7090 - val_loss: 0.4758 - val_accuracy: 0.7913 - val_auc_28: 0.7081\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 170s 341ms/step - loss: 0.4629 - accuracy: 0.7990 - auc_28: 0.7107 - val_loss: 0.4806 - val_accuracy: 0.7913 - val_auc_28: 0.7028\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 170s 341ms/step - loss: 0.4622 - accuracy: 0.7990 - auc_28: 0.7119 - val_loss: 0.4815 - val_accuracy: 0.7913 - val_auc_28: 0.6926\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 172s 343ms/step - loss: 0.4622 - accuracy: 0.7990 - auc_28: 0.7119 - val_loss: 0.4820 - val_accuracy: 0.7913 - val_auc_28: 0.6896\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 168s 336ms/step - loss: 0.4621 - accuracy: 0.7990 - auc_28: 0.7116 - val_loss: 0.4747 - val_accuracy: 0.7913 - val_auc_28: 0.7080\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 170s 339ms/step - loss: 0.4616 - accuracy: 0.7990 - auc_28: 0.7125 - val_loss: 0.4731 - val_accuracy: 0.7913 - val_auc_28: 0.7089\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 170s 340ms/step - loss: 0.4610 - accuracy: 0.7990 - auc_28: 0.7131 - val_loss: 0.4797 - val_accuracy: 0.7913 - val_auc_28: 0.7033\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 169s 339ms/step - loss: 0.4611 - accuracy: 0.7990 - auc_28: 0.7126 - val_loss: 0.4753 - val_accuracy: 0.7913 - val_auc_28: 0.7066\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 170s 339ms/step - loss: 0.4613 - accuracy: 0.7990 - auc_28: 0.7122 - val_loss: 0.4742 - val_accuracy: 0.7913 - val_auc_28: 0.7108\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 169s 339ms/step - loss: 0.4607 - accuracy: 0.7990 - auc_28: 0.7132 - val_loss: 0.4778 - val_accuracy: 0.7913 - val_auc_28: 0.7103\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 170s 340ms/step - loss: 0.4605 - accuracy: 0.7990 - auc_28: 0.7136 - val_loss: 0.4733 - val_accuracy: 0.7913 - val_auc_28: 0.7069\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 169s 339ms/step - loss: 0.4617 - accuracy: 0.7990 - auc_28: 0.7104 - val_loss: 0.4793 - val_accuracy: 0.7913 - val_auc_28: 0.7027\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 169s 337ms/step - loss: 0.4605 - accuracy: 0.7990 - auc_28: 0.7129 - val_loss: 0.4727 - val_accuracy: 0.7913 - val_auc_28: 0.7092\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 170s 341ms/step - loss: 0.4599 - accuracy: 0.7990 - auc_28: 0.7142 - val_loss: 0.4729 - val_accuracy: 0.7913 - val_auc_28: 0.7094\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 170s 339ms/step - loss: 0.4600 - accuracy: 0.7990 - auc_28: 0.7138 - val_loss: 0.4739 - val_accuracy: 0.7913 - val_auc_28: 0.7050\n",
            "finished inputw (512, 5, 256)\n",
            "test\n",
            "Tensor(\"cs_p2p/cs_self_attention/einsum_1/Einsum:0\", shape=(2048, 5, 64), dtype=float32)\n",
            "test\n",
            "Tensor(\"cs_p2p/cs_self_attention2/einsum_1/Einsum:0\", shape=(2048, 5, 64), dtype=float32)\n",
            "[[0.84118587]\n",
            " [0.5191077 ]\n",
            " [0.618066  ]\n",
            " ...\n",
            " [0.6679287 ]\n",
            " [0.80094725]\n",
            " [0.7188339 ]]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loan_status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fully Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fully Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Fully Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fully Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Fully Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260691</th>\n",
              "      <td>Charged Off</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260692</th>\n",
              "      <td>Fully Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260697</th>\n",
              "      <td>Charged Off</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260699</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260700</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1345383 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         loan_status\n",
              "0         Fully Paid\n",
              "1         Fully Paid\n",
              "2         Fully Paid\n",
              "4         Fully Paid\n",
              "5         Fully Paid\n",
              "...              ...\n",
              "2260691  Charged Off\n",
              "2260692   Fully Paid\n",
              "2260697  Charged Off\n",
              "2260699          NaN\n",
              "2260700          NaN\n",
              "\n",
              "[1345383 rows x 1 columns]"
            ]
          },
          "execution_count": 62,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FJMhrFucc4f"
      },
      "source": [
        "name='cs' \n",
        "num_heads=4\n",
        "use_masking=False\n",
        "batch_size = 512\n",
        "epochs = 20\n",
        "\n",
        "\"\"\"\n",
        "print(loans_raw)\n",
        "data = loans_raw\n",
        "#data = data[data.loan_status != 'Current']\n",
        "#data = data[data.loan_status != 'In Grace Period']\n",
        "data = data[data.loan_status == 'Charged Off']\n",
        "data = data[data.loan_status == 'Fully Paid']\n",
        "#print(data.loc[:100])\n",
        "train_dataset = pre_dataset(data[:1000],32)\n",
        "print(train_dataset)\n",
        "\"\"\"\n",
        "\n",
        "train_dataset = pre_dataset(train[:256000], batch_size)\n",
        "print(train_dataset.element_spec[0])\n",
        "#vali_dataset = pre_dataset(vali, 1)\n",
        "print(len(train[256000:288000]))\n",
        "vali_dataset = pre_dataset(train[256000:307200], batch_size)\n",
        "\n",
        "original_inputs = Input(shape=(5,), batch_size=batch_size, name=\"cs_p2p\")\n",
        "#inputw = inputW(32)(x_train)\n",
        "#inputw.shape\n",
        "#print(original_inputs.shape[0])\n",
        "output_inputw = inputW(256)(original_inputs)\n",
        "output_sa = MultiHeadSelfAttention(\n",
        "            num_heads, use_masking=use_masking, \n",
        "            name=f'{name}_self_attention')(output_inputw)\n",
        "post_residual1 = (Add(name=f'{name}_add')([output_sa, output_inputw]))\n",
        "norm1_output = LayerNormalization(name=f'{name}_normalization1')(post_residual1)\n",
        "output = TransformerTransition(name=f'{name}_transition', activation='relu')(norm1_output)\n",
        "post_residual2 = (Add(name=f'{name}_add2')([norm1_output,output]))\n",
        "output = LayerNormalization(name=f'{name}_normalization2')(post_residual2)\n",
        "\"\"\"\n",
        "output_sa = MultiHeadSelfAttention(\n",
        "            num_heads, use_masking=use_masking, \n",
        "            name=f'{name}_self_attention2')(output)\n",
        "post_residual1 = (Add(name=f'{name}_add12')([output_sa, output_inputw]))\n",
        "norm1_output = LayerNormalization(name=f'{name}_normalization12')(post_residual1)\n",
        "output = TransformerTransition(name=f'{name}_transition2', activation='relu')(norm1_output)\n",
        "post_residual2 = (Add(name=f'{name}_add22')([norm1_output,output]))\n",
        "output = LayerNormalization(name=f'{name}_normalization22')(post_residual2)\n",
        "\"\"\"\n",
        "output = Flatten()(output)\n",
        "\"\"\"\n",
        "output = Dense(1536, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dense(768, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "\"\"\"\n",
        "#output = Dense(256, activation='relu')(output)\n",
        "#output = Dropout(0.1)(output)\n",
        "output = Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "output = Dense(128, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "output = Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "output = Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "    bias_regularizer=regularizers.l2(1e-4),\n",
        "    activity_regularizer=regularizers.l2(1e-5))(output)\n",
        "output = Dropout(0.1)(output)\n",
        "#output = Dense(2, activation='softmax')(output)\n",
        "#output = Dense(2, activation='relu')(output)\n",
        "output = Dense(1, activation='sigmoid')(output)\n",
        "cs_p2p = Model(inputs=original_inputs, outputs=output, name=\"cs_p2p\")\n",
        "cs_p2p.summary()\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "#optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "cs_p2p.compile(optimizer, \n",
        "               loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "               #loss=tf.keras.metrics.MeanSquaredError(),\n",
        "        metrics=[\n",
        "        #tf.keras.metrics.MeanSquaredError(),\n",
        "        #tf.keras.metrics.Accuracy(),\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.AUC(),\n",
        "        ]\n",
        "        )\n",
        "cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=vali_dataset,validation_batch_size=batch_size)\n",
        "#cs_p2p.fit(train_dataset, epochs=epochs, batch_size=batch_size)\n",
        "print(cs_p2p.predict(train_dataset))\n",
        "train[['loan_status']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIFufVtqoooQ"
      },
      "source": [
        "## AutoInt  experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2PYWpuLrBv2",
        "outputId": "cd403559-abed-48cb-be77-6bda905c091a"
      },
      "source": [
        "128000 + 25600"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "153600"
            ]
          },
          "execution_count": 363,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wJmdKlZybsD"
      },
      "source": [
        "train = round(len(df_y_2) //128 *0.7)*128\n",
        "test = round(len(df_y_2) //128 *0.3)*128\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tVNcxUL6bVT"
      },
      "source": [
        "train = round(len(df_y_2) //128 *0.8*0.8)*128\n",
        "vali = round(len(df_y_2) //128 *0.8*0.2)*128\n",
        "test = round(len(df_y_2) //128 *0.2)*128\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nVakcCrlLbV"
      },
      "source": [
        "#@title less sample\n",
        "train = round(len(df_y_2) //128 *0.2)*128\n",
        "vali = round(len(df_y_2) //128 *0.3)*128\n",
        "test = round(len(df_y_2) //128 *0.2)*128\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIkV1j64b7Mu"
      },
      "source": [
        "outer = round(len(df_y_2) //128 *0.5)*128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xOfOU_7y2Jg",
        "outputId": "443d4721-44fe-45ec-91e9-4187cc270109"
      },
      "source": [
        "train , vali, test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(883712, 220928, 276224)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEBYvJLkC6i_"
      },
      "source": [
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uanJRD8Eu3nV",
        "outputId": "072b5ccd-686e-4ac2-a5cf-449a17f324f6"
      },
      "source": [
        "df_i_2.shape, df_emb_i_2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1380929, 64), (1380929, 2))"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4puXvHZyovVa"
      },
      "source": [
        "at_t_v = df_v_2[:train].copy()\n",
        "at_t_i = df_i_2[:train].copy()\n",
        "at_t_y = df_y_2[:train].copy()\n",
        "emb_at_i = df_emb_i_2[:train].copy()\n",
        "emb_at_v = df_emb_v_2[:train].copy()\n",
        "meta_at_i = df_m_2[:train].copy()\n",
        "meta_at_i_r = df_m_r_2[:train].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S97g8ykFDwiP"
      },
      "source": [
        "vali_t_v = df_v_2[train:train+vali].copy()\n",
        "vali_t_i = df_i_2[train:train+vali].copy()\n",
        "vali_t_y = df_y_2[train:train+vali].copy()\n",
        "vali_emb_at_i = df_emb_i_2[train:train+vali].copy()\n",
        "vali_emb_at_v = df_emb_v_2[train:train+vali].copy()\n",
        "vali_meta_at_i = df_m_2[train:train+vali].copy()\n",
        "vali_meta_at_i_r = df_m_r_2[train:train+vali].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65-zrhC6J86C"
      },
      "source": [
        "test_t_v = df_v_2[train+vali:test+vali+train].copy()\n",
        "test_t_i = df_i_2[train+vali:test+vali+train].copy()\n",
        "test_t_y = df_y_2[train+vali:test+vali+train].copy()\n",
        "test_emb_at_i = df_emb_i_2[train+vali:test+vali+train].copy()\n",
        "test_emb_at_v = df_emb_v_2[train+vali:test+vali+train].copy()\n",
        "test_meta_at_i = df_m_2[train+vali:test+vali+train].copy()\n",
        "test_meta_at_i_r = df_m_r_2[train+vali:test+vali+train].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q83iYTmC98X"
      },
      "source": [
        "at_t_v = deepcopy(df_v_2[:train])\n",
        "at_t_i = deepcopy(df_i_2[:train])\n",
        "at_t_y = deepcopy(df_y_2[:train])\n",
        "emb_at_i = deepcopy(df_emb_i_2[:train])\n",
        "emb_at_v = deepcopy(df_emb_v_2[:train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AhCoxv8DP9J"
      },
      "source": [
        "vali_t_v = deepcopy(df_v_2[train:train+vali])\n",
        "vali_t_i = deepcopy(df_i_2[train:train+vali])\n",
        "vali_t_y = deepcopy(df_y_2[train:train+vali])\n",
        "vali_emb_at_i = deepcopy(df_emb_i_2[train:train+vali])\n",
        "vali_emb_at_v = deepcopy(df_emb_v_2[train:train+vali])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "UH-eRqsZDcEu",
        "outputId": "611d6566-b813-4a7f-a450-b21254d0f9ae"
      },
      "source": [
        "test_t_v = deepcopy(df_v_2[train+vali:test+vali+train])\n",
        "test_t_i = deepcopy(df_i_2[train+vali:test+vali+train])\n",
        "test_t_y = deepcopy(df_y_2[train+vali:test+vali+train])\n",
        "test_emb_at_i = deepcopy(df_emb_i_2[train+vali:test+vali+train])\n",
        "test_emb_at_v = deepcopy(df_emb_v_2[train+vali:test+vali+train])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-900c6382cc4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_t_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_v_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvali\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvali\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_t_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_i_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvali\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvali\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_t_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_y_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvali\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvali\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_emb_at_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_emb_i_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvali\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvali\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_emb_at_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_emb_v_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvali\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvali\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'deepcopy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rYEDK-bb_tW"
      },
      "source": [
        "outer_t_v = df_v_2[train+vali:test+vali+train+outer].copy()\n",
        "outer_t_i = df_i_2[train+vali:test+vali+train+outer].copy()\n",
        "outer_t_y = df_y_2[train+vali:test+vali+train+outer].copy()\n",
        "outer_emb_at_i = df_emb_i_2[train+vali:test+vali+train+outer].copy()\n",
        "outer_emb_at_v = df_emb_v_2[train+vali:test+vali+train+outer].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCqzYlFf8A4Y"
      },
      "source": [
        "del df_v_2, df_i_2, df_y_2, df_emb_i_2, df_emb_v_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T656SGpfzgNd"
      },
      "source": [
        "feat_size = 410\n",
        "num_input = df_v_2.shape[-1]\n",
        "num_emb = test_emb_at_i.shape[-1]\n",
        "emb_d = 128\n",
        "# batch_size also needs to be auto\n",
        "directory = '/content/kt'\n",
        "batch_size = 128\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoTZUTxXBWHa",
        "outputId": "5384a840-cacf-450b-fc68-341dde6e4eea"
      },
      "source": [
        "num_input, num_emb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(64, 2)"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y_IXebepKn4",
        "outputId": "a740d0ce-0564-415b-dc2e-96f40e0a6e0a"
      },
      "source": [
        "emb_at_i.shape, emb_at_v.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((883712, 2, 1), (883712, 2, 128))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYvVKzvFpzoO",
        "outputId": "67c5a7a2-6fe4-49cb-c2bb-b968c5b4eb92"
      },
      "source": [
        "df_y_2[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 1.], dtype=float32)"
            ]
          },
          "execution_count": 37,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCEq80w2p8Cf",
        "outputId": "8596e41e-632d-4b15-b07c-911370e92d42"
      },
      "source": [
        "len(at_t_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "276224"
            ]
          },
          "execution_count": 221,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1R6qYprnRtu",
        "outputId": "ea78cacd-6757-4e9a-f09e-31d048a34ba1"
      },
      "source": [
        "meta_at_i.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(883712, 66)"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4upMyetMEl5z"
      },
      "source": [
        "emb_at_i = emb_at_i.reshape([-1, 2, 1])\n",
        "emb_at_v = emb_at_v.reshape([-1, 2, 128])\n",
        "at_t_i = at_t_i.reshape([-1, num_input, 1])\n",
        "at_t_v = at_t_v.reshape([-1, num_input, 1])\n",
        "\n",
        "at_t_y = at_t_y.reshape([-1, 2])\n",
        "\n",
        "vali_emb_at_i = vali_emb_at_i.reshape([-1, 2, 1])\n",
        "vali_emb_at_v = vali_emb_at_v.reshape([-1, 2, 128])\n",
        "vali_t_i = vali_t_i.reshape([-1, num_input, 1])\n",
        "vali_t_v = vali_t_v.reshape([-1, num_input, 1])\n",
        "\n",
        "vali_t_y = vali_t_y.reshape([-1, 2])\n",
        "\n",
        "test_emb_at_i = test_emb_at_i.reshape([-1, 2, 1])\n",
        "test_emb_at_v = test_emb_at_v.reshape([-1, 2, 128])\n",
        "test_t_i = test_t_i.reshape([-1, num_input, 1])\n",
        "test_t_v = test_t_v.reshape([-1, num_input, 1])\n",
        "\n",
        "test_t_y = test_t_y.reshape([-1, 2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzoXtSdRpLQ3"
      },
      "source": [
        "meta_at_i = meta_at_i.reshape([-1, num_input, 1])\n",
        "meta_at_i_r = meta_at_i_r.reshape([-1, num_input, 1])\n",
        "\n",
        "vali_meta_at_i = vali_meta_at_i.reshape([-1, num_input, 1])\n",
        "vali_meta_at_i_r = vali_meta_at_i_r.reshape([-1, num_input, 1])\n",
        "\n",
        "test_meta_at_i = test_meta_at_i.reshape([-1, num_input, 1])\n",
        "test_meta_at_i_r = test_meta_at_i_r.reshape([-1, num_input, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt-ZPa8WqcxV",
        "outputId": "8ab04fde-54ff-43c5-b97e-5a4f0f8d0da0"
      },
      "source": [
        "vali_meta_at_i.shape, vali_t_i.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((227832, 64, 1), (220928, 64, 1))"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TriyPaM1cI5A"
      },
      "source": [
        "douter_emb_at_i = outer_emb_at_i.reshape([-1, 2, 1])\n",
        "outer_emb_at_v = outer_emb_at_v.reshape([-1, 2, 128])\n",
        "outer_t_i = outer_t_i.reshape([-1, num_input, 1])\n",
        "outer_t_v = outer_t_v.reshape([-1, num_input, 1])\n",
        "outer_t_y = outer_t_y.reshape([-1, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv1JxiCTLJDI"
      },
      "source": [
        "emb_at_i = emb_at_i.reshape([-1, 2, 1])\n",
        "emb_at_v = emb_at_v.reshape([-1, 2, 128])\n",
        "at_t_i = at_t_i.reshape([-1, num_input, 1])\n",
        "at_t_v = at_t_v.reshape([-1, num_input, 1])\n",
        "at_t_y = at_t_y.reshape([-1, 3])\n",
        "\n",
        "vali_emb_at_i = vali_emb_at_i.reshape([-1, 2, 1])\n",
        "vali_emb_at_v = vali_emb_at_v.reshape([-1, 2, 128])\n",
        "vali_t_i = vali_t_i.reshape([-1, num_input, 1])\n",
        "vali_t_v = vali_t_v.reshape([-1, num_input, 1])\n",
        "vali_t_y = vali_t_y.reshape([-1, 3])\n",
        "\n",
        "test_emb_at_i = test_emb_at_i.reshape([-1, 2, 1])\n",
        "test_emb_at_v = test_emb_at_v.reshape([-1, 2, 128])\n",
        "test_t_i = test_t_i.reshape([-1, num_input, 1])\n",
        "test_t_v = test_t_v.reshape([-1, num_input, 1])\n",
        "test_t_y = test_t_y.reshape([-1, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3xWrlGZs14w"
      },
      "source": [
        "at_t_v = at_t_v[0:,26:]\n",
        "at_t_i = at_t_i[0:,26:]\n",
        "\n",
        "vali_t_v = vali_t_v[0:,26:]\n",
        "vali_t_i = vali_t_i[0:,26:]\n",
        "\n",
        "test_t_v = test_t_v[0:,26:]\n",
        "test_t_i = test_t_i[0:,26:]\n",
        "num_input = test_t_i.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tb0Ld69steM",
        "outputId": "d1f14fb1-0de1-479e-a0f7-2a5fe01554bf"
      },
      "source": [
        "num_input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 232,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn2CZRx8qhtU",
        "outputId": "1418e2ff-3953-4d09-f584-45db8e98c8a4"
      },
      "source": [
        "test_t_i.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(276224, 64, 1)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MizF6sW9gwx"
      },
      "source": [
        "model = CatBoostClassifier(\n",
        "    custom_loss=[metrics.Accuracy()],\n",
        "    random_seed=42,\n",
        "    logging_level='Silent'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBKB9WLxoclO"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(({\"value\": at_t_v, \"index\": at_t_i, 'emb_value':emb_at_v, 'emb_index':emb_at_i}, at_t_y))\n",
        "vali_ds = tf.data.Dataset.from_tensor_slices(({\"value\": vali_t_v, \"index\": vali_t_i, 'emb_value':vali_emb_at_v, 'emb_index':vali_emb_at_i}, vali_t_y))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(({\"value\": test_t_v, \"index\": test_t_i, 'emb_value':test_emb_at_v, 'emb_index':test_emb_at_i}, test_t_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoVGIy1XpYfF"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(({\"value\": at_t_v, \"index\": at_t_i, 'emb_value':emb_at_v, 'emb_index':emb_at_i, \n",
        "                                                'meta_index':meta_at_i, 'reverse_meta_index':meta_at_i_r}, at_t_y))\n",
        "vali_ds = tf.data.Dataset.from_tensor_slices(({\"value\": vali_t_v, \"index\": vali_t_i, 'emb_value':vali_emb_at_v, 'emb_index':vali_emb_at_i, \n",
        "                                              'meta_index':vali_meta_at_i, 'reverse_meta_index':vali_meta_at_i_r}, vali_t_y))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(({\"value\": test_t_v, \"index\": test_t_i, 'emb_value':test_emb_at_v, 'emb_index':test_emb_at_i, \n",
        "                                              'meta_index':test_meta_at_i, 'reverse_meta_index':test_meta_at_i_r}, test_t_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xqjNRp0cSzT"
      },
      "source": [
        "outer_ds = tf.data.Dataset.from_tensor_slices(({\"value\": outer_t_v, \"index\": outer_t_i, 'emb_value':outer_emb_at_v, 'emb_index':outer_emb_at_i}, outer_t_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoembUJYtaWr"
      },
      "source": [
        "train_ds = train_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
        "vali_ds = vali_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
        "test_ds = test_ds.batch(batch_size=batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKg0266xcaOn"
      },
      "source": [
        "outer_ds = outer_ds.batch(batch_size=batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIB9C8Gbt1ET"
      },
      "source": [
        "val_ds =  train_ds.take(round(len(train_ds) *0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZR7Ico7vsWZ"
      },
      "source": [
        "# np.shard or np.split\n",
        "train_ds_1 = train_ds.shard(5, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mLO3RT1zTcW"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR_BJhdBLoAy"
      },
      "source": [
        "Hyperparameter    |Value             |Best Value So Far \n",
        "num_of_d          |128               |?                 \n",
        "emb_layer_choice  |dense             |?                 \n",
        "EM_dropout_rate   |0.44              |?                 \n",
        "num_head          |8                 |?                 \n",
        "mha_residual      |True              |?                 \n",
        "num_MHA_layer     |6                 |?                 \n",
        "learning_rate     |0.00033171        |?                 \n",
        "decay_steps       |50000             |?                 \n",
        "decay_rate        |0.97              |?                 \n",
        "label_smoothing   |0.45              |?    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9MeKAXPINtm"
      },
      "source": [
        "\"\"\"Implements Multi-label confusion matrix scores.\"\"\"\n",
        "\n",
        "import warnings\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.metrics import Metric\n",
        "import numpy as np\n",
        "\n",
        "from typeguard import typechecked\n",
        "from tensorflow_addons.utils.types import AcceptableDTypes, FloatTensorLike\n",
        "\n",
        "\n",
        "class MCMdic(Metric):\n",
        "    \"\"\"Computes Multi-label confusion matrix.\n",
        "    Class-wise confusion matrix is computed for the\n",
        "    evaluation of classification.\n",
        "    If multi-class input is provided, it will be treated\n",
        "    as multilabel data.\n",
        "    Consider classification problem with two classes\n",
        "    (i.e num_classes=2).\n",
        "    Resultant matrix `M` will be in the shape of `(num_classes, 2, 2)`.\n",
        "    Every class `i` has a dedicated matrix of shape `(2, 2)` that contains:\n",
        "    - true negatives for class `i` in `M(0,0)`\n",
        "    - false positives for class `i` in `M(0,1)`\n",
        "    - false negatives for class `i` in `M(1,0)`\n",
        "    - true positives for class `i` in `M(1,1)`\n",
        "    Args:\n",
        "        num_classes: `int`, the number of labels the prediction task can have.\n",
        "        name: (Optional) string name of the metric instance.\n",
        "        dtype: (Optional) data type of the metric result.\n",
        "    Usage:\n",
        "    >>> # multilabel confusion matrix\n",
        "    >>> y_true = np.array([[1, 0, 1], [0, 1, 0]], dtype=np.int32)\n",
        "    >>> y_pred = np.array([[1, 0, 0], [0, 1, 1]], dtype=np.int32)\n",
        "    >>> metric = tfa.metrics.MultiLabelConfusionMatrix(num_classes=3)\n",
        "    >>> metric.update_state(y_true, y_pred)\n",
        "    >>> result = metric.result()\n",
        "    >>> result.numpy()  #doctest: -DONT_ACCEPT_BLANKLINE\n",
        "    array([[[1., 0.],\n",
        "            [0., 1.]],\n",
        "    <BLANKLINE>\n",
        "           [[1., 0.],\n",
        "            [0., 1.]],\n",
        "    <BLANKLINE>\n",
        "           [[0., 1.],\n",
        "            [1., 0.]]], dtype=float32)\n",
        "    >>> # if multiclass input is provided\n",
        "    >>> y_true = np.array([[1, 0, 0], [0, 1, 0]], dtype=np.int32)\n",
        "    >>> y_pred = np.array([[1, 0, 0], [0, 0, 1]], dtype=np.int32)\n",
        "    >>> metric = tfa.metrics.MultiLabelConfusionMatrix(num_classes=3)\n",
        "    >>> metric.update_state(y_true, y_pred)\n",
        "    >>> result = metric.result()\n",
        "    >>> result.numpy() #doctest: -DONT_ACCEPT_BLANKLINE\n",
        "    array([[[1., 0.],\n",
        "            [0., 1.]],\n",
        "    <BLANKLINE>\n",
        "           [[1., 0.],\n",
        "            [1., 0.]],\n",
        "    <BLANKLINE>\n",
        "           [[1., 1.],\n",
        "            [0., 0.]]], dtype=float32)\n",
        "    \"\"\"\n",
        "\n",
        "    @typechecked\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: FloatTensorLike,\n",
        "        name: str = \"confusion_matrix\",\n",
        "        dtype: AcceptableDTypes = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(name=name, dtype=dtype)\n",
        "        self.num_classes = num_classes\n",
        "        self.true_positives = self.add_weight(\n",
        "            \"true_positives\",\n",
        "            shape=[self.num_classes],\n",
        "            initializer=\"zeros\",\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "        self.false_positives = self.add_weight(\n",
        "            \"false_positives\",\n",
        "            shape=[self.num_classes],\n",
        "            initializer=\"zeros\",\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "        self.false_negatives = self.add_weight(\n",
        "            \"false_negatives\",\n",
        "            shape=[self.num_classes],\n",
        "            initializer=\"zeros\",\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "        self.true_negatives = self.add_weight(\n",
        "            \"true_negatives\",\n",
        "            shape=[self.num_classes],\n",
        "            initializer=\"zeros\",\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        if sample_weight is not None:\n",
        "            warnings.warn(\n",
        "                \"`sample_weight` is not None. Be aware that MultiLabelConfusionMatrix \"\n",
        "                \"does not take `sample_weight` into account when computing the metric \"\n",
        "                \"value.\"\n",
        "            )\n",
        "\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_pred = tf.argmax(y_pred, axis=-1)\n",
        "        y_pred = tf.one_hot(y_pred, 3)\n",
        "        y_pred = tf.cast(y_pred, tf.int32)\n",
        "        red = tf.reduce_sum(tf.cast(tf.math.equal(tf.math.count_nonzero(y_pred, axis=0, dtype=tf.int32), tf.constant([0,0,0])),tf.int32))\n",
        "        c =0\n",
        "        if red == tf.constant([3]):\n",
        "          if c > 0:\n",
        "            print(\"all zero error\")\n",
        "            raise ValueError\n",
        "          c +=1\n",
        "        # true positive\n",
        "        true_positive = tf.math.count_nonzero(y_true * y_pred, 0)\n",
        "        # predictions sum\n",
        "        pred_sum = tf.math.count_nonzero(y_pred, 0)\n",
        "        # true labels sum\n",
        "        true_sum = tf.math.count_nonzero(y_true, 0)\n",
        "        false_positive = pred_sum - true_positive\n",
        "        false_negative = true_sum - true_positive\n",
        "        y_true_negative = tf.math.not_equal(y_true, 1)\n",
        "        y_pred_negative = tf.math.not_equal(y_pred, 1)\n",
        "        true_negative = tf.math.count_nonzero(\n",
        "            tf.math.logical_and(y_true_negative, y_pred_negative), axis=0\n",
        "        )\n",
        "\n",
        "        # true positive state update\n",
        "        self.true_positives.assign_add(tf.cast(true_positive, self.dtype))\n",
        "        # false positive state update\n",
        "        self.false_positives.assign_add(tf.cast(false_positive, self.dtype))\n",
        "        # false negative state update\n",
        "        self.false_negatives.assign_add(tf.cast(false_negative, self.dtype))\n",
        "        # true negative state update\n",
        "        self.true_negatives.assign_add(tf.cast(true_negative, self.dtype))\n",
        "\n",
        "\n",
        "    def result(self):\n",
        "        flat_confusion_matrix = tf.convert_to_tensor(\n",
        "            [\n",
        "                self.true_negatives,\n",
        "                self.false_positives,\n",
        "                self.false_negatives,\n",
        "                self.true_positives,\n",
        "            ]\n",
        "        )\n",
        "        # reshape into 2*2 matrix\n",
        "        confusion_matrix = tf.reshape(tf.transpose(flat_confusion_matrix), [-1, 2, 2])\n",
        "\n",
        "        return flat_confusion_matrix\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Returns the serializable config of the metric.\"\"\"\n",
        "\n",
        "        config = {\n",
        "            \"num_classes\": self.num_classes,\n",
        "        }\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, **config}\n",
        "\n",
        "    def reset_state(self):\n",
        "        reset_value = np.zeros(self.num_classes, dtype=np.int32)\n",
        "        K.batch_set_value([(v, reset_value) for v in self.variables])\n",
        "\n",
        "    def reset_states(self):\n",
        "        # Backwards compatibility alias of `reset_state`. New classes should\n",
        "        # only implement `reset_state`.\n",
        "        # Required in Tensorflow < 2.5.0\n",
        "        return self.reset_state()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUPsKHta7D7o"
      },
      "source": [
        "\n",
        "class AUC_m(tf.keras.metrics.AUC):\n",
        "  def __init__(self,\n",
        "               num_thresholds=200,\n",
        "               curve='ROC',\n",
        "               summation_method='interpolation',\n",
        "               name=None,\n",
        "               dtype=None,\n",
        "               thresholds=None,\n",
        "               multi_label=False,\n",
        "               num_labels=None,\n",
        "               label_weights=None,\n",
        "               from_logits=False):\n",
        "    super().__init__(\n",
        "               num_thresholds,\n",
        "               curve,\n",
        "               summation_method,\n",
        "               name,\n",
        "               dtype,\n",
        "               thresholds,\n",
        "               multi_label,\n",
        "               num_labels,\n",
        "               label_weights,\n",
        "               from_logits)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.one_hot(y_pred, 3)\n",
        "\n",
        "    return super().update_state(y_true, y_pred, sample_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoYE6159N_zQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab518099-5c96-453b-cdbd-c61b6ca774f7"
      },
      "source": [
        "#@ mlp\n",
        "autodis = True\n",
        "num_y =3\n",
        "feat_size = 411\n",
        "\n",
        "meta_size =10\n",
        "t = 1e-3\n",
        "emb_layer = ['linear', 'dense', 'matrix'][2]\n",
        "\n",
        "mha_layer = 4\n",
        "mha_head = 4\n",
        "mha_residual = True\n",
        "mha_dropout_rate = 0.1\n",
        "\n",
        "em_dropout_rate = 0.1\n",
        "\n",
        "ffn_d = 512 #128\n",
        "pred_ffn_d = 128 #32\n",
        "ffn_drop = 0.1\n",
        "\n",
        "initial_learning_rate = 1e-3\n",
        "decay_steps=10000\n",
        "decay_rate=0.1\n",
        "\n",
        "with strategy.scope():\n",
        "  value = Input(shape=(num_input,), batch_size=batch_size, name='value', dtype=np.float32)\n",
        "  index = Input(shape=(num_input,), batch_size=batch_size, name='index', dtype=np.int32)\n",
        "  emb_value = Input(shape=(num_emb, emb_d, ), batch_size=batch_size, name='emb_value')\n",
        "  emb_index = Input(shape=(num_emb,), batch_size=batch_size, name='emb_index', dtype=np.int32)\n",
        "  if autodis:\n",
        "    meta_index =  Input(shape=(num_input,), batch_size=batch_size, name='meta_index', dtype=np.float32)\n",
        "    reverse_meta_index = Input(shape=(num_input,), batch_size=batch_size, \n",
        "                               name='reverse_meta_index', dtype=np.float32)\n",
        "  #hp_em_dropout_rate = hp.Fixed('EM_dropout_rate', value=0.0)\n",
        "  if autodis:\n",
        "    out = AutoDis_EM(emb_d=emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "            drop_rate=em_dropout_rate, em_em= emb_layer, meta_size=meta_size,\n",
        "            t=t\n",
        "            )(value, index, emb_value, emb_index, meta_index, reverse_meta_index) \n",
        "  else:\n",
        "    out = AutoInt_EM(emb_d=emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "            drop_rate=em_dropout_rate, em_em= emb_layer, \n",
        "          )(value, index, emb_value, emb_index)\n",
        "  # print(\"autoEM\", out.shape)\n",
        "\n",
        "  # ffn_d = hp.Int('hp_ffn_d_size', min_value=128, max_value=512, step=128)\n",
        "  # ffn_drop = hp.Float('hp_ffn_drop', min_value=0.0, max_value=0.9, step=0.3)\n",
        "\n",
        "  #for i in range(hp.Int('num_MHA_layer', min_value=1, max_value=4, step=1, default=1)):\n",
        "  for i in range(mha_layer):\n",
        "    out = FFN(ffn_d, drop_rate=ffn_drop, name=f'ffn_{i}_{ffn_drop}_{ffn_d}')(out)\n",
        "\n",
        "  out = layers.Flatten(name='flatten')(out)\n",
        "\n",
        "  out = layers.LayerNormalization()(out)\n",
        "  out = layers.Dropout(rate=ffn_drop)(out)\n",
        "\n",
        "  out = Dense(pred_ffn_d, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "              #kernel_regularizer=tf.keras.regularizers.l2(), \n",
        "              bias_initializer='random_normal')(out)\n",
        "\n",
        "  out = layers.LayerNormalization()(out)\n",
        "  out = layers.Dropout(rate=ffn_drop)(out)\n",
        "\n",
        "  if num_y == 2:\n",
        "    out = Dense(num_y, activation='relu', kernel_initializer='glorot_normal',\n",
        "                #kernel_regularizer=tf.keras.regularizers.l2(), \n",
        "                bias_initializer='random_normal')(out)\n",
        "    out = layers.Softmax()(out)\n",
        "  else:\n",
        "    out = Dense(num_y, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "                #kernel_regularizer=tf.keras.regularizers.l2(), \n",
        "                bias_initializer='random_normal')(out)\n",
        "    out = layers.Softmax()(out)\n",
        "  optimizer = 'adam'\n",
        "  # print(out.shape)\n",
        "\n",
        "\n",
        "  if autodis:\n",
        "    model = Model(inputs=[value, index, emb_value, emb_index, meta_index, reverse_meta_index], outputs=out, \n",
        "                name=f'autoint_{emb_d}_{emb_layer}_{mha_head}{em_dropout_rate}_{mha_residual}')\n",
        "                \n",
        "  else:\n",
        "    model = Model(inputs=[value, index, emb_value, emb_index], outputs=out, \n",
        "                name=f'autoint_{emb_d}_{emb_layer}_{mha_head}{em_dropout_rate}_{mha_residual}')\n",
        "\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=initial_learning_rate,\n",
        "      decay_steps=decay_steps,\n",
        "      #decay_steps=steps_each,\n",
        "      decay_rate=decay_rate)\n",
        "  weight_decay = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-2,\n",
        "      decay_steps=5000,\n",
        "      #decay_steps=steps_each,\n",
        "      decay_rate=0.1)\n",
        "\n",
        "  #optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "  #optimizer = AdamW(weight_decay=weight_decay , learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-8)\n",
        "  optimizer = RectifiedAdam(lr=1e-3, total_steps=69040, warmup_proportion=0.2, min_lr=1e-5, name='RectifiedAdam', beta_1=0.9, beta_2=0.98, epsilon=1e-8)\n",
        "  #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "\n",
        "  # label_smoothing = hp.Float('label_smoothing', min_value=0.0, max_value=0.60, step=0.3)\n",
        "  #checkpoint = tf.train.Checkpoint(model=model)\n",
        "  #local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost/replica:0/task:0/device:CPU:0\")\n",
        "  #checkpoint.write(\"/content\", options=local_device_option)\n",
        "\n",
        "  model.compile(optimizer, \n",
        "                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing) if num_y == 2 else tf.keras.losses.CategoricalCrossentropy(),\n",
        "          metrics=[\n",
        "                   tf.keras.metrics.CategoricalAccuracy(),\n",
        "          AUC_m(multi_label=True, num_labels=num_y, name=\"auc_m\"), \n",
        "          AUC_m(multi_label=True, num_labels=num_y, curve='PR', name=\"auc_m_PR\"), \n",
        "          MCMdic(num_classes=num_y)\n",
        "\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZMu5mO8Hsu1",
        "outputId": "3f647ce4-a42c-4288-f81a-ccf6f10a698b"
      },
      "source": [
        "#@ autoint\n",
        "autodis = False\n",
        "num_y =3\n",
        "feat_size = 411\n",
        "\n",
        "meta_size =20\n",
        "t = 1e-3\n",
        "emb_layer = ['linear', 'dense', 'matrix'][2]\n",
        "\n",
        "mha_layer = 4\n",
        "mha_head = 4\n",
        "mha_residual = True\n",
        "mha_dropout_rate = 0.1\n",
        "\n",
        "em_dropout_rate = 0.1\n",
        "\n",
        "ffn_d = 256 #512 #128\n",
        "pred_ffn_d = 128 #32\n",
        "ffn_drop = 0.1\n",
        "hp_ffn_boolean = True\n",
        "\n",
        "initial_learning_rate = 1e-3\n",
        "decay_steps=10000\n",
        "decay_rate=0.1\n",
        "\n",
        "with strategy.scope():\n",
        "  value = Input(shape=(num_input,), batch_size=batch_size, name='value', dtype=np.float32)\n",
        "  index = Input(shape=(num_input,), batch_size=batch_size, name='index', dtype=np.int32)\n",
        "  emb_value = Input(shape=(num_emb, emb_d, ), batch_size=batch_size, name='emb_value')\n",
        "  emb_index = Input(shape=(num_emb,), batch_size=batch_size, name='emb_index', dtype=np.int32)\n",
        "  if autodis:\n",
        "    meta_index =  Input(shape=(num_input,), batch_size=batch_size, name='meta_index', dtype=np.float32)\n",
        "    reverse_meta_index = Input(shape=(num_input,), batch_size=batch_size, \n",
        "                               name='reverse_meta_index', dtype=np.float32)\n",
        "  #hp_em_dropout_rate = hp.Fixed('EM_dropout_rate', value=0.0)\n",
        "  if autodis:\n",
        "    out = AutoDis_EM(emb_d=emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "            drop_rate=em_dropout_rate, em_em= emb_layer, meta_size=meta_size,\n",
        "            t=t\n",
        "            )(value, index, emb_value, emb_index, meta_index, reverse_meta_index) \n",
        "  else:\n",
        "    out = AutoInt_EM(emb_d=emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "            drop_rate=em_dropout_rate, em_em= emb_layer, \n",
        "          )(value, index, emb_value, emb_index)\n",
        "  # print(\"autoEM\", out.shape)\n",
        "\n",
        "  # ffn_d = hp.Int('hp_ffn_d_size', min_value=128, max_value=512, step=128)\n",
        "  # ffn_drop = hp.Float('hp_ffn_drop', min_value=0.0, max_value=0.9, step=0.3)\n",
        "\n",
        "  #for i in range(hp.Int('num_MHA_layer', min_value=1, max_value=4, step=1, default=1)):\n",
        "  for i in range(mha_layer):\n",
        "    out = AutoInt_MHS(num_heads=mha_head, has_residual=mha_residual, dropout=mha_dropout_rate, \n",
        "                      name=f'MHA_{i}_{mha_residual}_{mha_dropout_rate}_{mha_head}')(out)\n",
        "    if hp_ffn_boolean:\n",
        "      out = FFN(ffn_d, drop_rate=ffn_drop, name=f'ffn_{i}_{ffn_drop}_{ffn_d}')(out)\n",
        "\n",
        "  out = layers.Flatten(name='flatten')(out)\n",
        "\n",
        "  out = layers.LayerNormalization()(out)\n",
        "  out = layers.Dropout(rate=ffn_drop)(out)\n",
        "\n",
        "  out = Dense(pred_ffn_d, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "              #kernel_regularizer=tf.keras.regularizers.l2(), \n",
        "              bias_initializer='random_normal')(out)\n",
        "\n",
        "  out = layers.LayerNormalization()(out)\n",
        "  out = layers.Dropout(rate=ffn_drop)(out)\n",
        "\n",
        "  if num_y == 2:\n",
        "    out = Dense(num_y, activation='relu', kernel_initializer='glorot_normal',\n",
        "                #kernel_regularizer=tf.keras.regularizers.l2(), \n",
        "                bias_initializer='random_normal')(out)\n",
        "    out = layers.Softmax()(out)\n",
        "  else:\n",
        "    out = Dense(num_y, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "                #kernel_regularizer=tf.keras.regularizers.l2(), \n",
        "                bias_initializer='random_normal')(out)\n",
        "    out = layers.Softmax()(out)\n",
        "  optimizer = 'adam'\n",
        "  # print(out.shape)\n",
        "\n",
        "\n",
        "  if autodis:\n",
        "    model = Model(inputs=[value, index, emb_value, emb_index, meta_index, reverse_meta_index], outputs=out, \n",
        "                name=f'autoint_{emb_d}_{emb_layer}_{mha_head}{em_dropout_rate}_{mha_residual}')\n",
        "                \n",
        "  else:\n",
        "    model = Model(inputs=[value, index, emb_value, emb_index], outputs=out, \n",
        "                name=f'autoint_{emb_d}_{emb_layer}_{mha_head}{em_dropout_rate}_{mha_residual}')\n",
        "\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=initial_learning_rate,\n",
        "      decay_steps=decay_steps,\n",
        "      #decay_steps=steps_each,\n",
        "      decay_rate=decay_rate)\n",
        "  weight_decay = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-2,\n",
        "      decay_steps=5000,\n",
        "      #decay_steps=steps_each,\n",
        "      decay_rate=0.1)\n",
        "\n",
        "  #optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "  #optimizer = AdamW(weight_decay=weight_decay , learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-8)\n",
        "  optimizer = RectifiedAdam(lr=1e-3, total_steps=69040, warmup_proportion=0.2, min_lr=1e-5, name='RectifiedAdam', beta_1=0.9, beta_2=0.98, epsilon=1e-8)\n",
        "  #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "\n",
        "  # label_smoothing = hp.Float('label_smoothing', min_value=0.0, max_value=0.60, step=0.3)\n",
        "  #checkpoint = tf.train.Checkpoint(model=model)\n",
        "  #local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost/replica:0/task:0/device:CPU:0\")\n",
        "  #checkpoint.write(\"/content\", options=local_device_option)\n",
        "\n",
        "  model.compile(optimizer, \n",
        "                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing) if num_y == 2 else tf.keras.losses.CategoricalCrossentropy(),\n",
        "          metrics=[\n",
        "                   tf.keras.metrics.CategoricalAccuracy(),\n",
        "          AUC_m(multi_label=True, num_labels=num_y, name=\"auc_m\"), \n",
        "          AUC_m(multi_label=True, num_labels=num_y, curve='PR', name=\"auc_m_PR\"), \n",
        "          MCMdic(num_classes=num_y)\n",
        "\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "_Y7wt2A33wAI",
        "outputId": "6af5ccf9-da54-4491-b25f-617dac9005bb"
      },
      "source": [
        "#@ dynConv\n",
        "autodis = False\n",
        "num_y =3\n",
        "feat_size = 411\n",
        "\n",
        "meta_size =10\n",
        "t = 1e-3\n",
        "emb_layer = ['linear', 'dense', 'matrix'][1]\n",
        "\n",
        "mha_layer = 4\n",
        "mha_head = 4\n",
        "mha_residual = True\n",
        "mha_dropout_rate = 0.1\n",
        "\n",
        "K_size = 8#hp.Int('hp_kernel_size', min_value=2, max_value=12, step=2, default=4)\n",
        "H_size = 16#hp.Choice('hp_head_num', values=[4,8,16,32]) # remove 1, 32\n",
        "#ffn_d = 256#hp.Int('hp_ffn_d_size', min_value=128, max_value=512, step=128) # min 16 to 32\n",
        "#ffn_drop = hp.Float('hp_ffn_drop', min_value=0.0, max_value=0.9, step=0.3)\n",
        "DConv_droprate = 0.1#hp.Float('hp_DCconv_drop', min_value=0.1, max_value=0.5, step=0.1)\n",
        "\n",
        "em_dropout_rate = 0.1\n",
        "\n",
        "interactive_residual = True\n",
        "\n",
        "ffn_d = 256 #512 #128\n",
        "pred_ffn_d = 128 #32\n",
        "ffn_drop = 0.1\n",
        "hp_ffn_boolean = True\n",
        "\n",
        "initial_learning_rate = 1e-3\n",
        "decay_steps=10000\n",
        "decay_rate=0.1\n",
        "\n",
        "with strategy.scope():\n",
        "  value = Input(shape=(num_input,), batch_size=batch_size, name='value', dtype=np.float32)\n",
        "  index = Input(shape=(num_input,), batch_size=batch_size, name='index', dtype=np.int32)\n",
        "  emb_value = Input(shape=(num_emb, emb_d, ), batch_size=batch_size, name='emb_value')\n",
        "  emb_index = Input(shape=(num_emb,), batch_size=batch_size, name='emb_index', dtype=np.int32)\n",
        "  if autodis:\n",
        "    meta_index =  Input(shape=(num_input,), batch_size=batch_size, name='meta_index', dtype=np.float32)\n",
        "    reverse_meta_index = Input(shape=(num_input,), batch_size=batch_size, \n",
        "                               name='reverse_meta_index', dtype=np.float32)\n",
        "  #hp_em_dropout_rate = hp.Fixed('EM_dropout_rate', value=0.0)\n",
        "  if autodis:\n",
        "    out = AutoDis_EM(emb_d=emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "            drop_rate=em_dropout_rate, em_em= emb_layer, meta_size=meta_size,\n",
        "            t=t\n",
        "            )(value, index, emb_value, emb_index, meta_index, reverse_meta_index) \n",
        "  else:\n",
        "    out = AutoInt_EM(emb_d=emb_d, feat_size=feat_size, num_emb=num_emb, \n",
        "            drop_rate=em_dropout_rate, em_em= emb_layer, \n",
        "          )(value, index, emb_value, emb_index)\n",
        "  # print(\"autoEM\", out.shape)\n",
        "\n",
        "  # ffn_d = hp.Int('hp_ffn_d_size', min_value=128, max_value=512, step=128)\n",
        "  # ffn_drop = hp.Float('hp_ffn_drop', min_value=0.0, max_value=0.9, step=0.3)\n",
        "\n",
        "  #for i in range(hp.Int('num_MHA_layer', min_value=1, max_value=4, step=1, default=1)):\n",
        "  for i in range(mha_layer):\n",
        "    out = DynamicConvLayer(kernel_num=K_size, head_num=H_size, dropout_rate=DConv_droprate, \n",
        "                                 interactive_residual=interactive_residual, name=f'DynamicConv_{i}')(out)\n",
        "    if hp_ffn_boolean:\n",
        "      out = FFN(ffn_d, drop_rate=ffn_drop, name=f'ffn_{i}_{ffn_drop}_{ffn_d}')(out)\n",
        "\n",
        "  out = layers.Flatten(name='flatten')(out)\n",
        "\n",
        "  out = layers.LayerNormalization()(out)\n",
        "  out = layers.Dropout(rate=ffn_drop)(out)\n",
        "\n",
        "  out = Dense(pred_ffn_d, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(), \n",
        "              bias_initializer='random_normal')(out)\n",
        "\n",
        "  out = layers.LayerNormalization()(out)\n",
        "  out = layers.Dropout(rate=ffn_drop)(out)\n",
        "\n",
        "  if num_y == 2:\n",
        "    out = Dense(num_y, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(), bias_initializer='random_normal')(out)\n",
        "    out = layers.Softmax()(out)\n",
        "  else:\n",
        "    out = Dense(num_y, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(), bias_initializer='random_normal')(out)\n",
        "    out = layers.Softmax()(out)\n",
        "  #optimizer = 'adam'\n",
        "  # print(out.shape)\n",
        "\n",
        "\n",
        "  if autodis:\n",
        "    model = Model(inputs=[value, index, emb_value, emb_index, meta_index, reverse_meta_index], outputs=out, \n",
        "                name=f'autoint_{emb_d}_{emb_layer}_{mha_head}{em_dropout_rate}_{mha_residual}')\n",
        "                \n",
        "  else:\n",
        "    model = Model(inputs=[value, index, emb_value, emb_index], outputs=out, \n",
        "                name=f'autoint_{emb_d}_{emb_layer}_{mha_head}{em_dropout_rate}_{mha_residual}')\n",
        "\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=initial_learning_rate,\n",
        "      decay_steps=decay_steps,\n",
        "      #decay_steps=steps_each,\n",
        "      decay_rate=decay_rate)\n",
        "  weight_decay = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-1,\n",
        "      decay_steps=5000,\n",
        "      #decay_steps=steps_each,\n",
        "      decay_rate=0.2)\n",
        "\n",
        "  #optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "  #optimizer = AdamW(weight_decay=weight_decay , learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-8)\n",
        "  optimizer = RectifiedAdam(lr=1e-3, total_steps=69040, warmup_proportion=0.2, min_lr=1e-5, name='RectifiedAdam', beta_1=0.9, beta_2=0.98, epsilon=1e-8)\n",
        "  #optimizer = RAdamOptimizer(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5, name='RectifiedAdam')\n",
        "\n",
        "  # label_smoothing = hp.Float('label_smoothing', min_value=0.0, max_value=0.60, step=0.3)\n",
        "  #checkpoint = tf.train.Checkpoint(model=model)\n",
        "  #local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost/replica:0/task:0/device:CPU:0\")\n",
        "  #checkpoint.write(\"/content\", options=local_device_option)\n",
        "\n",
        "  model.compile(optimizer, \n",
        "                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing) if num_y == 2 else tf.keras.losses.CategoricalCrossentropy(),\n",
        "          metrics=[\n",
        "                   tf.keras.metrics.CategoricalAccuracy(),\n",
        "          AUC_m(multi_label=True, num_labels=num_y, name=\"auc_m\"), \n",
        "          AUC_m(multi_label=True, num_labels=num_y, curve='PR', name=\"auc_m_PR\"), \n",
        "          MCMdic(num_classes=num_y)\n",
        "\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-f345b22dec27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             )(value, index, emb_value, emb_index, meta_index, reverse_meta_index) \n\u001b[1;32m     49\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     out = AutoInt_EM(emb_d=emb_d, feat_size=feat_size, num_emb=num_emb, \n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mdrop_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mem_dropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem_em\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0memb_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           )(value, index, emb_value, emb_index)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoInt_EM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok9TMbYvdG7H"
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='/content',\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        options=tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "skVyUOL2_3x5",
        "outputId": "444de7e3-703b-4537-a540-10dd8c90d9ee"
      },
      "source": [
        "vali = model.fit(train_ds, epochs=10, validation_data=test_ds, validation_batch_size=batch_size,\n",
        "                 callbacks = [ ClearTrainingOutput(), checkpoint,\n",
        "                              EarlyStopping(monitor='val_auc_m', min_delta=0.01, \n",
        "                                                                patience=2, baseline=0.99),\n",
        "                          ], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "   4/7552 [..............................] - ETA: 2:48 - loss: 55.3770 - categorical_accuracy: 0.0840 - auc_m: 0.5083 - auc_m_PR: 0.3381 - confusion_matrix: 128.0000   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 1.8709s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 1.8709s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 133/7552 [..............................] - ETA: 2:28 - loss: 54.4745 - categorical_accuracy: 0.5173 - auc_m: 0.4847 - auc_m_PR: 0.3316 - confusion_matrix: 4256.0000"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-f05916eb703c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                               EarlyStopping(monitor='val_auc_m', min_delta=0.01, \n\u001b[1;32m      4\u001b[0m                                                                 patience=2, baseline=0.99),\n\u001b[0;32m----> 5\u001b[0;31m                           ], shuffle=True)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1186\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \"\"\"\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    513\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr1Z9whRbJp5",
        "outputId": "92cee0e1-fddf-4391-f3f3-240b09f1de5b"
      },
      "source": [
        "cm = tf.cast(vali.history['val_confusion_matrix'][1], tf.int32)\n",
        "cm\n",
        "#print(tf.reduce_sum(cm, axis=-1))\n",
        "tf.reshape(tf.transpose(cm), shape=(-1, 2, 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\n",
              "array([[174693, 212931,  44310],\n",
              "       [  9922,      0,      0],\n",
              "       [     0,   7997,   1925],\n",
              "       [ 36313,      0, 174693]], dtype=int32)>"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBdPHuZcfNwP",
        "outputId": "43dd69b4-88da-44dd-e3ad-89a444c8c5d5"
      },
      "source": [
        "(0.9965 + 0.987878 + 0.9912) / 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9918593333333333"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE40ylrip2CC"
      },
      "source": [
        "model.get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSaUjpWqmLSv"
      },
      "source": [
        "loaded = tf.keras.models.load_model('/content/autoint')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnf_dyVzX6Gb"
      },
      "source": [
        "def get_confusion_matrix_ops(self, predictions, labels, num_classes, unoccupied_class):\n",
        "        \"\"\" Get ops for maintaining a confusion matrix during training.\n",
        "\n",
        "            Args:\n",
        "            predictions: tf.tensor\n",
        "            labels: tf.tensor\n",
        "            num_classes: int\n",
        "            unoccupied_class: int, id of unoccupied class\n",
        "\n",
        "            Returns: tf.tensor, tf.tensor, tf.tensor\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        labels = tf.reshape(labels, [-1])\n",
        "        predictions_argmax = tf.reshape(tf.argmax(predictions, axis=2), [-1])\n",
        "        batch_confusion = tf.confusion_matrix(labels, predictions_argmax, num_classes=num_classes, name='batch_confusion')\n",
        "        confusion = tf.Variable( tf.zeros([num_classes, num_classes], dtype=tf.int32 ), name='confusion' )\n",
        "        confusion_update = confusion.assign( confusion + batch_confusion )\n",
        "        confusion_clear = confusion.assign(tf.zeros([num_classes, num_classes], dtype=tf.int32))\n",
        "\n",
        "        return confusion, confusion_update, confusion_clear "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "gQpa1ZWDdSlG",
        "outputId": "5acadb73-b6dd-4e0f-c147-92e0f85fa645"
      },
      "source": [
        "tf.constant([1,2]) == tf.constant([1,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e61f2c18fb6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA08REMEYcvA"
      },
      "source": [
        "class ConfusionMatrix(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name=\"CMatrix\", **kwargs):\n",
        "    super(ConfusionMatrix, self).__init__(name=name, **kwargs)\n",
        "    self.true_positives = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
        "    self.false_positives = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
        "    self.true_negatives = self.add_weight(name=\"tn\", initializer=\"zeros\")\n",
        "    self.false_negatives = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_true = tf.cast(y_true, tf.int32)\n",
        "    y_pred = tf.cast(y_pred, tf.int32)\n",
        "    result = y_true == y_pred\n",
        "    ans_indx = tf.argmax(y_true).numpy()\n",
        "    if result.numpy()[ans_indx]:\n",
        "      ins = [0,0,0]\n",
        "      ins[ans_indx] = 1\n",
        "      self.true_positives.assign_add(ins])\n",
        "    else:\n",
        "      ins = [0,0,0]\n",
        "      ins[ans_indx] = 1\n",
        "      self.true_positives.assign_add(ins])\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lehSA98f437h"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVoclafhdwP3"
      },
      "source": [
        "result = model.predict(test_ds.take(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozED8cBoeciS",
        "outputId": "2be41d06-2ca0-4501-d4f3-a078654e220c"
      },
      "source": [
        "firs = []\n",
        "for res in test_ds.take(1):\n",
        "  print(res[-1][0])\n",
        "  print(res[-1].numpy()[1])\n",
        "  res = res[-1].numpy()\n",
        "  print(type(res[0][0]), res[0])\n",
        "\n",
        "  for i in range(len(res)):\n",
        "    if res[i][0] == np.float32(1):\n",
        "      print(res[i], i)\n",
        "      firs.append(i)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)\n",
            "[0. 0. 1.]\n",
            "<class 'numpy.float32'> [0. 1. 0.]\n",
            "[1. 0. 0.] 16\n",
            "[1. 0. 0.] 26\n",
            "[1. 0. 0.] 31\n",
            "[1. 0. 0.] 32\n",
            "[1. 0. 0.] 35\n",
            "[1. 0. 0.] 36\n",
            "[1. 0. 0.] 42\n",
            "[1. 0. 0.] 48\n",
            "[1. 0. 0.] 52\n",
            "[1. 0. 0.] 54\n",
            "[1. 0. 0.] 66\n",
            "[1. 0. 0.] 70\n",
            "[1. 0. 0.] 74\n",
            "[1. 0. 0.] 87\n",
            "[1. 0. 0.] 88\n",
            "[1. 0. 0.] 93\n",
            "[1. 0. 0.] 105\n",
            "[1. 0. 0.] 108\n",
            "[1. 0. 0.] 115\n",
            "[1. 0. 0.] 117\n",
            "[1. 0. 0.] 118\n",
            "[1. 0. 0.] 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPwEvZ7Kv259"
      },
      "source": [
        "def confusionM(pred, data):\n",
        "  count =0\n",
        "  for i, j in zip(pred, data):\n",
        "    ans = tf.math.argmax(i)\n",
        "    if j.numpy()[int(ans)]:\n",
        "      count +=1\n",
        "\n",
        "    #print(j.numpy(), int(ans))\n",
        "    #for j in i:\n",
        "  print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAGxTTiezhjG"
      },
      "source": [
        "confusionM(result, list(test_ds.take(1))[0][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMK4u6cQAaYy",
        "outputId": "a3c0bfea-c10e-4a71-fa1b-00c850b422f3"
      },
      "source": [
        "ev = model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2158/2158 [==============================] - 38s 17ms/step - loss: 0.0023 - categorical_accuracy: 1.0000 - auc_15: 0.9999 - auc_m: 0.9999 - confusion_matrix: 69056.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjNj6r-fLKPY",
        "outputId": "e3ab8012-f366-43ab-fca7-03c3b5817a7d"
      },
      "source": [
        "ev # autodis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.002268890617415309,\n",
              " 0.9999529719352722,\n",
              " 0.9999264478683472,\n",
              " 0.9999264478683472,\n",
              " array([[2.20877e+05, 2.68529e+05, 6.30290e+04],\n",
              "        [0.00000e+00, 0.00000e+00, 1.30000e+01],\n",
              "        [1.30000e+01, 0.00000e+00, 0.00000e+00],\n",
              "        [5.53340e+04, 7.69500e+03, 2.13182e+05]], dtype=float32)]"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jROox60Cd_Su",
        "outputId": "a543cae5-944d-49e2-b1e9-23e2cd5229d2"
      },
      "source": [
        "pred = model.predict(test_ds.take(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "value (16, 64), value_emb (16, 2, 128)\n",
            "emb out:(16, 64, 128), vec emb:(16, 2, 128)\n",
            "autoint d (16, 2, 128) Tensor(\"em_dnese/BiasAdd:0\", shape=(16, 2, 128), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdxJi6ZRBAHA"
      },
      "source": [
        "list(test_ds.take(1))[0][-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kujngQiLP3Ox",
        "outputId": "7053b784-d695-4f30-9731-6ffebb5007bd"
      },
      "source": [
        "tf.math.count_nonzero(tf.cast(tf.equal(tf.compat.v1.argmax(list(test_ds.take(1))[0][-1].numpy(), axis=-1), tf.compat.v1.argmax(pred, axis=-1)), tf.int32), 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=111>"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-AWldS6xw0u"
      },
      "source": [
        "def split_datasets(dataset):\n",
        "    tensors = {}\n",
        "    names = list(dataset.element_spec.keys())\n",
        "    for name in names:\n",
        "        tensors[name] = dataset.map(lambda x: x[name])\n",
        "\n",
        "    return tensors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NLhgkIYyoB4",
        "outputId": "1194b3bf-2187-4d80-f283-72dcac5ab3bb"
      },
      "source": [
        "test_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'emb_index': TensorSpec(shape=(128, 2, 1), dtype=tf.int64, name=None),\n",
              "  'emb_value': TensorSpec(shape=(128, 2, 128), dtype=tf.float64, name=None),\n",
              "  'index': TensorSpec(shape=(128, 64, 1), dtype=tf.int64, name=None),\n",
              "  'value': TensorSpec(shape=(128, 64, 1), dtype=tf.float64, name=None)},\n",
              " TensorSpec(shape=(128, 3), dtype=tf.float32, name=None))"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zQyoiT6FH9-"
      },
      "source": [
        "pd.DataFrame(list(test_ds.unbatch())[0][-1].numpy()).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azJLazv3Zkzu",
        "outputId": "d0fe0a4e-25ca-4a77-c569-8022718abd4f"
      },
      "source": [
        "result[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[220877.,      0.],\n",
              "        [ 55347.,      0.]],\n",
              "\n",
              "       [[268529.,      0.],\n",
              "        [  7695.,      0.]],\n",
              "\n",
              "       [[ 63042.,      0.],\n",
              "        [213182.,      0.]]], dtype=float32)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvevVleRq9UP"
      },
      "source": [
        "!tar -xzf /content/autoint_model_l2_TPU.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOQ60GDAMYKq"
      },
      "source": [
        "del tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OwGlRyvhqIB"
      },
      "source": [
        "!rm -rf /content/autoint_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei5EcusjNyIy"
      },
      "source": [
        "!tar -zxvf /content/autoint_tk_2target_rm_issue_initial_stat.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMZZuFMnFH9D"
      },
      "source": [
        "mm = Autoint_builder_maker(feat_size=feat_size ,num_input=num_input, \n",
        "                          num_emb=num_emb, emb_d=emb_d, \n",
        "                          batch_size=batch_size, autodis=False, num_y=df_y_2.shape[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juwfOn_iFKMA",
        "outputId": "10fd8fa9-54e5-44d9-ad04-7e7bf4901ee1"
      },
      "source": [
        "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf8WyR4OsSZt",
        "outputId": "7db4f6c0-d8b7-4aa4-a08b-4c9e4bf23bbe"
      },
      "source": [
        "#@title keras tuner autoint\n",
        "tuner = kt.tuners.bayesian.BayesianOptimization(\n",
        "#tuner = kt.Hyperband(\n",
        "    Autoint_builder_maker(feat_size=feat_size ,num_input=num_input, \n",
        "                          num_emb=num_emb, emb_d=emb_d, \n",
        "                          batch_size=batch_size, autodis=False, num_y=df_y_2.shape[-1]),\n",
        "    #objective=kt.Objective('val_binary_accuracy', direction='max'),\n",
        "    objective=kt.Objective('val_auc', direction='max'),\n",
        "    #max_epochs=30,\n",
        "    #factor=3,\n",
        "    #hyperband_iterations=3,\n",
        "    #tune_new_entries=True,\n",
        "    max_trials=10,\n",
        "    directory = '/content/content',\n",
        "    distribution_strategy=strategy,\n",
        "    project_name='autoint', overwrite=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project /content/content/autoint/oracle.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project /content/content/autoint/oracle.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "linear 128\n",
            "value (16, 64), value_emb (16, 2, 128)\n",
            "emb out:(16, 64, 128), vec emb:(16, 2, 128)\n",
            "autoint l (16, 2, 128) Tensor(\"value_emb:0\", shape=(16, 2, 128), dtype=float32)\n",
            "value (16, 64), value_emb (16, 2, 128)\n",
            "emb out:(16, 64, 128), vec emb:(16, 2, 128)\n",
            "autoint l (16, 2, 128) Tensor(\"value_emb:0\", shape=(16, 2, 128), dtype=float32)\n",
            "autoEM (16, 66, 128)\n",
            "(16, 3)\n",
            "model compiled\n",
            "INFO:tensorflow:Reloading Tuner from /content/content/autoint/tuner0.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Tuner from /content/content/autoint/tuner0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3OxXsNtat-O",
        "outputId": "e7ad17c9-cfdb-46e2-865e-4124a6428f71"
      },
      "source": [
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tpu.cluster_spec().as_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'worker': ['10.114.126.18:8470']}"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtHX3hVny-er"
      },
      "source": [
        "!tar -zxf \"/content/autoint_kt (1).tar.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAcYd2Vdt9XU",
        "outputId": "764f8d80-03cb-4c04-fc39-0440659609eb"
      },
      "source": [
        "#@title keras tuner autoint autodis\n",
        "tuner = kt.tuners.bayesian.BayesianOptimization(\n",
        "#tuner = kt.Hyperband(\n",
        "    Autoint_builder_maker(feat_size=feat_size ,num_input=num_input, \n",
        "                          num_emb=num_emb, emb_d=emb_d, \n",
        "                          batch_size=batch_size, autodis=True, num_y=df_y_2.shape[-1]),\n",
        "    #objective=kt.Objective('val_binary_accuracy', direction='max'),\n",
        "    objective=kt.Objective('val_auc', direction='max'),\n",
        "    #max_epochs=30,\n",
        "    #factor=3,\n",
        "    #hyperband_iterations=3,\n",
        "    #tune_new_entries=True,\n",
        "    max_trials=10,\n",
        "    directory = '/content',\n",
        "    distribution_strategy=strategy,\n",
        "    project_name='autoint', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "autodis emb_out (16, 2, 128) emb_weight (410, 128) index_emb (16, 2)\n",
            "(16, 3)\n",
            "model compiled\n",
            "autodis emb_out (16, 2, 32) emb_weight (410, 32) index_emb (16, 2)\n",
            "Tensor(\"auto_dis_em/embedding_lookup_1/Identity_1:0\", shape=(16, 2, 32), dtype=float32) Tensor(\"Placeholder_2:0\", shape=(16, 2, 128), dtype=float32)\n",
            "(16, 3)\n",
            "model compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po2jvp20HnFR"
      },
      "source": [
        "#@title keras tuner Dynamic Cocv\n",
        "\n",
        "\n",
        "tuner = kt.tuners.bayesian.BayesianOptimization(\n",
        "#tuner = kt.Hyperband(\n",
        "    DynamicConvHyper(feat_size=feat_size ,num_input=num_input, num_y=at_t_y.shape[-1],\n",
        "                          num_emb=num_emb, emb_d=emb_d, batch_size=batch_size,\n",
        "                     autodis=False),\n",
        "    objective=kt.Objective('val_auc', direction='max'),\n",
        "    #max_epochs=30,\n",
        "    #factor=3,\n",
        "    #hyperband_iterations=3,\n",
        "    max_trials=10,\n",
        "    directory = '/content',\n",
        "    distribution_strategy=strategy,\n",
        "    project_name='DyConv', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTBIAikWhBWA",
        "outputId": "b7efdf14-0237-4242-e5fb-3a8bd436a744"
      },
      "source": [
        "#@title keras tuner MLP\n",
        "tuner = kt.tuners.bayesian.BayesianOptimization(\n",
        "#tuner = kt.Hyperband(\n",
        "    MLP_builder_maker(feat_size=feat_size ,num_input=num_input, \n",
        "                      num_emb=num_emb, emb_d=emb_d, batch_size=batch_size, \n",
        "                      autodis=True, num_y=at_t_y.shape[-1]),\n",
        "    objective=kt.Objective('val_auc', direction='max'),\n",
        "    #max_epochs=30,\n",
        "    #factor=3,\n",
        "    #hyperband_iterations=3,\n",
        "    max_trials=10,\n",
        "    directory = '/content',\n",
        "    distribution_strategy=strategy,\n",
        "    project_name='ffn_embed_l2', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "autodis (16, 2, 128) (410, 128) (16, 2)\n",
            "model compiled\n",
            "autodis (16, 2, 32) (410, 32) (16, 2)\n",
            "Tensor(\"auto_dis_em/embedding_lookup_1/Identity_1:0\", shape=(16, 2, 32), dtype=float32) Tensor(\"Placeholder_2:0\", shape=(16, 2, 128), dtype=float32)\n",
            "model compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w_0qVAKhM0u"
      },
      "source": [
        "!rm -rf /content/MLP_EMB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbtJQbzfXoDz"
      },
      "source": [
        "!tar -C /content -zxvf /content/autoint_kt.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEsH29DqWyDi",
        "outputId": "8e053553-7905-4322-e92c-dc227bee3f31"
      },
      "source": [
        "!tar -zcvf /content/dyconv_tk_2target_rm_issue_initial_stat.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: Cowardly refusing to create an empty archive\n",
            "Try 'tar --help' or 'tar --usage' for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_LUE-EE9wV_"
      },
      "source": [
        "import IPython\n",
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
        "  def on_train_end(*args, **kwargs):\n",
        "    IPython.display.clear_output(wait = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUvyoZ0FPfYd"
      },
      "source": [
        "import random\n",
        "def set_seed(seed=100):\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # optional\n",
        "    # for numpy.random\n",
        "    np.random.seed(seed)\n",
        "    # for built-in random\n",
        "    random.seed(seed)\n",
        "    # for hash seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-HORAaBFca_"
      },
      "source": [
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='/content',\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        options=tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzG_-PyLOhHv"
      },
      "source": [
        "#@title search"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnHkP-1LFsKW"
      },
      "source": [
        "set_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2CTLsU3ePJ6"
      },
      "source": [
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uUzfM3Dd6Lr",
        "outputId": "32b72a83-cb19-41c0-a6e3-212ce2b04720"
      },
      "source": [
        "tf.compat.v1.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KuutWd3JZdH"
      },
      "source": [
        "#tuner.search([at_t_v, at_t_i, emb_at_v, emb_at_i], at_t_y, epochs = 30, validation_split=0.2, callbacks = [ClearTrainingOutput(), EarlyStopping(monitor='val_loss', patience=5)])\n",
        "tuner.search(train_ds, epochs=10, validation_data=vali_ds, #validation_batch_size=batch_size, \n",
        "             callbacks = [ ClearTrainingOutput(), EarlyStopping(monitor='val_auc', min_delta=0.01, \n",
        "                                                                patience=2, baseline=0.99),\n",
        "                          ], shuffle=True)\n",
        " \n",
        "# Get the optimal hyperparamete\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkxOJZb9KfBO"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 2)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjgX7orKquOX",
        "outputId": "faaf9b6f-f31f-4136-d928-d7c58b0271d9"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 4)[3]\n",
        "model = tuner.hypermodel.build(best_hps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FWrE6Rwuw-88",
        "outputId": "dfae1e11-9700-40f4-8725-478c262cf323"
      },
      "source": [
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.fit(train_ds, validation_data=vali_ds, validation_batch_size=batch_size, batch_size=batch_size, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None 32\n",
            "value (16, 64), value_emb (16, 2, 128)\n",
            "emb out:(16, 64, 32), vec emb:(16, 2, 32)\n",
            "value (16, 64), value_emb (16, 2, 128)\n",
            "emb out:(16, 64, 32), vec emb:(16, 2, 32)\n",
            "autoEM (16, 66, 32)\n",
            "(16, 3)\n",
            "model compiled\n",
            "Epoch 1/100\n",
            "value (16, 64), value_emb (16, 2, 128)\n",
            "emb out:(16, 64, 32), vec emb:(16, 2, 32)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"Adam/gradients/PartitionedCall_1:2\", shape=(1056,), dtype=int32), values=Tensor(\"Adam/gradients/PartitionedCall_1:1\", shape=(1056, 32), dtype=float32), dense_shape=Tensor(\"Adam/gradients/PartitionedCall_1:3\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "value (16, 64), value_emb (16, 2, 128)\n",
            "emb out:(16, 64, 32), vec emb:(16, 2, 32)\n",
            "2158/2158 [==============================] - ETA: 0s - loss: 0.8622 - categorical_accuracy: 0.7936 - auc: 0.7838value (16, 64), value_emb (16, 2, 128)\n",
            "emb out:(16, 64, 32), vec emb:(16, 2, 32)\n",
            "2158/2158 [==============================] - 67s 28ms/step - loss: 0.8621 - categorical_accuracy: 0.7936 - auc: 0.7838 - val_loss: 0.7993 - val_categorical_accuracy: 0.7538 - val_auc: 0.7637\n",
            "Epoch 2/100\n",
            "2158/2158 [==============================] - 60s 28ms/step - loss: 0.6256 - categorical_accuracy: 0.7934 - auc: 0.8220 - val_loss: 0.7785 - val_categorical_accuracy: 0.7538 - val_auc: 0.7663\n",
            "Epoch 3/100\n",
            "2158/2158 [==============================] - 60s 28ms/step - loss: 0.5935 - categorical_accuracy: 0.7937 - auc: 0.8291 - val_loss: 0.7175 - val_categorical_accuracy: 0.7538 - val_auc: 0.7817\n",
            "Epoch 4/100\n",
            "2158/2158 [==============================] - 61s 28ms/step - loss: 0.5827 - categorical_accuracy: 0.7939 - auc: 0.8321 - val_loss: 0.7013 - val_categorical_accuracy: 0.7540 - val_auc: 0.7864\n",
            "Epoch 5/100\n",
            "2158/2158 [==============================] - 60s 28ms/step - loss: 0.5766 - categorical_accuracy: 0.7942 - auc: 0.8339 - val_loss: 0.6968 - val_categorical_accuracy: 0.7459 - val_auc: 0.7866\n",
            "Epoch 6/100\n",
            "2158/2158 [==============================] - 60s 28ms/step - loss: 0.5728 - categorical_accuracy: 0.7939 - auc: 0.8350 - val_loss: 0.6922 - val_categorical_accuracy: 0.7511 - val_auc: 0.7883\n",
            "Epoch 7/100\n",
            "2158/2158 [==============================] - 60s 28ms/step - loss: 0.5696 - categorical_accuracy: 0.7941 - auc: 0.8365 - val_loss: 0.6958 - val_categorical_accuracy: 0.7486 - val_auc: 0.7882\n",
            "Epoch 8/100\n",
            "2158/2158 [==============================] - 60s 28ms/step - loss: 0.5680 - categorical_accuracy: 0.7939 - auc: 0.8367 - val_loss: 0.6934 - val_categorical_accuracy: 0.7517 - val_auc: 0.7892\n",
            "Epoch 9/100\n",
            "2158/2158 [==============================] - 60s 28ms/step - loss: 0.5667 - categorical_accuracy: 0.7943 - auc: 0.8369 - val_loss: 0.6923 - val_categorical_accuracy: 0.7529 - val_auc: 0.7895\n",
            "Epoch 10/100\n",
            "2158/2158 [==============================] - 63s 29ms/step - loss: 0.5653 - categorical_accuracy: 0.7942 - auc: 0.8375 - val_loss: 0.6876 - val_categorical_accuracy: 0.7530 - val_auc: 0.7893\n",
            "Epoch 11/100\n",
            "2158/2158 [==============================] - 68s 32ms/step - loss: 0.5645 - categorical_accuracy: 0.7946 - auc: 0.8380 - val_loss: 0.6859 - val_categorical_accuracy: 0.7543 - val_auc: 0.7896\n",
            "Epoch 12/100\n",
            "2158/2158 [==============================] - 67s 31ms/step - loss: 0.5634 - categorical_accuracy: 0.7946 - auc: 0.8387 - val_loss: 0.6873 - val_categorical_accuracy: 0.7542 - val_auc: 0.7895\n",
            "Epoch 13/100\n",
            "2158/2158 [==============================] - 68s 31ms/step - loss: 0.5629 - categorical_accuracy: 0.7947 - auc: 0.8391 - val_loss: 0.6865 - val_categorical_accuracy: 0.7544 - val_auc: 0.7896\n",
            "Epoch 14/100\n",
            "2158/2158 [==============================] - 68s 31ms/step - loss: 0.5626 - categorical_accuracy: 0.7946 - auc: 0.8388 - val_loss: 0.6857 - val_categorical_accuracy: 0.7544 - val_auc: 0.7899\n",
            "Epoch 15/100\n",
            "2158/2158 [==============================] - 68s 31ms/step - loss: 0.5625 - categorical_accuracy: 0.7947 - auc: 0.8385 - val_loss: 0.6868 - val_categorical_accuracy: 0.7542 - val_auc: 0.7904\n",
            "Epoch 16/100\n",
            "2158/2158 [==============================] - 68s 32ms/step - loss: 0.5622 - categorical_accuracy: 0.7945 - auc: 0.8391 - val_loss: 0.6863 - val_categorical_accuracy: 0.7542 - val_auc: 0.7904\n",
            "Epoch 17/100\n",
            "2158/2158 [==============================] - 68s 31ms/step - loss: 0.5618 - categorical_accuracy: 0.7945 - auc: 0.8390 - val_loss: 0.6857 - val_categorical_accuracy: 0.7545 - val_auc: 0.7904\n",
            "Epoch 18/100\n",
            "2158/2158 [==============================] - 69s 32ms/step - loss: 0.5617 - categorical_accuracy: 0.7944 - auc: 0.8391 - val_loss: 0.6857 - val_categorical_accuracy: 0.7544 - val_auc: 0.7905\n",
            "Epoch 19/100\n",
            "2158/2158 [==============================] - 67s 31ms/step - loss: 0.5615 - categorical_accuracy: 0.7948 - auc: 0.8392 - val_loss: 0.6854 - val_categorical_accuracy: 0.7544 - val_auc: 0.7903\n",
            "Epoch 20/100\n",
            "2158/2158 [==============================] - 68s 31ms/step - loss: 0.5614 - categorical_accuracy: 0.7946 - auc: 0.8392 - val_loss: 0.6852 - val_categorical_accuracy: 0.7546 - val_auc: 0.7903\n",
            "Epoch 21/100\n",
            "2158/2158 [==============================] - 65s 30ms/step - loss: 0.5610 - categorical_accuracy: 0.7947 - auc: 0.8395 - val_loss: 0.6857 - val_categorical_accuracy: 0.7545 - val_auc: 0.7903\n",
            "Epoch 22/100\n",
            "2158/2158 [==============================] - 66s 31ms/step - loss: 0.5609 - categorical_accuracy: 0.7946 - auc: 0.8399 - val_loss: 0.6856 - val_categorical_accuracy: 0.7545 - val_auc: 0.7902\n",
            "Epoch 23/100\n",
            "2157/2158 [============================>.] - ETA: 0s - loss: 0.5611 - categorical_accuracy: 0.7945 - auc: 0.8394"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-ba4af10670b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_hps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvali_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m       data_handler = data_adapter.get_data_handler(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1392\u001b[0m           should not be specified (since targets will be obtained from the\n\u001b[1;32m   1393\u001b[0m           iterator/dataset).\n\u001b[0;32m-> 1394\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInteger\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mNumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0msamples\u001b[0m \u001b[0mper\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m           \u001b[0mcomputation\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0munspecified\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mto\u001b[0m \u001b[0;36m32.\u001b[0m \u001b[0mDo\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m           \u001b[0mspecify\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0myour\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mform\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInteger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mwithin\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mAggregated\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0mresults\u001b[0m \u001b[0mup\u001b[0m \u001b[0muntil\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m    477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_test_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         logging.warning(warning_msg.format(\n\u001b[1;32m    358\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_hook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mbatch_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavg_batch_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             hook_time=avg_end_hook_time))\n\u001b[1;32m    361\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m   \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m     \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m   \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m   \u001b[0mThis\u001b[0m \u001b[0mcorrectly\u001b[0m \u001b[0mrepacks\u001b[0m \u001b[0mdicts\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0ms\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mbeen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   \u001b[0mflattened\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0malso\u001b[0m \u001b[0mallows\u001b[0m \u001b[0mflattening\u001b[0m \u001b[0man\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mrepacking\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m   \u001b[0mback\u001b[0m \u001b[0musing\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mplain\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvice\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m   \u001b[0mDictionaries\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mflattened\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m   \u001b[0mThis\u001b[0m \u001b[0mcorrectly\u001b[0m \u001b[0mrepacks\u001b[0m \u001b[0mdicts\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0ms\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mbeen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   \u001b[0mflattened\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0malso\u001b[0m \u001b[0mallows\u001b[0m \u001b[0mflattening\u001b[0m \u001b[0man\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mrepacking\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m   \u001b[0mback\u001b[0m \u001b[0musing\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mplain\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvice\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m   \u001b[0mDictionaries\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mflattened\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m   \u001b[0mAsync\u001b[0m \u001b[0mstrategies\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msuch\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTPUStrategy\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mParameterServerStrategy\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m   \u001b[0mforced\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m   \u001b[0msync\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mUnlike\u001b[0m \u001b[0mNumPy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensors\u001b[0m \u001b[0mare\u001b[0m \u001b[0mimmutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mso\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_custom_summarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m       \u001b[0mvalue_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"value=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summarize_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m       \u001b[0mvalue_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"numpy=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumpy_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_repr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     return \"<tf.Tensor: shape=%s, dtype=%s, %s>\" % (self.shape, self.dtype.name,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "zJyBi0YN3GIC",
        "outputId": "69502c87-7b30-4947-d7b2-9b94ff18e794"
      },
      "source": [
        "model = tuner.get_best_models()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None 64\n",
            "value (16, 64), value_emb (16, 2, 128)\n",
            "emb out:(16, 64, 64), vec emb:(16, 2, 64)\n",
            "value (16, 64), value_emb (16, 2, 128)\n",
            "emb out:(16, 64, 64), vec emb:(16, 2, 64)\n",
            "autoEM (16, 66, 64)\n",
            "(16, 3)\n",
            "model compiled\n"
          ]
        },
        {
          "ename": "NotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \"\"\"\n\u001b[0;32m---> 95\u001b[0;31m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /content/content/autoint/trial_d2831994c5e44b8df630246d6e0b3530/checkpoints/epoch_0/checkpoint",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-abc9b71fb66c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mget_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \"\"\"\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m# Method only exists in this class for the docstring override.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36mget_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \"\"\"\n\u001b[1;32m    286\u001b[0m         \u001b[0mbest_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_trials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \"\"\"\n\u001b[1;32m    286\u001b[0m         \u001b[0mbest_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_trials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhm_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_distribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             model.load_weights(\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_checkpoint_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             )\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2197\u001b[0m         \u001b[0msave_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEither\u001b[0m \u001b[0;34m'tf'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mending\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'.h5'\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m             \u001b[0;34m'.keras'\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mto\u001b[0m \u001b[0mHDF5\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msave_format\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2199\u001b[0;31m             \u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2200\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointOptions\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mspecifies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0moptions\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msaving\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     37\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /content/content/autoint/trial_d2831994c5e44b8df630246d6e0b3530/checkpoints/epoch_0/checkpoint"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIuYtu6O22aP",
        "outputId": "3bff1f91-4224-4324-ead5-8805ad034c71"
      },
      "source": [
        "model.evaluate(test_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2158/2158 [==============================] - 29s 11ms/step - loss: 0.2583 - categorical_accuracy: 0.9774 - auc: 1.0000 - auc_1: 0.9999\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.2582692503929138, 0.9773806929588318, 0.9999987483024597, 0.999900221824646]"
            ]
          },
          "execution_count": 111,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA9AXLfL3UIp",
        "outputId": "b22bf67e-df51-4e11-f5ea-f054886ec0d8"
      },
      "source": [
        "model_2 = tuner.get_best_models(4)[1]\n",
        "model_2.evaluate(test_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n",
            "model compiled\n",
            "model compiled\n",
            "model compiled\n",
            "2158/2158 [==============================] - 28s 11ms/step - loss: 0.0996 - categorical_accuracy: 0.9774 - auc: 1.0000 - auc_1: 0.9998\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.09963809698820114,\n",
              " 0.9773843288421631,\n",
              " 0.9999977946281433,\n",
              " 0.9998387098312378]"
            ]
          },
          "execution_count": 112,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeKMFGJ8r_4Q",
        "outputId": "4e1a9bca-2289-47fe-831f-389388c00eed"
      },
      "source": [
        "model_3 = tuner.get_best_models(4)[2]\n",
        "model_3.evaluate(test_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n",
            "model compiled\n",
            "model compiled\n",
            "model compiled\n",
            "2158/2158 [==============================] - 28s 11ms/step - loss: 0.0833 - categorical_accuracy: 0.9774 - auc: 1.0000 - auc_1: 0.9998\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.08330965042114258,\n",
              " 0.9773843288421631,\n",
              " 0.9999977946281433,\n",
              " 0.9998387098312378]"
            ]
          },
          "execution_count": 113,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qw2QEfksUEQ",
        "outputId": "efc3b7f0-31b6-46e4-a670-c62078993ecf"
      },
      "source": [
        "model_4 = tuner.get_best_models(4)[3]\n",
        "model_4.evaluate(test_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n",
            "model compiled\n",
            "model compiled\n",
            "model compiled\n",
            "2158/2158 [==============================] - 28s 11ms/step - loss: 0.0837 - categorical_accuracy: 0.9774 - auc: 1.0000 - auc_1: 0.9998\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.08372887969017029,\n",
              " 0.9773843288421631,\n",
              " 0.9999977946281433,\n",
              " 0.9998387098312378]"
            ]
          },
          "execution_count": 114,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOd-lxIMMwYx"
      },
      "source": [
        "#tpu\n",
        "\n",
        "localhost_save_option = tf.saved_model.SaveOptions(experimental_io_device=\"/job:localhost\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X2xaoTZn09P",
        "outputId": "b8de5e43-fb8c-4f56-d97b-92b4e43de1ab"
      },
      "source": [
        "# dyconv tpu y3 l2 wo dnn pre ffn \n",
        "# dycon layer = dycon + ffn \n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.3,\n",
              " 'decay_rate': 0.51,\n",
              " 'decay_steps': 10000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'hp_DCconv_drop': 0.0,\n",
              " 'hp_ffn_d_size': 960,\n",
              " 'hp_ffn_drop': 0.0,\n",
              " 'hp_head_num': 1,\n",
              " 'hp_kernel_size': 6,\n",
              " 'hp_pred_ffn_d_size': 16,\n",
              " 'hp_pred_ffn_drop': 0.31,\n",
              " 'interac_residu138025': False,\n",
              " 'interac_residu14218': False,\n",
              " 'interac_residu229780': False,\n",
              " 'interac_residu233453': False,\n",
              " 'interac_residu346956': False,\n",
              " 'interac_residu395188': False,\n",
              " 'interac_residu508521': False,\n",
              " 'interac_residu807929': False,\n",
              " 'interac_residu845717': False,\n",
              " 'interac_residu882509': False,\n",
              " 'interac_residu958424': False,\n",
              " 'label_smoothing': 0.13,\n",
              " 'layers_num': 1,\n",
              " 'learning_rate': 0.000223579210259422,\n",
              " 'num_of_d': 96}"
            ]
          },
          "execution_count": 68,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T43gZC5Co7ZQ"
      },
      "source": [
        "model = tuner.hypermodel.build(best_hps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhy4vXjDpLTN",
        "outputId": "872b037e-b46f-4066-c0cd-2bc31e391bb7"
      },
      "source": [
        "history = model.fit(train_ds, epochs=10, validation_data=vali_ds, validation_batch_size=batch_size, \n",
        "             callbacks = [ ClearTrainingOutput(), EarlyStopping(monitor='val_loss', min_delta=0.01, patience=2)], shuffle=True)\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.callbacks.History object at 0x7f674b92d510>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCwshNcMrEnh",
        "outputId": "861874a6-f20b-481a-9b4b-bbbb510a742d"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': [0.999203622341156,\n",
              "  0.9999980926513672,\n",
              "  0.9999997019767761,\n",
              "  0.9999949932098389],\n",
              " 'categorical_accuracy': [0.9865471720695496,\n",
              "  0.9996814131736755,\n",
              "  0.9998153448104858,\n",
              "  0.999808132648468],\n",
              " 'loss': [2.7785279750823975,\n",
              "  0.37188711762428284,\n",
              "  0.3680680990219116,\n",
              "  0.3663812577724457],\n",
              " 'val_auc': [0.9992232322692871, 0.9999993443489075, 0.9999881386756897, 1.0],\n",
              " 'val_categorical_accuracy': [0.9611523747444153,\n",
              "  0.9992950558662415,\n",
              "  0.9954177737236023,\n",
              "  0.999954104423523],\n",
              " 'val_loss': [0.3903200924396515,\n",
              "  0.3614693582057953,\n",
              "  0.3648163378238678,\n",
              "  0.3598661422729492]}"
            ]
          },
          "execution_count": 78,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "4pTUCnRmo-dA",
        "outputId": "e7be77e7-e3c2-42cd-e451-b6bb6d53eb0b"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 5)[1]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(train_ds, epochs=10, validation_data=vali_ds, validation_batch_size=batch_size, \n",
        "             callbacks = [ ClearTrainingOutput(), EarlyStopping(monitor='val_loss', min_delta=0.01, patience=2)], shuffle=True)\n",
        "print(history.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "   4/2158 [..............................] - ETA: 39s - loss: 16.3417 - categorical_accuracy: 0.3563 - auc: 0.5730    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.5941s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.5941s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2158/2158 [==============================] - 91s 38ms/step - loss: 6.2148 - categorical_accuracy: 0.8133 - auc: 0.9307 - val_loss: 0.8130 - val_categorical_accuracy: 0.9612 - val_auc: 0.9986\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_binary_accuracy` which is not available. Available metrics are: loss,categorical_accuracy,auc,val_loss,val_categorical_accuracy,val_auc\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_binary_accuracy` which is not available. Available metrics are: loss,categorical_accuracy,auc,val_loss,val_categorical_accuracy,val_auc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n",
            "2158/2158 [==============================] - 80s 37ms/step - loss: 0.7976 - categorical_accuracy: 0.9924 - auc: 0.9999 - val_loss: 0.8024 - val_categorical_accuracy: 0.9612 - val_auc: 0.9992\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_binary_accuracy` which is not available. Available metrics are: loss,categorical_accuracy,auc,val_loss,val_categorical_accuracy,val_auc\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_binary_accuracy` which is not available. Available metrics are: loss,categorical_accuracy,auc,val_loss,val_categorical_accuracy,val_auc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10\n",
            "2158/2158 [==============================] - 81s 37ms/step - loss: 0.7933 - categorical_accuracy: 0.9960 - auc: 1.0000 - val_loss: 0.7899 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_binary_accuracy` which is not available. Available metrics are: loss,categorical_accuracy,auc,val_loss,val_categorical_accuracy,val_auc\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_binary_accuracy` which is not available. Available metrics are: loss,categorical_accuracy,auc,val_loss,val_categorical_accuracy,val_auc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10\n",
            "1768/2158 [=======================>......] - ETA: 7s - loss: 0.7910 - categorical_accuracy: 1.0000 - auc: 1.0000"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-ebc674531dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_hps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m history = model.fit(train_ds, epochs=10, validation_data=vali_ds, validation_batch_size=batch_size, \n\u001b[0;32m----> 4\u001b[0;31m              callbacks = [ ClearTrainingOutput(), EarlyStopping(monitor='val_binary_accuracy', min_delta=0.01, patience=2)], shuffle=True)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3313\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3314\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_function_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3315\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3316\u001b[0m       \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m       \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m       return (inputs, kwargs, flat_inputs + flat_kwargs,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_convert_numpy_inputs\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   2735\u001b[0m   \u001b[0;31m# are eventually passed to ConcreteFunction()._call_flat, which requires\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m   \u001b[0;31m# expanded composites.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2737\u001b[0;31m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m   \u001b[0;31m# Check for NumPy arrays in arguments and convert them to Tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mexpand_composites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m_type_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1790\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_type_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     return _SingleWorkerDatasetIteratorSpec(self._worker, self._devices,\n\u001b[0;32m-> 1792\u001b[0;31m                                             self._element_spec, self._options)\n\u001b[0m\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, worker, devices, element_spec, options)\u001b[0m\n\u001b[1;32m   1679\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1679\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/device_util.py\u001b[0m in \u001b[0;36mcanonicalize\u001b[0;34m(d, default)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# may return remote devices as well, but we're already doing this elsewhere.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     host_cpu = tf_device.DeviceSpec.from_string(\n\u001b[0;32m---> 63\u001b[0;31m         config.list_logical_devices(\"CPU\")[0].name)\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhost_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_merged_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/device_spec.py\u001b[0m in \u001b[0;36mfrom_string\u001b[0;34m(cls, spec)\u001b[0m\n\u001b[1;32m    156\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mDeviceSpec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \"\"\"\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_string_to_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mparse_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/device_spec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, job, replica, task, device_type, device_index)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n\u001b[1;32m    124\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_str_or_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replica\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_int_or_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplica\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_int_or_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_device_str_or_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIYL4mQAsWU-",
        "outputId": "469ed1c3-987d-4a53-f6ec-16e1ca28cc23"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 5)[1]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(train_ds, epochs=10, validation_data=vali_ds, validation_batch_size=batch_size, \n",
        "             callbacks = [ ClearTrainingOutput(), EarlyStopping(monitor='val_loss', min_delta=0.01, patience=2)], shuffle=True)\n",
        "print(history.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': [2.797792434692383, 0.7957286238670349, 0.7933281064033508], 'categorical_accuracy': [0.9490993022918701, 0.9934400916099548, 0.9934437274932861], 'auc': [0.994138240814209, 0.999966025352478, 0.9999743103981018], 'val_loss': [0.8111476898193359, 0.8028169274330139, 0.801870584487915], 'val_categorical_accuracy': [0.9611378908157349, 0.9611595869064331, 0.9611354470252991], 'val_auc': [0.9985485076904297, 0.9992387294769287, 0.9992125630378723]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k7tKWR1oCOZ",
        "outputId": "5ace3d94-482b-41b1-8eb0-6563ce27aa30"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 5)[2]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(train_ds, epochs=10, validation_data=vali_ds, validation_batch_size=batch_size, \n",
        "             callbacks = [ ClearTrainingOutput(), EarlyStopping(monitor='val_loss', min_delta=0.01, patience=2)], shuffle=True)\n",
        "print(history.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': [2.2038564682006836, 0.04667557030916214, 0.019489634782075882, 0.008513448759913445, 0.005303633399307728], 'categorical_accuracy': [0.902991771697998, 0.9932699799537659, 0.9953117966651917, 0.99908047914505, 0.9995691776275635], 'auc': [0.9775574207305908, 0.9991524815559387, 0.9998765587806702, 0.9999475479125977, 0.9999722242355347], 'val_loss': [0.23294846713542938, 0.07955635339021683, 0.007959450595080853, 0.004068322945386171, 0.0032292711548507214], 'val_categorical_accuracy': [0.9611137509346008, 0.9611523747444153, 0.9998575448989868, 0.9999517202377319, 0.9999517202377319], 'val_auc': [0.9912970066070557, 0.9988660216331482, 0.9999966621398926, 0.9999666810035706, 0.9999739527702332]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY6VHr-oNFET"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 5)[2]\n",
        "model = tuner.hypermodel.build(best_hps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roBKTblFw4zU"
      },
      "source": [
        "model.save('/content/dyconv_y3_l2_wo_dnn_tpu', options=localhost_save_option)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdeqI7PNxYEs",
        "outputId": "abc2f06a-f156-4136-f860-d67881197a1b"
      },
      "source": [
        "!tar -zcvf 'dyconv_y3_l2_wo_dnn_tpu.tar.gz' /content/dyconv_y3_l2_wo_dnn_tpu/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "/content/dyconv_y3_l2_wo_dnn_tpu/\n",
            "/content/dyconv_y3_l2_wo_dnn_tpu/saved_model.pb\n",
            "/content/dyconv_y3_l2_wo_dnn_tpu/assets/\n",
            "/content/dyconv_y3_l2_wo_dnn_tpu/variables/\n",
            "/content/dyconv_y3_l2_wo_dnn_tpu/variables/variables.index\n",
            "/content/dyconv_y3_l2_wo_dnn_tpu/variables/variables.data-00000-of-00001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vesUm8pjwtYZ",
        "outputId": "4d6faef1-f2d7-499c-dbd4-8556aaaaa9f1"
      },
      "source": [
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.0,\n",
              " 'decay_rate': 0.2,\n",
              " 'decay_steps': 15000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'hp_DCconv_drop': 0.52,\n",
              " 'hp_ffn_d_size': 992,\n",
              " 'hp_ffn_drop': 0.01,\n",
              " 'hp_head_num': 1,\n",
              " 'hp_kernel_size': 6,\n",
              " 'hp_pred_ffn_d_size': 80,\n",
              " 'hp_pred_ffn_drop': 0.71,\n",
              " 'interac_residu138025': False,\n",
              " 'interac_residu14218': False,\n",
              " 'interac_residu233453': False,\n",
              " 'interac_residu346956': False,\n",
              " 'interac_residu395188': False,\n",
              " 'interac_residu812250': False,\n",
              " 'interac_residu845717': False,\n",
              " 'label_smoothing': 0.0,\n",
              " 'layers_num': 1,\n",
              " 'learning_rate': 0.0001,\n",
              " 'num_of_d': 32}"
            ]
          },
          "execution_count": 83,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s54eysCc3QFT",
        "outputId": "d7dc2454-713b-45c7-ece7-7c6cddcec036"
      },
      "source": [
        "# dyconv  tpu y2 l2 w/o dnn and only pre ffn\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.73,\n",
              " 'decay_rate': 0.26,\n",
              " 'decay_steps': 100000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'hp_DCconv_drop': 0.5,\n",
              " 'hp_ffn_d_size': 608,\n",
              " 'hp_ffn_drop': 0.17,\n",
              " 'hp_head_num': 8,\n",
              " 'hp_kernel_size': 12,\n",
              " 'hp_pred_ffn_d_size': 800,\n",
              " 'hp_pred_ffn_drop': 0.09,\n",
              " 'label_smoothing': 0.49,\n",
              " 'layers_num': 8,\n",
              " 'learning_rate': 0.0001,\n",
              " 'num_of_d': 32}"
            ]
          },
          "execution_count": 40,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3Uk5CS73D8j",
        "outputId": "86561cc6-84ad-4001-be24-6805138904cc"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 5)\n",
        "for i in best_hps:\n",
        "  print(i.values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'num_of_d': 32, 'emb_layer_choice': 'matrix', 'EM_dropout_rate': 0.73, 'hp_kernel_size': 12, 'hp_head_num': 8, 'hp_ffn_d_size': 608, 'hp_ffn_drop': 0.17, 'hp_DCconv_drop': 0.5, 'layers_num': 8, 'hp_pred_ffn_drop': 0.09, 'hp_pred_ffn_d_size': 800, 'learning_rate': 0.0001, 'decay_steps': 100000, 'decay_rate': 0.26, 'label_smoothing': 0.49}\n",
            "{'num_of_d': 32, 'emb_layer_choice': 'matrix', 'EM_dropout_rate': 0.9500000000000001, 'hp_kernel_size': 12, 'hp_head_num': 8, 'hp_ffn_d_size': 528, 'hp_ffn_drop': 0.02, 'hp_DCconv_drop': 0.47000000000000003, 'layers_num': 7, 'hp_pred_ffn_drop': 0.19, 'hp_pred_ffn_d_size': 704, 'learning_rate': 0.00011202631862509904, 'decay_steps': 100000, 'decay_rate': 0.02, 'label_smoothing': 0.76}\n",
            "{'num_of_d': 96, 'emb_layer_choice': 'dense', 'EM_dropout_rate': 0.52, 'hp_kernel_size': 12, 'hp_head_num': 32, 'hp_ffn_d_size': 816, 'hp_ffn_drop': 0.14, 'hp_DCconv_drop': 0.1, 'layers_num': 8, 'hp_pred_ffn_drop': 0.08, 'hp_pred_ffn_d_size': 176, 'learning_rate': 0.007765767620141046, 'decay_steps': 75000, 'decay_rate': 0.31, 'label_smoothing': 0.3}\n",
            "{'num_of_d': 32, 'emb_layer_choice': 'matrix', 'EM_dropout_rate': 0.99, 'hp_kernel_size': 12, 'hp_head_num': 32, 'hp_ffn_d_size': 1024, 'hp_ffn_drop': 0.0, 'hp_DCconv_drop': 0.04, 'layers_num': 12, 'hp_pred_ffn_drop': 0.0, 'hp_pred_ffn_d_size': 864, 'learning_rate': 0.0001, 'decay_steps': 100000, 'decay_rate': 0.0, 'label_smoothing': 0.98}\n",
            "{'num_of_d': 32, 'emb_layer_choice': 'matrix', 'EM_dropout_rate': 0.51, 'hp_kernel_size': 12, 'hp_head_num': 16, 'hp_ffn_d_size': 528, 'hp_ffn_drop': 0.22, 'hp_DCconv_drop': 0.4, 'layers_num': 4, 'hp_pred_ffn_drop': 0.0, 'hp_pred_ffn_d_size': 928, 'learning_rate': 0.0001489809415370005, 'decay_steps': 100000, 'decay_rate': 0.21, 'label_smoothing': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBZUPdYxQOPd"
      },
      "source": [
        "model = tuner.hypermodel.build(best_hps[0])\n",
        "model.save('/content/dyconv_y2_l2_wo_dnn_tpu', options=localhost_save_option)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGH1QfzMQd9y"
      },
      "source": [
        "!tar -zcvf 'dyconv_y2_l2_wo_dnn_tpu.tar.gz' /content/dyconv_y2_l2_wo_dnn_tpu/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDW_OYgyLjFR",
        "outputId": "1de3dbd5-9986-4a66-d5c5-506541c66f54"
      },
      "source": [
        "model.fit(train_ds, epochs=10, validation_data=vali_ds, validation_batch_size=batch_size, \n",
        "             callbacks = [ ClearTrainingOutput(), EarlyStopping(monitor='val_binary_accuracy', min_delta=0.01, patience=2)], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d42e0f750>"
            ]
          },
          "execution_count": 44,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgRkPb_2L39b",
        "outputId": "abd9f890-ceac-4e8c-c436-48c5ff0af240"
      },
      "source": [
        "model.save('/content/autoint_model_l2_TPU', options=localhost_save_option)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as em_dropout_layer_call_and_return_conditional_losses, em_dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses while saving (showing 5 of 385). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as em_dropout_layer_call_and_return_conditional_losses, em_dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses while saving (showing 5 of 385). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/autoint_model_l2_TPU/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/autoint_model_l2_TPU/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lzl_KixRT-a",
        "outputId": "b2bf771f-9f1e-4991-f4fa-7429f481ca93"
      },
      "source": [
        "!tar -zcvf 'autoint_model_l2_TPU.tar.gz' /content/autoint_model_l2_TPU/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "/content/autoint_model_l2_TPU/\n",
            "/content/autoint_model_l2_TPU/assets/\n",
            "/content/autoint_model_l2_TPU/saved_model.pb\n",
            "/content/autoint_model_l2_TPU/variables/\n",
            "/content/autoint_model_l2_TPU/variables/variables.data-00000-of-00001\n",
            "/content/autoint_model_l2_TPU/variables/variables.index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9awSbY6nN7l3",
        "outputId": "62be543c-9914-4131-b646-091b428a562b"
      },
      "source": [
        "set_seed()\n",
        "with strategy.scope():\n",
        "  new_model = tf.keras.models.load_model('/content/autoint_model', options=localhost_save_option)\n",
        "new_model.evaluate(test_ds, batch_size=batch_size)\n",
        "#new_model.fit(train_ds, epochs=10, validation_data=vali_ds, validation_batch_size=batch_size, \n",
        "#             callbacks = [ ClearTrainingOutput(), EarlyStopping(monitor='val_binary_accuracy', min_delta=0.01, patience=2)], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2158/2158 [==============================] - 41s 17ms/step - loss: 0.7557 - binary_accuracy: 0.7394 - auc: 0.4811\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.755675733089447, 0.7394325137138367, 0.4811224341392517]"
            ]
          },
          "execution_count": 187,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpqP3FYrB8xw",
        "outputId": "1a32c2df-7c3b-4d69-9da2-ddb320c81c6d"
      },
      "source": [
        "#best_hps = tuner.get_best_hyperparameters(num_trials=5)[3]\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "no_w_model = tuner.hypermodel.build(best_hps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF3vjvYlS27l"
      },
      "source": [
        "no_w_model.fit(train_ds, epochs=10, validation_data=vali_ds, validation_batch_size=batch_size, \n",
        "             callbacks = [ ClearTrainingOutput(), EarlyStopping(monitor='val_binary_accuracy', min_delta=0.01, patience=2)], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KXUbVmTWxoC"
      },
      "source": [
        "!tar -zcvf dyconv_tk_2target_rm_issue_initial_stat.tar.gz /content/DyConv/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vM5Hn7jKWGhq",
        "outputId": "33635186-2b8c-4111-8d02-260cfc721f7e"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('dyconv_tk_2target_rm_issue_initial_stat.tar.gz') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_8a75b892-2834-4cee-8d0f-4b4572711aa3\", \"untitled\", 0)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_u4xbsMTcZt",
        "outputId": "98c78326-caf2-4799-cde8-60c81a052a73"
      },
      "source": [
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.36,\n",
              " 'decay_rate': 0.81,\n",
              " 'decay_steps': 55000,\n",
              " 'emb_layer_choice': 'dense',\n",
              " 'hp_DCconv_drop': 0.77,\n",
              " 'hp_ffn_d_size': 416,\n",
              " 'hp_ffn_drop': 0.03,\n",
              " 'hp_head_num': 32,\n",
              " 'hp_kernel_size': 4,\n",
              " 'label_smoothing': 0.6,\n",
              " 'layers_num': 8,\n",
              " 'learning_rate': 0.00034060994466741175,\n",
              " 'num_of_d': 64}"
            ]
          },
          "execution_count": 45,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z61E1lX4M7_z",
        "outputId": "85fbee84-9f06-45bd-b6f2-e9766b657b6a"
      },
      "source": [
        "tuner.get_best_hyperparameters(num_trials=5)[3].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout': True,\n",
              " 'EM_dropout_rate': 0.02,\n",
              " 'decay_rate': 0.1,\n",
              " 'decay_steps': 90000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'label_smoothing': 0.0,\n",
              " 'learning_rate': 0.0001,\n",
              " 'mha_residual': False,\n",
              " 'num_MHA_layer': 6,\n",
              " 'num_head': 8,\n",
              " 'num_of_d': 32}"
            ]
          },
          "execution_count": 73,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldGKyUxKSpoy",
        "outputId": "8c0d72cf-c494-4a88-f904-b26f6ae932d3"
      },
      "source": [
        "model = tuner.get_best_models(2)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n",
            "model compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2VmG99QeoDF"
      },
      "source": [
        "answer = outer_t_y[0:128]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8SbR_BSd95m"
      },
      "source": [
        "predict = model.predict(outer_ds.take(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mecwXT8VgfBk",
        "outputId": "25b61489-b07e-46e9-f921-b3cd92b8c241"
      },
      "source": [
        "len(answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "execution_count": 184,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWZVWsT_gZiR",
        "outputId": "97076b1b-076d-4e39-e2b8-21b82750fe1d"
      },
      "source": [
        "ans_num = 0\n",
        "for i in range(len(predict)):\n",
        "  ans = 0\n",
        "  if predict[i] >0.5:\n",
        "    ans = 1\n",
        "  \n",
        "  if ans == answer[i]:\n",
        "    ans_num +=1\n",
        "\n",
        "ans_num"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "execution_count": 186,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsDmTENMKLuy",
        "outputId": "d1cfb0c3-7e31-4dc1-ffe2-38a5820642ac"
      },
      "source": [
        "model.evaluate(outer_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5394/5394 [==============================] - 95s 17ms/step - loss: 44.2004 - binary_accuracy: 0.9976 - auc: 0.9985\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[44.202171325683594, 0.9972119331359863, 0.9982379078865051]"
            ]
          },
          "execution_count": 161,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsqUKrq8RHrv",
        "outputId": "091bf8ea-d427-4c3f-ab7b-4d285d5e20b6"
      },
      "source": [
        "model.save('/content/autoint_model_best', options=localhost_save_option)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as em_dropout_layer_call_and_return_conditional_losses, em_dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses while saving (showing 5 of 155). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as em_dropout_layer_call_and_return_conditional_losses, em_dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses while saving (showing 5 of 155). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/autoint_model_best/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/autoint_model_best/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb1HAYh4NcDA"
      },
      "source": [
        "with strategy.scope():\n",
        "  new_model = tf.keras.models.load_model('/content/autoint_model_best', options=localhost_save_option)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD70gsskRd3s",
        "outputId": "54dce8b3-67d6-4357-ab3e-fa68a9159cd3"
      },
      "source": [
        "new_model.evaluate(test_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2158/2158 [==============================] - 41s 18ms/step - loss: 0.4971 - binary_accuracy: 1.0000 - auc: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.49706342816352844, 1.0, 0.9999999403953552]"
            ]
          },
          "execution_count": 141,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3BPG9j4SzFm",
        "outputId": "4df702e1-19d0-49b7-ba1a-4dc95f99ff65"
      },
      "source": [
        "!tar -zcvf 'autoint_model_best.tar.gz' /content/autoint_model_best"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "/content/autoint_model_best/\n",
            "/content/autoint_model_best/assets/\n",
            "/content/autoint_model_best/saved_model.pb\n",
            "/content/autoint_model_best/variables/\n",
            "/content/autoint_model_best/variables/variables.data-00000-of-00001\n",
            "/content/autoint_model_best/variables/variables.index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YusMJDtQT1Iv"
      },
      "source": [
        "history = model.fit(train_ds, epochs = 30, validation_data=test_ds, validation_batch_size=batch_size, callbacks = [ClearTrainingOutput(), EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c92Rov837eFT"
      },
      "source": [
        "history = model.fit(vali_ds, epochs = 30, validation_data=train_ds, validation_batch_size=batch_size, callbacks = [ClearTrainingOutput(), EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRIMKHyfLdIR"
      },
      "source": [
        "model.fit(test_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdmAsM_xCYjD"
      },
      "source": [
        "model.load_weights('/content/checkpoint')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeypYkQHXYly",
        "outputId": "438c25aa-c61f-4297-c938-4ad8492c6477"
      },
      "source": [
        "best_model = tuner.get_best_models(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n",
            "model compiled\n",
            "model compiled\n",
            "model compiled\n",
            "model compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhnxI9M82qhs"
      },
      "source": [
        "best_model[4].evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQoUDxlL5kp7"
      },
      "source": [
        "best_model[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnSfpJNs3M3D",
        "outputId": "8d5f7485-dc1c-47de-9139-dd8dbc82ec42"
      },
      "source": [
        "\n",
        "for i in range(10):\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'emb_index': TensorSpec(shape=(128, 2, 1), dtype=tf.int64, name=None),\n",
              "  'emb_value': TensorSpec(shape=(128, 2, 128), dtype=tf.float64, name=None),\n",
              "  'index': TensorSpec(shape=(128, 67, 1), dtype=tf.int64, name=None),\n",
              "  'value': TensorSpec(shape=(128, 67, 1), dtype=tf.float64, name=None)},\n",
              " TensorSpec(shape=(128, 1), dtype=tf.float32, name=None))"
            ]
          },
          "execution_count": 52,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVTUgm_h_UvJ",
        "outputId": "8e9635c6-a0d0-47e6-8c70-40b0fb74e2e9"
      },
      "source": [
        "best_model[0].save('/content/best_autoint')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as em_dropout_layer_call_fn, em_dropout_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn while saving (showing 5 of 105). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TREpNbTUAd3W"
      },
      "source": [
        "!tar -C /content -zcvf autoint_kt.tar.gz /content/autoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yR4qCxv5Wyc"
      },
      "source": [
        "# CV test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4TvxkKolt-R",
        "outputId": "e9475705-51a5-4f0d-ccd6-9ab995f7fb9f"
      },
      "source": [
        "df_y_2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1380929, 2)"
            ]
          },
          "execution_count": 56,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bJlx-CYKS3K"
      },
      "source": [
        "train = round(len(df_y_2) //128 *0.7)*128\n",
        "test = round(len(df_y_2) //128 *0.3)*128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBuiSSRLKT40"
      },
      "source": [
        "train = round(len(df_y_2) //128 *0.3)*128\n",
        "test = round(len(df_y_2) //128 *0.7)*128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffXyKVoeryhv"
      },
      "source": [
        " \n",
        " \n",
        "at_t_v = df_v_2[:train].copy()\n",
        "at_t_i = df_i_2[:train].copy()\n",
        "at_t_y = df_y_2[:train].copy()\n",
        "emb_at_i = df_emb_i_2[:train].copy()\n",
        "emb_at_v = df_emb_v_2[:train].copy()\n",
        " \n",
        "test_t_v = df_v_2[train:test+train].copy()\n",
        "test_t_i = df_i_2[train:test+train].copy()\n",
        "test_t_y = df_y_2[train:test+train].copy()\n",
        "test_emb_at_i = df_emb_i_2[train:test+train].copy()\n",
        "test_emb_at_v = df_emb_v_2[train:test+train].copy()\n",
        " \n",
        "emb_at_i = emb_at_i.reshape([-1, 2, 1])\n",
        "emb_at_v = emb_at_v.reshape([-1, 2, 128])\n",
        "at_t_i = at_t_i.reshape([-1, num_input, 1])\n",
        "at_t_v = at_t_v.reshape([-1, num_input, 1])\n",
        "at_t_y = at_t_y.reshape([-1, 2])\n",
        " \n",
        "test_emb_at_i = test_emb_at_i.reshape([-1, 2, 1])\n",
        "test_emb_at_v = test_emb_at_v.reshape([-1, 2, 128])\n",
        "test_t_i = test_t_i.reshape([-1, num_input, 1])\n",
        "test_t_v = test_t_v.reshape([-1, num_input, 1])\n",
        "test_t_y = test_t_y.reshape([-1, 2])\n",
        " \n",
        "train_ds = tf.data.Dataset.from_tensor_slices(({\"value\": at_t_v, \"index\": at_t_i, 'emb_value':emb_at_v, 'emb_index':emb_at_i}, at_t_y))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(({\"value\": test_t_v, \"index\": test_t_i, 'emb_value':test_emb_at_v, 'emb_index':test_emb_at_i}, test_t_y))\n",
        " \n",
        "train_ds = train_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
        "test_ds = test_ds.batch(batch_size=batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFcjy8-4lpKM"
      },
      "source": [
        "train = round(len(df_y_2) //128 *0.7)*128\n",
        "test = round(len(df_y_2) //128 *0.3)*128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCERmUUTvIgM"
      },
      "source": [
        "train = round(len(df_y_2) //128 *0.3)*128\n",
        "test = round(len(df_y_2) //128 *0.7)*128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVkUNAvLMPra"
      },
      "source": [
        "train = round(len(df_y_2) //128 *0.01 *0.2)*128\n",
        "test = round(len(df_y_2) //128 *0.9*0.2)*128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd9LSzPCx90G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "fc4a37c6-638b-43a8-d5c3-9cf15986e0d5"
      },
      "source": [
        " \n",
        " \n",
        "at_t_v = df_v_2[:train].copy()\n",
        "at_t_i = df_i_2[:train].copy()\n",
        "at_t_y = df_y_2[:train].copy()\n",
        "emb_at_i = df_emb_i_2[:train].copy()\n",
        "emb_at_v = df_emb_v_2[:train].copy()\n",
        " \n",
        "test_t_v = df_v_2[train:test+train].copy()\n",
        "test_t_i = df_i_2[train:test+train].copy()\n",
        "test_t_y = df_y_2[train:test+train].copy()\n",
        "test_emb_at_i = df_emb_i_2[train:test+train].copy()\n",
        "test_emb_at_v = df_emb_v_2[train:test+train].copy()\n",
        " \n",
        "emb_at_i = emb_at_i.reshape([-1, 2, 1])\n",
        "emb_at_v = emb_at_v.reshape([-1, 2, 128])\n",
        "at_t_i = at_t_i.reshape([-1, num_input, 1])\n",
        "at_t_v = at_t_v.reshape([-1, num_input, 1])\n",
        "at_t_y = at_t_y.reshape([-1, 3])\n",
        " \n",
        "test_emb_at_i = test_emb_at_i.reshape([-1, 2, 1])\n",
        "test_emb_at_v = test_emb_at_v.reshape([-1, 2, 128])\n",
        "test_t_i = test_t_i.reshape([-1, num_input, 1])\n",
        "test_t_v = test_t_v.reshape([-1, num_input, 1])\n",
        "test_t_y = test_t_y.reshape([-1, 3])\n",
        " \n",
        "train_ds = tf.data.Dataset.from_tensor_slices(({\"value\": at_t_v, \"index\": at_t_i, 'emb_value':emb_at_v, 'emb_index':emb_at_i}, at_t_y))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(({\"value\": test_t_v, \"index\": test_t_i, 'emb_value':test_emb_at_v, 'emb_index':test_emb_at_i}, test_t_y))\n",
        " \n",
        "train_ds = train_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
        "test_ds = test_ds.batch(batch_size=batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-3634ad34af36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtest_t_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_t_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mat_t_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mat_t_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'emb_value'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0memb_at_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'emb_index'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0memb_at_i\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mat_t_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_t_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_t_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'emb_value'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_emb_at_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'emb_index'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_emb_at_i\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_t_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    758\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \"\"\"\n\u001b[0;32m--> 760\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3328\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0;32m-> 3330\u001b[0;31m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[0m\u001b[1;32m   3331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3332\u001b[0m     variant_tensor = gen_dataset_ops.tensor_slice_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0;32m--> 289\u001b[0;31m                        (self, other))\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions 483328 and 966656 are not compatible"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd3tshl4Q1m3"
      },
      "source": [
        "feat_size = 410\n",
        "num_input = df_v_2.shape[-1]\n",
        "num_emb = test_emb_at_i.shape[-1]\n",
        "emb_d = 128\n",
        "# batch_size also needs to be auto\n",
        "directory = '/content/kt'\n",
        "batch_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xR7kMGrK_7s"
      },
      "source": [
        "st_v_y_2 = pd.DataFrame(df_y_2[:train])\n",
        "st_t_y_2 = pd.DataFrame(df_y_2[train:test+train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B__5dcaLDY8"
      },
      "source": [
        "V_st = st_v_y_2.value_counts().values\n",
        "T_st = st_t_y_2.value_counts().values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAL3xsCpLzIg"
      },
      "source": [
        "def st_example(st_array, name):\n",
        "  all_num = st_array.sum()\n",
        "  print(name)\n",
        "  for i in st_array:\n",
        "    pst = i /all_num\n",
        "    print(i, ' :', round(pst, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJuz8znRNRUW",
        "outputId": "241bafd5-5a39-45ca-dc23-71891bdb77e9"
      },
      "source": [
        "st_example(V_st, \"train\"), st_example(T_st, \"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train\n",
            "753264  : 0.779\n",
            "189247  : 0.196\n",
            "24145  : 0.025\n",
            "test\n",
            "324349  : 0.783\n",
            "79830  : 0.193\n",
            "10029  : 0.024\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "execution_count": 183,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdhTQ62nLlYb",
        "outputId": "04d02218-b3c9-4a11-854e-c2e084a0b8ca"
      },
      "source": [
        " 10029/(324349 + 79830 +10029)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.024212472960444993"
            ]
          },
          "execution_count": 118,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4FTXmgbcPTm",
        "outputId": "1d08cf24-ab6b-4a49-ce47-8132e1caff23"
      },
      "source": [
        "st_example(V_st), st_example(T_st)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2300  : 0.817\n",
            "496  : 0.176\n",
            "20  : 0.007\n",
            "197291  : 0.794\n",
            "49612  : 0.2\n",
            "1673  : 0.007\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "execution_count": 76,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdbpBDjebTnE"
      },
      "source": [
        "train_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqj_fH7DdIc5"
      },
      "source": [
        "### Simple emb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaO1GeHx28m8",
        "outputId": "2c2a93ee-badb-4a6c-ec12-01c3baebec2f"
      },
      "source": [
        "#@ mlp\n",
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train num: 7552 test : 3236\n",
            "trials: 4\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 105s 17ms/step - loss: 0.0312 - categorical_accuracy: 0.9742 - auc_m: 0.8267 - auc_m_PR: 0.6362 - confusion_matrix: 193344.0000 - val_loss: 0.0277 - val_categorical_accuracy: 0.9748 - val_auc_m: 0.8281 - val_auc_m_PR: 0.6371 - val_confusion_matrix: 48320.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 104s 17ms/step - loss: 0.0274 - categorical_accuracy: 0.9751 - auc_m: 0.8282 - auc_m_PR: 0.6373 - confusion_matrix: 193344.0000 - val_loss: 0.0277 - val_categorical_accuracy: 0.9748 - val_auc_m: 0.8281 - val_auc_m_PR: 0.6371 - val_confusion_matrix: 48320.0000\n",
            "Epoch 3/10\n",
            "6042/6042 [==============================] - 104s 17ms/step - loss: 0.0274 - categorical_accuracy: 0.9751 - auc_m: 0.8282 - auc_m_PR: 0.6373 - confusion_matrix: 193344.0000 - val_loss: 0.0277 - val_categorical_accuracy: 0.9748 - val_auc_m: 0.8281 - val_auc_m_PR: 0.6371 - val_confusion_matrix: 48320.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 43s 13ms/step - loss: 0.0760 - categorical_accuracy: 0.9711 - auc_m: 0.8264 - auc_m_PR: 0.6311 - confusion_matrix: 103552.0000\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXGdcqr93NCK",
        "outputId": "1a08716b-2822-4842-c165-06dba33083b1"
      },
      "source": [
        "avg_metrics([i[0:-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09425875, 0.98905767, 0.96231354, 0.87615207])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59dWYjfg3OLH",
        "outputId": "b1d073be-0047-471a-9a6b-2978118d4e5e"
      },
      "source": [
        "avg_cm_scores([i[-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('precision', 0.9288496879875311),\n",
              "             ('recall', 0.9429557124773661),\n",
              "             ('F1', 0.8997003915855505),\n",
              "             ('specificity', 0.9907955487569172),\n",
              "             ('accuracy', 0.9288112544279392)])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YujewME13PQV",
        "outputId": "2b9eb815-f0e9-4d6a-a53d-93d6e53951a6"
      },
      "source": [
        "tf.reshape(tf.transpose(avg_metrics([i[-1] for i in cv.evs])), [-1,2,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
              "array([[[331602.2,   2775.8],\n",
              "        [   601.6,  79228.4]],\n",
              "\n",
              "       [[402422.4,   1756.6],\n",
              "        [  2005.8,   8023.2]],\n",
              "\n",
              "       [[ 89859. ,      0. ],\n",
              "        [  1925. , 322424. ]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5CKcROOnHwU",
        "outputId": "c0a1c55f-73d8-4403-fc5f-55ea15a22df8"
      },
      "source": [
        "mm = [[[331602.2,   2775.8],\n",
        "        [   601.6,  79228.4]],\n",
        "\n",
        "       [[402422.4,   1756.6],\n",
        "        [  2005.8,   8023.2]],\n",
        "\n",
        "       [[ 89859. ,      0. ],\n",
        "        [  1925. , 322424. ]]]\n",
        "fp = [i[0][1] for i in mm] \n",
        "fn = [ i[1][0] for i in mm]\n",
        "a, b, c, d, e, f = bala(fn[0], fn[1], fn[2], fp[0], fp[1], fp[2], trial=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False True False True False False\n",
            "(3.3306690738754696e-16, 601.6000000000001)==601.6, (2005.8, 770.0000000000002)==2775.8\n",
            "(0.0, 2005.8)==2005.8, (1155.0, 601.6000000000001)==1756.6\n",
            "(1155.0, 770.0000000000002)==1925.0, (0.0, 3.3306690738754696e-16)==0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8MZxj2A-qVd",
        "outputId": "6b54ab72-68a8-47b9-c8cb-341bae3cfbcd"
      },
      "source": [
        "a, b, c, d, e, f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.3306690738754696e-16,\n",
              " 601.6000000000001,\n",
              " 0.0,\n",
              " 2005.8,\n",
              " 1155.0,\n",
              " 770.0000000000002)"
            ]
          },
          "metadata": {},
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibZMYU9ar416",
        "outputId": "f281d38e-5ee8-4050-ce07-ca10d79c8dcd"
      },
      "source": [
        "fn[2], fp[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1925.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giuMgX7trtZF",
        "outputId": "082d18f4-b3f9-4469-f6ca-3f53223c448e"
      },
      "source": [
        "1756.6 -1925.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-168.4000000000001"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wncRro_q0b4",
        "outputId": "095bb0f9-2707-46ad-9823-eb60930335a0"
      },
      "source": [
        "np.sum(mm[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414208.0"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io0UEz4vp055"
      },
      "source": [
        "  print(f\"{a, b}=={fx}, {a, d}=={px}\")\n",
        "  print(f\"{c, d}=={fy}, {b, e}=={py}\")\n",
        "  print(f\"{e, f}=={fz}, {c, f}=={pz}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5LDkO-3k5Qy"
      },
      "source": [
        "##### autoint y2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkIM5AdoF8RK",
        "outputId": "c52c9d1c-ff3c-416d-f344-9a40a0f5a795"
      },
      "source": [
        "#l2 autoint\n",
        "print('train num:', len(train_ds), \"test :\", len(test_ds))\n",
        "k_fold = 5\n",
        "ds_shard = [ train_ds.shard(k_fold, i) for i in range(k_fold)]\n",
        "for i in range(k_fold):\n",
        "  index = [ i for i in range(k_fold)]\n",
        "  set_seed(i)\n",
        "  vali = ds_shard[i]\n",
        "  index.remove(i)\n",
        "  train = 1\n",
        "  for i in index:\n",
        "    if train ==1:\n",
        "      train = ds_shard[i]\n",
        "    else:\n",
        "      train = train.concatenate(ds_shard[i])\n",
        "  #model = tuner.hypermodel.build(best_hps)\n",
        "  with strategy.scope():\n",
        "    model = tf.keras.models.load_model('/content/autoint_model_l2_TPU', options=localhost_save_option)\n",
        "  print('train stars')\n",
        "  model.fit(train, epochs=10, validation_data=vali, validation_batch_size=batch_size, \n",
        "             callbacks = [ EarlyStopping(monitor='val_binary_accuracy', min_delta=0.01, patience=2)], shuffle=True)\n",
        "  print('test evaluation')\n",
        "  model.evaluate(test_ds, batch_size=batch_size)\n",
        "  print('ensure delete model')\n",
        "  del model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 7552 test : 3236\n",
            "train stars\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"Adam/gradients/PartitionedCall_14:2\", shape=(1072,), dtype=int32), values=Tensor(\"Adam/gradients/PartitionedCall_14:1\", shape=(1072, 128), dtype=float32), dense_shape=Tensor(\"Adam/gradients/PartitionedCall_14:3\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6041/6041 [==============================] - 267s 39ms/step - loss: 2.0632 - binary_accuracy: 0.9963 - auc: 0.9990 - val_loss: 0.0278 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 261s 43ms/step - loss: 0.0215 - binary_accuracy: 0.9989 - auc: 0.9996 - val_loss: 0.0041 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6041/6041 [==============================] - 261s 43ms/step - loss: 0.0025 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0013 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 80s 24ms/step - loss: 0.0744 - binary_accuracy: 0.9954 - auc: 0.9954\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 282s 41ms/step - loss: 2.0495 - binary_accuracy: 0.9941 - auc: 0.9989 - val_loss: 0.0184 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 237s 39ms/step - loss: 0.0146 - binary_accuracy: 0.9971 - auc: 0.9998 - val_loss: 0.0015 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6041/6041 [==============================] - 235s 39ms/step - loss: 0.0222 - binary_accuracy: 0.9968 - auc: 0.9995 - val_loss: 0.0024 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 71s 21ms/step - loss: 0.0486 - binary_accuracy: 0.9954 - auc: 0.9954\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 274s 40ms/step - loss: 2.0734 - binary_accuracy: 0.9943 - auc: 0.9992 - val_loss: 0.0325 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 252s 42ms/step - loss: 0.0401 - binary_accuracy: 0.9897 - auc: 0.9989 - val_loss: 0.0031 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6042/6042 [==============================] - 266s 44ms/step - loss: 0.0085 - binary_accuracy: 0.9997 - auc: 0.9997 - val_loss: 0.0015 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 81s 24ms/step - loss: 0.0544 - binary_accuracy: 0.9954 - auc: 0.9954\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 306s 44ms/step - loss: 2.0731 - binary_accuracy: 0.9941 - auc: 0.9991 - val_loss: 0.0293 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 267s 44ms/step - loss: 0.0277 - binary_accuracy: 0.9964 - auc: 0.9994 - val_loss: 0.0056 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6042/6042 [==============================] - 263s 43ms/step - loss: 0.0020 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0010 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 69s 21ms/step - loss: 0.0643 - binary_accuracy: 0.9954 - auc: 0.9954\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 268s 39ms/step - loss: 2.0735 - binary_accuracy: 0.9940 - auc: 0.9991 - val_loss: 0.0293 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 237s 39ms/step - loss: 0.0162 - binary_accuracy: 0.9981 - auc: 0.9998 - val_loss: 0.0021 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6042/6042 [==============================] - 237s 39ms/step - loss: 0.0095 - binary_accuracy: 0.9988 - auc: 0.9997 - val_loss: 0.0014 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 68s 21ms/step - loss: 0.0557 - binary_accuracy: 0.9954 - auc: 0.9954\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB0ooSAa6olP",
        "outputId": "c6b2eef0-804f-4b21-f1a9-f35fab07fbc9"
      },
      "source": [
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.0,\n",
              " 'decay_rate': 0.29,\n",
              " 'decay_steps': 10000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'ffn': True,\n",
              " 'ffn_d_size_0': 1024,\n",
              " 'ffn_d_size_1': 16,\n",
              " 'ffn_d_size_2': 176,\n",
              " 'ffn_d_size_3': 16,\n",
              " 'ffn_d_size_4': 16,\n",
              " 'ffn_d_size_5': 16,\n",
              " 'ffn_d_size_6': 16,\n",
              " 'ffn_drop_0': 0.0,\n",
              " 'ffn_drop_1': 0.0,\n",
              " 'ffn_drop_2': 0.12,\n",
              " 'ffn_drop_3': 0.0,\n",
              " 'ffn_drop_4': 0.9400000000000001,\n",
              " 'ffn_drop_5': 0.0,\n",
              " 'ffn_drop_6': 0.0,\n",
              " 'ffn_layer': False,\n",
              " 'hp_ffn_d_size': 64,\n",
              " 'hp_ffn_drop': 0.99,\n",
              " 'label_smoothing': 0.0,\n",
              " 'layers_num': 1,\n",
              " 'learning_rate': 0.0001,\n",
              " 'mha_residual': True,\n",
              " 'num_MHA_layer': 12,\n",
              " 'num_head': 4,\n",
              " 'num_of_d': 128}"
            ]
          },
          "execution_count": 60,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j2FhlNzk_e1"
      },
      "source": [
        "##### autoint y3 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbL2M_6klFtY",
        "outputId": "12c87fba-24f8-45bd-eb8c-2ffbc0b1e77e"
      },
      "source": [
        "# ffnwl2 \n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.2,\n",
              " 'decay_rate': 1.0,\n",
              " 'decay_steps': 50000,\n",
              " 'emb_layer_choice': 'linear',\n",
              " 'ffn': True,\n",
              " 'ffn_layer': True,\n",
              " 'hp_ffn_d_size': 128,\n",
              " 'hp_ffn_drop': 0.0,\n",
              " 'hp_pred_ffn_d_size': 64,\n",
              " 'hp_pred_ffn_drop': 0.8999999999999999,\n",
              " 'label_smoothing': 0.0,\n",
              " 'learning_rate': 0.00021797945520173441,\n",
              " 'mha_dropout_rate': 0.0,\n",
              " 'mha_residual': True,\n",
              " 'num_MHA_layer': 4,\n",
              " 'num_head': 4,\n",
              " 'num_of_d': 128}"
            ]
          },
          "execution_count": 83,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8fV2nJeUh_b",
        "outputId": "1ab2171e-fd10-45e0-f7b4-0f8867d85b9d"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 7552 test : 3236\n",
            "trials: 4\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 143s 24ms/step - loss: 0.0040 - categorical_accuracy: 0.9986 - auc_m: 0.9950 - auc_m_PR: 0.9910 - confusion_matrix: 193344.0000 - val_loss: 7.6564e-05 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 1.0000 - val_confusion_matrix: 48320.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 143s 24ms/step - loss: 6.0160e-05 - categorical_accuracy: 1.0000 - auc_m: 1.0000 - auc_m_PR: 0.9999 - confusion_matrix: 193344.0000 - val_loss: 3.3305e-11 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 1.0000 - val_confusion_matrix: 48320.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 53s 16ms/step - loss: 0.0673 - categorical_accuracy: 0.9935 - auc_m: 0.9952 - auc_m_PR: 0.9432 - confusion_matrix: 103552.0000\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ORWLGQVXDHw",
        "outputId": "d8bd640a-bd6d-4be0-9012-1969e926ea72"
      },
      "source": [
        "avg_metrics([i[0:-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.06518932, 0.99575142, 0.9967682 , 0.96226329])"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8A3UJx_XFmP",
        "outputId": "15be7ecd-881b-4d09-c0de-f7483b748ac4"
      },
      "source": [
        "avg_cm_scores([i[-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('precision', 0.9962876399358113),\n",
              "             ('recall', 0.9639376242955526),\n",
              "             ('F1', 0.978328828016917),\n",
              "             ('specificity', 0.9951921900113424),\n",
              "             ('accuracy', 0.9962492390507186)])"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdSKYcv7XH69",
        "outputId": "4927fe3a-bb44-4c65-d3b1-4a7565b435e3"
      },
      "source": [
        "tf.reshape(tf.transpose(avg_metrics([i[-1] for i in cv.evs])), [-1,2,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
              "array([[[334378. ,      0. ],\n",
              "        [   604.8,  79225.2]],\n",
              "\n",
              "       [[402869.8,   1309.2],\n",
              "        [     0. ,  10029. ]],\n",
              "\n",
              "       [[ 89408.4,    450.6],\n",
              "        [  1155. , 323194. ]]], dtype=float32)>"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa9p2pM9sqlf",
        "outputId": "828c129c-6b97-4d4a-be5e-d76d1034e7f6"
      },
      "source": [
        "mm = [[[334378. ,      0. ],\n",
        "        [   604.8,  79225.2]],\n",
        "\n",
        "       [[402869.8,   1309.2],\n",
        "        [     0. ,  10029. ]],\n",
        "\n",
        "       [[ 89408.4,    450.6],\n",
        "        [  1155. , 323194. ]]]\n",
        "fp = [i[0][1] for i in mm] \n",
        "fn = [ i[1][0] for i in mm]\n",
        "a, b, c, d, e, f = bala(fn[0], fn[1], fn[2], fp[0], fp[1], fp[2], trial=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False True True True False False\n",
            "(450.6999999999999, 154.1)==604.8, (0.0, 0.0)==0.0\n",
            "(0.0, 0.0)==0.0, (1155.0, 154.1)==1309.2\n",
            "(1155.0, 0.0)==1155.0, (0.0, 450.6999999999999)==450.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onhODt_I-VEY",
        "outputId": "94bffa12-d6e3-4c7a-efb1-6212b3ad64e8"
      },
      "source": [
        "a, b, c, d, e, f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(450.6999999999999, 154.1, 0.0, 0.0, 1155.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 326
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmNc7QBUvbXC",
        "outputId": "691e2813-974d-40c4-fc40-3e78751c234b"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2588/2588 [==============================] - 66s 25ms/step - loss: 0.0158 - categorical_accuracy: 0.9970 - auc: 0.9998 - val_loss: 0.0016 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2588/2588 [==============================] - 65s 25ms/step - loss: 0.0090 - categorical_accuracy: 0.9978 - auc: 1.0000 - val_loss: 0.0012 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 124s 16ms/step - loss: 0.0164 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 66s 25ms/step - loss: 0.0600 - categorical_accuracy: 0.9899 - auc: 0.9975 - val_loss: 0.0939 - val_categorical_accuracy: 0.9855 - val_auc: 0.9913\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 66s 25ms/step - loss: 0.1018 - categorical_accuracy: 0.9849 - auc: 0.9929 - val_loss: 0.1189 - val_categorical_accuracy: 0.9855 - val_auc: 0.9913\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 124s 16ms/step - loss: 0.1535 - categorical_accuracy: 0.9742 - auc: 0.9844\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 66s 25ms/step - loss: 0.1079 - categorical_accuracy: 0.9825 - auc: 0.9931 - val_loss: 0.1334 - val_categorical_accuracy: 0.9847 - val_auc: 0.9908\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 65s 25ms/step - loss: 0.1088 - categorical_accuracy: 0.9830 - auc: 0.9918 - val_loss: 0.1341 - val_categorical_accuracy: 0.9847 - val_auc: 0.9908\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 126s 16ms/step - loss: 0.1751 - categorical_accuracy: 0.9705 - auc: 0.9821\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 65s 25ms/step - loss: 0.0205 - categorical_accuracy: 0.9964 - auc: 0.9996 - val_loss: 6.8277e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 66s 25ms/step - loss: 0.0091 - categorical_accuracy: 0.9978 - auc: 1.0000 - val_loss: 4.0936e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 125s 16ms/step - loss: 0.0287 - categorical_accuracy: 0.9972 - auc: 0.9979\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 65s 25ms/step - loss: 0.0346 - categorical_accuracy: 0.9953 - auc: 0.9987 - val_loss: 0.0016 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 65s 25ms/step - loss: 0.0307 - categorical_accuracy: 0.9948 - auc: 0.9989 - val_loss: 6.7661e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 126s 16ms/step - loss: 0.0118 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ317GbbqRqs",
        "outputId": "74223b42-cea0-4693-fe13-9811a54f912d"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/10\n",
            "648/648 [==============================] - 51s 79ms/step - loss: 0.0146 - categorical_accuracy: 0.9969 - auc: 0.9999 - val_loss: 0.0010 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "648/648 [==============================] - 51s 79ms/step - loss: 0.0173 - categorical_accuracy: 0.9968 - auc: 0.9998 - val_loss: 0.0028 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 125s 16ms/step - loss: 0.0145 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 51s 78ms/step - loss: 0.0158 - categorical_accuracy: 0.9973 - auc: 0.9999 - val_loss: 0.0020 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 49s 76ms/step - loss: 0.0131 - categorical_accuracy: 0.9973 - auc: 1.0000 - val_loss: 0.0013 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 126s 16ms/step - loss: 0.0133 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 53s 81ms/step - loss: 0.0175 - categorical_accuracy: 0.9968 - auc: 0.9998 - val_loss: 0.0080 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 49s 77ms/step - loss: 0.0155 - categorical_accuracy: 0.9976 - auc: 1.0000 - val_loss: 0.0036 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 124s 16ms/step - loss: 0.0159 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 51s 79ms/step - loss: 0.0142 - categorical_accuracy: 0.9973 - auc: 1.0000 - val_loss: 0.0014 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 49s 75ms/step - loss: 0.0117 - categorical_accuracy: 0.9973 - auc: 1.0000 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 124s 16ms/step - loss: 0.0137 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 51s 80ms/step - loss: 0.0133 - categorical_accuracy: 0.9975 - auc: 1.0000 - val_loss: 0.0012 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 50s 77ms/step - loss: 0.0116 - categorical_accuracy: 0.9974 - auc: 1.0000 - val_loss: 9.5572e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 126s 16ms/step - loss: 0.0139 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f27bZvtC5NuX",
        "outputId": "bb2261ac-4aaf-49d1-b334-37e1a330a830"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/10\n",
            "648/648 [==============================] - 52s 80ms/step - loss: 0.0235 - categorical_accuracy: 0.9964 - auc: 0.9995 - val_loss: 0.0036 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "648/648 [==============================] - 49s 76ms/step - loss: 0.0133 - categorical_accuracy: 0.9976 - auc: 1.0000 - val_loss: 0.0020 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 125s 16ms/step - loss: 0.0140 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 50s 78ms/step - loss: 0.0141 - categorical_accuracy: 0.9972 - auc: 1.0000 - val_loss: 0.0012 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 49s 76ms/step - loss: 0.0353 - categorical_accuracy: 0.9952 - auc: 0.9987 - val_loss: 0.0169 - val_categorical_accuracy: 0.9982 - val_auc: 0.9987\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 125s 16ms/step - loss: 0.0373 - categorical_accuracy: 0.9955 - auc: 0.9966\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 51s 79ms/step - loss: 0.0138 - categorical_accuracy: 0.9975 - auc: 1.0000 - val_loss: 0.0013 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 49s 77ms/step - loss: 0.0117 - categorical_accuracy: 0.9975 - auc: 1.0000 - val_loss: 0.0010 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 126s 16ms/step - loss: 0.0128 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 51s 79ms/step - loss: 0.0138 - categorical_accuracy: 0.9973 - auc: 1.0000 - val_loss: 0.0010 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 48s 75ms/step - loss: 0.0112 - categorical_accuracy: 0.9975 - auc: 1.0000 - val_loss: 7.5091e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 125s 16ms/step - loss: 0.0132 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 51s 79ms/step - loss: 0.0138 - categorical_accuracy: 0.9974 - auc: 0.9999 - val_loss: 8.1320e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 49s 75ms/step - loss: 0.0119 - categorical_accuracy: 0.9974 - auc: 0.9999 - val_loss: 0.0036 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 125s 16ms/step - loss: 0.0159 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U-MHCXh_HGc",
        "outputId": "04fff97c-50aa-4888-9c38-7dd4c57c853c"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 863 test : 7767\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 14s 80ms/step - loss: 0.0147 - categorical_accuracy: 0.9973 - auc: 1.0000 - val_loss: 0.0012 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 14s 79ms/step - loss: 0.0119 - categorical_accuracy: 0.9982 - auc: 1.0000 - val_loss: 9.4252e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 129s 16ms/step - loss: 0.0119 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 14s 81ms/step - loss: 0.0138 - categorical_accuracy: 0.9977 - auc: 1.0000 - val_loss: 0.0012 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 14s 79ms/step - loss: 0.0118 - categorical_accuracy: 0.9979 - auc: 1.0000 - val_loss: 8.7172e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 128s 16ms/step - loss: 0.0119 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 14s 80ms/step - loss: 0.0125 - categorical_accuracy: 0.9983 - auc: 1.0000 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 14s 83ms/step - loss: 0.0182 - categorical_accuracy: 0.9971 - auc: 0.9996 - val_loss: 0.0126 - val_categorical_accuracy: 0.9991 - val_auc: 0.9993\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 129s 16ms/step - loss: 0.0555 - categorical_accuracy: 0.9959 - auc: 0.9969\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 14s 81ms/step - loss: 0.0168 - categorical_accuracy: 0.9976 - auc: 0.9998 - val_loss: 0.0019 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 14s 80ms/step - loss: 0.0134 - categorical_accuracy: 0.9975 - auc: 0.9999 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 127s 16ms/step - loss: 0.0119 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 14s 82ms/step - loss: 0.0139 - categorical_accuracy: 0.9979 - auc: 1.0000 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 14s 80ms/step - loss: 0.0140 - categorical_accuracy: 0.9968 - auc: 1.0000 - val_loss: 8.2235e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 128s 16ms/step - loss: 0.0117 - categorical_accuracy: 0.9992 - auc: 0.9994\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U2elmO3lNHk"
      },
      "source": [
        "#### dyconv y2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZsfeCvQoLS1",
        "outputId": "47642f9a-c9b2-4b6b-bf8f-4c491d18b00b"
      },
      "source": [
        "# dv conv l2 y2  without dnn layer parts only one layer predction ffn wo residual interacting layer\n",
        "# train 0.54 vali 0.16 test 0.3\n",
        "\n",
        "print('train num:', len(train_ds), \"test :\", len(test_ds))\n",
        "k_fold = 5\n",
        "ds_shard = [ train_ds.shard(k_fold, i) for i in range(k_fold)]\n",
        "for i in range(k_fold):\n",
        "  index = [ i for i in range(k_fold)]\n",
        "  set_seed(i)\n",
        "  vali = ds_shard[i]\n",
        "  index.remove(i)\n",
        "  train = 1\n",
        "  for i in index:\n",
        "    if train ==1:\n",
        "      train = ds_shard[i]\n",
        "    else:\n",
        "      train = train.concatenate(ds_shard[i])\n",
        "  model = tuner.hypermodel.build(best_hps)\n",
        "  #with strategy.scope():\n",
        "    #model = tuner.hypermodel.build(best_hps)\n",
        "    #model = tf.keras.models.load_model('/content/autoint_model_l2_TPU', options=localhost_save_option)\n",
        "  print('train stars')\n",
        "  model.fit(train, epochs=10, validation_data=vali, validation_batch_size=batch_size, \n",
        "             callbacks = [ EarlyStopping(monitor='val_binary_accuracy', min_delta=0.01, patience=2)], shuffle=True)\n",
        "  print('test evaluation')\n",
        "  model.evaluate(test_ds, batch_size=batch_size)\n",
        "  print('ensure delete model')\n",
        "  del model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 7552 test : 3236\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 516s 79ms/step - loss: 2.5908 - binary_accuracy: 0.9251 - auc: 0.9561 - val_loss: 0.5583 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 470s 78ms/step - loss: 0.5585 - binary_accuracy: 0.9996 - auc: 0.9999 - val_loss: 0.5577 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6041/6041 [==============================] - 474s 78ms/step - loss: 0.5579 - binary_accuracy: 0.9997 - auc: 0.9999 - val_loss: 0.5576 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 106s 32ms/step - loss: 0.5603 - binary_accuracy: 0.9954 - auc: 0.9936\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 525s 80ms/step - loss: 2.5904 - binary_accuracy: 0.9382 - auc: 0.9656 - val_loss: 0.5582 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 472s 78ms/step - loss: 0.5586 - binary_accuracy: 0.9996 - auc: 0.9999 - val_loss: 0.5579 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6041/6041 [==============================] - 474s 78ms/step - loss: 0.5581 - binary_accuracy: 0.9997 - auc: 0.9999 - val_loss: 0.5577 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 105s 32ms/step - loss: 0.5601 - binary_accuracy: 0.9954 - auc: 0.9953\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 522s 79ms/step - loss: 2.5967 - binary_accuracy: 0.9286 - auc: 0.9581 - val_loss: 0.5582 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 470s 78ms/step - loss: 0.5585 - binary_accuracy: 0.9996 - auc: 0.9999 - val_loss: 0.5578 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6042/6042 [==============================] - 473s 78ms/step - loss: 0.5580 - binary_accuracy: 0.9997 - auc: 1.0000 - val_loss: 0.5577 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 105s 32ms/step - loss: 0.5597 - binary_accuracy: 0.9954 - auc: 0.9998\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 527s 80ms/step - loss: 2.6356 - binary_accuracy: 0.8631 - auc: 0.9049 - val_loss: 0.5580 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 475s 79ms/step - loss: 0.5583 - binary_accuracy: 0.9996 - auc: 0.9999 - val_loss: 0.5578 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6042/6042 [==============================] - 472s 78ms/step - loss: 0.5580 - binary_accuracy: 0.9997 - auc: 0.9999 - val_loss: 0.5577 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 104s 32ms/step - loss: 0.5604 - binary_accuracy: 0.9954 - auc: 0.9932\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 522s 79ms/step - loss: 2.6071 - binary_accuracy: 0.9374 - auc: 0.9650 - val_loss: 0.5582 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 471s 78ms/step - loss: 0.5585 - binary_accuracy: 0.9996 - auc: 0.9999 - val_loss: 0.5578 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6042/6042 [==============================] - 472s 78ms/step - loss: 0.5580 - binary_accuracy: 0.9997 - auc: 0.9999 - val_loss: 0.5577 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 106s 32ms/step - loss: 0.5605 - binary_accuracy: 0.9954 - auc: 0.9922\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx9L50KrTmhD",
        "outputId": "8776bcb9-58fc-4f85-c9b9-2579d204a660"
      },
      "source": [
        "best_hps[0].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.73,\n",
              " 'decay_rate': 0.26,\n",
              " 'decay_steps': 100000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'hp_DCconv_drop': 0.5,\n",
              " 'hp_ffn_d_size': 608,\n",
              " 'hp_ffn_drop': 0.17,\n",
              " 'hp_head_num': 8,\n",
              " 'hp_kernel_size': 12,\n",
              " 'hp_pred_ffn_d_size': 800,\n",
              " 'hp_pred_ffn_drop': 0.09,\n",
              " 'label_smoothing': 0.49,\n",
              " 'layers_num': 8,\n",
              " 'learning_rate': 0.0001,\n",
              " 'num_of_d': 32}"
            ]
          },
          "execution_count": 52,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EDS5sU4_L62"
      },
      "source": [
        "#### dyconv y3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyv61PLseRd_",
        "outputId": "67cd220b-bf7d-46d2-fe84-f6d3a2dcc46a"
      },
      "source": [
        "# ffnwl2 \n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.2,\n",
              " 'decay_rate': 1.0,\n",
              " 'decay_steps': 10000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'hp_DCconv_drop': 0.1,\n",
              " 'hp_ffn_d_size': 256,\n",
              " 'hp_ffn_drop': 0.0,\n",
              " 'hp_head_num': 16,\n",
              " 'hp_kernel_size': 12,\n",
              " 'hp_pred_ffn_d_size': 64,\n",
              " 'hp_pred_ffn_drop': 0.6,\n",
              " 'interac_residu': False,\n",
              " 'label_smoothing': 0.0,\n",
              " 'layers_num': 4,\n",
              " 'learning_rate': 0.0001,\n",
              " 'num_of_d': 128}"
            ]
          },
          "execution_count": 63,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfSL_mSQgkDJ",
        "outputId": "97357626-f5bd-4e9c-f024-b989321f5b15"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train num: 7552 test : 3236\n",
            "trials: 4\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 244s 40ms/step - loss: 0.0300 - categorical_accuracy: 0.9892 - auc_m: 0.9742 - auc_m_PR: 0.9466 - confusion_matrix: 193344.0000 - val_loss: 1.4778e-05 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 0.9999 - val_confusion_matrix: 48320.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 244s 40ms/step - loss: 6.3836e-05 - categorical_accuracy: 1.0000 - auc_m: 1.0000 - auc_m_PR: 0.9998 - confusion_matrix: 193344.0000 - val_loss: 1.7450e-05 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 0.9999 - val_confusion_matrix: 48320.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 70s 21ms/step - loss: 0.0553 - categorical_accuracy: 0.9935 - auc_m: 0.9963 - auc_m_PR: 0.9275 - confusion_matrix: 103552.0000\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdpC0PtQhYX3",
        "outputId": "de33beab-6ce9-4ce9-8c79-09d198ac62d9"
      },
      "source": [
        "avg_metrics([i[0:-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.07262216, 0.9935192 , 0.99631841, 0.92745713])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_kLdK15hZqP",
        "outputId": "d3057074-c503-4668-b7bc-b421e81de731"
      },
      "source": [
        "avg_cm_scores([i[-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('precision', 0.9948508143424988),\n",
              "             ('recall', 0.9296231667200724),\n",
              "             ('F1', 0.9580733656883239),\n",
              "             ('specificity', 0.9922536373138428),\n",
              "             ('accuracy', 0.9948123824589605)])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Izsof0Nha9N",
        "outputId": "63a4a392-1295-4c09-e523-18c08d6dff9f"
      },
      "source": [
        "tf.reshape(tf.transpose(avg_metrics([i[-1] for i in cv.evs])), [-1,2,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
              "array([[[334378. ,      0. ],\n",
              "        [   759.4,  79070.6]],\n",
              "\n",
              "       [[401494.6,   2684.4],\n",
              "        [     0. ,  10029. ]],\n",
              "\n",
              "       [[ 89859. ,      0. ],\n",
              "        [  1925. , 322424. ]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7bKgF1DvyOO",
        "outputId": "fbd9d683-4204-45db-8ba1-bb11777c45a6"
      },
      "source": [
        "mm = [[[334378. ,      0. ],\n",
        "        [   759.4,  79070.6]],\n",
        "\n",
        "       [[401494.6,   2684.4],\n",
        "        [     0. ,  10029. ]],\n",
        "\n",
        "       [[ 89859. ,      0. ],\n",
        "        [  1925. , 322424. ]]]\n",
        "fp = [i[0][1] for i in mm] \n",
        "fn = [ i[1][0] for i in mm]\n",
        "a, b, c, d, e, f = bala(fn[0], fn[1], fn[2], fp[0], fp[1], fp[2], trial=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False True True True False False\n",
            "(0.09999999999997158, 759.3000000000001)==759.4, (0.0, 0.0)==0.0\n",
            "(0.0, 0.0)==0.0, (1925.0, 759.3000000000001)==2684.4\n",
            "(1925.0, 0.0)==1925.0, (0.0, 0.09999999999997158)==0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hysjF_oUwEik",
        "outputId": "648e65d5-f81c-4658-e460-9e16a9aace25"
      },
      "source": [
        "fn[0], fn[1], fn[2], fp[0], fp[1], fp[2],"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(764.0, 0.0, 1925.0, 0.0, 2689.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgSCwpS3JeYB",
        "outputId": "7774748f-065a-407f-d47a-eab15a8ebd5e"
      },
      "source": [
        "#3 :7\n",
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2588/2588 [==============================] - 128s 49ms/step - loss: 8.4688e-04 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0012 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 186s 24ms/step - loss: 0.0285 - categorical_accuracy: 0.9972 - auc: 0.9979\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 128s 49ms/step - loss: 4.9117e-04 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 4.5632e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 187s 24ms/step - loss: 0.0289 - categorical_accuracy: 0.9972 - auc: 0.9979\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 128s 50ms/step - loss: 5.8800e-04 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.6183e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 188s 24ms/step - loss: 0.0313 - categorical_accuracy: 0.9972 - auc: 0.9979\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 129s 50ms/step - loss: 7.7185e-04 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 4.3937e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 195s 25ms/step - loss: 0.0331 - categorical_accuracy: 0.9972 - auc: 0.9979\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 130s 50ms/step - loss: 6.2136e-04 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.2187e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 191s 25ms/step - loss: 0.0333 - categorical_accuracy: 0.9972 - auc: 0.9979\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikc5Oo3VgPjH",
        "outputId": "5d431364-3893-47f6-bfd0-1a8e07fd200d"
      },
      "source": [
        "#3:7\n",
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/10\n",
            "648/648 [==============================] - 87s 135ms/step - loss: 8.7524e-04 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 191s 25ms/step - loss: 0.0228 - categorical_accuracy: 0.9973 - auc: 0.9983\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 86s 133ms/step - loss: 5.3764e-04 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 6.9429e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 187s 24ms/step - loss: 0.0291 - categorical_accuracy: 0.9972 - auc: 0.9979\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 86s 134ms/step - loss: 7.0536e-04 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 5.0566e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 186s 24ms/step - loss: 0.0323 - categorical_accuracy: 0.9972 - auc: 0.9979\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 86s 133ms/step - loss: 4.4907e-04 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4402e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 188s 24ms/step - loss: 0.0342 - categorical_accuracy: 0.9972 - auc: 0.9979\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 86s 134ms/step - loss: 9.7483e-04 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0013 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 189s 25ms/step - loss: 0.0288 - categorical_accuracy: 0.9972 - auc: 0.9979\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFZXq0IrJCqD",
        "outputId": "af05a9b8-24b3-4605-f5c3-5cf751513eb2"
      },
      "source": [
        "# 1:9\n",
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 863 test : 7767\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 50s 158ms/step - loss: 2.4679 - categorical_accuracy: 0.7661 - auc: 0.8708 - val_loss: 1.3752 - val_categorical_accuracy: 0.9919 - val_auc: 0.9976\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 192s 24ms/step - loss: 1.4804 - categorical_accuracy: 0.9723 - auc: 0.9916\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 23s 136ms/step - loss: 2.1491 - categorical_accuracy: 0.8331 - auc: 0.9204 - val_loss: 1.3475 - val_categorical_accuracy: 0.9926 - val_auc: 0.9991\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 193s 24ms/step - loss: 1.4514 - categorical_accuracy: 0.9727 - auc: 0.9938\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 23s 135ms/step - loss: 2.1036 - categorical_accuracy: 0.8433 - auc: 0.9318 - val_loss: 1.3476 - val_categorical_accuracy: 0.9923 - val_auc: 0.9970\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 191s 24ms/step - loss: 1.4571 - categorical_accuracy: 0.9724 - auc: 0.9892\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 2.0553 - categorical_accuracy: 0.8546 - auc: 0.9437 - val_loss: 1.3372 - val_categorical_accuracy: 0.9924 - val_auc: 0.9999\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 199s 25ms/step - loss: 1.4271 - categorical_accuracy: 0.9723 - auc: 0.9975\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 24s 137ms/step - loss: 2.1531 - categorical_accuracy: 0.8198 - auc: 0.9140 - val_loss: 1.3448 - val_categorical_accuracy: 0.9924 - val_auc: 0.9981\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 194s 24ms/step - loss: 1.4472 - categorical_accuracy: 0.9723 - auc: 0.9935\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "3LsUaf2fbIV5",
        "outputId": "54318a95-5b9c-450c-9d99-08d5bd26e4be"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 22 test : 1942\n",
            "train stars\n",
            "Epoch 1/10\n",
            "5/5 [==============================] - 2s 368ms/step - loss: 2.5348 - categorical_accuracy: 0.8125 - auc: 0.8696 - val_loss: 2.4254 - val_categorical_accuracy: 0.8134 - val_auc: 0.9670\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 1s 355ms/step - loss: 2.4481 - categorical_accuracy: 0.8313 - auc: 0.8859 - val_loss: 2.4184 - val_categorical_accuracy: 0.8134 - val_auc: 0.9693\n",
            "test evaluation\n",
            "1942/1942 [==============================] - 49s 24ms/step - loss: 2.4730 - categorical_accuracy: 0.7937 - auc: 0.9630\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "5/5 [==============================] - 2s 389ms/step - loss: 2.5420 - categorical_accuracy: 0.8047 - auc: 0.8626 - val_loss: 2.4146 - val_categorical_accuracy: 0.8189 - val_auc: 0.9695\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 1s 353ms/step - loss: 2.4983 - categorical_accuracy: 0.7984 - auc: 0.8685 - val_loss: 2.3847 - val_categorical_accuracy: 0.8189 - val_auc: 0.9728\n",
            "test evaluation\n",
            " 152/1942 [=>............................] - ETA: 43s - loss: 2.4728 - categorical_accuracy: 0.7840 - auc: 0.9615"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-cfe10e1372f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mless_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCV_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-4ac2ce2765a9>\u001b[0m in \u001b[0;36mCV_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m                  callbacks = [ EarlyStopping(monitor='val_auc', min_delta=0.01, patience=1, baseline=0.99)], shuffle=True)\n\u001b[1;32m     58\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test evaluation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ensure delete model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWsave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1392\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \"\"\"\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_test_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_called_in_fit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_predict_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHE4sXXbTooD",
        "outputId": "807794ff-90f8-478d-ea05-68a09e5711b4"
      },
      "source": [
        "# dv conv l2 y3  without dnn layer parts only one layer predction ffn wo residual interacting layer\n",
        "# train 0.54 vali 0.16 test 0.3\n",
        "\n",
        "print('train num:', len(train_ds), \"test :\", len(test_ds))\n",
        "k_fold = 5\n",
        "ds_shard = [ train_ds.shard(k_fold, i) for i in range(k_fold)]\n",
        "for i in range(k_fold):\n",
        "  index = [ i for i in range(k_fold)]\n",
        "  set_seed(i)\n",
        "  vali = ds_shard[i]\n",
        "  index.remove(i)\n",
        "  train = 1\n",
        "  for i in index:\n",
        "    if train ==1:\n",
        "      train = ds_shard[i]\n",
        "    else:\n",
        "      train = train.concatenate(ds_shard[i])\n",
        "  model = tuner.hypermodel.build(best_hps)\n",
        "  #with strategy.scope():\n",
        "    #model = tuner.hypermodel.build(best_hps)\n",
        "    #model = tf.keras.models.load_model('/content/autoint_model_l2_TPU', options=localhost_save_option)\n",
        "  print('train stars')\n",
        "  model.fit(train, epochs=10, validation_data=vali, validation_batch_size=batch_size, \n",
        "             callbacks = [ EarlyStopping(monitor='val_loss', min_delta=0.01, patience=2)], shuffle=True)\n",
        "  print('test evaluation')\n",
        "  model.evaluate(test_ds, batch_size=batch_size)\n",
        "  print('ensure delete model')\n",
        "  del model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 7552 test : 3236\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 144s 23ms/step - loss: 2.5071 - categorical_accuracy: 0.8648 - auc: 0.9474 - val_loss: 0.0083 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 133s 22ms/step - loss: 0.0085 - categorical_accuracy: 0.9997 - auc: 1.0000 - val_loss: 0.0030 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6041/6041 [==============================] - 133s 22ms/step - loss: 0.0032 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0017 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 55s 17ms/step - loss: 0.0733 - categorical_accuracy: 0.9935 - auc: 0.9952\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 143s 22ms/step - loss: 2.4974 - categorical_accuracy: 0.8708 - auc: 0.9484 - val_loss: 0.0118 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 133s 22ms/step - loss: 0.0110 - categorical_accuracy: 0.9996 - auc: 1.0000 - val_loss: 0.0032 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6041/6041 [==============================] - 133s 22ms/step - loss: 0.0034 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0017 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 4/10\n",
            "6041/6041 [==============================] - 134s 22ms/step - loss: 0.0019 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0012 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 5/10\n",
            "6041/6041 [==============================] - 133s 22ms/step - loss: 0.0016 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 56s 17ms/step - loss: 0.0646 - categorical_accuracy: 0.9935 - auc: 0.9952\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 144s 23ms/step - loss: 2.5374 - categorical_accuracy: 0.8524 - auc: 0.9417 - val_loss: 0.0112 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 135s 22ms/step - loss: 0.0106 - categorical_accuracy: 0.9996 - auc: 1.0000 - val_loss: 0.0031 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6042/6042 [==============================] - 134s 22ms/step - loss: 0.0033 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0016 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 58s 17ms/step - loss: 0.0671 - categorical_accuracy: 0.9935 - auc: 0.9952\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 145s 23ms/step - loss: 2.4438 - categorical_accuracy: 0.8896 - auc: 0.9622 - val_loss: 0.0106 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 134s 22ms/step - loss: 0.0087 - categorical_accuracy: 0.9997 - auc: 1.0000 - val_loss: 0.0026 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6042/6042 [==============================] - 135s 22ms/step - loss: 0.0026 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0013 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 57s 17ms/step - loss: 0.0740 - categorical_accuracy: 0.9935 - auc: 0.9952\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 144s 23ms/step - loss: 2.4936 - categorical_accuracy: 0.8613 - auc: 0.9466 - val_loss: 0.0125 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 134s 22ms/step - loss: 0.0118 - categorical_accuracy: 0.9995 - auc: 1.0000 - val_loss: 0.0037 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "6042/6042 [==============================] - 136s 22ms/step - loss: 0.0037 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0019 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 4/10\n",
            "6042/6042 [==============================] - 135s 22ms/step - loss: 0.0021 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0013 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 5/10\n",
            "6042/6042 [==============================] - 135s 22ms/step - loss: 0.0016 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 56s 17ms/step - loss: 0.0692 - categorical_accuracy: 0.9935 - auc: 0.9952\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZknO8hJxw3V",
        "outputId": "c73ab346-65b5-4893-a408-58293815c876"
      },
      "source": [
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.0,\n",
              " 'decay_rate': 0.2,\n",
              " 'decay_steps': 15000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'hp_DCconv_drop': 0.52,\n",
              " 'hp_ffn_d_size': 992,\n",
              " 'hp_ffn_drop': 0.01,\n",
              " 'hp_head_num': 1,\n",
              " 'hp_kernel_size': 6,\n",
              " 'hp_pred_ffn_d_size': 80,\n",
              " 'hp_pred_ffn_drop': 0.71,\n",
              " 'interac_residu138025': False,\n",
              " 'interac_residu14218': False,\n",
              " 'interac_residu233453': False,\n",
              " 'interac_residu346956': False,\n",
              " 'interac_residu395188': False,\n",
              " 'interac_residu812250': False,\n",
              " 'interac_residu845717': False,\n",
              " 'label_smoothing': 0.0,\n",
              " 'layers_num': 1,\n",
              " 'learning_rate': 0.0001,\n",
              " 'num_of_d': 32}"
            ]
          },
          "execution_count": 86,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLbFsy124nxt"
      },
      "source": [
        "### AutoDis CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk9TTBz6Q9ES"
      },
      "source": [
        "train = round(len(df_y_2) //128 *0.7)*128\n",
        "test = round(len(df_y_2) //128 *0.3)*128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FgWGhZlQ9rq"
      },
      "source": [
        "train = round(len(df_y_2) //128 *0.3)*128\n",
        "test = round(len(df_y_2) //128 *0.7)*128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOUVNJqBB6Lv"
      },
      "source": [
        "train = round(len(df_y_2) //128 *0.2)*128\n",
        "test = round(len(df_y_2) //128 *0.8)*128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRsiK9lNXmQU"
      },
      "source": [
        "train = round(len(df_y_2) //128 *0.1*0.8)*128\n",
        "test = round(len(df_y_2) //128 *0.9*0.8)*128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmrDrSvrSIjJ"
      },
      "source": [
        "st_v_y_2 = pd.DataFrame(df_y_2[:train])\n",
        "st_t_y_2 = pd.DataFrame(df_y_2[train:test+train])\n",
        "V_st = st_v_y_2.value_counts().values\n",
        "T_st = st_t_y_2.value_counts().values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hfaP-2osXPA",
        "outputId": "01a46ae4-ee8d-4a3b-ebac-0d0270ec6f0a"
      },
      "source": [
        "print('train: ')\n",
        "st_example(V_st)\n",
        "print('test: ')\n",
        "st_example(T_st)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: \n",
            "753264  : 0.779\n",
            "189247  : 0.196\n",
            "24145  : 0.025\n",
            "test: \n",
            "324349  : 0.783\n",
            "79830  : 0.193\n",
            "10029  : 0.024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0aO2dm3jws_",
        "outputId": "98fc01bd-7f84-48a3-d4ba-f89796465276"
      },
      "source": [
        "np.sum(mm)/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414208.0"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0zKQMsbmvjd",
        "outputId": "51e70865-e044-44de-8323-a30f7a66c0a7"
      },
      "source": [
        "322809 + 299.2  + 0.0 + 1706.4 + 79082 +684.4 + 1540.1 + 63.7 + 8023"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414207.80000000005"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGttb2iek512",
        "outputId": "234b926a-e357-48c8-bbdb-ab744ba94771"
      },
      "source": [
        "alln"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414208"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEqoTqikjVNw",
        "outputId": "15baa726-1fe4-4391-dc67-b50f51ce7061"
      },
      "source": [
        "alln - mm[0][0][0], mm[0][1][1], mm[1][0][0], alln - mm[2][0][0], mm[2][0][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(82220.79999999999, 79082.0, 402575.2, 324648.2, 299.2)"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPIgk3T0k-sL",
        "outputId": "e3edf57e-c22e-42c0-cdac-de618d9cca08"
      },
      "source": [
        "mm[0][0][0] + mm[0][0][1] + mm[0][1][0] + mm[0][1][1], mm[0][0][0], mm[0][0][1], mm[0][1][0], mm[0][1][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(414208.0, 331987.2, 2390.8, 748.0, 79082.0)"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TINHcgb7izz6",
        "outputId": "aa60cd44-466d-49cc-c8b6-2fab6f88c4e4"
      },
      "source": [
        "np.sum([mm[0][0][0] for i in mm])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "995961.6000000001"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQzT_MEhSGUE",
        "outputId": "aa80589f-3553-42da-dcfc-26610ac550fb"
      },
      "source": [
        "st_example(V_st)\n",
        "st_example(T_st)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "325305  : 0.785\n",
            "82510  : 0.199\n",
            "6393  : 0.015\n",
            "752308  : 0.778\n",
            "186567  : 0.193\n",
            "27781  : 0.029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuMVi558sMeA",
        "outputId": "2444eb4a-831e-4ffe-a9bc-de757ea85d61"
      },
      "source": [
        "st_example(V_st)\n",
        "st_example(T_st)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87757  : 0.794\n",
            "21858  : 0.198\n",
            "849  : 0.008\n",
            "776674  : 0.781\n",
            "191872  : 0.193\n",
            "25630  : 0.026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnzUZ2x542DF"
      },
      "source": [
        "num_y = df_y_2.shape[-1]\n",
        "\n",
        "at_t_v = df_v_2[:train].copy()\n",
        "at_t_i = df_i_2[:train].copy()\n",
        "at_t_y = df_y_2[:train].copy()\n",
        "emb_at_i = df_emb_i_2[:train].copy()\n",
        "emb_at_v = df_emb_v_2[:train].copy()\n",
        "meta_at_i = df_m_2[:train].copy()\n",
        "meta_at_i_r = df_m_r_2[:train].copy()\n",
        "\n",
        "test_t_v = df_v_2[train:test+train].copy()\n",
        "test_t_i = df_i_2[train:test+train].copy()\n",
        "test_t_y = df_y_2[train:test+train].copy()\n",
        "test_emb_at_i = df_emb_i_2[train:test+train].copy()\n",
        "test_emb_at_v = df_emb_v_2[train:test+train].copy()\n",
        "test_meta_at_i = df_m_2[train:test+train].copy()\n",
        "test_meta_at_i_r = df_m_r_2[train:test+train].copy()\n",
        "\n",
        "emb_at_i = emb_at_i.reshape([-1, 2, 1])\n",
        "emb_at_v = emb_at_v.reshape([-1, 2, 128])\n",
        "at_t_i = at_t_i.reshape([-1, num_input, 1])\n",
        "at_t_v = at_t_v.reshape([-1, num_input, 1])\n",
        "at_t_y = at_t_y.reshape([-1, num_y])\n",
        "\n",
        "test_emb_at_i = test_emb_at_i.reshape([-1, 2, 1])\n",
        "test_emb_at_v = test_emb_at_v.reshape([-1, 2, 128])\n",
        "test_t_i = test_t_i.reshape([-1, num_input, 1])\n",
        "test_t_v = test_t_v.reshape([-1, num_input, 1])\n",
        "test_t_y = test_t_y.reshape([-1, num_y])\n",
        "\n",
        "meta_at_i = meta_at_i.reshape([-1, num_input, 1])\n",
        "meta_at_i_r = meta_at_i_r.reshape([-1, num_input, 1])\n",
        "\n",
        "test_meta_at_i = test_meta_at_i.reshape([-1, num_input, 1])\n",
        "test_meta_at_i_r = test_meta_at_i_r.reshape([-1, num_input, 1])\n",
        "\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(({\"value\": at_t_v, \"index\": at_t_i, 'emb_value':emb_at_v, 'emb_index':emb_at_i, \n",
        "                                                'meta_index':meta_at_i, 'reverse_meta_index':meta_at_i_r}, at_t_y))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(({\"value\": test_t_v, \"index\": test_t_i, 'emb_value':test_emb_at_v, 'emb_index':test_emb_at_i, \n",
        "                                              'meta_index':test_meta_at_i, 'reverse_meta_index':test_meta_at_i_r}, test_t_y))\n",
        "\n",
        "train_ds = train_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
        "test_ds = test_ds.batch(batch_size=batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxU8N20Ysa_f"
      },
      "source": [
        "# cv test function \n",
        "from copy import deepcopy\n",
        "import IPython\n",
        "\n",
        "class CV():\n",
        "  def __init__(self, model, train, test, k_fold, epochs=10, less_sample = False, patience=2):\n",
        "    self.model = model\n",
        "    self.ds_shard = [ train_ds.shard(k_fold, i) for i in range(k_fold)]\n",
        "    self.k_fold = k_fold\n",
        "    self.train_num = len(train)\n",
        "    self.test_num = len(test)\n",
        "    self.less_sample = less_sample\n",
        "    self.epochs=epochs\n",
        "    self.patience =patience\n",
        "    self.evs = []\n",
        "\n",
        "  def set_seed(self, seed):\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # optional\n",
        "    # for numpy.random\n",
        "    np.random.seed(seed)\n",
        "    # for built-in random\n",
        "    random.seed(seed)\n",
        "    # for hash seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "  def split_ds(self, i):\n",
        "    index = [ i for i in range(self.k_fold)]\n",
        "    vali = self.ds_shard[i]\n",
        "    index.remove(i)\n",
        "    train = 1\n",
        "    for i in index:\n",
        "      if train ==1:\n",
        "        train = self.ds_shard[i]\n",
        "      else:\n",
        "        train = train.concatenate(self.ds_shard[i])\n",
        "    if self.less_sample:\n",
        "      return vali, train\n",
        "    else:\n",
        "      return train, vali\n",
        "\n",
        "  def CV_test(self):\n",
        "    '''\n",
        "    model, tarin, test, k-fold\n",
        "    '''\n",
        "    for i in range(self.k_fold):\n",
        "      print('train num:', self.train_num, \"test :\", self.test_num)\n",
        "      print(f\"trials: {i}\")\n",
        "      train_model = self.model\n",
        "      Wsave = self.model.get_weights()\n",
        "      \n",
        "      self.set_seed(i)\n",
        "      train, vali = self.split_ds(i)\n",
        "        \n",
        "      #model = tuner.hypermodel.build(best_hps)\n",
        "      #with strategy.scope():\n",
        "        #model = tuner.hypermodel.build(best_hps)\n",
        "        #model = tf.keras.models.load_model('/content/autoint_model_l2_TPU', options=localhost_save_option)\n",
        "      print('train stars')\n",
        "      train_model.fit(train, epochs=self.epochs, validation_data=vali, validation_batch_size=batch_size, \n",
        "                 callbacks = [ EarlyStopping(monitor='val_auc_m', min_delta=0.001, patience=self.patience, \n",
        "                                             baseline=0.99, restore_best_weights=True)], shuffle=True)\n",
        "      print('test evaluation')\n",
        "      ev = train_model.evaluate(test_ds, batch_size=batch_size)\n",
        "      self.evs.append(ev)\n",
        "      print('ensure delete model')\n",
        "      self.model.set_weights(Wsave)\n",
        "      del train_model\n",
        "      IPython.display.clear_output(wait = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMJ6mPZtIomC"
      },
      "source": [
        "def avg_metrics(data):\n",
        "  result = []\n",
        "  n_metrics = len(data[0])\n",
        "  for i in range(len(data)):\n",
        "    for j in range(n_metrics):\n",
        "      if i == 0:\n",
        "        result.append(data[i][j])\n",
        "      else:\n",
        "        result[j] += data[i][j]\n",
        "  \n",
        "  return np.array(result)/len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6Tg0yxoNTxP",
        "outputId": "f8827da8-119a-44b7-94f6-3ea9794a3a49"
      },
      "source": [
        "ss =np.array([1,2,3])\n",
        "ss += np.array([1,2,3])\n",
        "ss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 4, 6])"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWVzxXPxLkkh"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def cm_scores(cm, tp=3, fp=2, fn=1, tn=0):\n",
        "  precision = (cm[tp]+1)/(cm[tp]+cm[fp]+1)\n",
        "  recall = (cm[tp]+1)/(cm[tp]+cm[fn]+1)\n",
        "  F1 = (2*precision*recall)/(precision+recall)\n",
        "  specificity = cm[tn]/(cm[tn]+cm[fp])\n",
        "  accuracy = (cm[tp]+[tn])/(cm[tp]+[tn]+cm[fp]+[fn])\n",
        "  return np.array([precision, recall, F1, specificity, accuracy])\n",
        "\n",
        "def avg_cm_scores(cms):\n",
        "  scores_n = [\"precision\", \"recall\", \"F1\", \"specificity\", \"accuracy\"]\n",
        "  scores = None\n",
        "  num_cla = len(cms[0][0])\n",
        "  for i in range(len(cms)):\n",
        "    if i == 0:\n",
        "      scores = cm_scores(cms[i])\n",
        "    else:\n",
        "      scores += cm_scores(cms[i])\n",
        "\n",
        "    #print(scores)\n",
        "  scores = np.sum(np.array(scores)/len(cms),axis=1)/num_cla\n",
        "  #scores = np.array(scores)/len(cms)\n",
        "  return OrderedDict({i : j for i, j in zip(scores_n, scores)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99unyCJtIZdI"
      },
      "source": [
        "cs_evs= cv.evs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "11j5E4YFKEKQ",
        "outputId": "f50cf5e1-fb1d-4517-996d-0a09504c2c4f"
      },
      "source": [
        "avg_metrics([i[0:-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-5386c64323dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavg_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha-B5IwGPEyu",
        "outputId": "dc6f3d89-40b5-41c6-8783-e8b000fdfac9"
      },
      "source": [
        "avg_cm_scores([i[-1] for i in cs_evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('precision', 0.9999230961004892),\n",
              "             ('recall', 0.9995675762494405),\n",
              "             ('F1', 0.9997451543807984),\n",
              "             ('specificity', 0.9999807397524516),\n",
              "             ('accuracy', 0.9998770253557329)])"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "OeptIBhXMKxR",
        "outputId": "ebf3fc47-65f9-487a-b68e-270ea3c70948"
      },
      "source": [
        "avg_metrics([i[-1] for i in cs_evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-1948dd736cbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavg_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcs_evs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cs_evs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktIuefXV1Atp"
      },
      "source": [
        "#### mlp autodis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_E1_vhF1JWF"
      },
      "source": [
        "###### autodis mlp y2 l2 layer 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpXvar63aJYZ",
        "outputId": "4f0a2ea8-9c27-4ead-a26d-b74c383ac284"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 2)[1]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.0,\n",
              " 'decay_rate': 1.0,\n",
              " 'decay_steps': 10000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'ffn_d_size': 96,\n",
              " 'ffn_emb_drop': 0.9,\n",
              " 'label_smoothing': 0.0,\n",
              " 'learning_rate': 0.0003964709931796167,\n",
              " 'meta_size': 20,\n",
              " 'num_mlp_layer': 4,\n",
              " 'num_of_d': 128,\n",
              " 'temperatue_rate': 3.6301936559448597e-06}"
            ]
          },
          "execution_count": 118,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4imM9eOkZkKG",
        "outputId": "226445f5-481e-40c3-eb32-4943ac54657f"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 7552 test : 3236\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 147s 23ms/step - loss: 0.5443 - binary_accuracy: 0.8815 - auc: 0.8405 - val_loss: 0.0050 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 135s 22ms/step - loss: 0.0075 - binary_accuracy: 0.9997 - auc: 0.9997 - val_loss: 0.0088 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 59s 18ms/step - loss: 0.0873 - binary_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 139s 23ms/step - loss: 0.0078 - binary_accuracy: 0.9997 - auc: 0.9995 - val_loss: 0.0047 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 134s 22ms/step - loss: 0.0039 - binary_accuracy: 0.9999 - auc: 0.9998 - val_loss: 0.0054 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 56s 17ms/step - loss: 0.0784 - binary_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 136s 23ms/step - loss: 0.0037 - binary_accuracy: 0.9999 - auc: 0.9998 - val_loss: 0.0069 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 135s 22ms/step - loss: 0.0029 - binary_accuracy: 0.9999 - auc: 0.9999 - val_loss: 0.0017 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 55s 16ms/step - loss: 0.0949 - binary_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 133s 22ms/step - loss: 0.0031 - binary_accuracy: 0.9998 - auc: 0.9999 - val_loss: 0.0025 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 133s 22ms/step - loss: 0.0027 - binary_accuracy: 0.9999 - auc: 0.9999 - val_loss: 0.0042 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 53s 16ms/step - loss: 0.0532 - binary_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 133s 22ms/step - loss: 0.0021 - binary_accuracy: 0.9999 - auc: 0.9999 - val_loss: 0.0034 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 132s 22ms/step - loss: 0.0023 - binary_accuracy: 0.9999 - auc: 0.9999 - val_loss: 0.0019 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 55s 16ms/step - loss: 0.0767 - binary_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTk_2e3T0ZS7",
        "outputId": "e4b6e2c7-6882-43dd-8905-089df6fa71f8"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 3)[2]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.0,\n",
              " 'decay_rate': 0.7000000000000001,\n",
              " 'decay_steps': 10000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'ffn_d_size': 16,\n",
              " 'ffn_emb_drop': 0.7000000000000001,\n",
              " 'label_smoothing': 0.2,\n",
              " 'learning_rate': 0.0001,\n",
              " 'meta_size': 20,\n",
              " 'num_mlp_layer': 4,\n",
              " 'num_of_d': 128,\n",
              " 'temperatue_rate': 2.001529651224235e-06}"
            ]
          },
          "execution_count": 96,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBPjbchN0pUL",
        "outputId": "1019feef-b71f-4786-b005-f2236901c221"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 7552 test : 3236\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 134s 21ms/step - loss: 1.0625 - binary_accuracy: 0.9018 - auc: 0.9015 - val_loss: 0.3819 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 128s 21ms/step - loss: 0.3279 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3618 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 53s 16ms/step - loss: 0.3736 - binary_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 130s 22ms/step - loss: 0.3259 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3453 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 127s 21ms/step - loss: 0.3255 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3358 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 53s 16ms/step - loss: 0.3475 - binary_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 134s 22ms/step - loss: 0.3252 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3302 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 131s 22ms/step - loss: 0.3252 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3312 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 56s 17ms/step - loss: 0.3430 - binary_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 133s 22ms/step - loss: 0.3251 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3300 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 131s 22ms/step - loss: 0.3251 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3305 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 55s 16ms/step - loss: 0.3414 - binary_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 133s 22ms/step - loss: 0.3251 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3305 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 133s 22ms/step - loss: 0.3251 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3305 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 55s 16ms/step - loss: 0.3412 - binary_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzoFlmQLvkPu"
      },
      "source": [
        "# ffnwl2 \n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 3)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yj_XtY4o-4s",
        "outputId": "edbf0682-197f-47a4-9a99-3e56b790b977"
      },
      "source": [
        "# ffnwl2 \n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 3)[1]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.0,\n",
              " 'decay_rate': 0.7,\n",
              " 'decay_steps': 55000,\n",
              " 'emb_layer_choice': 'linear',\n",
              " 'ffn_d_size': 64,\n",
              " 'ffn_emb_drop': 0.9,\n",
              " 'label_smoothing': 0.0,\n",
              " 'learning_rate': 0.00015908172817343834,\n",
              " 'meta_size': 16,\n",
              " 'num_mlp_layer': 4,\n",
              " 'num_of_d': 128,\n",
              " 'temperatue_rate': 0.01}"
            ]
          },
          "execution_count": 159,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yAHBEbWWpqs5",
        "outputId": "8bf372be-029a-4527-d67b-febc82a2bb31"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 131s 20ms/step - loss: 2.6194 - binary_accuracy: 0.8721 - auc: 0.8316 - val_loss: 0.0870 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 120s 20ms/step - loss: 0.0756 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0442 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 131s 17ms/step - loss: 0.0489 - binary_accuracy: 0.9980 - auc: 0.9987\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 124s 21ms/step - loss: 0.0320 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0217 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 122s 20ms/step - loss: 0.5459 - binary_accuracy: 0.8008 - auc: 0.8686 - val_loss: 1.9936 - val_binary_accuracy: 0.2166 - val_auc: 0.5000\n",
            "Epoch 3/10\n",
            "6041/6041 [==============================] - 121s 20ms/step - loss: 1.7067 - binary_accuracy: 0.2203 - auc: 0.5036 - val_loss: 1.4581 - val_binary_accuracy: 0.2166 - val_auc: 0.5000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 131s 17ms/step - loss: 1.4497 - binary_accuracy: 0.2217 - auc: 0.5000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 124s 21ms/step - loss: 1.2338 - binary_accuracy: 0.2213 - auc: 0.5023 - val_loss: 1.0456 - val_binary_accuracy: 0.2121 - val_auc: 0.5000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 123s 20ms/step - loss: 0.8843 - binary_accuracy: 0.2213 - auc: 0.5021 - val_loss: 0.7553 - val_binary_accuracy: 0.2121 - val_auc: 0.5000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 131s 17ms/step - loss: 0.7534 - binary_accuracy: 0.2217 - auc: 0.5000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 123s 20ms/step - loss: 0.6652 - binary_accuracy: 0.6116 - auc: 0.5024 - val_loss: 0.5940 - val_binary_accuracy: 0.7847 - val_auc: 0.5000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 120s 20ms/step - loss: 0.5604 - binary_accuracy: 0.7791 - auc: 0.5021 - val_loss: 0.5322 - val_binary_accuracy: 0.7847 - val_auc: 0.5000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 129s 17ms/step - loss: 0.5383 - binary_accuracy: 0.7783 - auc: 0.5000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 121s 20ms/step - loss: 0.5308 - binary_accuracy: 0.7793 - auc: 0.5031 - val_loss: 0.5196 - val_binary_accuracy: 0.7863 - val_auc: 0.5000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 120s 20ms/step - loss: 0.5278 - binary_accuracy: 0.7793 - auc: 0.5011 - val_loss: 0.5189 - val_binary_accuracy: 0.7863 - val_auc: 0.5000\n",
            "test evaluation\n",
            "2632/7552 [=========>....................] - ETA: 1:20 - loss: 0.5564 - binary_accuracy: 0.7565 - auc: 0.5000"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-160-acc02c0178b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCV_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-116-43d153763378>\u001b[0m in \u001b[0;36mCV_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m                  callbacks = [ EarlyStopping(monitor='val_auc', min_delta=0.01, patience=0)], shuffle=True)\n\u001b[1;32m     57\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test evaluation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m       \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ensure delete model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0;31m#model.load_weights(Wsave)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1392\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \"\"\"\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_test_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhdOCYDDpuiQ",
        "outputId": "12f2f60c-2fe0-4e2a-e70d-dac15df8b18d"
      },
      "source": [
        "# ffnwl2 \n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 3)[2]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.0,\n",
              " 'decay_rate': 0.5,\n",
              " 'decay_steps': 70000,\n",
              " 'emb_layer_choice': 'dense',\n",
              " 'ffn_d_size': 96,\n",
              " 'ffn_emb_drop': 0.9,\n",
              " 'label_smoothing': 0.0,\n",
              " 'learning_rate': 0.0001,\n",
              " 'meta_size': 8,\n",
              " 'num_mlp_layer': 4,\n",
              " 'num_of_d': 128,\n",
              " 'temperatue_rate': 0.01}"
            ]
          },
          "execution_count": 161,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "ZhMshzjkpxNw",
        "outputId": "2fed4c97-b55e-4685-b06b-e7dda724c9d3"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, epochs=2)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/2\n",
            "6041/6041 [==============================] - 115s 19ms/step - loss: 0.1085 - binary_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0858 - val_binary_accuracy: 1.0000 - val_auc: 1.0000\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-b5516cf91617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCV_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-165-47ec40ce44c0>\u001b[0m in \u001b[0;36mCV_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m       train_model.fit(train, epochs=self.epochs, validation_data=vali, validation_batch_size=batch_size, \n\u001b[1;32m     57\u001b[0m                  callbacks = [ EarlyStopping(monitor='val_auc', min_delta=0.01, patience=0, \n\u001b[0;32m---> 58\u001b[0;31m                                              baseline=0.5, restore_best_weights=True)], shuffle=True)\n\u001b[0m\u001b[1;32m     59\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test evaluation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1776\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Restoring model weights from the end of the best epoch.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0mexpected_num_weights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mexpected_num_weights\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m       raise ValueError(\n\u001b[1;32m   1854\u001b[0m           \u001b[0;34m'You called `set_weights(weights)` on layer \"%s\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vtnWcyadAeo"
      },
      "source": [
        "##### autodis mlp y3 layer 4 wo l2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xva1EwQdEdV",
        "outputId": "da3e5f5d-841c-4999-b434-799ba6d9cc76"
      },
      "source": [
        "# ffnwl2 \n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.0,\n",
              " 'decay_rate': 0.5,\n",
              " 'decay_steps': 40000,\n",
              " 'emb_layer_choice': 'linear',\n",
              " 'ffn_d_size': 512,\n",
              " 'ffn_emb_drop': 0.9,\n",
              " 'label_smoothing': 0.0,\n",
              " 'learning_rate': 0.00029573142758853525,\n",
              " 'meta_size': 20,\n",
              " 'num_mlp_layer': 4,\n",
              " 'num_of_d': 128,\n",
              " 'temperatue_rate': 0.01}"
            ]
          },
          "execution_count": 183,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGbccGglio5a",
        "outputId": "9da1b9c3-61da-4e64-e632-ff92e0d945af"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, epochs=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train num: 7552 test : 3236\n",
            "trials: 4\n",
            "train stars\n",
            "Epoch 1/5\n",
            "6042/6042 [==============================] - 110s 18ms/step - loss: 0.0052 - categorical_accuracy: 0.9979 - auc_m: 0.9915 - auc_m_PR: 0.9845 - confusion_matrix: 193344.0000 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 1.0000 - val_confusion_matrix: 48320.0000\n",
            "Epoch 2/5\n",
            "6042/6042 [==============================] - 110s 18ms/step - loss: 1.5616e-10 - categorical_accuracy: 1.0000 - auc_m: 1.0000 - auc_m_PR: 1.0000 - confusion_matrix: 193344.0000 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 1.0000 - val_confusion_matrix: 48320.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 45s 13ms/step - loss: 0.0579 - categorical_accuracy: 0.9935 - auc_m: 0.9963 - auc_m_PR: 0.9277 - confusion_matrix: 103552.0000\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWy4xGAA-OCi",
        "outputId": "8b2f1ca1-2104-4377-a3be-9fd20fffb3b7"
      },
      "source": [
        "avg_metrics([i[0:-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.12517757, 0.98963372, 0.96190569, 0.88145264])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "722J9N7A-Pg_",
        "outputId": "f4a2f52c-d774-4e85-be09-53d9c663e038"
      },
      "source": [
        "avg_cm_scores([i[-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('precision', 0.9286340622245915),\n",
              "             ('recall', 0.9485206325848896),\n",
              "             ('F1', 0.9025808615435381),\n",
              "             ('specificity', 0.9920320550600689),\n",
              "             ('accuracy', 0.9285956261960702)])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHSWfszf-QeV",
        "outputId": "7feb36d3-3a5c-4295-e070-c8735cca2348"
      },
      "source": [
        "tf.reshape(tf.transpose(avg_metrics([i[-1] for i in cv.evs])), [-1,2,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
              "array([[[3.319872e+05, 2.390800e+03],\n",
              "        [7.480000e+02, 7.908200e+04]],\n",
              "\n",
              "       [[4.025752e+05, 1.603800e+03],\n",
              "        [2.005800e+03, 8.023200e+03]],\n",
              "\n",
              "       [[8.955980e+04, 2.992000e+02],\n",
              "        [1.540000e+03, 3.228090e+05]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EQ25DPmfezS"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C52J9WeGfWKS"
      },
      "source": [
        "mm = [[[3.319872e+05, 2.390800e+03],\n",
        "        [7.480000e+02, 7.908200e+04]],\n",
        "\n",
        "       [[4.025752e+05, 1.603800e+03],\n",
        "        [2.005800e+03, 8.023200e+03]],\n",
        "\n",
        "       [[8.955980e+04, 2.992000e+02],\n",
        "        [1.540000e+03, 3.228090e+05]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLNFfrY0fH4g",
        "outputId": "85d6c6b9-141d-4f0d-f45d-426d2cf87ae4"
      },
      "source": [
        "fp = [i[0][1] for i in mm] \n",
        "fn = [ i[1][0] for i in mm]\n",
        "a, b, c, d, e, f = bala(fn[0], fn[1], fn[2], fp[0], fp[1], fp[2], trial=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False True False True False False\n",
            "(3.3306690738754696e-16, 601.6000000000001)==601.6, (2005.8, 770.0000000000002)==2775.8\n",
            "(0.0, 2005.8)==2005.8, (1155.0, 601.6000000000001)==1756.6\n",
            "(1155.0, 770.0000000000002)==1925.0, (0.0, 3.3306690738754696e-16)==0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2IYzY5v_WrY",
        "outputId": "7a0582dc-0754-409b-c919-52727b002dc1"
      },
      "source": [
        "a, b, c, d, e, f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.3306690738754696e-16,\n",
              " 601.6000000000001,\n",
              " 0.0,\n",
              " 2005.8,\n",
              " 1155.0,\n",
              " 770.0000000000002)"
            ]
          },
          "metadata": {},
          "execution_count": 320
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-7vPm8FgILJ",
        "outputId": "1e7c47c4-fb30-4142-d2f2-76be2de330e4"
      },
      "source": [
        "3236 * 128"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414208"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pecJaEuPvmex",
        "outputId": "6314176a-f50b-42f1-f34f-ae02e47b92a8"
      },
      "source": [
        "np.argmax(0, -1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPkdSaaOmqCh"
      },
      "source": [
        "def bala(fp0,fp1,fp2, fn0,fn1,fn2, sm_u=0.1, trial=5):\n",
        "  a, b, c, d, e, f = [0.0]*6\n",
        "  a, d, e = fp0, fp1, fp2\n",
        "  for i in range(trial):\n",
        "    u =  round(comp(b,a,c,fn2), 1)\n",
        "    a -= u\n",
        "    b += u\n",
        "    u = round(comp(c,d,f, fn0), 1)\n",
        "    c -= u\n",
        "    d += u\n",
        "    u = round(comp(f, e, b, fn1), 1)\n",
        "    e -= u\n",
        "    f += u\n",
        "\n",
        "\n",
        "  print(\n",
        "    a+b == fp0,\n",
        "    c + d == fp1,\n",
        "    e + f == fp2,\n",
        "    fn0 == d+f,\n",
        "    fn1 == e+b,\n",
        "    fn2 == c+a)\n",
        "  print(f\"{a, b}=={fp0}, {d, f}=={fn0}\")\n",
        "  print(f\"{c, d}=={fp1}, {e, b}=={fn1}\")\n",
        "  print(f\"{e, f}=={fp2}, {c, a}=={fn2}\")\n",
        "\n",
        "  return(a,b,c,d,e,f)\n",
        "\n",
        "def min_u(a, b, c, x):\n",
        "  return np.argmin((x-(b+c)/2, a/2))\n",
        "\n",
        "\n",
        "def comp(a, b, c, x):\n",
        "    if (x - b -c) > 0:\n",
        "      if (a != 0 and a > 0 )or b < 0:\n",
        "        l = [(x-b+c)/2, a/2]\n",
        "        u = round(l[np.argmin(l)], 1)\n",
        "      else:\n",
        "        u=0   \n",
        "    elif (x -b -c) < 0:\n",
        "      if (b != 0 and b > 0) or a < 0:\n",
        "        l = [(b+c-x)/2, b/2]\n",
        "        u = round(l[np.argmin(l)], 1)\n",
        "      else:\n",
        "        u=0 \n",
        "    else:\n",
        "      u = 0\n",
        "\n",
        "    if abs(u) < 0.1:\n",
        "      u = 0\n",
        "\n",
        "    #print(u)\n",
        "\n",
        "\n",
        "    return u"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHgkycS-Tfvf",
        "outputId": "39ad32fd-9c47-475b-fcae-8c221577262d"
      },
      "source": [
        "np.argmin([1,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7kkQWtSIYi3",
        "outputId": "27b01ffe-14e8-4efb-b18d-7e475b628136"
      },
      "source": [
        "a= mm[0][0][1]\n",
        "b =0\n",
        "c = 0\n",
        "x = mm[1][1][0]\n",
        "for i in range(5): \n",
        "\n",
        "  l =  comp(a, b, c, x)\n",
        "  a -= l\n",
        "  b += l\n",
        "\n",
        "print(l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9mfZnKfNRrS",
        "outputId": "9c5bc199-f018-445d-ec42-658bcfd1b767"
      },
      "source": [
        "fp[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2390.8"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgsytDGvRz73",
        "outputId": "f603252f-4745-46ce-b5f9-e8c71f09144c"
      },
      "source": [
        "comp(0.0, 1540.0, 0.0, 1603.8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ABXSCFLTSyk",
        "outputId": "12209415-b4b1-490c-e026-e036b97b1e72"
      },
      "source": [
        "685.4 +1706.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2391.8"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pzkae83UVU1E",
        "outputId": "30c7838f-c3f1-4dff-9a04-f312a9f53b79"
      },
      "source": [
        "1 - -1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z40v4YQxYO8z",
        "outputId": "6e46b6ab-b8e0-412b-aa25-3e833af161f8"
      },
      "source": [
        "a + b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2390.7999999999993"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUxBG0TYYUld",
        "outputId": "d4bf8807-dc3a-45bc-c939-65c0eabf066d"
      },
      "source": [
        "fp[0], fp[1], fp[2],"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2390.8, 1603.8, 299.2)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaXVwutGl8Ss"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GHw1SN-sGjc",
        "outputId": "1691ef04-7a02-4a58-e9f7-a1a9e92d3ece"
      },
      "source": [
        "fp = [i[0][1] for i in mm] \n",
        "fn = [ i[1][0] for i in mm]\n",
        "a, b, c, d, e, f = bala(fn[0], fn[1], fn[2], fp[0], fp[1], fp[2], trial=100)\n",
        "fn[0], fn[1], fn[2], a+d, b+e, c+f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True True True False False False\n",
            "(684.3999999999999, 1706.3999999999994)==2390.8, (684.3999999999999, 63.7)==748.0\n",
            "(1540.1, 63.7)==1603.8, (1706.3999999999994, 299.2)==2005.8\n",
            "(299.2, 0.0)==299.2, (1540.1, 0.0)==1540.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(748.0, 2005.8, 1540.0, 748.0999999999999, 2005.5999999999995, 1540.1)"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYa_7UVgMYjL",
        "outputId": "f0423269-d420-4523-c482-1270b50a0f77"
      },
      "source": [
        "a+d, a, d, b+e, c+f, "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(748.0999999999999,\n",
              " 684.3999999999999,\n",
              " 1706.3999999999994,\n",
              " 2005.5999999999995,\n",
              " 1540.1)"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znYkaZKeN4Pp",
        "outputId": "7d29a84b-1d76-4cd3-d5f7-1cbac8ee15d1"
      },
      "source": [
        "[ i[1][1] for i in mm], [ i[0][1] for i in mm]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([79082.0, 8023.2, 322809.0], [2390.8, 1603.8, 299.2])"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g5MvEzlfpHZ",
        "outputId": "672fbe59-2992-4127-c132-e63c71e68c0d"
      },
      "source": [
        "{\"fp\": [i[0][1] for i in mm], \"fn\":[ i[1][0] for i in mm]}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fn': [748.0, 2005.8, 1540.0], 'fp': [2390.8, 1603.8, 299.2]}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnYGW1J5hSTu",
        "outputId": "742b4dfc-7195-468e-9454-ec33800db63e"
      },
      "source": [
        "np.sum([ i[0][0] for i in mm]) + "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "824122.2000000001"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcM9SHQLgMb0",
        "outputId": "698c08d3-9c4b-439d-9b61-7b3db6f48a2c"
      },
      "source": [
        "def mm_mul(confusion):\n",
        "  for i in "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4293.799999999988"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoqppznawN4q",
        "outputId": "d26e8ccf-c17a-4f0c-bcc8-179f34bd436a"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, epochs=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/5\n",
            "2588/2588 [==============================] - 56s 22ms/step - loss: 10.3300 - categorical_accuracy: 0.7656 - auc: 0.9094 - val_loss: 2.2775 - val_categorical_accuracy: 0.7994 - val_auc: 0.9571\n",
            "Epoch 2/5\n",
            "2588/2588 [==============================] - 56s 22ms/step - loss: 0.6716 - categorical_accuracy: 0.9767 - auc: 0.9987 - val_loss: 0.1094 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/5\n",
            "2588/2588 [==============================] - 56s 21ms/step - loss: 0.0296 - categorical_accuracy: 0.9996 - auc: 1.0000 - val_loss: 0.0030 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 119s 15ms/step - loss: 0.0108 - categorical_accuracy: 0.9972 - auc: 1.0000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "2589/2589 [==============================] - 55s 21ms/step - loss: 13.0463 - categorical_accuracy: 0.7481 - auc: 0.8830 - val_loss: 3.7263 - val_categorical_accuracy: 0.7834 - val_auc: 0.9683\n",
            "Epoch 2/5\n",
            "2589/2589 [==============================] - 55s 21ms/step - loss: 1.3999 - categorical_accuracy: 0.9266 - auc: 0.9932 - val_loss: 0.3219 - val_categorical_accuracy: 0.9985 - val_auc: 1.0000\n",
            "Epoch 3/5\n",
            "2589/2589 [==============================] - 56s 21ms/step - loss: 0.1035 - categorical_accuracy: 0.9971 - auc: 1.0000 - val_loss: 0.0143 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 120s 15ms/step - loss: 0.0234 - categorical_accuracy: 0.9972 - auc: 1.0000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "2589/2589 [==============================] - 56s 22ms/step - loss: 15.5183 - categorical_accuracy: 0.7360 - auc: 0.8696 - val_loss: 5.4088 - val_categorical_accuracy: 0.7879 - val_auc: 0.9641\n",
            "Epoch 2/5\n",
            "2589/2589 [==============================] - 56s 21ms/step - loss: 2.4556 - categorical_accuracy: 0.8378 - auc: 0.9703 - val_loss: 0.6764 - val_categorical_accuracy: 0.9909 - val_auc: 0.9978\n",
            "Epoch 3/5\n",
            "2589/2589 [==============================] - 56s 22ms/step - loss: 0.2730 - categorical_accuracy: 0.9948 - auc: 0.9999 - val_loss: 0.0632 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 121s 15ms/step - loss: 0.0722 - categorical_accuracy: 0.9973 - auc: 1.0000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "2589/2589 [==============================] - 56s 22ms/step - loss: 18.4659 - categorical_accuracy: 0.7163 - auc: 0.8621 - val_loss: 7.7155 - val_categorical_accuracy: 0.7847 - val_auc: 0.9456\n",
            "Epoch 2/5\n",
            "2589/2589 [==============================] - 55s 21ms/step - loss: 3.9659 - categorical_accuracy: 0.7945 - auc: 0.9376 - val_loss: 1.4708 - val_categorical_accuracy: 0.9836 - val_auc: 0.9791\n",
            "Epoch 3/5\n",
            "2589/2589 [==============================] - 55s 21ms/step - loss: 0.6544 - categorical_accuracy: 0.9854 - auc: 0.9976 - val_loss: 0.2304 - val_categorical_accuracy: 0.9995 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 121s 15ms/step - loss: 0.2483 - categorical_accuracy: 0.9979 - auc: 0.9995\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "2589/2589 [==============================] - 56s 22ms/step - loss: 21.8693 - categorical_accuracy: 0.7029 - auc: 0.8350 - val_loss: 10.6660 - val_categorical_accuracy: 0.7863 - val_auc: 0.9064\n",
            "Epoch 2/5\n",
            "2589/2589 [==============================] - 55s 21ms/step - loss: 6.0155 - categorical_accuracy: 0.7851 - auc: 0.8994 - val_loss: 2.9259 - val_categorical_accuracy: 0.7863 - val_auc: 0.9703\n",
            "Epoch 3/5\n",
            "2589/2589 [==============================] - 55s 21ms/step - loss: 1.4422 - categorical_accuracy: 0.7852 - auc: 0.9693 - val_loss: 0.5720 - val_categorical_accuracy: 0.7863 - val_auc: 0.9752\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 119s 15ms/step - loss: 0.5843 - categorical_accuracy: 0.7783 - auc: 0.9719\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-ORw9XM3ueS",
        "outputId": "f4e8c009-8ac8-418e-876d-fdf35cb860a8"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, epochs=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/5\n",
            "648/648 [==============================] - 51s 79ms/step - loss: 32.4384 - categorical_accuracy: 0.6820 - auc: 0.7975 - val_loss: 26.3549 - val_categorical_accuracy: 0.7856 - val_auc: 0.8850\n",
            "Epoch 2/5\n",
            "648/648 [==============================] - 49s 76ms/step - loss: 22.6929 - categorical_accuracy: 0.7211 - auc: 0.8490 - val_loss: 19.5468 - val_categorical_accuracy: 0.2194 - val_auc: 0.6173\n",
            "Epoch 3/5\n",
            "648/648 [==============================] - 49s 75ms/step - loss: 16.9550 - categorical_accuracy: 0.4753 - auc: 0.7715 - val_loss: 14.5317 - val_categorical_accuracy: 0.7856 - val_auc: 0.8697\n",
            "Epoch 4/5\n",
            "648/648 [==============================] - 48s 74ms/step - loss: 12.7440 - categorical_accuracy: 0.7846 - auc: 0.8866 - val_loss: 11.1350 - val_categorical_accuracy: 0.7856 - val_auc: 0.8876\n",
            "Epoch 5/5\n",
            "648/648 [==============================] - 49s 76ms/step - loss: 9.7647 - categorical_accuracy: 0.7846 - auc: 0.8875 - val_loss: 8.5247 - val_categorical_accuracy: 0.7856 - val_auc: 0.9149\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 126s 16ms/step - loss: 8.5801 - categorical_accuracy: 0.7783 - auc: 0.9052\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "647/647 [==============================] - 52s 80ms/step - loss: 35.4118 - categorical_accuracy: 0.6790 - auc: 0.7671 - val_loss: 30.3278 - val_categorical_accuracy: 0.7859 - val_auc: 0.8853\n",
            "Epoch 2/5\n",
            "647/647 [==============================] - 47s 73ms/step - loss: 26.8031 - categorical_accuracy: 0.7731 - auc: 0.8612 - val_loss: 23.6819 - val_categorical_accuracy: 0.7859 - val_auc: 0.8853\n",
            "Epoch 3/5\n",
            "647/647 [==============================] - 46s 71ms/step - loss: 21.1532 - categorical_accuracy: 0.7821 - auc: 0.8785 - val_loss: 18.8320 - val_categorical_accuracy: 0.7859 - val_auc: 0.8394\n",
            "Epoch 4/5\n",
            "647/647 [==============================] - 46s 71ms/step - loss: 16.8663 - categorical_accuracy: 0.7828 - auc: 0.8792 - val_loss: 15.0380 - val_categorical_accuracy: 0.7859 - val_auc: 0.8853\n",
            "Epoch 5/5\n",
            "647/647 [==============================] - 46s 71ms/step - loss: 13.4640 - categorical_accuracy: 0.7828 - auc: 0.8722 - val_loss: 11.9927 - val_categorical_accuracy: 0.7859 - val_auc: 0.8853\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 124s 16ms/step - loss: 11.9943 - categorical_accuracy: 0.7783 - auc: 0.8748\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "647/647 [==============================] - 52s 80ms/step - loss: 36.1123 - categorical_accuracy: 0.6108 - auc: 0.6841 - val_loss: 31.4342 - val_categorical_accuracy: 0.4755 - val_auc: 0.7237\n",
            "Epoch 2/5\n",
            "647/647 [==============================] - 49s 76ms/step - loss: 27.9043 - categorical_accuracy: 0.4225 - auc: 0.7295 - val_loss: 24.5970 - val_categorical_accuracy: 0.7847 - val_auc: 0.8685\n",
            "Epoch 3/5\n",
            "647/647 [==============================] - 48s 75ms/step - loss: 22.0901 - categorical_accuracy: 0.7879 - auc: 0.8878 - val_loss: 19.8217 - val_categorical_accuracy: 0.7847 - val_auc: 0.8811\n",
            "Epoch 4/5\n",
            "647/647 [==============================] - 49s 75ms/step - loss: 17.8183 - categorical_accuracy: 0.7879 - auc: 0.8884 - val_loss: 16.0122 - val_categorical_accuracy: 0.7847 - val_auc: 0.8939\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 127s 16ms/step - loss: 16.0767 - categorical_accuracy: 0.7783 - auc: 0.8873\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "647/647 [==============================] - 51s 80ms/step - loss: 36.8674 - categorical_accuracy: 0.6929 - auc: 0.7629 - val_loss: 32.5691 - val_categorical_accuracy: 0.7855 - val_auc: 0.8392\n",
            "Epoch 2/5\n",
            "647/647 [==============================] - 48s 74ms/step - loss: 29.3251 - categorical_accuracy: 0.7743 - auc: 0.8689 - val_loss: 26.3824 - val_categorical_accuracy: 0.7855 - val_auc: 0.8392\n",
            "Epoch 3/5\n",
            "647/647 [==============================] - 48s 75ms/step - loss: 23.9084 - categorical_accuracy: 0.7821 - auc: 0.8697 - val_loss: 21.6046 - val_categorical_accuracy: 0.7855 - val_auc: 0.8851\n",
            "Epoch 4/5\n",
            "647/647 [==============================] - 49s 76ms/step - loss: 19.6035 - categorical_accuracy: 0.7841 - auc: 0.8630 - val_loss: 17.7217 - val_categorical_accuracy: 0.7855 - val_auc: 0.8851\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 126s 16ms/step - loss: 17.7228 - categorical_accuracy: 0.7783 - auc: 0.8748\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "647/647 [==============================] - 52s 80ms/step - loss: 37.2094 - categorical_accuracy: 0.6816 - auc: 0.7092 - val_loss: 33.1306 - val_categorical_accuracy: 0.7851 - val_auc: 0.8388\n",
            "Epoch 2/5\n",
            "647/647 [==============================] - 48s 74ms/step - loss: 29.9941 - categorical_accuracy: 0.7572 - auc: 0.8354 - val_loss: 27.1331 - val_categorical_accuracy: 0.7851 - val_auc: 0.8848\n",
            "Epoch 3/5\n",
            "647/647 [==============================] - 49s 76ms/step - loss: 24.6846 - categorical_accuracy: 0.5051 - auc: 0.7378 - val_loss: 22.3366 - val_categorical_accuracy: 0.1993 - val_auc: 0.6168\n",
            "Epoch 4/5\n",
            "647/647 [==============================] - 49s 76ms/step - loss: 20.0910 - categorical_accuracy: 0.6367 - auc: 0.8556 - val_loss: 18.1372 - val_categorical_accuracy: 0.7851 - val_auc: 0.8845\n",
            "Epoch 5/5\n",
            "647/647 [==============================] - 49s 75ms/step - loss: 16.4538 - categorical_accuracy: 0.7863 - auc: 0.8869 - val_loss: 14.9235 - val_categorical_accuracy: 0.7851 - val_auc: 0.8849\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 126s 16ms/step - loss: 14.9921 - categorical_accuracy: 0.7783 - auc: 0.8776\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi_otxm9-4Dd",
        "outputId": "0bf6b463-3335-472d-8851-d4f9f63ac914"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, epochs=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 863 test : 7767\n",
            "train stars\n",
            "Epoch 1/5\n",
            "173/173 [==============================] - 13s 74ms/step - loss: 40.6088 - categorical_accuracy: 0.5580 - auc: 0.4777 - val_loss: 39.1446 - val_categorical_accuracy: 0.7952 - val_auc: 0.5000\n",
            "Epoch 2/5\n",
            "173/173 [==============================] - 13s 76ms/step - loss: 37.9031 - categorical_accuracy: 0.6661 - auc: 0.7508 - val_loss: 36.7176 - val_categorical_accuracy: 0.7952 - val_auc: 0.8936\n",
            "Epoch 3/5\n",
            "173/173 [==============================] - 13s 75ms/step - loss: 35.6572 - categorical_accuracy: 0.7123 - auc: 0.8116 - val_loss: 34.6288 - val_categorical_accuracy: 0.7952 - val_auc: 0.8464\n",
            "Epoch 4/5\n",
            "173/173 [==============================] - 13s 75ms/step - loss: 33.6882 - categorical_accuracy: 0.7454 - auc: 0.8302 - val_loss: 32.7684 - val_categorical_accuracy: 0.7952 - val_auc: 0.8464\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 124s 15ms/step - loss: 32.7682 - categorical_accuracy: 0.7812 - auc: 0.8359\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "173/173 [==============================] - 13s 76ms/step - loss: 40.8939 - categorical_accuracy: 0.5223 - auc: 0.4704 - val_loss: 39.6956 - val_categorical_accuracy: 0.7950 - val_auc: 0.5002\n",
            "Epoch 2/5\n",
            "173/173 [==============================] - 13s 75ms/step - loss: 38.6199 - categorical_accuracy: 0.6674 - auc: 0.6375 - val_loss: 37.5705 - val_categorical_accuracy: 0.7950 - val_auc: 0.8463\n",
            "Epoch 3/5\n",
            "173/173 [==============================] - 13s 75ms/step - loss: 36.6022 - categorical_accuracy: 0.7170 - auc: 0.8106 - val_loss: 35.6515 - val_categorical_accuracy: 0.7950 - val_auc: 0.8463\n",
            "Epoch 4/5\n",
            "173/173 [==============================] - 13s 76ms/step - loss: 34.7645 - categorical_accuracy: 0.7495 - auc: 0.8287 - val_loss: 33.8903 - val_categorical_accuracy: 0.7950 - val_auc: 0.8463\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 124s 15ms/step - loss: 33.8909 - categorical_accuracy: 0.7812 - auc: 0.8359\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "173/173 [==============================] - 13s 76ms/step - loss: 41.0122 - categorical_accuracy: 0.5377 - auc: 0.4630 - val_loss: 39.9165 - val_categorical_accuracy: 0.7938 - val_auc: 0.5000\n",
            "Epoch 2/5\n",
            "173/173 [==============================] - 13s 75ms/step - loss: 38.9161 - categorical_accuracy: 0.6835 - auc: 0.5691 - val_loss: 37.9323 - val_categorical_accuracy: 0.7938 - val_auc: 0.8454\n",
            "Epoch 3/5\n",
            "173/173 [==============================] - 13s 75ms/step - loss: 37.0130 - categorical_accuracy: 0.7298 - auc: 0.8199 - val_loss: 36.1059 - val_categorical_accuracy: 0.7938 - val_auc: 0.8454\n",
            "Epoch 4/5\n",
            "173/173 [==============================] - 13s 76ms/step - loss: 35.2518 - categorical_accuracy: 0.7548 - auc: 0.8321 - val_loss: 34.4069 - val_categorical_accuracy: 0.7938 - val_auc: 0.8454\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 124s 15ms/step - loss: 34.4069 - categorical_accuracy: 0.7812 - auc: 0.8359\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "172/172 [==============================] - 13s 77ms/step - loss: 41.0771 - categorical_accuracy: 0.5394 - auc: 0.4606 - val_loss: 40.0388 - val_categorical_accuracy: 0.7940 - val_auc: 0.5000\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - 13s 76ms/step - loss: 39.0838 - categorical_accuracy: 0.6882 - auc: 0.5153 - val_loss: 38.1417 - val_categorical_accuracy: 0.7940 - val_auc: 0.7443\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - 13s 75ms/step - loss: 37.2556 - categorical_accuracy: 0.7376 - auc: 0.8233 - val_loss: 36.3791 - val_categorical_accuracy: 0.7940 - val_auc: 0.8455\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - 13s 75ms/step - loss: 35.5501 - categorical_accuracy: 0.7555 - auc: 0.8330 - val_loss: 34.7280 - val_categorical_accuracy: 0.7940 - val_auc: 0.8455\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 124s 15ms/step - loss: 34.7298 - categorical_accuracy: 0.7812 - auc: 0.8359\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/5\n",
            "172/172 [==============================] - 13s 78ms/step - loss: 41.1128 - categorical_accuracy: 0.5325 - auc: 0.4626 - val_loss: 40.1058 - val_categorical_accuracy: 0.7942 - val_auc: 0.5000\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - 13s 76ms/step - loss: 39.1772 - categorical_accuracy: 0.6672 - auc: 0.5106 - val_loss: 38.2592 - val_categorical_accuracy: 0.7942 - val_auc: 0.5000\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - 13s 76ms/step - loss: 37.3927 - categorical_accuracy: 0.6960 - auc: 0.7989 - val_loss: 36.5343 - val_categorical_accuracy: 0.7942 - val_auc: 0.8456\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - 13s 75ms/step - loss: 35.7202 - categorical_accuracy: 0.7128 - auc: 0.8154 - val_loss: 34.9122 - val_categorical_accuracy: 0.7942 - val_auc: 0.8456\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 123s 15ms/step - loss: 34.9105 - categorical_accuracy: 0.7812 - auc: 0.8359\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wXbyP84uGPK"
      },
      "source": [
        "#### autodis autoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8uEZ2PguUCG"
      },
      "source": [
        "##### autodis autoin y2 l2 wo ffn "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FXH--ul5org",
        "outputId": "5600131c-1b8e-471b-e4a7-6166456fcbd6"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "model = model = tuner.hypermodel.build(best_hps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9ueO0tm4gv_",
        "outputId": "07ad7ec9-0bb2-441a-eae0-9614b1b4dfae"
      },
      "source": [
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.0,\n",
              " 'decay_rate': 0.9400000000000001,\n",
              " 'decay_steps': 10000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'ffn': True,\n",
              " 'ffn_layer': True,\n",
              " 'hp_ffn_d_size': 448,\n",
              " 'hp_ffn_drop': 0.6000000000000001,\n",
              " 'hp_pred_ffn_d_size': 320,\n",
              " 'hp_pred_ffn_drop': 0.15000000000000002,\n",
              " 'label_smoothing': 0.16,\n",
              " 'learning_rate': 0.0001,\n",
              " 'meta_size': 8,\n",
              " 'mha_dropout_rate': 0.6000000000000001,\n",
              " 'mha_residual': False,\n",
              " 'num_MHA_layer': 4,\n",
              " 'num_head': 4,\n",
              " 'num_of_d': 32,\n",
              " 'temperatue_rate': 3.0406227989841867e-06}"
            ]
          },
          "execution_count": 145,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-VEgzLr4jo6",
        "outputId": "eb8d082f-73e1-4eb1-93e1-1b62c73130bc"
      },
      "source": [
        "# trainvali 7 test 3\n",
        "print('train num:', len(train_ds), \"test :\", len(test_ds))\n",
        "k_fold = 5\n",
        "ds_shard = [ train_ds.shard(k_fold, i) for i in range(k_fold)]\n",
        "for i in range(k_fold):\n",
        "  index = [ i for i in range(k_fold)]\n",
        "  set_seed(i)\n",
        "  vali = ds_shard[i]\n",
        "  index.remove(i)\n",
        "  train = 1\n",
        "  for i in index:\n",
        "    if train ==1:\n",
        "      train = ds_shard[i]\n",
        "    else:\n",
        "      train = train.concatenate(ds_shard[i])\n",
        "  model = tuner.hypermodel.build(best_hps)\n",
        "  #with strategy.scope():\n",
        "    #model = tuner.hypermodel.build(best_hps)\n",
        "    #model = tf.keras.models.load_model('/content/autoint_model_l2_TPU', options=localhost_save_option)\n",
        "  print('train stars')\n",
        "  model.fit(train, epochs=10, validation_data=vali, validation_batch_size=batch_size, \n",
        "             callbacks = [ EarlyStopping(monitor='val_auc', min_delta=0.01, patience=0)], shuffle=True)\n",
        "  print('test evaluation')\n",
        "  model.evaluate(test_ds, batch_size=batch_size)\n",
        "  print('ensure delete model')\n",
        "  del model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 7552 test : 3236\n",
            "model compiled\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 169s 25ms/step - loss: 0.5894 - categorical_accuracy: 0.9256 - auc: 0.9275 - val_loss: 0.3064 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 149s 25ms/step - loss: 0.3043 - categorical_accuracy: 0.9997 - auc: 0.9999 - val_loss: 0.2968 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 58s 17ms/step - loss: 0.3063 - categorical_accuracy: 0.9954 - auc: 0.9954\n",
            "ensure delete model\n",
            "model compiled\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 169s 25ms/step - loss: 0.6680 - categorical_accuracy: 0.8550 - auc: 0.7836 - val_loss: 0.2869 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 150s 25ms/step - loss: 0.2864 - categorical_accuracy: 0.9999 - auc: 0.9999 - val_loss: 0.2838 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 59s 17ms/step - loss: 0.2933 - categorical_accuracy: 0.9954 - auc: 0.9978\n",
            "ensure delete model\n",
            "model compiled\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 168s 25ms/step - loss: 0.5948 - categorical_accuracy: 0.9124 - auc: 0.9080 - val_loss: 0.4476 - val_categorical_accuracy: 0.9104 - val_auc: 0.9951\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 150s 25ms/step - loss: 0.3049 - categorical_accuracy: 0.9998 - auc: 0.9999 - val_loss: 0.2957 - val_categorical_accuracy: 0.9997 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 58s 17ms/step - loss: 0.3065 - categorical_accuracy: 0.9943 - auc: 0.9975\n",
            "ensure delete model\n",
            "model compiled\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 166s 25ms/step - loss: 0.6284 - categorical_accuracy: 0.9203 - auc: 0.9283 - val_loss: 0.7528 - val_categorical_accuracy: 0.7834 - val_auc: 0.5073\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 150s 25ms/step - loss: 0.3339 - categorical_accuracy: 0.9861 - auc: 0.9893 - val_loss: 0.3054 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 57s 17ms/step - loss: 0.3149 - categorical_accuracy: 0.9953 - auc: 0.9958\n",
            "ensure delete model\n",
            "model compiled\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 167s 25ms/step - loss: 0.6635 - categorical_accuracy: 0.9219 - auc: 0.9114 - val_loss: 0.4003 - val_categorical_accuracy: 0.9988 - val_auc: 0.9999\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 150s 25ms/step - loss: 0.3973 - categorical_accuracy: 0.9989 - auc: 0.9994 - val_loss: 0.3904 - val_categorical_accuracy: 0.9998 - val_auc: 0.9999\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 60s 18ms/step - loss: 0.3907 - categorical_accuracy: 0.9951 - auc: 0.9969\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-TAoQqB7VMj",
        "outputId": "8f8e47ee-67e9-421b-fe89-cd957e9ec716"
      },
      "source": [
        "# trainvali 3 test 7\n",
        "print('train num:', len(train_ds), \"test :\", len(test_ds))\n",
        "k_fold = 5\n",
        "ds_shard = [ train_ds.shard(k_fold, i) for i in range(k_fold)]\n",
        "for i in range(k_fold):\n",
        "  index = [ i for i in range(k_fold)]\n",
        "  set_seed(i)\n",
        "  vali = ds_shard[i]\n",
        "  index.remove(i)\n",
        "  train = 1\n",
        "  for i in index:\n",
        "    if train ==1:\n",
        "      train = ds_shard[i]\n",
        "    else:\n",
        "      train = train.concatenate(ds_shard[i])\n",
        "  model = tuner.hypermodel.build(best_hps)\n",
        "  #with strategy.scope():\n",
        "    #model = tuner.hypermodel.build(best_hps)\n",
        "    #model = tf.keras.models.load_model('/content/autoint_model_l2_TPU', options=localhost_save_option)\n",
        "  print('train stars')\n",
        "  model.fit(train, epochs=10, validation_data=vali, validation_batch_size=batch_size, \n",
        "             callbacks = [ EarlyStopping(monitor='val_auc', min_delta=0.01, patience=0)], shuffle=True)\n",
        "  print('test evaluation')\n",
        "  model.evaluate(test_ds, batch_size=batch_size)\n",
        "  print('ensure delete model')\n",
        "  del model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "model compiled\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2588/2588 [==============================] - 83s 26ms/step - loss: 0.8391 - categorical_accuracy: 0.8153 - auc: 0.7959 - val_loss: 0.7136 - val_categorical_accuracy: 0.8211 - val_auc: 0.9161\n",
            "Epoch 2/10\n",
            "2588/2588 [==============================] - 66s 25ms/step - loss: 0.4220 - categorical_accuracy: 0.9954 - auc: 0.9967 - val_loss: 0.4035 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 142s 18ms/step - loss: 0.4070 - categorical_accuracy: 0.9983 - auc: 0.9990\n",
            "ensure delete model\n",
            "model compiled\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 85s 27ms/step - loss: 0.8239 - categorical_accuracy: 0.8588 - auc: 0.8244 - val_loss: 0.3531 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 66s 26ms/step - loss: 0.3432 - categorical_accuracy: 0.9993 - auc: 0.9999 - val_loss: 0.3203 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 143s 18ms/step - loss: 0.3224 - categorical_accuracy: 0.9989 - auc: 0.9990\n",
            "ensure delete model\n",
            "model compiled\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 84s 27ms/step - loss: 0.7661 - categorical_accuracy: 0.8845 - auc: 0.8861 - val_loss: 0.3437 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 66s 26ms/step - loss: 0.3346 - categorical_accuracy: 0.9996 - auc: 0.9999 - val_loss: 0.3148 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 143s 18ms/step - loss: 0.3188 - categorical_accuracy: 0.9980 - auc: 0.9989\n",
            "ensure delete model\n",
            "model compiled\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 84s 27ms/step - loss: 0.8604 - categorical_accuracy: 0.8004 - auc: 0.6770 - val_loss: 0.3990 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 67s 26ms/step - loss: 0.3921 - categorical_accuracy: 0.9993 - auc: 0.9997 - val_loss: 0.3817 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 140s 18ms/step - loss: 0.3876 - categorical_accuracy: 0.9976 - auc: 0.9986\n",
            "ensure delete model\n",
            "model compiled\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 83s 26ms/step - loss: 0.9115 - categorical_accuracy: 0.7626 - auc: 0.5249 - val_loss: 0.3082 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 65s 25ms/step - loss: 0.3020 - categorical_accuracy: 0.9998 - auc: 1.0000 - val_loss: 0.2890 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 136s 17ms/step - loss: 0.2928 - categorical_accuracy: 0.9980 - auc: 0.9998\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKo87LY501gH"
      },
      "source": [
        "##### autoint y3 l2 ffnl2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "516-HY4xdQ4h",
        "outputId": "601ecea6-5e7d-402f-be79-9cb911354b4b"
      },
      "source": [
        "#7:3\n",
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 7552 test : 3236\n",
            "trials: 4\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 148s 24ms/step - loss: 4.5382e-10 - categorical_accuracy: 1.0000 - auc_m: 1.0000 - auc_m_PR: 1.0000 - confusion_matrix: 193344.0000 - val_loss: 1.8505e-11 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 1.0000 - val_confusion_matrix: 48320.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 147s 24ms/step - loss: 4.7724e-10 - categorical_accuracy: 1.0000 - auc_m: 1.0000 - auc_m_PR: 1.0000 - confusion_matrix: 193344.0000 - val_loss: 1.6038e-11 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 1.0000 - val_confusion_matrix: 48320.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 54s 16ms/step - loss: 0.1732 - categorical_accuracy: 0.9935 - auc_m: 0.9962 - auc_m_PR: 0.9669 - confusion_matrix: 103552.0000\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIsfWhZFY46h",
        "outputId": "0a32a0c1-fbd0-4ab6-df35-297f09c9bdfd"
      },
      "source": [
        "avg_metrics([i[0:-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.17366705, 0.99354672, 0.99618119, 0.96687502])"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWpeh6zAUpUP",
        "outputId": "12762b1e-2f5f-4e3f-a301-47799ed56bbe"
      },
      "source": [
        "avg_cm_scores([i[-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('precision', 0.9948983788490295),\n",
              "             ('recall', 0.968943198521932),\n",
              "             ('F1', 0.9814844131469727),\n",
              "             ('specificity', 0.9922606547673544),\n",
              "             ('accuracy', 0.9948672096306258)])"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcMXwjB7MaNA",
        "outputId": "c912510d-8ee8-430c-bf80-81634487a4ba"
      },
      "source": [
        "tf.reshape(tf.transpose(avg_metrics([i[-1] for i in cv.evs])), [-1,2,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
              "array([[[1928227.4,   11165. ],\n",
              "        [   4338.4,  458675.6]],\n",
              "\n",
              "       [[2339899.8,    4338.4],\n",
              "        [      0. ,   58168.2]],\n",
              "\n",
              "       [[ 521182.2,       0. ],\n",
              "        [  11165. , 1870059.2]]], dtype=float32)>"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMUTc-GctjHG",
        "outputId": "24a58021-ea1c-4804-f0c7-5fbc1ddba87e"
      },
      "source": [
        "mm = [[[1928227.4,   11165. ],\n",
        "        [   4338.4,  458675.6]],\n",
        "\n",
        "       [[2339899.8,    4338.4],\n",
        "        [      0. ,   58168.2]],\n",
        "\n",
        "       [[ 521182.2,       0. ],\n",
        "        [  11165. , 1870059.2]]]\n",
        "fp = [i[0][1] for i in mm] \n",
        "fn = [ i[1][0] for i in mm]\n",
        "a, b, c, d, e, f = bala( fn[0], fn[1], fn[2], fp[0], fp[1], fp[2], trial=100)\n",
        "a, b, c, d, e, f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False True True False False False\n",
            "(0.0999999999999428, 4338.3)==4338.4, (0.0, 11164.9)==11165.0\n",
            "(0.0, 0.0)==0.0, (0.1000000000002875, 4338.3)==4338.4\n",
            "(0.1000000000002875, 11164.9)==11165.0, (0.0, 0.0999999999999428)==0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0999999999999428, 4338.3, 0.0, 0.0, 0.1000000000002875, 11164.9)"
            ]
          },
          "metadata": {},
          "execution_count": 322
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5zRu_sD2YYB",
        "outputId": "d8f3eba6-040a-4318-b443-34fb07e3886e"
      },
      "source": [
        "458675.6/5.8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79082.0"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSxzvgj2xvai"
      },
      "source": [
        "  print(f\"{a, b}=={fp0}, {d, f}=={fn0}\")\n",
        "  print(f\"{c, d}=={fp1}, {e, b}=={fn1}\")\n",
        "  print(f\"{e, f}=={fp2}, {c, a}=={fn2}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBMp-BhD2IRE",
        "outputId": "0e2c103b-18a1-41e8-e6ac-f8b8c0782d25"
      },
      "source": [
        "for i in [a, b, c, d, e, f]:\n",
        "  print(i/5.8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.017241379310334967\n",
            "747.9827586206898\n",
            "0.0\n",
            "0.0\n",
            "0.017241379310394395\n",
            "1924.9827586206898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sul7hpDY2WAD",
        "outputId": "e8afa12d-cf0d-47fa-8f66-22017ffa553f"
      },
      "source": [
        "17.4/3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.8"
            ]
          },
          "metadata": {},
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPqh5u42t1G9",
        "outputId": "7519f28c-69ed-46d6-adb2-68e31ac0be8a"
      },
      "source": [
        "np.sum(mm) /17.4, 414208"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(414208.00000000006, 414208)"
            ]
          },
          "metadata": {},
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzH-8gO9EeDe",
        "outputId": "3dc555b4-8b9e-4149-8940-dfe82692de3f"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "trials: 4\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 64s 25ms/step - loss: 2.9176e-10 - categorical_accuracy: 1.0000 - auc_m: 1.0000 - auc_m_PR: 1.0000 - confusion_matrix: 82848.0000 - val_loss: 8.6377e-12 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 1.0000 - val_confusion_matrix: 20704.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 64s 25ms/step - loss: 3.5255e-10 - categorical_accuracy: 1.0000 - auc_m: 1.0000 - auc_m_PR: 1.0000 - confusion_matrix: 82848.0000 - val_loss: 1.1517e-11 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 1.0000 - val_confusion_matrix: 20704.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 128s 16ms/step - loss: 0.0742 - categorical_accuracy: 0.9972 - auc_m: 0.9984 - auc_m_PR: 0.9869 - confusion_matrix: 241664.0000\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNqlAMIJQqXv",
        "outputId": "ea39524a-eed1-4aca-8d52-fb5867d13c2c"
      },
      "source": [
        "avg_metrics([i[0:-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.07421215, 0.99723482, 0.99836129, 0.9869405 ])"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzjuf8n4QslB",
        "outputId": "bf14b6b1-d2dd-4985-b607-4ec5a6bfac65"
      },
      "source": [
        "avg_cm_scores([i[-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('precision', 0.9978106419245402),\n",
              "             ('recall', 0.9878425796826681),\n",
              "             ('F1', 0.9927647511164347),\n",
              "             ('specificity', 0.9967129826545715),\n",
              "             ('accuracy', 0.9977964217357972)])"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5m_xH_OQuVs",
        "outputId": "48126210-434a-4918-b1ed-3ba549fc2153"
      },
      "source": [
        "tf.reshape(tf.transpose(avg_metrics([i[-1] for i in cv.evs])), [-1,2,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
              "array([[[7.78164e+05, 1.92500e+03],\n",
              "        [7.48000e+02, 1.85819e+05]],\n",
              "\n",
              "       [[9.38127e+05, 7.48000e+02],\n",
              "        [0.00000e+00, 2.77810e+04]],\n",
              "\n",
              "       [[2.14348e+05, 0.00000e+00],\n",
              "        [1.92500e+03, 7.50383e+05]]], dtype=float32)>"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjOGj_nQt8Fr"
      },
      "source": [
        "mm = [[[7.78164e+05, 1.92500e+03],\n",
        "        [7.48000e+02, 1.85819e+05]],\n",
        "\n",
        "       [[9.38127e+05, 7.48000e+02],\n",
        "        [0.00000e+00, 2.77810e+04]],\n",
        "\n",
        "       [[2.14348e+05, 0.00000e+00],\n",
        "        [1.92500e+03, 7.50383e+05]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wsXKS8bt9zL",
        "outputId": "f38888d1-2932-47c3-87ee-5f3b1ed60a5e"
      },
      "source": [
        "np.sum(mm) /3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "966656.0"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5NNjdh7MCQ0",
        "outputId": "c0fcfa61-1f80-4734-851e-1b23591c0824"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/10\n",
            "648/648 [==============================] - 54s 83ms/step - loss: 0.5736 - categorical_accuracy: 0.9726 - auc: 0.9906 - val_loss: 0.1052 - val_categorical_accuracy: 0.9844 - val_auc: 0.9999\n",
            "Epoch 2/10\n",
            "648/648 [==============================] - 49s 76ms/step - loss: 0.0525 - categorical_accuracy: 0.9988 - auc: 1.0000 - val_loss: 0.0304 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 125s 16ms/step - loss: 0.0465 - categorical_accuracy: 0.9972 - auc: 0.9990\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 53s 82ms/step - loss: 0.6677 - categorical_accuracy: 0.9576 - auc: 0.9970 - val_loss: 0.1745 - val_categorical_accuracy: 0.9848 - val_auc: 0.9999\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 50s 77ms/step - loss: 0.0942 - categorical_accuracy: 0.9962 - auc: 1.0000 - val_loss: 0.0512 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 126s 16ms/step - loss: 0.0655 - categorical_accuracy: 0.9977 - auc: 0.9990\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 53s 81ms/step - loss: 0.8789 - categorical_accuracy: 0.9695 - auc: 0.9749 - val_loss: 0.4056 - val_categorical_accuracy: 0.9845 - val_auc: 0.9768\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 50s 77ms/step - loss: 0.2041 - categorical_accuracy: 0.9956 - auc: 0.9948 - val_loss: 0.0493 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "647/647 [==============================] - 50s 78ms/step - loss: 0.0350 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0250 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 126s 16ms/step - loss: 0.0422 - categorical_accuracy: 0.9976 - auc: 0.9990\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 53s 82ms/step - loss: 0.6347 - categorical_accuracy: 0.9861 - auc: 0.9989 - val_loss: 0.1588 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 50s 77ms/step - loss: 0.0857 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0451 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 125s 16ms/step - loss: 0.0608 - categorical_accuracy: 0.9980 - auc: 0.9990\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 53s 82ms/step - loss: 0.8884 - categorical_accuracy: 0.9691 - auc: 0.9743 - val_loss: 0.4065 - val_categorical_accuracy: 0.9844 - val_auc: 0.9769\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 50s 77ms/step - loss: 0.3230 - categorical_accuracy: 0.9852 - auc: 0.9772 - val_loss: 0.2792 - val_categorical_accuracy: 0.9844 - val_auc: 0.9769\n",
            "Epoch 3/10\n",
            "647/647 [==============================] - 50s 78ms/step - loss: 0.2629 - categorical_accuracy: 0.9852 - auc: 0.9772 - val_loss: 0.2545 - val_categorical_accuracy: 0.9844 - val_auc: 0.9769\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 126s 16ms/step - loss: 0.2987 - categorical_accuracy: 0.9479 - auc: 0.9699\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYQsvWjpPz89",
        "outputId": "c02c52a0-cfd6-4d28-8dbf-c0053a33e334"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 863 test : 7767\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 14s 83ms/step - loss: 1.5801 - categorical_accuracy: 0.9393 - auc: 0.9658 - val_loss: 0.9889 - val_categorical_accuracy: 0.9919 - val_auc: 0.9789\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 14s 81ms/step - loss: 0.7947 - categorical_accuracy: 0.9937 - auc: 0.9782 - val_loss: 0.6415 - val_categorical_accuracy: 0.9919 - val_auc: 0.9790\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 14s 81ms/step - loss: 0.5589 - categorical_accuracy: 0.9937 - auc: 0.9782 - val_loss: 0.4847 - val_categorical_accuracy: 0.9919 - val_auc: 0.9790\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 129s 16ms/step - loss: 0.5021 - categorical_accuracy: 0.9733 - auc: 0.9759\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 14s 82ms/step - loss: 1.7393 - categorical_accuracy: 0.9307 - auc: 0.9650 - val_loss: 1.1653 - val_categorical_accuracy: 0.9926 - val_auc: 0.9790\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 14s 81ms/step - loss: 0.9456 - categorical_accuracy: 0.9912 - auc: 0.9783 - val_loss: 0.7671 - val_categorical_accuracy: 0.9926 - val_auc: 0.9790\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 14s 82ms/step - loss: 0.6613 - categorical_accuracy: 0.9912 - auc: 0.9783 - val_loss: 0.5672 - val_categorical_accuracy: 0.9926 - val_auc: 0.9790\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 129s 16ms/step - loss: 0.5850 - categorical_accuracy: 0.9729 - auc: 0.9759\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 14s 80ms/step - loss: 1.8118 - categorical_accuracy: 0.9300 - auc: 0.9650 - val_loss: 1.2570 - val_categorical_accuracy: 0.9923 - val_auc: 0.9787\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 14s 80ms/step - loss: 1.0208 - categorical_accuracy: 0.9952 - auc: 0.9800 - val_loss: 0.8332 - val_categorical_accuracy: 1.0000 - val_auc: 0.9803\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 14s 80ms/step - loss: 0.7102 - categorical_accuracy: 0.9997 - auc: 0.9809 - val_loss: 0.6119 - val_categorical_accuracy: 1.0000 - val_auc: 0.9803\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 128s 16ms/step - loss: 0.6096 - categorical_accuracy: 0.9990 - auc: 0.9811\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 14s 83ms/step - loss: 1.8312 - categorical_accuracy: 0.9405 - auc: 0.9683 - val_loss: 1.3173 - val_categorical_accuracy: 0.9924 - val_auc: 0.9788\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 14s 82ms/step - loss: 1.0820 - categorical_accuracy: 0.9918 - auc: 0.9792 - val_loss: 0.8939 - val_categorical_accuracy: 0.9924 - val_auc: 0.9788\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 14s 81ms/step - loss: 0.7646 - categorical_accuracy: 0.9920 - auc: 0.9792 - val_loss: 0.6590 - val_categorical_accuracy: 0.9924 - val_auc: 0.9788\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 131s 16ms/step - loss: 0.6757 - categorical_accuracy: 0.9723 - auc: 0.9757\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 14s 83ms/step - loss: 1.8514 - categorical_accuracy: 0.9402 - auc: 0.9685 - val_loss: 1.3502 - val_categorical_accuracy: 0.9924 - val_auc: 0.9788\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 14s 82ms/step - loss: 1.1153 - categorical_accuracy: 0.9918 - auc: 0.9790 - val_loss: 0.9231 - val_categorical_accuracy: 0.9924 - val_auc: 0.9788\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 14s 81ms/step - loss: 0.7915 - categorical_accuracy: 0.9921 - auc: 0.9791 - val_loss: 0.6807 - val_categorical_accuracy: 0.9924 - val_auc: 0.9788\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 128s 16ms/step - loss: 0.6978 - categorical_accuracy: 0.9734 - auc: 0.9759\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbybMLuLulty"
      },
      "source": [
        "#### autodis dyconv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS4UWvcdRTTG"
      },
      "source": [
        "#@title autodis dyconv y2 l2 wo ffn ll4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJQ_5c0Ds2XN",
        "outputId": "ec95bf8f-338f-4247-9b79-e251fa44c1fb"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.0,\n",
              " 'decay_rate': 0.97,\n",
              " 'decay_steps': 55000,\n",
              " 'emb_layer_choice': 'linear',\n",
              " 'hp_DCconv_drop': 0.30000000000000004,\n",
              " 'hp_ffn_d_size': 16,\n",
              " 'hp_ffn_drop': 0.0,\n",
              " 'hp_head_num': 1,\n",
              " 'hp_kernel_size': 2,\n",
              " 'hp_pred_ffn_d_size': 176,\n",
              " 'hp_pred_ffn_drop': 0.0,\n",
              " 'interac_residu': True,\n",
              " 'label_smoothing': 0.0,\n",
              " 'layers_num': 4,\n",
              " 'learning_rate': 0.0001,\n",
              " 'meta_size': 4,\n",
              " 'num_of_d': 128,\n",
              " 'temperatue_rate': 0.01}"
            ]
          },
          "execution_count": 207,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEQYTYecvnoW"
      },
      "source": [
        "!rm -rf '/content/autodis_dyconv_y2_l2_wo_ffn_tpu/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlVZ2s7BtGtT"
      },
      "source": [
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.save('/content/autodis_dyconv_y2_l2_wo_ffn_tpu', options=localhost_save_option)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMc6RRfNtR9b"
      },
      "source": [
        "!tar -zcvf 'kt_reload_autodis_dyconv_y2_l2_wo_ffn_tpu.tar.gz' /content/DyConv/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ac8H4zqswxC",
        "outputId": "e43a2ab9-2f26-422e-ce91-a28ed8d5ac50"
      },
      "source": [
        "# trainvali 7 test 3\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print('train num:', len(train_ds), \"test :\", len(test_ds))\n",
        "k_fold = 5\n",
        "ds_shard = [ train_ds.shard(k_fold, i) for i in range(k_fold)]\n",
        "for i in range(k_fold):\n",
        "  index = [ i for i in range(k_fold)]\n",
        "  set_seed(i)\n",
        "  vali = ds_shard[i]\n",
        "  index.remove(i)\n",
        "  train = 1\n",
        "  for i in index:\n",
        "    if train ==1:\n",
        "      train = ds_shard[i]\n",
        "    else:\n",
        "      train = train.concatenate(ds_shard[i])\n",
        "  model = tuner.hypermodel.build(best_hps)\n",
        "  #with strategy.scope():\n",
        "    #model = tuner.hypermodel.build(best_hps)\n",
        "    #model = tf.keras.models.load_model('/content/autodis_dyconv_y2_l2_wo_ffn_tpu', options=localhost_save_option)\n",
        "  print('train stars')\n",
        "  model.fit(train, batch_size=batch_size, epochs=10, validation_data=vali, validation_batch_size=batch_size, \n",
        "             callbacks = [ EarlyStopping(monitor='val_auc', min_delta=0.01, patience=1)], shuffle=True)\n",
        "  print('test evaluation')\n",
        "  model.evaluate(test_ds, batch_size=batch_size)\n",
        "  print('ensure delete model')\n",
        "  del model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 7552 test : 3236\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 220s 32ms/step - loss: 0.5024 - categorical_accuracy: 0.9888 - auc: 0.9937 - val_loss: 6.9237e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 189s 31ms/step - loss: 7.6117e-04 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.1398e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 68s 21ms/step - loss: 0.0475 - categorical_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 220s 33ms/step - loss: 0.4939 - categorical_accuracy: 0.9876 - auc: 0.9932 - val_loss: 9.6857e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 191s 32ms/step - loss: 9.0062e-04 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 3.6314e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 68s 21ms/step - loss: 0.0459 - categorical_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 219s 32ms/step - loss: 0.4880 - categorical_accuracy: 0.9903 - auc: 0.9939 - val_loss: 7.9448e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 190s 31ms/step - loss: 9.9558e-04 - categorical_accuracy: 0.9998 - auc: 1.0000 - val_loss: 1.8919e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 68s 20ms/step - loss: 0.0486 - categorical_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 218s 32ms/step - loss: 0.5008 - categorical_accuracy: 0.9852 - auc: 0.9910 - val_loss: 0.0013 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 192s 32ms/step - loss: 9.5926e-04 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4452e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 69s 21ms/step - loss: 0.0476 - categorical_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 217s 32ms/step - loss: 0.4737 - categorical_accuracy: 0.9886 - auc: 0.9939 - val_loss: 8.0665e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 190s 31ms/step - loss: 9.7251e-04 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 4.5157e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 68s 20ms/step - loss: 0.0515 - categorical_accuracy: 0.9954 - auc: 0.9970\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1VAjTOattEq",
        "outputId": "5e89f4fa-5cbd-4ad7-9521-0f9577720a65"
      },
      "source": [
        "# trainvali 3 test 7\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print('train num:', len(train_ds), \"test :\", len(test_ds))\n",
        "k_fold = 5\n",
        "ds_shard = [ train_ds.shard(k_fold, i) for i in range(k_fold)]\n",
        "for i in range(k_fold):\n",
        "  index = [ i for i in range(k_fold)]\n",
        "  set_seed(i)\n",
        "  vali = ds_shard[i]\n",
        "  index.remove(i)\n",
        "  train = 1\n",
        "  for i in index:\n",
        "    if train ==1:\n",
        "      train = ds_shard[i]\n",
        "    else:\n",
        "      train = train.concatenate(ds_shard[i])\n",
        "  model = tuner.hypermodel.build(best_hps)\n",
        "  #with strategy.scope():\n",
        "    #model = tuner.hypermodel.build(best_hps)\n",
        "    #model = tf.keras.models.load_model('/content/autodis_dyconv_y2_l2_wo_ffn_tpu', options=localhost_save_option)\n",
        "  print('train stars')\n",
        "  model.fit(train, batch_size=batch_size, epochs=10, validation_data=vali, validation_batch_size=batch_size, \n",
        "             callbacks = [ EarlyStopping(monitor='val_auc', min_delta=0.01, patience=1)], shuffle=True)\n",
        "  print('test evaluation')\n",
        "  model.evaluate(test_ds, batch_size=batch_size)\n",
        "  print('ensure delete model')\n",
        "  del model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2588/2588 [==============================] - 110s 33ms/step - loss: 0.8837 - categorical_accuracy: 0.9768 - auc: 0.9835 - val_loss: 0.0142 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2588/2588 [==============================] - 84s 32ms/step - loss: 0.0084 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0750 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 161s 21ms/step - loss: 0.0909 - categorical_accuracy: 0.9980 - auc: 0.9987\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 111s 34ms/step - loss: 0.9028 - categorical_accuracy: 0.9654 - auc: 0.9716 - val_loss: 0.0142 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 85s 33ms/step - loss: 0.0078 - categorical_accuracy: 0.9999 - auc: 0.9999 - val_loss: 0.0024 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 161s 21ms/step - loss: 0.0088 - categorical_accuracy: 0.9980 - auc: 0.9998\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 112s 34ms/step - loss: 0.8434 - categorical_accuracy: 0.9731 - auc: 0.9823 - val_loss: 0.0062 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 83s 32ms/step - loss: 0.0049 - categorical_accuracy: 0.9997 - auc: 1.0000 - val_loss: 0.0013 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 161s 21ms/step - loss: 0.0168 - categorical_accuracy: 0.9980 - auc: 0.9987\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 110s 33ms/step - loss: 0.8541 - categorical_accuracy: 0.9758 - auc: 0.9836 - val_loss: 0.0096 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 83s 32ms/step - loss: 0.0056 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 9.1957e-04 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 160s 21ms/step - loss: 0.0164 - categorical_accuracy: 0.9980 - auc: 0.9987\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 109s 33ms/step - loss: 0.8618 - categorical_accuracy: 0.9783 - auc: 0.9857 - val_loss: 0.0127 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 84s 32ms/step - loss: 0.0076 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0018 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 160s 21ms/step - loss: 0.0174 - categorical_accuracy: 0.9980 - auc: 0.9987\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBMjFIrf9Ai5",
        "outputId": "272c3e06-7be3-48d4-d11f-f97264ea821f"
      },
      "source": [
        "# train 0.6 vali 0.24 test 0.7\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print('train num:', len(train_ds), \"test :\", len(test_ds))\n",
        "k_fold = 5\n",
        "ds_shard = [ train_ds.shard(k_fold, i) for i in range(k_fold)]\n",
        "for i in range(k_fold):\n",
        "  index = [ i for i in range(k_fold)]\n",
        "  set_seed(i)\n",
        "  vali = ds_shard[i]\n",
        "  index.remove(i)\n",
        "  train = 1\n",
        "  for i in index:\n",
        "    if train ==1:\n",
        "      train = ds_shard[i]\n",
        "    else:\n",
        "      train = train.concatenate(ds_shard[i])\n",
        "  model = tuner.hypermodel.build(best_hps)\n",
        "  #with strategy.scope():\n",
        "    #model = tuner.hypermodel.build(best_hps)\n",
        "    #model = tf.keras.models.load_model('/content/autodis_dyconv_y2_l2_wo_ffn_tpu', options=localhost_save_option)\n",
        "  print('train stars')\n",
        "  model.fit(vali, batch_size=batch_size, epochs=10, validation_data=train, validation_batch_size=batch_size, \n",
        "             callbacks = [ EarlyStopping(monitor='val_auc', min_delta=0.01, patience=1)], shuffle=True)\n",
        "  print('test evaluation')\n",
        "  model.evaluate(test_ds, batch_size=batch_size)\n",
        "  print('ensure delete model')\n",
        "  del model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/10\n",
            "648/648 [==============================] - 94s 109ms/step - loss: 1.6714 - categorical_accuracy: 0.9357 - auc: 0.9365 - val_loss: 0.4844 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "648/648 [==============================] - 65s 100ms/step - loss: 0.3505 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1114 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 159s 20ms/step - loss: 0.1248 - categorical_accuracy: 0.9980 - auc: 0.9987\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 94s 108ms/step - loss: 1.6380 - categorical_accuracy: 0.9296 - auc: 0.9357 - val_loss: 0.4337 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 64s 99ms/step - loss: 0.3073 - categorical_accuracy: 0.9999 - auc: 1.0000 - val_loss: 0.0878 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 160s 21ms/step - loss: 0.0997 - categorical_accuracy: 0.9980 - auc: 0.9987\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 95s 109ms/step - loss: 1.6757 - categorical_accuracy: 0.9085 - auc: 0.8901 - val_loss: 0.4187 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 65s 100ms/step - loss: 0.2952 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0818 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 163s 21ms/step - loss: 0.0944 - categorical_accuracy: 0.9980 - auc: 0.9987\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 95s 110ms/step - loss: 1.6625 - categorical_accuracy: 0.9251 - auc: 0.9275 - val_loss: 0.4497 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 64s 99ms/step - loss: 0.3199 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0927 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 162s 21ms/step - loss: 0.1048 - categorical_accuracy: 0.9980 - auc: 0.9987\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 93s 108ms/step - loss: 1.6850 - categorical_accuracy: 0.9124 - auc: 0.9053 - val_loss: 0.4426 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 64s 100ms/step - loss: 0.3139 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0917 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 162s 21ms/step - loss: 0.1035 - categorical_accuracy: 0.9980 - auc: 0.9987\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cACq0UgFDz1"
      },
      "source": [
        "# train 0.4 vali 0.16 test 0.8\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print('train num:', len(train_ds), \"test :\", len(test_ds))\n",
        "k_fold = 5\n",
        "ds_shard = [ train_ds.shard(k_fold, i) for i in range(k_fold)]\n",
        "for i in range(k_fold):\n",
        "  index = [ i for i in range(k_fold)]\n",
        "  set_seed(i)\n",
        "  vali = ds_shard[i]\n",
        "  index.remove(i)\n",
        "  train = 1\n",
        "  for i in index:\n",
        "    if train ==1:\n",
        "      train = ds_shard[i]\n",
        "    else:\n",
        "      train = train.concatenate(ds_shard[i])\n",
        "  model = tuner.hypermodel.build(best_hps)\n",
        "  #with strategy.scope():\n",
        "    #model = tuner.hypermodel.build(best_hps)\n",
        "    #model = tf.keras.models.load_model('/content/autodis_dyconv_y2_l2_wo_ffn_tpu', options=localhost_save_option)\n",
        "  print('train stars')\n",
        "  model.fit(vali, batch_size=batch_size, epochs=10, validation_data=train, validation_batch_size=batch_size, \n",
        "             callbacks = [ EarlyStopping(monitor='val_auc', min_delta=0.01, patience=1)], shuffle=True)\n",
        "  print('test evaluation')\n",
        "  model.evaluate(test_ds, batch_size=batch_size)\n",
        "  print('ensure delete model')\n",
        "  del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrSDn5_EvGka"
      },
      "source": [
        "##### autodis dyconv y3 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khK6RxfSYbzd"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhGE3JcD8FVc",
        "outputId": "b2173f22-3ef7-4a1b-aba5-f10ae01c4fd1"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train num: 7552 test : 3236\n",
            "trials: 4\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 245s 41ms/step - loss: 0.0042 - categorical_accuracy: 0.9985 - auc_m: 0.9954 - auc_m_PR: 0.9914 - confusion_matrix: 193344.0000 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 1.0000 - val_confusion_matrix: 48320.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 245s 40ms/step - loss: 4.5323e-11 - categorical_accuracy: 1.0000 - auc_m: 1.0000 - auc_m_PR: 1.0000 - confusion_matrix: 193344.0000 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000 - val_auc_m: 1.0000 - val_auc_m_PR: 1.0000 - val_confusion_matrix: 48320.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 69s 20ms/step - loss: 0.1187 - categorical_accuracy: 0.9935 - auc_m: 0.9962 - auc_m_PR: 0.9669 - confusion_matrix: 103552.0000\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkkHsTwrtNn7",
        "outputId": "b2434dc0-a6ae-484b-952c-461501f416c2"
      },
      "source": [
        "avg_metrics([i[0:-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09026108, 0.99393542, 0.99667103, 0.94380264])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu033XpatPFw",
        "outputId": "af9e2c6a-d4ce-423c-b7e2-84cd505b44fa"
      },
      "source": [
        "avg_cm_scores([i[-1] for i in cv.evs])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('precision', 0.9955668926239013),\n",
              "             ('recall', 0.9455134471257528),\n",
              "             ('F1', 0.9680514454841612),\n",
              "             ('specificity', 0.99242551724116),\n",
              "             ('accuracy', 0.9955284591347633)])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR86nS_6tQZa",
        "outputId": "0e215acd-c29e-45f0-b953-49897a247122"
      },
      "source": [
        "tf.reshape(tf.transpose(avg_metrics([i[-1] for i in cv.evs])), [-1,2,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
              "array([[[3.337806e+05, 5.974000e+02],\n",
              "        [5.882000e+02, 7.924180e+04]],\n",
              "\n",
              "       [[4.022818e+05, 1.897200e+03],\n",
              "        [0.000000e+00, 1.002900e+04]],\n",
              "\n",
              "       [[8.984160e+04, 1.740000e+01],\n",
              "        [1.923800e+03, 3.224252e+05]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0mpFgj_t-b9",
        "outputId": "b5dc69d6-1a57-4b5a-bb98-402b90311abe"
      },
      "source": [
        "mm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[333780.6, 597.4], [588.2, 79241.8]],\n",
              " [[402281.8, 1897.2], [0.0, 10029.0]],\n",
              " [[89841.6, 17.4], [1923.8, 322425.2]]]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF_5D1UstRnU",
        "outputId": "48300238-5164-40be-8108-868ad9f0f0ee"
      },
      "source": [
        "mm = [[[3.337806e+05, 5.974000e+02],\n",
        "        [5.882000e+02, 7.924180e+04]],\n",
        "\n",
        "       [[4.022818e+05, 1.897200e+03],\n",
        "        [0.000000e+00, 1.002900e+04]],\n",
        "\n",
        "       [[8.984160e+04, 1.740000e+01],\n",
        "        [1.923800e+03, 3.224252e+05]]]\n",
        "fp = [i[0][1] for i in mm] \n",
        "fn = [ i[1][0] for i in mm]\n",
        "a, b, c, d, e, f = bala( fn[0], fn[1], fn[2], fp[0], fp[1], fp[2], trial=100)\n",
        "a, b, c, d, e, f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False True False False False False\n",
            "(17.40000000000007, 570.8000000000001)==588.2, (0.0, 597.4000000000001)==597.4\n",
            "(0.0, 0.0)==0.0, (1326.4, 570.8000000000001)==1897.2\n",
            "(1326.4, 597.4000000000001)==1923.8, (0.0, 17.40000000000007)==17.4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17.40000000000007, 570.8000000000001, 0.0, 0.0, 1326.4, 597.4000000000001)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09x7Se7zvUs3"
      },
      "source": [
        "  print(f\"{a, b}=={fp0}, {d, f}=={fn0}\")\n",
        "  print(f\"{c, d}=={fp1}, {e, b}=={fn1}\")\n",
        "  print(f\"{e, f}=={fp2}, {c, a}=={fn2}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1Fl1C9jvXRI",
        "outputId": "387e572e-61d3-4035-8235-f5e8728acab8"
      },
      "source": [
        "#train 0.7 test0.3\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.05,\n",
              " 'decay_rate': 0.6000000000000001,\n",
              " 'decay_steps': 55000,\n",
              " 'emb_layer_choice': 'dense',\n",
              " 'hp_DCconv_drop': 0.25,\n",
              " 'hp_ffn_d_size': 368,\n",
              " 'hp_ffn_drop': 0.5,\n",
              " 'hp_head_num': 4,\n",
              " 'hp_kernel_size': 10,\n",
              " 'hp_pred_ffn_d_size': 64,\n",
              " 'hp_pred_ffn_drop': 0.8,\n",
              " 'interac_residu': True,\n",
              " 'label_smoothing': 0.1,\n",
              " 'layers_num': 4,\n",
              " 'learning_rate': 0.0011003795189368764,\n",
              " 'meta_size': 8,\n",
              " 'num_of_d': 64,\n",
              " 'temperatue_rate': 0.004181823729519305}"
            ]
          },
          "execution_count": 50,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IALK5zHvMdD",
        "outputId": "9f79eab8-2aa5-4226-c06f-1dd98b6814e7"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 7552 test : 3236\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 266s 44ms/step - loss: 0.4808 - categorical_accuracy: 0.9777 - auc: 0.9933 - val_loss: 0.3290 - val_categorical_accuracy: 0.9967 - val_auc: 0.9999\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 264s 44ms/step - loss: 0.3166 - categorical_accuracy: 0.9926 - auc: 0.9981 - val_loss: 0.3018 - val_categorical_accuracy: 1.0000 - val_auc: 0.9999\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 70s 21ms/step - loss: 0.3074 - categorical_accuracy: 0.9980 - auc: 0.9976\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6041/6041 [==============================] - 266s 44ms/step - loss: 0.5307 - categorical_accuracy: 0.9630 - auc: 0.9819 - val_loss: 0.2979 - val_categorical_accuracy: 1.0000 - val_auc: 0.9999\n",
            "Epoch 2/10\n",
            "6041/6041 [==============================] - 265s 44ms/step - loss: 0.2996 - categorical_accuracy: 0.9991 - auc: 0.9998 - val_loss: 0.2940 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 71s 21ms/step - loss: 0.3048 - categorical_accuracy: 0.9953 - auc: 0.9999\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 266s 44ms/step - loss: 0.5040 - categorical_accuracy: 0.9823 - auc: 0.9947 - val_loss: 0.2955 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 264s 44ms/step - loss: 0.3024 - categorical_accuracy: 0.9979 - auc: 0.9957 - val_loss: 0.2938 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 70s 21ms/step - loss: 0.3149 - categorical_accuracy: 0.9935 - auc: 0.9941\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 267s 44ms/step - loss: 0.5345 - categorical_accuracy: 0.9754 - auc: 0.9921 - val_loss: 0.2978 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 264s 44ms/step - loss: 0.2986 - categorical_accuracy: 0.9993 - auc: 1.0000 - val_loss: 0.2918 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 70s 21ms/step - loss: 0.3096 - categorical_accuracy: 0.9935 - auc: 0.9992\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "6042/6042 [==============================] - 265s 44ms/step - loss: 0.5686 - categorical_accuracy: 0.9693 - auc: 0.9896 - val_loss: 0.2956 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "6042/6042 [==============================] - 265s 44ms/step - loss: 0.2991 - categorical_accuracy: 0.9993 - auc: 0.9997 - val_loss: 0.2940 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "3236/3236 [==============================] - 70s 21ms/step - loss: 0.3161 - categorical_accuracy: 0.9935 - auc: 0.9939\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNAMsqeg1lXR",
        "outputId": "3adfb2d7-2200-4d1d-d1cd-dc5a30f6ea9b"
      },
      "source": [
        "#train 0.7 test0.3\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.05,\n",
              " 'decay_rate': 0.6000000000000001,\n",
              " 'decay_steps': 55000,\n",
              " 'emb_layer_choice': 'dense',\n",
              " 'hp_DCconv_drop': 0.25,\n",
              " 'hp_ffn_d_size': 368,\n",
              " 'hp_ffn_drop': 0.5,\n",
              " 'hp_head_num': 4,\n",
              " 'hp_kernel_size': 10,\n",
              " 'hp_pred_ffn_d_size': 64,\n",
              " 'hp_pred_ffn_drop': 0.8,\n",
              " 'interac_residu': True,\n",
              " 'label_smoothing': 0.1,\n",
              " 'layers_num': 4,\n",
              " 'learning_rate': 0.0011003795189368764,\n",
              " 'meta_size': 8,\n",
              " 'num_of_d': 64,\n",
              " 'temperatue_rate': 0.004181823729519305}"
            ]
          },
          "execution_count": 55,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy8LBr77bctB",
        "outputId": "8c1c0df1-623e-4102-a6a3-e36ab43daf01"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2588/2588 [==============================] - 140s 45ms/step - loss: 5.6053 - categorical_accuracy: 0.8804 - auc: 0.8366 - val_loss: 0.3095 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2588/2588 [==============================] - 113s 44ms/step - loss: 0.3096 - categorical_accuracy: 0.9996 - auc: 1.0000 - val_loss: 0.2964 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 164s 21ms/step - loss: 0.3037 - categorical_accuracy: 0.9973 - auc: 0.9999\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 115s 44ms/step - loss: 0.6430 - categorical_accuracy: 0.9742 - auc: 0.9872 - val_loss: 0.3077 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 114s 44ms/step - loss: 0.3068 - categorical_accuracy: 0.9986 - auc: 0.9933 - val_loss: 0.2924 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 164s 21ms/step - loss: 0.3041 - categorical_accuracy: 0.9972 - auc: 0.9974\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 114s 44ms/step - loss: 0.6646 - categorical_accuracy: 0.9683 - auc: 0.9839 - val_loss: 0.2944 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 114s 44ms/step - loss: 0.3043 - categorical_accuracy: 0.9993 - auc: 0.9999 - val_loss: 0.2935 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 165s 21ms/step - loss: 0.3008 - categorical_accuracy: 0.9972 - auc: 1.0000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 114s 44ms/step - loss: 0.7387 - categorical_accuracy: 0.9367 - auc: 0.9585 - val_loss: 0.2941 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 114s 44ms/step - loss: 0.3059 - categorical_accuracy: 0.9992 - auc: 0.9992 - val_loss: 0.3045 - val_categorical_accuracy: 0.9982 - val_auc: 0.9831\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 164s 21ms/step - loss: 0.3188 - categorical_accuracy: 0.9935 - auc: 0.9783\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "2589/2589 [==============================] - 114s 44ms/step - loss: 0.6728 - categorical_accuracy: 0.9742 - auc: 0.9894 - val_loss: 0.2949 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "2589/2589 [==============================] - 114s 44ms/step - loss: 0.3056 - categorical_accuracy: 0.9992 - auc: 0.9999 - val_loss: 0.2988 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 164s 21ms/step - loss: 0.3087 - categorical_accuracy: 0.9972 - auc: 0.9977\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t23aA9JYb27x",
        "outputId": "41e5b319-8575-4197-ac8b-fb9221a16d5b"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 3236 test : 7552\n",
            "train stars\n",
            "Epoch 1/10\n",
            "648/648 [==============================] - 78s 121ms/step - loss: 1.9010 - categorical_accuracy: 0.8752 - auc: 0.8387 - val_loss: 0.3203 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "648/648 [==============================] - 75s 116ms/step - loss: 0.3292 - categorical_accuracy: 0.9957 - auc: 0.9992 - val_loss: 0.3005 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 164s 21ms/step - loss: 0.3101 - categorical_accuracy: 0.9972 - auc: 0.9979\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 78s 120ms/step - loss: 2.4957 - categorical_accuracy: 0.8271 - auc: 0.6937 - val_loss: 0.3459 - val_categorical_accuracy: 0.9848 - val_auc: 0.9999\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 75s 117ms/step - loss: 0.3560 - categorical_accuracy: 0.9835 - auc: 0.9895 - val_loss: 0.3231 - val_categorical_accuracy: 0.9999 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 163s 21ms/step - loss: 0.3290 - categorical_accuracy: 0.9981 - auc: 1.0000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 78s 120ms/step - loss: 2.7186 - categorical_accuracy: 0.8197 - auc: 0.7023 - val_loss: 0.3379 - val_categorical_accuracy: 0.9996 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 76s 117ms/step - loss: 0.3389 - categorical_accuracy: 0.9932 - auc: 0.9973 - val_loss: 0.3057 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 165s 21ms/step - loss: 0.3107 - categorical_accuracy: 0.9978 - auc: 1.0000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 79s 122ms/step - loss: 2.7302 - categorical_accuracy: 0.8328 - auc: 0.7464 - val_loss: 0.3461 - val_categorical_accuracy: 0.9997 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 75s 116ms/step - loss: 0.3542 - categorical_accuracy: 0.9884 - auc: 0.9881 - val_loss: 0.4047 - val_categorical_accuracy: 0.9847 - val_auc: 0.9832\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 165s 21ms/step - loss: 0.4310 - categorical_accuracy: 0.9693 - auc: 0.9806\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 79s 122ms/step - loss: 3.4102 - categorical_accuracy: 0.8156 - auc: 0.6830 - val_loss: 0.4801 - val_categorical_accuracy: 0.9844 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 75s 116ms/step - loss: 0.4207 - categorical_accuracy: 0.9873 - auc: 0.9935 - val_loss: 0.3496 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7552/7552 [==============================] - 165s 21ms/step - loss: 0.3552 - categorical_accuracy: 0.9977 - auc: 0.9998\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CssabqZ8j_Gy",
        "outputId": "8b3893a6-e1f0-4ec6-fec3-5237e2e6a793"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 863 test : 7767\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 21s 120ms/step - loss: 9.2823 - categorical_accuracy: 0.7565 - auc: 0.5008 - val_loss: 1.9813 - val_categorical_accuracy: 0.7952 - val_auc: 0.5000\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 21s 120ms/step - loss: 1.3363 - categorical_accuracy: 0.7910 - auc: 0.5027 - val_loss: 0.9840 - val_categorical_accuracy: 0.7952 - val_auc: 0.5008\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 21s 120ms/step - loss: 0.8090 - categorical_accuracy: 0.8428 - auc: 0.7224 - val_loss: 0.4421 - val_categorical_accuracy: 0.9919 - val_auc: 0.9312\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 171s 21ms/step - loss: 0.4862 - categorical_accuracy: 0.9723 - auc: 0.9570\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 21s 122ms/step - loss: 10.6387 - categorical_accuracy: 0.7480 - auc: 0.5059 - val_loss: 2.5185 - val_categorical_accuracy: 0.7950 - val_auc: 0.5000\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 21s 121ms/step - loss: 1.5383 - categorical_accuracy: 0.7917 - auc: 0.5059 - val_loss: 1.0166 - val_categorical_accuracy: 0.7950 - val_auc: 0.8157\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 21s 119ms/step - loss: 0.6814 - categorical_accuracy: 0.9095 - auc: 0.8632 - val_loss: 0.4047 - val_categorical_accuracy: 0.9926 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 169s 21ms/step - loss: 0.4406 - categorical_accuracy: 0.9734 - auc: 0.9971\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 21s 121ms/step - loss: 11.4376 - categorical_accuracy: 0.7550 - auc: 0.4994 - val_loss: 2.9440 - val_categorical_accuracy: 0.7938 - val_auc: 0.5000\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 21s 120ms/step - loss: 1.7188 - categorical_accuracy: 0.7962 - auc: 0.4888 - val_loss: 1.0694 - val_categorical_accuracy: 0.7938 - val_auc: 0.5147\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 21s 120ms/step - loss: 0.8957 - categorical_accuracy: 0.8078 - auc: 0.5944 - val_loss: 0.4460 - val_categorical_accuracy: 0.9923 - val_auc: 0.9992\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 170s 21ms/step - loss: 0.4727 - categorical_accuracy: 0.9734 - auc: 0.9992\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 21s 122ms/step - loss: 11.9942 - categorical_accuracy: 0.7535 - auc: 0.4996 - val_loss: 3.2749 - val_categorical_accuracy: 0.7940 - val_auc: 0.5215\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 21s 121ms/step - loss: 1.8746 - categorical_accuracy: 0.7958 - auc: 0.4878 - val_loss: 1.1007 - val_categorical_accuracy: 0.7940 - val_auc: 0.9375\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 21s 121ms/step - loss: 0.6481 - categorical_accuracy: 0.9448 - auc: 0.8969 - val_loss: 0.4072 - val_categorical_accuracy: 0.9924 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 169s 21ms/step - loss: 0.4407 - categorical_accuracy: 0.9734 - auc: 0.9976\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 21s 121ms/step - loss: 12.4600 - categorical_accuracy: 0.7591 - auc: 0.5013 - val_loss: 3.5856 - val_categorical_accuracy: 0.7942 - val_auc: 0.5001\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 21s 120ms/step - loss: 2.0298 - categorical_accuracy: 0.7934 - auc: 0.4898 - val_loss: 1.1796 - val_categorical_accuracy: 0.7942 - val_auc: 0.5037\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 20s 120ms/step - loss: 0.9482 - categorical_accuracy: 0.7914 - auc: 0.5125 - val_loss: 0.7713 - val_categorical_accuracy: 0.7942 - val_auc: 0.6878\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 169s 21ms/step - loss: 0.8075 - categorical_accuracy: 0.7812 - auc: 0.6884\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQOBuEkCrh0Z",
        "outputId": "77476b10-5d37-4df5-fc7d-d9dca1093662"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, less_sample=True)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 863 test : 7767\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 21s 122ms/step - loss: 12.4681 - categorical_accuracy: 0.7539 - auc: 0.5042 - val_loss: 3.6119 - val_categorical_accuracy: 0.7952 - val_auc: 0.5043\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 21s 120ms/step - loss: 2.0116 - categorical_accuracy: 0.8001 - auc: 0.6295 - val_loss: 0.9296 - val_categorical_accuracy: 0.9919 - val_auc: 0.9783\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 21s 121ms/step - loss: 0.6058 - categorical_accuracy: 0.9836 - auc: 0.9231 - val_loss: 0.4061 - val_categorical_accuracy: 0.9919 - val_auc: 1.0000\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 21s 120ms/step - loss: 0.3933 - categorical_accuracy: 0.9931 - auc: 0.9609 - val_loss: 0.3326 - val_categorical_accuracy: 0.9919 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 169s 21ms/step - loss: 0.3582 - categorical_accuracy: 0.9734 - auc: 0.9999\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 21s 122ms/step - loss: 12.2785 - categorical_accuracy: 0.7486 - auc: 0.5038 - val_loss: 3.4526 - val_categorical_accuracy: 0.7950 - val_auc: 0.5013\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 21s 121ms/step - loss: 1.9643 - categorical_accuracy: 0.7906 - auc: 0.4944 - val_loss: 1.1222 - val_categorical_accuracy: 0.7950 - val_auc: 0.9652\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 21s 120ms/step - loss: 0.6967 - categorical_accuracy: 0.9161 - auc: 0.8855 - val_loss: 0.4017 - val_categorical_accuracy: 0.9926 - val_auc: 0.9929\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 21s 120ms/step - loss: 0.3958 - categorical_accuracy: 0.9901 - auc: 0.9708 - val_loss: 0.3303 - val_categorical_accuracy: 0.9926 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 169s 21ms/step - loss: 0.3576 - categorical_accuracy: 0.9734 - auc: 0.9990\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 21s 122ms/step - loss: 12.1288 - categorical_accuracy: 0.7550 - auc: 0.5107 - val_loss: 3.3643 - val_categorical_accuracy: 0.7938 - val_auc: 0.5000\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 21s 121ms/step - loss: 1.9080 - categorical_accuracy: 0.7962 - auc: 0.4971 - val_loss: 1.1143 - val_categorical_accuracy: 0.7938 - val_auc: 0.5000\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 21s 120ms/step - loss: 0.7692 - categorical_accuracy: 0.8722 - auc: 0.8143 - val_loss: 0.3987 - val_categorical_accuracy: 0.9923 - val_auc: 0.9886\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 21s 120ms/step - loss: 0.4008 - categorical_accuracy: 0.9887 - auc: 0.9345 - val_loss: 0.3316 - val_categorical_accuracy: 0.9923 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 171s 21ms/step - loss: 0.3631 - categorical_accuracy: 0.9734 - auc: 0.9998\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 21s 124ms/step - loss: 12.1609 - categorical_accuracy: 0.7574 - auc: 0.5009 - val_loss: 3.3814 - val_categorical_accuracy: 0.7940 - val_auc: 0.5003\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 21s 121ms/step - loss: 1.8754 - categorical_accuracy: 0.8028 - auc: 0.6144 - val_loss: 0.7577 - val_categorical_accuracy: 0.9924 - val_auc: 0.9891\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 21s 121ms/step - loss: 0.5497 - categorical_accuracy: 0.9885 - auc: 0.9520 - val_loss: 0.3834 - val_categorical_accuracy: 0.9924 - val_auc: 1.0000\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 21s 120ms/step - loss: 0.3813 - categorical_accuracy: 0.9916 - auc: 0.9792 - val_loss: 0.3253 - val_categorical_accuracy: 0.9924 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 170s 21ms/step - loss: 0.3537 - categorical_accuracy: 0.9723 - auc: 0.9962\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 21s 122ms/step - loss: 12.0986 - categorical_accuracy: 0.7448 - auc: 0.4888 - val_loss: 3.3348 - val_categorical_accuracy: 0.7942 - val_auc: 0.5001\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 20s 119ms/step - loss: 1.8919 - categorical_accuracy: 0.7949 - auc: 0.4963 - val_loss: 1.0975 - val_categorical_accuracy: 0.7942 - val_auc: 0.8963\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 21s 120ms/step - loss: 0.6820 - categorical_accuracy: 0.9191 - auc: 0.8782 - val_loss: 0.3927 - val_categorical_accuracy: 0.9924 - val_auc: 1.0000\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 21s 121ms/step - loss: 0.3869 - categorical_accuracy: 0.9911 - auc: 0.9743 - val_loss: 0.3264 - val_categorical_accuracy: 0.9924 - val_auc: 1.0000\n",
            "test evaluation\n",
            "7767/7767 [==============================] - 170s 21ms/step - loss: 0.3519 - categorical_accuracy: 0.9733 - auc: 0.9995\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvvMZrHkQGXj"
      },
      "source": [
        "# less feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNhLYd74xogu",
        "outputId": "a0e7e161-64c2-4f55-d501-35988b8c3fec"
      },
      "source": [
        "df_v.columns[26:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['inq_last_12m', 'acc_open_past_24mths', 'bc_open_to_buy', 'bc_util',\n",
              "       'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mort_acc',\n",
              "       'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl',\n",
              "       'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl',\n",
              "       'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats',\n",
              "       'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75',\n",
              "       'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim',\n",
              "       'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit',\n",
              "       'credit_age', 'grade', 'sub_grade', 'emp_length', 'home_ownership',\n",
              "       'verification_status', 'purpose', 'term', 'zip_code', 'addr_state',\n",
              "       'application_type', 'verification_status_joint'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 190,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRs3GaGeRcjU",
        "outputId": "fc10ceda-bc9d-4391-cbbf-79c07fc33732"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 2)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.1,\n",
              " 'decay_rate': 0.2,\n",
              " 'decay_steps': 90000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'ffn': True,\n",
              " 'ffn_layer': False,\n",
              " 'hp_ffn_d_size': 512,\n",
              " 'hp_ffn_drop': 0.3,\n",
              " 'hp_pred_ffn_d_size': 64,\n",
              " 'hp_pred_ffn_drop': 0.3,\n",
              " 'label_smoothing': 0.6,\n",
              " 'learning_rate': 0.00027084489969062176,\n",
              " 'mha_dropout_rate': 0.1,\n",
              " 'mha_residual': True,\n",
              " 'num_MHA_layer': 4,\n",
              " 'num_head': 4,\n",
              " 'num_of_d': 128}"
            ]
          },
          "execution_count": 205,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "V-byHBueRjhY",
        "outputId": "6598aab7-a42e-4c4f-def7-966744444796"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5, patience=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 2158 test : 2158\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 48s 22ms/step - loss: 1.1465 - categorical_accuracy: 0.8829 - auc: 0.9473 - val_loss: 0.6110 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/10\n",
            "1726/1726 [==============================] - 37s 21ms/step - loss: 0.6117 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.6109 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/10\n",
            "1726/1726 [==============================] - 36s 21ms/step - loss: 0.6114 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.6109 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 4/10\n",
            "1726/1726 [==============================] - 37s 21ms/step - loss: 0.6112 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.6109 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 5/10\n",
            "1726/1726 [==============================] - 37s 21ms/step - loss: 0.6111 - categorical_accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.6112 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.6112 - categorical_accuracy: 1.0000 - auc: 1.0000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 37s 22ms/step - loss: 0.7473 - categorical_accuracy: 0.2064 - auc: 0.5004 - val_loss: 0.6948 - val_categorical_accuracy: 0.2099 - val_auc: 0.5000\n",
            "Epoch 2/10\n",
            "1726/1726 [==============================] - 37s 22ms/step - loss: 0.6940 - categorical_accuracy: 0.2059 - auc: 0.5000 - val_loss: 0.6935 - val_categorical_accuracy: 0.2099 - val_auc: 0.5000\n",
            "Epoch 3/10\n",
            "1726/1726 [==============================] - 36s 21ms/step - loss: 0.6933 - categorical_accuracy: 0.2059 - auc: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.2099 - val_auc: 0.5000\n",
            "Epoch 4/10\n",
            "1726/1726 [==============================] - 38s 22ms/step - loss: 0.6932 - categorical_accuracy: 0.2059 - auc: 0.5000 - val_loss: 0.6931 - val_categorical_accuracy: 0.2099 - val_auc: 0.5000\n",
            "Epoch 5/10\n",
            "1726/1726 [==============================] - 37s 21ms/step - loss: 0.6932 - categorical_accuracy: 0.2059 - auc: 0.5000 - val_loss: 0.6931 - val_categorical_accuracy: 0.2099 - val_auc: 0.5000\n",
            "Epoch 6/10\n",
            "1726/1726 [==============================] - 37s 21ms/step - loss: 0.6932 - categorical_accuracy: 0.2059 - auc: 0.5000 - val_loss: 0.6931 - val_categorical_accuracy: 0.2099 - val_auc: 0.5000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.6931 - categorical_accuracy: 0.1967 - auc: 0.5000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1368/1726 [======================>.......] - ETA: 6s - loss: 0.7586 - categorical_accuracy: 0.2093 - auc: 0.5005"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-206-c90c47fa9b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCV_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-203-2bdbfee1eac9>\u001b[0m in \u001b[0;36mCV_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train stars'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m       train_model.fit(train, epochs=self.epochs, validation_data=vali, validation_batch_size=batch_size, \n\u001b[0;32m---> 58\u001b[0;31m                  callbacks = [ EarlyStopping(monitor='val_auc', min_delta=0.01, patience=self.patience, baseline=0.99)], shuffle=True)\n\u001b[0m\u001b[1;32m     59\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test evaluation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDqrnHL4ULuO",
        "outputId": "efe5e15e-51ab-4dd0-ae6d-2f3d87a65ae8"
      },
      "source": [
        "df_v.columns[54:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['sub_grade', 'emp_length', 'home_ownership', 'verification_status',\n",
              "       'purpose', 'term', 'zip_code', 'addr_state', 'application_type',\n",
              "       'verification_status_joint'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 301,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GLqRi8Y43E9",
        "outputId": "d91e6c16-31ef-4fb7-a7a5-2299e50244f2"
      },
      "source": [
        "train_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'emb_index': TensorSpec(shape=(128, 2, 1), dtype=tf.int64, name=None),\n",
              "  'emb_value': TensorSpec(shape=(128, 2, 128), dtype=tf.float64, name=None),\n",
              "  'index': TensorSpec(shape=(128, 64, 1), dtype=tf.int64, name=None),\n",
              "  'value': TensorSpec(shape=(128, 64, 1), dtype=tf.float64, name=None)},\n",
              " TensorSpec(shape=(128, 2), dtype=tf.float32, name=None))"
            ]
          },
          "execution_count": 302,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiVO24UQke73"
      },
      "source": [
        "### less column 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYHJ_9TCkcRK",
        "outputId": "84d4d48e-e29c-482f-d33c-a32629590fb7"
      },
      "source": [
        "less_col"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['annual_inc',\n",
              " 'loan_amnt',\n",
              " 'dti',\n",
              " 'delinq_2yrs',\n",
              " 'fico_range_low',\n",
              " 'fico_range_high',\n",
              " 'sub_grade',\n",
              " 'emp_length',\n",
              " 'home_ownership',\n",
              " 'verification_status',\n",
              " 'purpose',\n",
              " 'term']"
            ]
          },
          "execution_count": 66,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z_06Nbukftc",
        "outputId": "c2e47175-ce4e-46b5-9224-f91638100e18"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.2,\n",
              " 'decay_rate': 0.4,\n",
              " 'decay_steps': 50000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'ffn': True,\n",
              " 'ffn_layer': False,\n",
              " 'hp_ffn_d_size': 384,\n",
              " 'hp_ffn_drop': 0.8999999999999999,\n",
              " 'hp_pred_ffn_d_size': 128,\n",
              " 'hp_pred_ffn_drop': 0.3,\n",
              " 'label_smoothing': 0.0,\n",
              " 'learning_rate': 0.00011098725667045414,\n",
              " 'mha_dropout_rate': 0.4,\n",
              " 'mha_residual': True,\n",
              " 'num_MHA_layer': 4,\n",
              " 'num_head': 4,\n",
              " 'num_of_d': 128}"
            ]
          },
          "execution_count": 68,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itHL0yBikvz0",
        "outputId": "6d7eafa4-cfbe-4fa8-c7d3-a2e57abbd954"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 2158 test : 2158\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 47s 21ms/step - loss: 1.5594 - categorical_accuracy: 0.7785 - auc: 0.8974 - val_loss: 0.5842 - val_categorical_accuracy: 0.7962 - val_auc: 0.9322\n",
            "Epoch 2/10\n",
            "1726/1726 [==============================] - 38s 22ms/step - loss: 0.5729 - categorical_accuracy: 0.7931 - auc: 0.9276 - val_loss: 0.5074 - val_categorical_accuracy: 0.7980 - val_auc: 0.9326\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.5873 - categorical_accuracy: 0.7965 - auc: 0.9208\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 36s 21ms/step - loss: 0.8875 - categorical_accuracy: 0.7936 - auc: 0.8951 - val_loss: 0.6727 - val_categorical_accuracy: 0.7901 - val_auc: 0.9006\n",
            "Epoch 2/10\n",
            "1726/1726 [==============================] - 38s 22ms/step - loss: 0.6381 - categorical_accuracy: 0.7945 - auc: 0.9006 - val_loss: 0.6224 - val_categorical_accuracy: 0.7901 - val_auc: 0.9007\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 34s 15ms/step - loss: 0.6201 - categorical_accuracy: 0.8033 - auc: 0.8995\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 36s 21ms/step - loss: 0.7852 - categorical_accuracy: 0.7899 - auc: 0.9230 - val_loss: 0.5508 - val_categorical_accuracy: 0.7998 - val_auc: 0.9338\n",
            "Epoch 2/10\n",
            "1726/1726 [==============================] - 37s 21ms/step - loss: 0.5316 - categorical_accuracy: 0.7944 - auc: 0.9291 - val_loss: 0.4959 - val_categorical_accuracy: 0.8007 - val_auc: 0.9340\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 34s 15ms/step - loss: 0.5768 - categorical_accuracy: 0.8005 - auc: 0.9212\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1727/1727 [==============================] - 35s 20ms/step - loss: 0.8974 - categorical_accuracy: 0.7927 - auc: 0.8945 - val_loss: 0.6740 - val_categorical_accuracy: 0.7931 - val_auc: 0.9039\n",
            "Epoch 2/10\n",
            "1727/1727 [==============================] - 35s 20ms/step - loss: 0.6491 - categorical_accuracy: 0.7937 - auc: 0.8994 - val_loss: 0.6184 - val_categorical_accuracy: 0.7931 - val_auc: 0.9042\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.6246 - categorical_accuracy: 0.8033 - auc: 0.8997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1727/1727 [==============================] - 36s 21ms/step - loss: 0.8668 - categorical_accuracy: 0.7898 - auc: 0.9115 - val_loss: 0.5721 - val_categorical_accuracy: 0.7961 - val_auc: 0.9328\n",
            "Epoch 2/10\n",
            "1727/1727 [==============================] - 37s 22ms/step - loss: 0.5479 - categorical_accuracy: 0.7942 - auc: 0.9287 - val_loss: 0.5080 - val_categorical_accuracy: 0.7972 - val_auc: 0.9331\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 35s 16ms/step - loss: 0.5798 - categorical_accuracy: 0.8024 - auc: 0.9212\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzgpSvMu9ZIr"
      },
      "source": [
        "### less column 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHw5OyC49bgl",
        "outputId": "a9cf0b30-5196-4bc2-9a75-80e7a6573aa8"
      },
      "source": [
        "less_col"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['annual_inc',\n",
              " 'loan_amnt',\n",
              " 'dti',\n",
              " 'delinq_2yrs',\n",
              " 'fico_range_low',\n",
              " 'fico_range_high',\n",
              " 'annual_inc_joint',\n",
              " 'dti_joint',\n",
              " 'sub_grade',\n",
              " 'emp_length',\n",
              " 'home_ownership',\n",
              " 'verification_status',\n",
              " 'purpose',\n",
              " 'term',\n",
              " 'verification_status_joint']"
            ]
          },
          "execution_count": 48,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKrP0SoL9ld7",
        "outputId": "cb149ce8-605f-4de3-b18d-5c6a146a3794"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.2,\n",
              " 'decay_rate': 0.8,\n",
              " 'decay_steps': 90000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'ffn': False,\n",
              " 'ffn_layer': True,\n",
              " 'hp_ffn_d_size': 128,\n",
              " 'hp_ffn_drop': 0.8999999999999999,\n",
              " 'hp_pred_ffn_d_size': 192,\n",
              " 'hp_pred_ffn_drop': 0.3,\n",
              " 'label_smoothing': 0.0,\n",
              " 'learning_rate': 0.00028123220676523223,\n",
              " 'mha_dropout_rate': 0.4,\n",
              " 'mha_residual': True,\n",
              " 'num_MHA_layer': 4,\n",
              " 'num_head': 4,\n",
              " 'num_of_d': 128}"
            ]
          },
          "execution_count": 49,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbjflIUg9oZN",
        "outputId": "f424f10a-7ecb-4088-fa5c-6c99907feae3"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 2158 test : 2158\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 55s 24ms/step - loss: 1.2817 - categorical_accuracy: 0.7697 - auc: 0.8743 - val_loss: 0.5944 - val_categorical_accuracy: 0.7922 - val_auc: 0.9035\n",
            "Epoch 2/10\n",
            "1726/1726 [==============================] - 39s 23ms/step - loss: 0.5825 - categorical_accuracy: 0.7924 - auc: 0.8928 - val_loss: 0.5443 - val_categorical_accuracy: 0.7922 - val_auc: 0.8931\n",
            "Epoch 3/10\n",
            "1726/1726 [==============================] - 39s 23ms/step - loss: 0.5486 - categorical_accuracy: 0.7924 - auc: 0.8938 - val_loss: 0.5390 - val_categorical_accuracy: 0.7922 - val_auc: 0.8931\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 34s 15ms/step - loss: 0.5874 - categorical_accuracy: 0.8033 - auc: 0.8903\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 39s 23ms/step - loss: 0.7593 - categorical_accuracy: 0.7922 - auc: 0.8456 - val_loss: 0.6692 - val_categorical_accuracy: 0.7901 - val_auc: 0.8426\n",
            "Epoch 2/10\n",
            "1726/1726 [==============================] - 38s 22ms/step - loss: 0.5517 - categorical_accuracy: 0.7940 - auc: 0.8940 - val_loss: 0.5465 - val_categorical_accuracy: 0.7901 - val_auc: 0.8914\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 34s 15ms/step - loss: 0.5847 - categorical_accuracy: 0.8033 - auc: 0.8903\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 39s 22ms/step - loss: 0.6968 - categorical_accuracy: 0.7909 - auc: 0.8821 - val_loss: 0.5451 - val_categorical_accuracy: 0.7963 - val_auc: 0.8949\n",
            "Epoch 2/10\n",
            "1726/1726 [==============================] - 39s 23ms/step - loss: 0.5463 - categorical_accuracy: 0.7925 - auc: 0.8938 - val_loss: 0.5359 - val_categorical_accuracy: 0.7963 - val_auc: 0.8949\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 34s 15ms/step - loss: 0.5855 - categorical_accuracy: 0.8033 - auc: 0.8903\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1727/1727 [==============================] - 39s 23ms/step - loss: 0.6565 - categorical_accuracy: 0.7906 - auc: 0.8923 - val_loss: 0.5500 - val_categorical_accuracy: 0.7931 - val_auc: 0.8946\n",
            "Epoch 2/10\n",
            "1727/1727 [==============================] - 39s 22ms/step - loss: 0.5444 - categorical_accuracy: 0.7934 - auc: 0.8937 - val_loss: 0.5415 - val_categorical_accuracy: 0.7931 - val_auc: 0.8931\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 34s 15ms/step - loss: 0.5866 - categorical_accuracy: 0.8033 - auc: 0.8903\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1727/1727 [==============================] - 39s 22ms/step - loss: 0.6580 - categorical_accuracy: 0.7899 - auc: 0.8922 - val_loss: 0.5446 - val_categorical_accuracy: 0.7949 - val_auc: 0.8942\n",
            "Epoch 2/10\n",
            "1727/1727 [==============================] - 40s 23ms/step - loss: 0.5457 - categorical_accuracy: 0.7929 - auc: 0.8932 - val_loss: 0.5374 - val_categorical_accuracy: 0.7949 - val_auc: 0.8942\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 34s 15ms/step - loss: 0.5805 - categorical_accuracy: 0.8033 - auc: 0.8903\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb-NB-FvYdXL"
      },
      "source": [
        "### less 3 column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhfoU_SgYfsu",
        "outputId": "3a0d5785-db68-4d90-a1e9-0613fde54966"
      },
      "source": [
        "less_col"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['annual_inc',\n",
              " 'loan_amnt',\n",
              " 'dti',\n",
              " 'delinq_2yrs',\n",
              " 'fico_range_low',\n",
              " 'fico_range_high',\n",
              " 'sub_grade',\n",
              " 'emp_length',\n",
              " 'home_ownership',\n",
              " 'verification_status',\n",
              " 'purpose',\n",
              " 'term',\n",
              " 'zip_code',\n",
              " 'addr_state']"
            ]
          },
          "execution_count": 71,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_PBP9X4Yj49",
        "outputId": "e8076794-34c5-4b36-a70d-52d9d0579a35"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.2,\n",
              " 'decay_rate': 1.0,\n",
              " 'decay_steps': 90000,\n",
              " 'emb_layer_choice': 'dense',\n",
              " 'ffn': True,\n",
              " 'ffn_layer': False,\n",
              " 'hp_ffn_d_size': 128,\n",
              " 'hp_ffn_drop': 0.0,\n",
              " 'hp_pred_ffn_d_size': 128,\n",
              " 'hp_pred_ffn_drop': 0.8999999999999999,\n",
              " 'label_smoothing': 0.0,\n",
              " 'learning_rate': 0.0001,\n",
              " 'mha_dropout_rate': 0.0,\n",
              " 'mha_residual': True,\n",
              " 'num_MHA_layer': 4,\n",
              " 'num_head': 4,\n",
              " 'num_of_d': 128}"
            ]
          },
          "execution_count": 72,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB_RBvdjYlfn",
        "outputId": "5b64560d-22d9-4621-fc89-8ed0ba951d74"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 2158 test : 2158\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 47s 22ms/step - loss: 1.8885 - categorical_accuracy: 0.7565 - auc: 0.8855 - val_loss: 0.3544 - val_categorical_accuracy: 0.9941 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.4026 - categorical_accuracy: 0.9774 - auc: 0.9981\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 36s 21ms/step - loss: 0.7614 - categorical_accuracy: 0.9430 - auc: 0.9904 - val_loss: 0.2761 - val_categorical_accuracy: 0.9927 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 14ms/step - loss: 0.2941 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 36s 21ms/step - loss: 0.6733 - categorical_accuracy: 0.9565 - auc: 0.9933 - val_loss: 0.2442 - val_categorical_accuracy: 0.9936 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.2798 - categorical_accuracy: 0.9774 - auc: 0.9990\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1727/1727 [==============================] - 35s 21ms/step - loss: 0.6582 - categorical_accuracy: 0.9540 - auc: 0.9932 - val_loss: 0.2345 - val_categorical_accuracy: 0.9932 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.2562 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1727/1727 [==============================] - 35s 21ms/step - loss: 0.6392 - categorical_accuracy: 0.9582 - auc: 0.9939 - val_loss: 0.2293 - val_categorical_accuracy: 0.9936 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 14ms/step - loss: 0.2475 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GL-_njZcM9m"
      },
      "source": [
        "### less 4 column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDbAEOYfcSf4",
        "outputId": "2fccd4fb-a8e2-4ab5-c8aa-4a6aabeb59fd"
      },
      "source": [
        "less_col"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['annual_inc',\n",
              " 'loan_amnt',\n",
              " 'dti',\n",
              " 'delinq_2yrs',\n",
              " 'fico_range_low',\n",
              " 'fico_range_high',\n",
              " 'annual_inc_joint',\n",
              " 'dti_joint',\n",
              " 'sub_grade',\n",
              " 'emp_length',\n",
              " 'home_ownership',\n",
              " 'verification_status',\n",
              " 'purpose',\n",
              " 'term',\n",
              " 'credit_age',\n",
              " 'verification_status_joint',\n",
              " 'zip_code',\n",
              " 'addr_state']"
            ]
          },
          "execution_count": 97,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ19_2thcTfM",
        "outputId": "fedc5474-acaf-4e2c-ff18-d1e932c6dbf0"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 2)[1]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.0,\n",
              " 'decay_rate': 1.0,\n",
              " 'decay_steps': 70000,\n",
              " 'emb_layer_choice': 'matrix',\n",
              " 'ffn': True,\n",
              " 'ffn_layer': False,\n",
              " 'hp_ffn_d_size': 128,\n",
              " 'hp_ffn_drop': 0.0,\n",
              " 'hp_pred_ffn_d_size': 192,\n",
              " 'hp_pred_ffn_drop': 0.0,\n",
              " 'label_smoothing': 0.0,\n",
              " 'learning_rate': 0.0001,\n",
              " 'mha_dropout_rate': 0.0,\n",
              " 'mha_residual': True,\n",
              " 'num_MHA_layer': 4,\n",
              " 'num_head': 4,\n",
              " 'num_of_d': 128}"
            ]
          },
          "execution_count": 100,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_ZuBmOlcUqx",
        "outputId": "c1b27904-0402-46c9-fcf5-08e783641670"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 2158 test : 2158\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 48s 22ms/step - loss: 1.0034 - categorical_accuracy: 0.9869 - auc: 0.9986 - val_loss: 0.0490 - val_categorical_accuracy: 0.9941 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.0699 - categorical_accuracy: 0.9776 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 37s 21ms/step - loss: 0.3041 - categorical_accuracy: 0.9930 - auc: 1.0000 - val_loss: 0.0313 - val_categorical_accuracy: 0.9927 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.0482 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 37s 21ms/step - loss: 0.2674 - categorical_accuracy: 0.9928 - auc: 1.0000 - val_loss: 0.0288 - val_categorical_accuracy: 0.9936 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.0467 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1727/1727 [==============================] - 36s 21ms/step - loss: 0.2524 - categorical_accuracy: 0.9929 - auc: 1.0000 - val_loss: 0.0283 - val_categorical_accuracy: 0.9932 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 14ms/step - loss: 0.0464 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1727/1727 [==============================] - 36s 21ms/step - loss: 0.2466 - categorical_accuracy: 0.9928 - auc: 1.0000 - val_loss: 0.0283 - val_categorical_accuracy: 0.9936 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.0460 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6F-ZNLHoNGH",
        "outputId": "a424ef29-eed6-4c43-b47b-309efa6a8bb7"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=10)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 2158 test : 2158\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1942/1942 [==============================] - 38s 20ms/step - loss: 0.2284 - categorical_accuracy: 0.9926 - auc: 1.0000 - val_loss: 0.0255 - val_categorical_accuracy: 0.9945 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.0445 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1942/1942 [==============================] - 39s 20ms/step - loss: 0.2118 - categorical_accuracy: 0.9930 - auc: 1.0000 - val_loss: 0.0227 - val_categorical_accuracy: 0.9928 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 15ms/step - loss: 0.0404 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1942/1942 [==============================] - 38s 20ms/step - loss: 0.2092 - categorical_accuracy: 0.9929 - auc: 1.0000 - val_loss: 0.0213 - val_categorical_accuracy: 0.9937 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 15ms/step - loss: 0.0395 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1942/1942 [==============================] - 38s 20ms/step - loss: 0.2066 - categorical_accuracy: 0.9929 - auc: 1.0000 - val_loss: 0.0216 - val_categorical_accuracy: 0.9935 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 15ms/step - loss: 0.0393 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1942/1942 [==============================] - 38s 20ms/step - loss: 0.2052 - categorical_accuracy: 0.9930 - auc: 1.0000 - val_loss: 0.0212 - val_categorical_accuracy: 0.9934 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.0405 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1942/1942 [==============================] - 39s 20ms/step - loss: 0.2052 - categorical_accuracy: 0.9929 - auc: 1.0000 - val_loss: 0.0211 - val_categorical_accuracy: 0.9937 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.0394 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1942/1942 [==============================] - 39s 20ms/step - loss: 0.2054 - categorical_accuracy: 0.9930 - auc: 1.0000 - val_loss: 0.0221 - val_categorical_accuracy: 0.9927 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.0388 - categorical_accuracy: 0.9777 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1942/1942 [==============================] - 39s 20ms/step - loss: 0.2053 - categorical_accuracy: 0.9929 - auc: 1.0000 - val_loss: 0.0215 - val_categorical_accuracy: 0.9935 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 15ms/step - loss: 0.0422 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1943/1943 [==============================] - 40s 20ms/step - loss: 0.2059 - categorical_accuracy: 0.9930 - auc: 1.0000 - val_loss: 0.0223 - val_categorical_accuracy: 0.9929 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 15ms/step - loss: 0.0394 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1943/1943 [==============================] - 39s 20ms/step - loss: 0.2049 - categorical_accuracy: 0.9929 - auc: 1.0000 - val_loss: 0.0208 - val_categorical_accuracy: 0.9937 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 14ms/step - loss: 0.0388 - categorical_accuracy: 0.9774 - auc: 0.9997\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdJ3XhABXttc"
      },
      "source": [
        "### less column 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOT7hO7MX3RT",
        "outputId": "b7ea28ef-818b-410c-ebd3-a5d798348012"
      },
      "source": [
        "less_col"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['sub_grade', 'emp_length', 'pymnt_plan', 'home_ownership',\n",
              "       'verification_status', 'purpose', 'term', 'zip_code', 'addr_state',\n",
              "       'application_type', 'verification_status_joint'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 129,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdmrrejFX2Uf",
        "outputId": "a7ab98f4-2781-4159-dec9-3414bf9219fd"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 2)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model compiled\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'EM_dropout_rate': 0.2,\n",
              " 'decay_rate': 1.0,\n",
              " 'decay_steps': 90000,\n",
              " 'emb_layer_choice': 'linear',\n",
              " 'ffn': True,\n",
              " 'ffn_layer': False,\n",
              " 'hp_ffn_d_size': 512,\n",
              " 'hp_ffn_drop': 0.0,\n",
              " 'hp_pred_ffn_d_size': 256,\n",
              " 'hp_pred_ffn_drop': 0.3,\n",
              " 'label_smoothing': 0.0,\n",
              " 'learning_rate': 0.0001,\n",
              " 'mha_dropout_rate': 0.2,\n",
              " 'mha_residual': True,\n",
              " 'num_MHA_layer': 4,\n",
              " 'num_head': 4,\n",
              " 'num_of_d': 128}"
            ]
          },
          "execution_count": 131,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1EG6A90X1FM",
        "outputId": "225a34cf-25c2-4248-e227-1c001690220e"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train num: 2158 test : 2158\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 47s 21ms/step - loss: 1.0758 - categorical_accuracy: 0.9810 - auc: 0.9965 - val_loss: 0.0632 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 15ms/step - loss: 0.0633 - categorical_accuracy: 1.0000 - auc: 1.0000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 35s 20ms/step - loss: 0.3351 - categorical_accuracy: 0.9953 - auc: 0.9999 - val_loss: 0.0273 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 15ms/step - loss: 0.0274 - categorical_accuracy: 1.0000 - auc: 1.0000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1726/1726 [==============================] - 36s 21ms/step - loss: 0.2879 - categorical_accuracy: 0.9945 - auc: 0.9999 - val_loss: 0.0209 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 15ms/step - loss: 0.0210 - categorical_accuracy: 1.0000 - auc: 1.0000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1727/1727 [==============================] - 36s 21ms/step - loss: 0.2703 - categorical_accuracy: 0.9954 - auc: 0.9999 - val_loss: 0.0183 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 32s 14ms/step - loss: 0.0182 - categorical_accuracy: 1.0000 - auc: 1.0000\n",
            "ensure delete model\n",
            "train stars\n",
            "Epoch 1/10\n",
            "1727/1727 [==============================] - 36s 21ms/step - loss: 0.2601 - categorical_accuracy: 0.9944 - auc: 0.9999 - val_loss: 0.0164 - val_categorical_accuracy: 1.0000 - val_auc: 1.0000\n",
            "test evaluation\n",
            "2158/2158 [==============================] - 33s 14ms/step - loss: 0.0165 - categorical_accuracy: 1.0000 - auc: 1.0000\n",
            "ensure delete model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCFi2J66X5Od"
      },
      "source": [
        "### less col 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvtpfC56X7gh"
      },
      "source": [
        "less_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLJXnvI2X8H6"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 2)[1]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ6wxd9_X9tW"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no7oyCuiX--5"
      },
      "source": [
        "### less col 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kREyHT3aYBCX"
      },
      "source": [
        "less_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eONY6CxIYCBe"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 2)[1]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uce-Sp8oYD0h"
      },
      "source": [
        "cv =CV(model=model, train=train_ds, test=test_ds, k_fold=5)\n",
        "cv.CV_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH197zwlRt7b"
      },
      "source": [
        "# stastic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzdA7XZC5Pnb",
        "outputId": "2ce26c1d-b21a-4a71-cf47-cfbfca34a475"
      },
      "source": [
        "for i in df_v_2.columns:\n",
        "  print(df_v_2[i].describe().apply(lambda x: format(x, 'f')), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count     1380929.000000\n",
            "mean        76300.997962\n",
            "std         70209.031972\n",
            "min            16.000000\n",
            "25%         45800.000000\n",
            "50%         65000.000000\n",
            "75%         90000.000000\n",
            "max      10999200.000000\n",
            "Name: annual_inc, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean       14479.115834\n",
            "std         8751.371724\n",
            "min          500.000000\n",
            "25%         8000.000000\n",
            "50%        12000.000000\n",
            "75%        20000.000000\n",
            "max        40000.000000\n",
            "Name: loan_amnt, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          18.324109\n",
            "std           11.353462\n",
            "min           -1.000000\n",
            "25%           11.800000\n",
            "50%           17.639999\n",
            "75%           24.100000\n",
            "max          999.000000\n",
            "Name: dti, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           0.318904\n",
            "std            0.880246\n",
            "min            0.000000\n",
            "25%            0.000000\n",
            "50%            0.000000\n",
            "75%            0.000000\n",
            "max           39.000000\n",
            "Name: delinq_2yrs, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean         696.052114\n",
            "std           31.782506\n",
            "min          610.000000\n",
            "25%          670.000000\n",
            "50%          690.000000\n",
            "75%          710.000000\n",
            "max          845.000000\n",
            "Name: fico_range_low, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean         700.052252\n",
            "std           31.783153\n",
            "min          614.000000\n",
            "25%          674.000000\n",
            "50%          694.000000\n",
            "75%          714.000000\n",
            "max          850.000000\n",
            "Name: fico_range_high, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           0.661041\n",
            "std            0.957734\n",
            "min            0.000000\n",
            "25%            0.000000\n",
            "50%            0.000000\n",
            "75%            1.000000\n",
            "max           33.000000\n",
            "Name: inq_last_6mths, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          17.523621\n",
            "std           22.701289\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%           31.000000\n",
            "max          226.000000\n",
            "Name: mths_since_last_delinq, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          12.855897\n",
            "std           28.385730\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max          129.000000\n",
            "Name: mths_since_last_record, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          11.597466\n",
            "std            5.483908\n",
            "min            1.000000\n",
            "25%            8.000000\n",
            "50%           11.000000\n",
            "75%           14.000000\n",
            "max           90.000000\n",
            "Name: open_acc, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           0.215543\n",
            "std            0.603683\n",
            "min            0.000000\n",
            "25%            0.000000\n",
            "50%            0.000000\n",
            "75%            0.000000\n",
            "max           86.000000\n",
            "Name: pub_rec, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean       16256.659793\n",
            "std        22422.390702\n",
            "min            0.000000\n",
            "25%         5933.000000\n",
            "50%        11126.000000\n",
            "75%        19745.000000\n",
            "max      2904836.000000\n",
            "Name: revol_bal, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          51.782156\n",
            "std           24.541560\n",
            "min            0.000000\n",
            "25%           33.400002\n",
            "50%           52.099998\n",
            "75%           70.699997\n",
            "max          892.299988\n",
            "Name: revol_util, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          24.937181\n",
            "std           12.010917\n",
            "min            1.000000\n",
            "25%           16.000000\n",
            "50%           23.000000\n",
            "75%           32.000000\n",
            "max          176.000000\n",
            "Name: total_acc, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean        2481.680655\n",
            "std        18974.881588\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max      1837000.000000\n",
            "Name: annual_inc_joint, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.381154\n",
            "std            2.830282\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max           69.489998\n",
            "Name: dti_joint, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean         235.819384\n",
            "std        10622.867409\n",
            "min            0.000000\n",
            "25%            0.000000\n",
            "50%            0.000000\n",
            "75%            0.000000\n",
            "max      9152545.000000\n",
            "Name: tot_coll_amt, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           0.907133\n",
            "std            0.645843\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max           25.000000\n",
            "Name: open_il_12m, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.304510\n",
            "std            1.140669\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max           51.000000\n",
            "Name: open_il_24m, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.174219\n",
            "std            1.032800\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max           28.000000\n",
            "Name: open_rv_12m, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.827775\n",
            "std            1.999164\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            2.000000\n",
            "max           53.000000\n",
            "Name: open_rv_24m, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean        2274.419276\n",
            "std         4453.024899\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%         3281.000000\n",
            "max       776843.000000\n",
            "Name: max_bal_bc, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          24.555766\n",
            "std           31.204101\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%           54.000000\n",
            "max          204.000000\n",
            "Name: all_util, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean       31102.111360\n",
            "std        36280.219646\n",
            "min            0.000000\n",
            "25%        12600.000000\n",
            "50%        22800.000000\n",
            "75%        39300.000000\n",
            "max      9999999.000000\n",
            "Name: total_rev_hi_lim, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.041055\n",
            "std            1.022579\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max           48.000000\n",
            "Name: inq_fi, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.248247\n",
            "std            1.834714\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max           79.000000\n",
            "Name: total_cu_tl, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.542199\n",
            "std            1.757242\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max           67.000000\n",
            "Name: inq_last_12m, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           4.568870\n",
            "std            3.210757\n",
            "min            0.000000\n",
            "25%            2.000000\n",
            "50%            4.000000\n",
            "75%            6.000000\n",
            "max           64.000000\n",
            "Name: acc_open_past_24mths, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean        9706.272302\n",
            "std        15119.589238\n",
            "min            0.000000\n",
            "25%         1146.000000\n",
            "50%         4283.000000\n",
            "75%        11765.000000\n",
            "max       559912.000000\n",
            "Name: bc_open_to_buy, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          57.155709\n",
            "std           30.297182\n",
            "min            0.000000\n",
            "25%           34.000000\n",
            "50%           60.900002\n",
            "75%           83.800003\n",
            "max          339.600006\n",
            "Name: bc_util, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean         115.849889\n",
            "std           60.483875\n",
            "min            0.000000\n",
            "25%           81.000000\n",
            "50%          126.000000\n",
            "75%          150.000000\n",
            "max          999.000000\n",
            "Name: mo_sin_old_il_acct, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean         172.073954\n",
            "std          100.354015\n",
            "min            1.000000\n",
            "25%          108.000000\n",
            "50%          158.000000\n",
            "75%          225.000000\n",
            "max          852.000000\n",
            "Name: mo_sin_old_rev_tl_op, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.636308\n",
            "std            1.961646\n",
            "min            0.000000\n",
            "25%            0.000000\n",
            "50%            1.000000\n",
            "75%            3.000000\n",
            "max           51.000000\n",
            "Name: mort_acc, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           0.535755\n",
            "std            1.298511\n",
            "min            0.000000\n",
            "25%            0.000000\n",
            "50%            0.000000\n",
            "75%            1.000000\n",
            "max           51.000000\n",
            "Name: num_accts_ever_120_pd, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           3.513305\n",
            "std            2.270597\n",
            "min            0.000000\n",
            "25%            2.000000\n",
            "50%            3.000000\n",
            "75%            5.000000\n",
            "max           35.000000\n",
            "Name: num_actv_bc_tl, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           5.415433\n",
            "std            3.382159\n",
            "min            0.000000\n",
            "25%            3.000000\n",
            "50%            5.000000\n",
            "75%            7.000000\n",
            "max           63.000000\n",
            "Name: num_actv_rev_tl, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           4.580644\n",
            "std            2.990207\n",
            "min            0.000000\n",
            "25%            3.000000\n",
            "50%            4.000000\n",
            "75%            6.000000\n",
            "max           63.000000\n",
            "Name: num_bc_sats, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           7.720506\n",
            "std            4.924387\n",
            "min            0.000000\n",
            "25%            4.000000\n",
            "50%            7.000000\n",
            "75%           10.000000\n",
            "max           70.000000\n",
            "Name: num_bc_tl, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           8.180666\n",
            "std            7.392465\n",
            "min            0.000000\n",
            "25%            3.000000\n",
            "50%            6.000000\n",
            "75%           11.000000\n",
            "max          159.000000\n",
            "Name: num_il_tl, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           7.913382\n",
            "std            4.725564\n",
            "min            0.000000\n",
            "25%            5.000000\n",
            "50%            7.000000\n",
            "75%           10.000000\n",
            "max           83.000000\n",
            "Name: num_op_rev_tl, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          13.890369\n",
            "std            8.451978\n",
            "min            1.000000\n",
            "25%            8.000000\n",
            "50%           13.000000\n",
            "75%           18.000000\n",
            "max          128.000000\n",
            "Name: num_rev_accts, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           5.366206\n",
            "std            3.297246\n",
            "min            0.000000\n",
            "25%            3.000000\n",
            "50%            5.000000\n",
            "75%            7.000000\n",
            "max           45.000000\n",
            "Name: num_rev_tl_bal_gt_0, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          11.193365\n",
            "std            5.785472\n",
            "min            0.000000\n",
            "25%            7.000000\n",
            "50%           10.000000\n",
            "75%           14.000000\n",
            "max           90.000000\n",
            "Name: num_sats, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           2.122945\n",
            "std            1.819531\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%            2.000000\n",
            "75%            3.000000\n",
            "max           32.000000\n",
            "Name: num_tl_op_past_12m, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          89.401350\n",
            "std           22.178154\n",
            "min            0.000000\n",
            "25%           89.699997\n",
            "50%           97.099998\n",
            "75%          100.000000\n",
            "max          100.000000\n",
            "Name: pct_tl_nvr_dlq, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean          43.053487\n",
            "std           36.379428\n",
            "min            0.000000\n",
            "25%            1.000000\n",
            "50%           40.000000\n",
            "75%           75.000000\n",
            "max          100.000000\n",
            "Name: percent_bc_gt_75, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           0.135547\n",
            "std            0.378881\n",
            "min            0.000000\n",
            "25%            0.000000\n",
            "50%            0.000000\n",
            "75%            0.000000\n",
            "max           12.000000\n",
            "Name: pub_rec_bankruptcies, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           0.052294\n",
            "std            0.400965\n",
            "min            0.000000\n",
            "25%            0.000000\n",
            "50%            0.000000\n",
            "75%            0.000000\n",
            "max           85.000000\n",
            "Name: tax_liens, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean      165326.435019\n",
            "std       177467.070862\n",
            "min            1.000000\n",
            "25%        43571.000000\n",
            "50%       101104.000000\n",
            "75%       242950.000000\n",
            "max      9999999.000000\n",
            "Name: tot_hi_cred_lim, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean       47948.950538\n",
            "std        47860.751193\n",
            "min            0.000000\n",
            "25%        19122.000000\n",
            "50%        35976.000000\n",
            "75%        61289.000000\n",
            "max      3408095.000000\n",
            "Name: total_bal_ex_mort, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean       20837.224288\n",
            "std        21524.013354\n",
            "min            0.000000\n",
            "25%         7000.000000\n",
            "50%        14500.000000\n",
            "75%        27400.000000\n",
            "max      1105500.000000\n",
            "Name: total_bc_limit, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean       40092.376475\n",
            "std        43232.491612\n",
            "min            0.000000\n",
            "25%        11500.000000\n",
            "50%        29858.000000\n",
            "75%        54910.000000\n",
            "max      2101913.000000\n",
            "Name: total_il_high_credit_limit, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean         197.294894\n",
            "std           91.521288\n",
            "min            6.000000\n",
            "25%          135.000000\n",
            "50%          179.000000\n",
            "75%          243.000000\n",
            "max         1013.000000\n",
            "Name: credit_age, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: grade, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: sub_grade, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: emp_length, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: pymnt_plan, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: home_ownership, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: verification_status, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: purpose, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: term, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: zip_code, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: addr_state, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: application_type, dtype: object \n",
            "\n",
            "count    1380929.000000\n",
            "mean           1.000000\n",
            "std            0.000000\n",
            "min            1.000000\n",
            "25%            1.000000\n",
            "50%            1.000000\n",
            "75%            1.000000\n",
            "max            1.000000\n",
            "Name: verification_status_joint, dtype: object \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "cehb6vixShYM",
        "outputId": "932585a7-d9d0-46ae-9781-0e4b38b2a97f"
      },
      "source": [
        "df_v_2.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annual_inc</th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>dti</th>\n",
              "      <th>delinq_2yrs</th>\n",
              "      <th>fico_range_low</th>\n",
              "      <th>fico_range_high</th>\n",
              "      <th>inq_last_6mths</th>\n",
              "      <th>mths_since_last_delinq</th>\n",
              "      <th>mths_since_last_record</th>\n",
              "      <th>open_acc</th>\n",
              "      <th>pub_rec</th>\n",
              "      <th>revol_bal</th>\n",
              "      <th>revol_util</th>\n",
              "      <th>total_acc</th>\n",
              "      <th>annual_inc_joint</th>\n",
              "      <th>dti_joint</th>\n",
              "      <th>tot_coll_amt</th>\n",
              "      <th>open_il_12m</th>\n",
              "      <th>open_il_24m</th>\n",
              "      <th>open_rv_12m</th>\n",
              "      <th>open_rv_24m</th>\n",
              "      <th>max_bal_bc</th>\n",
              "      <th>all_util</th>\n",
              "      <th>total_rev_hi_lim</th>\n",
              "      <th>inq_fi</th>\n",
              "      <th>total_cu_tl</th>\n",
              "      <th>inq_last_12m</th>\n",
              "      <th>acc_open_past_24mths</th>\n",
              "      <th>bc_open_to_buy</th>\n",
              "      <th>bc_util</th>\n",
              "      <th>mo_sin_old_il_acct</th>\n",
              "      <th>mo_sin_old_rev_tl_op</th>\n",
              "      <th>mort_acc</th>\n",
              "      <th>num_accts_ever_120_pd</th>\n",
              "      <th>num_actv_bc_tl</th>\n",
              "      <th>num_actv_rev_tl</th>\n",
              "      <th>num_bc_sats</th>\n",
              "      <th>num_bc_tl</th>\n",
              "      <th>num_il_tl</th>\n",
              "      <th>num_op_rev_tl</th>\n",
              "      <th>num_rev_accts</th>\n",
              "      <th>num_rev_tl_bal_gt_0</th>\n",
              "      <th>num_sats</th>\n",
              "      <th>num_tl_op_past_12m</th>\n",
              "      <th>pct_tl_nvr_dlq</th>\n",
              "      <th>percent_bc_gt_75</th>\n",
              "      <th>pub_rec_bankruptcies</th>\n",
              "      <th>tax_liens</th>\n",
              "      <th>tot_hi_cred_lim</th>\n",
              "      <th>total_bal_ex_mort</th>\n",
              "      <th>total_bc_limit</th>\n",
              "      <th>total_il_high_credit_limit</th>\n",
              "      <th>credit_age</th>\n",
              "      <th>grade</th>\n",
              "      <th>sub_grade</th>\n",
              "      <th>emp_length</th>\n",
              "      <th>pymnt_plan</th>\n",
              "      <th>home_ownership</th>\n",
              "      <th>verification_status</th>\n",
              "      <th>purpose</th>\n",
              "      <th>term</th>\n",
              "      <th>zip_code</th>\n",
              "      <th>addr_state</th>\n",
              "      <th>application_type</th>\n",
              "      <th>verification_status_joint</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "      <td>1.38093e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>76301</td>\n",
              "      <td>14479.1</td>\n",
              "      <td>18.3241</td>\n",
              "      <td>0.318904</td>\n",
              "      <td>696.052</td>\n",
              "      <td>700.052</td>\n",
              "      <td>0.661041</td>\n",
              "      <td>17.5236</td>\n",
              "      <td>12.8559</td>\n",
              "      <td>11.5975</td>\n",
              "      <td>0.215543</td>\n",
              "      <td>16256.7</td>\n",
              "      <td>51.7822</td>\n",
              "      <td>24.9372</td>\n",
              "      <td>2481.68</td>\n",
              "      <td>1.38115</td>\n",
              "      <td>235.819</td>\n",
              "      <td>0.907133</td>\n",
              "      <td>1.30451</td>\n",
              "      <td>1.17422</td>\n",
              "      <td>1.82777</td>\n",
              "      <td>2274.42</td>\n",
              "      <td>24.5558</td>\n",
              "      <td>31102.1</td>\n",
              "      <td>1.04105</td>\n",
              "      <td>1.24825</td>\n",
              "      <td>1.5422</td>\n",
              "      <td>4.56887</td>\n",
              "      <td>9706.27</td>\n",
              "      <td>57.1557</td>\n",
              "      <td>115.85</td>\n",
              "      <td>172.074</td>\n",
              "      <td>1.63631</td>\n",
              "      <td>0.535755</td>\n",
              "      <td>3.51331</td>\n",
              "      <td>5.41543</td>\n",
              "      <td>4.58064</td>\n",
              "      <td>7.72051</td>\n",
              "      <td>8.18067</td>\n",
              "      <td>7.91338</td>\n",
              "      <td>13.8904</td>\n",
              "      <td>5.36621</td>\n",
              "      <td>11.1934</td>\n",
              "      <td>2.12294</td>\n",
              "      <td>89.4013</td>\n",
              "      <td>43.0535</td>\n",
              "      <td>0.135547</td>\n",
              "      <td>0.0522938</td>\n",
              "      <td>165326</td>\n",
              "      <td>47949</td>\n",
              "      <td>20837.2</td>\n",
              "      <td>40092.4</td>\n",
              "      <td>197.295</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>70209</td>\n",
              "      <td>8751.37</td>\n",
              "      <td>11.3535</td>\n",
              "      <td>0.880246</td>\n",
              "      <td>31.7825</td>\n",
              "      <td>31.7832</td>\n",
              "      <td>0.957734</td>\n",
              "      <td>22.7013</td>\n",
              "      <td>28.3857</td>\n",
              "      <td>5.48391</td>\n",
              "      <td>0.603683</td>\n",
              "      <td>22422.4</td>\n",
              "      <td>24.5416</td>\n",
              "      <td>12.0109</td>\n",
              "      <td>18974.9</td>\n",
              "      <td>2.83028</td>\n",
              "      <td>10622.9</td>\n",
              "      <td>0.645843</td>\n",
              "      <td>1.14067</td>\n",
              "      <td>1.0328</td>\n",
              "      <td>1.99916</td>\n",
              "      <td>4453.02</td>\n",
              "      <td>31.2041</td>\n",
              "      <td>36280.2</td>\n",
              "      <td>1.02258</td>\n",
              "      <td>1.83471</td>\n",
              "      <td>1.75724</td>\n",
              "      <td>3.21076</td>\n",
              "      <td>15119.6</td>\n",
              "      <td>30.2972</td>\n",
              "      <td>60.4839</td>\n",
              "      <td>100.354</td>\n",
              "      <td>1.96165</td>\n",
              "      <td>1.29851</td>\n",
              "      <td>2.2706</td>\n",
              "      <td>3.38216</td>\n",
              "      <td>2.99021</td>\n",
              "      <td>4.92439</td>\n",
              "      <td>7.39246</td>\n",
              "      <td>4.72556</td>\n",
              "      <td>8.45198</td>\n",
              "      <td>3.29725</td>\n",
              "      <td>5.78547</td>\n",
              "      <td>1.81953</td>\n",
              "      <td>22.1782</td>\n",
              "      <td>36.3794</td>\n",
              "      <td>0.378881</td>\n",
              "      <td>0.400965</td>\n",
              "      <td>177467</td>\n",
              "      <td>47860.8</td>\n",
              "      <td>21524</td>\n",
              "      <td>43232.5</td>\n",
              "      <td>91.5213</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>16</td>\n",
              "      <td>500</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>610</td>\n",
              "      <td>614</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>45800</td>\n",
              "      <td>8000</td>\n",
              "      <td>11.8</td>\n",
              "      <td>0</td>\n",
              "      <td>670</td>\n",
              "      <td>674</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>5933</td>\n",
              "      <td>33.4</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12600</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1146</td>\n",
              "      <td>34</td>\n",
              "      <td>81</td>\n",
              "      <td>108</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>89.7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43571</td>\n",
              "      <td>19122</td>\n",
              "      <td>7000</td>\n",
              "      <td>11500</td>\n",
              "      <td>135</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>65000</td>\n",
              "      <td>12000</td>\n",
              "      <td>17.64</td>\n",
              "      <td>0</td>\n",
              "      <td>690</td>\n",
              "      <td>694</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>11126</td>\n",
              "      <td>52.1</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>22800</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4283</td>\n",
              "      <td>60.9</td>\n",
              "      <td>126</td>\n",
              "      <td>158</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>97.1</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>101104</td>\n",
              "      <td>35976</td>\n",
              "      <td>14500</td>\n",
              "      <td>29858</td>\n",
              "      <td>179</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>90000</td>\n",
              "      <td>20000</td>\n",
              "      <td>24.1</td>\n",
              "      <td>0</td>\n",
              "      <td>710</td>\n",
              "      <td>714</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>19745</td>\n",
              "      <td>70.7</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3281</td>\n",
              "      <td>54</td>\n",
              "      <td>39300</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>11765</td>\n",
              "      <td>83.8</td>\n",
              "      <td>150</td>\n",
              "      <td>225</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>242950</td>\n",
              "      <td>61289</td>\n",
              "      <td>27400</td>\n",
              "      <td>54910</td>\n",
              "      <td>243</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.09992e+07</td>\n",
              "      <td>40000</td>\n",
              "      <td>999</td>\n",
              "      <td>39</td>\n",
              "      <td>845</td>\n",
              "      <td>850</td>\n",
              "      <td>33</td>\n",
              "      <td>226</td>\n",
              "      <td>129</td>\n",
              "      <td>90</td>\n",
              "      <td>86</td>\n",
              "      <td>2.90484e+06</td>\n",
              "      <td>892.3</td>\n",
              "      <td>176</td>\n",
              "      <td>1.837e+06</td>\n",
              "      <td>69.49</td>\n",
              "      <td>9.15254e+06</td>\n",
              "      <td>25</td>\n",
              "      <td>51</td>\n",
              "      <td>28</td>\n",
              "      <td>53</td>\n",
              "      <td>776843</td>\n",
              "      <td>204</td>\n",
              "      <td>1e+07</td>\n",
              "      <td>48</td>\n",
              "      <td>79</td>\n",
              "      <td>67</td>\n",
              "      <td>64</td>\n",
              "      <td>559912</td>\n",
              "      <td>339.6</td>\n",
              "      <td>999</td>\n",
              "      <td>852</td>\n",
              "      <td>51</td>\n",
              "      <td>51</td>\n",
              "      <td>35</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>70</td>\n",
              "      <td>159</td>\n",
              "      <td>83</td>\n",
              "      <td>128</td>\n",
              "      <td>45</td>\n",
              "      <td>90</td>\n",
              "      <td>32</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>12</td>\n",
              "      <td>85</td>\n",
              "      <td>1e+07</td>\n",
              "      <td>3.4081e+06</td>\n",
              "      <td>1.1055e+06</td>\n",
              "      <td>2.10191e+06</td>\n",
              "      <td>1013</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        annual_inc    loan_amnt  ... application_type verification_status_joint\n",
              "count  1.38093e+06  1.38093e+06  ...      1.38093e+06               1.38093e+06\n",
              "mean         76301      14479.1  ...                1                         1\n",
              "std          70209      8751.37  ...                0                         0\n",
              "min             16          500  ...                1                         1\n",
              "25%          45800         8000  ...                1                         1\n",
              "50%          65000        12000  ...                1                         1\n",
              "75%          90000        20000  ...                1                         1\n",
              "max    1.09992e+07        40000  ...                1                         1\n",
              "\n",
              "[8 rows x 65 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O87RgeUESzuk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}